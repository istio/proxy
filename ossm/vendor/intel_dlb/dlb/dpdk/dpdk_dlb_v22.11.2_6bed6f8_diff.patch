diff --git a/app/dlb_monitor/main.c b/app/dlb_monitor/main.c
new file mode 100644
index 0000000..b152ac0
--- /dev/null
+++ b/app/dlb_monitor/main.c
@@ -0,0 +1,1317 @@
+/* SPDX-License-Identifier: BSD-3-Clause
+ * Copyright(c) 2019-2021 Intel Corporation
+ */
+
+#include <getopt.h>
+#include <inttypes.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <stdbool.h>
+#include <unistd.h>
+
+#include <rte_cycles.h>
+#include <rte_debug.h>
+#include <rte_eal.h>
+#include <rte_eventdev.h>
+#include <rte_string_fns.h>
+
+/* Note: port_queue_id in xstats APIs is 8 bits, hence maximum of 256 */
+#define MAX_PORTS_QUEUES 256
+#define DLB2_HW_V2 0
+#define DLB2_HW_V2_5 1
+static uint32_t num_ports;
+static uint32_t num_queues;
+static struct rte_event_port_conf port_confs[MAX_PORTS_QUEUES];
+static struct rte_event_queue_conf queue_confs[MAX_PORTS_QUEUES];
+
+static int dev_id;
+static bool do_reset;
+static bool do_watch;
+static bool skip_zero;
+
+static uint32_t measure_time_us = 1 * US_PER_S;
+
+static void
+usage(void)
+{
+	const char *usage_str =
+		"Usage: dlb_monitor [options]\n"
+		"Options:\n"
+		" -i <dev_id>	Eventdev id (default: 0)\n"
+		" -r		Reset stats after displaying them\n"
+		" -t <duration> Measurement duration (seconds) (min: 1s, default: 1s)\n"
+		" -w            Repeatedly print stats\n"
+		" -z            Don't print ports or queues with 0 enqueue/dequeue/depth stats\n"
+		"\n";
+
+	printf("%s\n", usage_str);
+	exit(1);
+}
+
+static void
+parse_app_args(int argc, char **argv)
+{
+	int option_index, c;
+
+	opterr = 0;
+
+	for (;;) {
+		c = getopt_long(argc, argv, "i:rt:wz", NULL,
+				&option_index);
+		if (c == -1)
+			break;
+
+		switch (c) {
+		case 'i':
+			dev_id = atoi(optarg);
+			break;
+		case 'r':
+			do_reset = true;
+			break;
+		case 't':
+			if (atoi(optarg) < 1)
+				usage();
+			measure_time_us = atoi(optarg) * US_PER_S;
+			break;
+		case 'w':
+			do_watch = true;
+			break;
+		case 'z':
+			skip_zero = true;
+			break;
+		default:
+			usage();
+		}
+	}
+}
+
+static const char * const dev_xstat_strs[] = {
+	"dev_inflight_events",
+	"dev_nb_events_limit",
+	"dev_ldb_pool_size",
+	"dev_dir_pool_size",
+	"dev_pool_size",
+	"dev_hw_version",
+};
+
+enum dlb_dev_xstats {
+	DEV_INFL_EVENTS,
+	DEV_NB_EVENTS_LIMIT,
+	DEV_LDB_POOL_SIZE,
+	DEV_DIR_POOL_SIZE,
+	DEV_POOL_SIZE,
+	DEV_HW_VERSION,
+};
+
+static const char * const port_xstat_strs[] = {
+	"tx_ok",
+	"tx_new",
+	"tx_fwd",
+	"tx_rel",
+	"tx_sched_ordered",
+	"tx_sched_unordered",
+	"tx_sched_atomic",
+	"tx_sched_directed",
+	"tx_invalid",
+	"tx_nospc_ldb_hw_credits",
+	"tx_nospc_dir_hw_credits",
+	"tx_nospc_hw_credits",
+	"tx_nospc_inflight_max",
+	"tx_nospc_new_event_limit",
+	"tx_nospc_inflight_credits",
+	"outstanding_releases",
+	"max_outstanding_releases",
+	"total_polls",
+	"zero_polls",
+	"rx_ok",
+	"rx_sched_ordered",
+	"rx_sched_unordered",
+	"rx_sched_atomic",
+	"rx_sched_directed",
+	"rx_sched_invalid",
+	"is_configured",
+	"is_load_balanced",
+};
+
+enum dlb_port_xstats {
+	TX_OK,
+	TX_NEW,
+	TX_FWD,
+	TX_REL,
+	TX_SCHED_ORDERED,
+	TX_SCHED_UNORDERED,
+	TX_SCHED_ATOMIC,
+	TX_SCHED_DIRECTED,
+	TX_SCHED_INVALID,
+	TX_NOSPC_LDB_HW_CREDITS,
+	TX_NOSPC_DIR_HW_CREDITS,
+	TX_NOSPC_HW_CREDITS,
+	TX_NOSPC_INFL_MAX,
+	TX_NOSPC_NEW_EVENT_LIM,
+	TX_NOSPC_INFL_CREDITS,
+	OUTSTANDING_RELEASES,
+	MAX_OUTSTANDING_RELEASES,
+	TOTAL_POLLS,
+	ZERO_POLLS,
+	RX_OK,
+	RX_SCHED_ORDERED,
+	RX_SCHED_UNORDERED,
+	RX_SCHED_ATOMIC,
+	RX_SCHED_DIRECTED,
+	RX_SCHED_INVALID,
+	IS_CONFIGURED,
+	PORT_IS_LOAD_BALANCED,
+};
+
+static const char * const queue_xstat_strs[] = {
+	"current_depth",
+	"is_load_balanced",
+};
+
+enum dlb_queue_xstats {
+	CURRENT_DEPTH,
+	QUEUE_IS_LOAD_BALANCED,
+};
+
+uint64_t dev_xstat_ids[RTE_DIM(dev_xstat_strs)];
+uint64_t port_xstat_ids[MAX_PORTS_QUEUES][RTE_DIM(port_xstat_strs)];
+uint64_t queue_xstat_ids[MAX_PORTS_QUEUES][RTE_DIM(queue_xstat_strs)];
+
+uint64_t dev_xstat_vals[RTE_DIM(dev_xstat_strs)];
+uint64_t port_xstat_vals[MAX_PORTS_QUEUES][RTE_DIM(port_xstat_strs)];
+uint64_t queue_xstat_vals[MAX_PORTS_QUEUES][RTE_DIM(queue_xstat_strs)] = {0};
+
+uint64_t prev_sched_throughput[MAX_PORTS_QUEUES] = {0};
+
+#define MAX_QUEUES_PER_PORT 8
+struct port_link_info {
+	int num_links;
+	uint8_t queues[MAX_QUEUES_PER_PORT];
+	uint8_t priorities[MAX_QUEUES_PER_PORT];
+};
+
+struct port_link_info port_links[MAX_PORTS_QUEUES];
+
+static void
+get_xstats_ids(uint8_t dev_id,
+	       enum rte_event_dev_xstats_mode mode,
+	       const char * const *names,
+	       uint64_t *ids,
+	       unsigned int len,
+	       uint8_t queue_port_id)
+{
+	struct rte_event_dev_xstats_name *xstats_names;
+	uint64_t *xstats_ids;
+	unsigned int size, i, j;
+	int ret;
+
+	/* Get amount of storage required */
+	ret = rte_event_dev_xstats_names_get(dev_id,
+					     mode,
+					     queue_port_id,
+					     NULL, /* names */
+					     NULL, /* ids */
+					     0);   /* num */
+	if (ret <= 0)
+		rte_panic("rte_event_dev_xstats_names_get err %d\n", ret);
+
+	size = (unsigned int)ret;
+
+	xstats_names = malloc(sizeof(struct rte_event_dev_xstats_name) * size);
+	xstats_ids = malloc(sizeof(uint64_t) * size);
+
+	if (!xstats_names || !xstats_ids)
+		rte_panic("unable to alloc memory for stats retrieval\n");
+
+	ret = rte_event_dev_xstats_names_get(dev_id, mode, queue_port_id,
+					     xstats_names, xstats_ids,
+					     size);
+	if (ret != (int)size)
+		rte_panic("rte_event_dev_xstats_names_get err %d\n", ret);
+
+	for (i = 0; i < len; i++) {
+		char name[RTE_EVENT_DEV_XSTATS_NAME_SIZE];
+
+		if (mode == RTE_EVENT_DEV_XSTATS_DEVICE)
+			rte_strlcpy(name,
+				    names[i],
+				    RTE_EVENT_DEV_XSTATS_NAME_SIZE - 1);
+		else if (mode == RTE_EVENT_DEV_XSTATS_PORT)
+			snprintf(name,
+				 RTE_EVENT_DEV_XSTATS_NAME_SIZE - 1,
+				 "port_%u_%s",
+				 queue_port_id,
+				 names[i]);
+		else
+			snprintf(name,
+				 RTE_EVENT_DEV_XSTATS_NAME_SIZE - 1,
+				 "qid_%u_%s",
+				 queue_port_id,
+				 names[i]);
+
+		for (j = 0; j < size; j++) {
+			if (strncmp(name,
+				    xstats_names[j].name,
+				    RTE_EVENT_DEV_XSTATS_NAME_SIZE) == 0) {
+				ids[i] = xstats_ids[j];
+				break;
+			}
+		}
+
+		if (j == size)
+			rte_panic("Couldn't find xstat %s\n", name);
+	}
+
+	free(xstats_names);
+	free(xstats_ids);
+}
+
+static void
+init_xstats(void)
+{
+	unsigned int i;
+
+	/* Lookup xstats IDs in advance */
+	get_xstats_ids(dev_id,
+		       RTE_EVENT_DEV_XSTATS_DEVICE,
+		       dev_xstat_strs,
+		       dev_xstat_ids,
+		       RTE_DIM(dev_xstat_strs),
+		       0);
+
+	for (i = 0; i < num_ports; i++) {
+		get_xstats_ids(dev_id,
+			       RTE_EVENT_DEV_XSTATS_PORT,
+			       port_xstat_strs,
+			       port_xstat_ids[i],
+			       RTE_DIM(port_xstat_strs),
+			       i);
+	}
+
+	for (i = 0; i < num_queues; i++) {
+		get_xstats_ids(dev_id,
+			       RTE_EVENT_DEV_XSTATS_QUEUE,
+			       queue_xstat_strs,
+			       queue_xstat_ids[i],
+			       RTE_DIM(queue_xstat_strs),
+			       i);
+	}
+
+	/* Initialize prev_sched_throughput[i] */
+	for (i = 0; i < num_ports; i++) {
+		int ret;
+		ret = rte_event_dev_xstats_get(dev_id,
+					       RTE_EVENT_DEV_XSTATS_PORT,
+					       i,
+					       port_xstat_ids[i],
+					       port_xstat_vals[i],
+					       RTE_DIM(port_xstat_strs));
+		if (ret != RTE_DIM(port_xstat_strs))
+			rte_panic("Failed to get port %u's xstats (ret: %d)\n",
+				  i, ret);
+		prev_sched_throughput[i] = port_xstat_vals[i][RX_OK];
+	}
+}
+
+static void
+collect_config(void)
+{
+	uint32_t attr_id;
+	unsigned int i;
+	int ret;
+
+	attr_id = RTE_EVENT_DEV_ATTR_PORT_COUNT;
+	if (rte_event_dev_attr_get(dev_id, attr_id, &num_ports))
+		rte_panic("Failed to get the device's port count\n");
+
+	attr_id = RTE_EVENT_DEV_ATTR_QUEUE_COUNT;
+	if (rte_event_dev_attr_get(dev_id, attr_id, &num_queues))
+		rte_panic("Failed to get the device's queue count\n");
+
+	init_xstats();
+
+	for (i = 0; i < num_ports; i++) {
+		uint32_t attr;
+
+		attr_id = RTE_EVENT_PORT_ATTR_NEW_EVENT_THRESHOLD;
+		if (rte_event_port_attr_get(dev_id, i, attr_id, &attr))
+			rte_panic("Failed to get port %u's new event threshold\n",
+				  i);
+
+		port_confs[i].new_event_threshold = attr;
+
+		attr_id = RTE_EVENT_PORT_ATTR_ENQ_DEPTH;
+		if (rte_event_port_attr_get(dev_id, i, attr_id, &attr))
+			rte_panic("Failed to get port %u's enqueue depth\n",
+				  i);
+
+		port_confs[i].enqueue_depth = attr;
+
+		attr_id = RTE_EVENT_PORT_ATTR_DEQ_DEPTH;
+		if (rte_event_port_attr_get(dev_id, i, attr_id, &attr))
+			rte_panic("Failed to get port %u's dequeue depth\n",
+				  i);
+
+		port_confs[i].dequeue_depth = attr;
+
+		attr_id = RTE_EVENT_PORT_ATTR_IMPLICIT_RELEASE_DISABLE;
+		if (rte_event_port_attr_get(dev_id, i, attr_id, &attr))
+			rte_panic("Failed to get port %u's implicit release attr\n",
+				  i);
+
+		port_confs[i].event_port_cfg = 0;
+		if (attr)
+			port_confs[i].event_port_cfg |=
+				RTE_EVENT_PORT_CFG_DISABLE_IMPL_REL;
+	}
+
+	for (i = 0; i < num_queues; i++) {
+		uint32_t attr;
+
+		attr_id = RTE_EVENT_QUEUE_ATTR_NB_ATOMIC_ORDER_SEQUENCES;
+		if (rte_event_queue_attr_get(dev_id, i, attr_id, &attr))
+			rte_panic("Failed to get queue %u's ordered SN config\n",
+				  i);
+
+		queue_confs[i].nb_atomic_order_sequences = attr;
+
+		attr_id = RTE_EVENT_QUEUE_ATTR_EVENT_QUEUE_CFG;
+		if (rte_event_queue_attr_get(dev_id, i, attr_id, &attr))
+			rte_panic("Failed to get queue %u's queue cfg\n",
+				  i);
+
+		queue_confs[i].event_queue_cfg = attr;
+
+		/* This function returns -EOVERFLOW when the queue was
+		 * configured with RTE_EVENT_QUEUE_CFG_ALL_TYPES. In that
+		 * case, schedule_type is a don't-care value.
+		 */
+		attr_id = RTE_EVENT_QUEUE_ATTR_SCHEDULE_TYPE;
+
+		ret = rte_event_queue_attr_get(dev_id, i, attr_id, &attr);
+		if (ret && ret != -EOVERFLOW)
+			rte_panic("Failed to get queue %u's schedule type\n",
+				  i);
+
+		queue_confs[i].schedule_type = attr;
+	}
+
+	/* Lookup port->queue link information */
+	for (i = 0; i < num_ports; i++) {
+		ret = rte_event_port_links_get(dev_id, i,
+					       port_links[i].queues,
+					       port_links[i].priorities);
+		if (ret < 0)
+			rte_panic("Failed to get port %u's links\n", i);
+
+		port_links[i].num_links = ret;
+	}
+
+	for (i = 0; i < num_ports; i++) {
+		ret = rte_event_dev_xstats_get(
+			dev_id,
+			RTE_EVENT_DEV_XSTATS_PORT,
+			i,
+			&port_xstat_ids[i][PORT_IS_LOAD_BALANCED],
+			&port_xstat_vals[i][PORT_IS_LOAD_BALANCED],
+			1);
+		if (ret != 1)
+			rte_panic("Failed to get port %u's is_load_balanced xstat (ret: %d)\n",
+				  i, ret);
+	}
+	ret = rte_event_dev_xstats_get(dev_id,
+				       RTE_EVENT_DEV_XSTATS_DEVICE,
+				       0,
+				       &dev_xstat_ids[DEV_LDB_POOL_SIZE],
+				       &dev_xstat_vals[DEV_LDB_POOL_SIZE],
+				       1);
+
+	if (ret != 1)
+		rte_panic("Failed to get ldb pool size\n");
+
+	ret = rte_event_dev_xstats_get(dev_id,
+				       RTE_EVENT_DEV_XSTATS_DEVICE,
+				       0,
+				       &dev_xstat_ids[DEV_DIR_POOL_SIZE],
+				       &dev_xstat_vals[DEV_DIR_POOL_SIZE],
+				       1);
+
+	if (ret != 1)
+		rte_panic("Failed to get dir pool size\n");
+
+	ret = rte_event_dev_xstats_get(dev_id,
+				       RTE_EVENT_DEV_XSTATS_DEVICE,
+				       0,
+				       &dev_xstat_ids[DEV_POOL_SIZE],
+				       &dev_xstat_vals[DEV_POOL_SIZE],
+				       1);
+	if (ret != 1)
+		rte_panic("Failed to get pool size\n");
+
+	ret = rte_event_dev_xstats_get(dev_id,
+				       RTE_EVENT_DEV_XSTATS_DEVICE,
+				       0,
+				       &dev_xstat_ids[DEV_HW_VERSION],
+				       &dev_xstat_vals[DEV_HW_VERSION],
+				       1);
+	if (ret != 1)
+		rte_panic("Failed to get hw version\n");
+}
+
+static void
+collect_stats(void)
+{
+	unsigned int i;
+	int ret;
+
+	/* Lookup port->queue link information */
+	for (i = 0; i < num_ports; i++) {
+		ret = rte_event_port_links_get(dev_id, i,
+					       port_links[i].queues,
+					       port_links[i].priorities);
+		if (ret < 0)
+			rte_panic("Failed to get port %u's links\n", i);
+
+		port_links[i].num_links = ret;
+	}
+
+	/* Collect xstats */
+	ret = rte_event_dev_xstats_get(dev_id,
+				       RTE_EVENT_DEV_XSTATS_DEVICE,
+				       0,
+				       dev_xstat_ids,
+				       dev_xstat_vals,
+				       RTE_DIM(dev_xstat_strs));
+
+	if (ret != RTE_DIM(dev_xstat_strs))
+		rte_panic("Failed to get device xstats\n");
+
+	for (i = 0; i < num_ports; i++) {
+		ret = rte_event_dev_xstats_get(dev_id,
+					       RTE_EVENT_DEV_XSTATS_PORT,
+					       i,
+					       port_xstat_ids[i],
+					       port_xstat_vals[i],
+					       RTE_DIM(port_xstat_strs));
+		if (ret != RTE_DIM(port_xstat_strs))
+			rte_panic("Failed to get port %u's xstats (ret: %d)\n",
+				  i, ret);
+	}
+
+	for (i = 0; i < num_queues; i++) {
+		ret = rte_event_dev_xstats_get(dev_id,
+					       RTE_EVENT_DEV_XSTATS_QUEUE,
+					       i,
+					       queue_xstat_ids[i],
+					       queue_xstat_vals[i],
+					       RTE_DIM(queue_xstat_strs));
+		if (ret != RTE_DIM(queue_xstat_strs))
+			rte_panic("Failed to get queue %u's xstats\n", i);
+	}
+}
+
+#define PCT_STR_LEN 5
+static void
+format_percent_str(float pct, char *str)
+{
+	if (pct > 0.0f && pct < 1.0f)
+		snprintf(str, PCT_STR_LEN, " <1%%");
+	else
+		snprintf(str, PCT_STR_LEN, "%3.0f%%", pct);
+}
+
+#define COL_RED "\x1b[31m"
+#define COL_RESET "\x1b[0m"
+#define LINK_STR_LEN 11
+
+static void
+display_port_config(void)
+{
+	unsigned int i;
+
+	printf("                                            Port Configuration\n");
+	printf("------------------------------------------------------------------------------------------------------------------\n");
+	printf("    |      |     |     |  New   |Implicit| Link 0 | Link 1 | Link 2 | Link 3 | Link 4 | Link 5 | Link 6 | Link 7 |\n");
+	printf("    |      | Enq | Deq | event  | release|(Queue, |(Queue, |(Queue, |(Queue, |(Queue, |(Queue, |(Queue, |(Queue, |\n");
+	printf("Port| Type |Depth|Depth| thresh | enabled|  Prio) |  Prio) |  Prio) |  Prio) |  Prio) |  Prio) |  Prio) |  Prio) |\n");
+	printf("----|------|-----|-----|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|\n");
+
+	for (i = 0; i < num_ports; i++) {
+		char link_str[MAX_QUEUES_PER_PORT][LINK_STR_LEN] = {
+			"        ",
+			"        ",
+			"        ",
+			"        ",
+			"        ",
+			"        ",
+			"        ",
+			"        ",
+		};
+		bool is_ldb, impl_rel_enab;
+		int j;
+
+		is_ldb = port_xstat_vals[i][PORT_IS_LOAD_BALANCED];
+
+		for (j = 0; j < port_links[i].num_links; j++) {
+			if (port_links[i].queues[j] < 10)
+				snprintf(link_str[j], 11, "  Q%1u,P%u ",
+					 port_links[i].queues[j],
+					 port_links[i].priorities[j] >> 5);
+			else if (port_links[i].queues[j] < 100)
+				snprintf(link_str[j], 10, " Q%2u,P%u ",
+					 port_links[i].queues[j],
+					 port_links[i].priorities[j] >> 5);
+			else
+				snprintf(link_str[j], 10, "Q%3u,P%u ",
+					 port_links[i].queues[j],
+					 port_links[i].priorities[j] >> 5);
+		}
+
+		impl_rel_enab = (port_confs[i].event_port_cfg &
+				 RTE_EVENT_PORT_CFG_DISABLE_IMPL_REL) == 0;
+
+		printf("%3u |  %s | %3u | %3u |  %5u |    %u   |%s|%s|%s|%s|%s|%s|%s|%s|\n",
+			i, is_ldb ? "LDB" : "DIR",
+			port_confs[i].enqueue_depth,
+			port_confs[i].dequeue_depth,
+			port_confs[i].new_event_threshold,
+			impl_rel_enab,
+			link_str[0], link_str[1], link_str[2], link_str[3],
+			link_str[4], link_str[5], link_str[6], link_str[7]);
+	}
+
+	printf("------------------------------------------------------------------------------------------------------------------\n");
+
+	printf("\n");
+}
+
+static void
+display_queue_config(void)
+{
+	bool print_ord_sn_warning;
+	unsigned int i;
+
+	print_ord_sn_warning = false;
+
+	printf("      Queue Configuration\n");
+	printf("-----------------------------------\n");
+	printf("     |  Sched  | Ordered sequence |\n");
+	printf("Queue|  Types  |      numbers     |\n");
+	printf("-----|---------|------------------|\n");
+
+	for (i = 0; i < num_queues; i++) {
+		bool is_dir, is_uno, too_few_sns, is_all_types;
+		unsigned int j, ord_sns;
+		int num_linked_ports;
+
+		is_dir = queue_confs[i].event_queue_cfg &
+			RTE_EVENT_QUEUE_CFG_SINGLE_LINK;
+
+		is_uno = false;
+
+		is_all_types = queue_confs[i].event_queue_cfg &
+			RTE_EVENT_QUEUE_CFG_ALL_TYPES;
+
+		if (is_all_types &&
+		    queue_confs[i].nb_atomic_order_sequences == 0)
+			is_uno = true;
+
+		if (!is_all_types &&
+		    queue_confs[i].schedule_type != RTE_SCHED_TYPE_ORDERED)
+			is_uno = true;
+
+		ord_sns = queue_confs[i].nb_atomic_order_sequences;
+		if (is_dir || is_uno)
+			ord_sns = 0;
+
+		num_linked_ports = 0;
+		for (j = 0; j < num_ports; j++) {
+			int k;
+			for (k = 0; k < port_links[j].num_links; k++) {
+				if (port_links[j].queues[k] == i)
+					num_linked_ports++;
+			}
+		}
+
+		too_few_sns = (ord_sns > 0) &&
+			      (num_linked_ports > 0) &&
+			      ((ord_sns / num_linked_ports) <= 4);
+
+		print_ord_sn_warning |= too_few_sns;
+
+		printf(" %3u | %s |       %s%4u%s       |\n",
+			i,
+			is_dir ? "  DIR  " : is_uno ? "ATM+PAR" : "ATM+ORD",
+			too_few_sns ? COL_RED : "", ord_sns, COL_RESET);
+	}
+
+	printf("-----------------------------------\n");
+
+	if (print_ord_sn_warning)
+		printf("\n%sOrdered sequence numbers%s: an ordered queue's nb_atomic_order_sequences sets\n"
+		       "			  its inflight event limit: the number of the\n"
+		       "			  queue's events that can be scheduled and awaiting\n"
+		       "			  release at any time. If this number is too low,\n"
+		       "			  it can limit the parallelism of the application.\n"
+		       "			  Consider increasing the queue's SN config.\n",
+			COL_RED, COL_RESET);
+	printf("\n");
+}
+
+static void
+display_device_config(void)
+{
+	printf("\n");
+	printf("          Device Configuration\n");
+	if(dev_xstat_vals[DEV_HW_VERSION] == DLB2_HW_V2) {
+		printf("-----------------------------------------\n");
+		printf("      |  LDB pool size |  DIR pool size |\n");
+		printf("Device|    (DLB 2.0)   |    (DLB 2.0)   |\n");
+		printf("------|----------------|----------------|\n");
+
+		printf("  %2u  |     %5"PRIu64"      |      %4"PRIu64"      |\n",
+			dev_id,
+			dev_xstat_vals[DEV_LDB_POOL_SIZE],
+			dev_xstat_vals[DEV_DIR_POOL_SIZE]);
+		printf("-----------------------------------------\n");
+	}
+	else if(dev_xstat_vals[DEV_HW_VERSION] == DLB2_HW_V2_5) {
+                printf("-------------------------\n");
+                printf("      |  COMB pool size |\n");
+                printf("Device|     (DLB 2.5)   |\n");  
+                printf("------|-----------------|\n");
+
+                printf("  %2u  |     %5"PRIu64"       |\n",     
+                        dev_id, 
+                        dev_xstat_vals[DEV_POOL_SIZE]);
+                printf("-------------------------\n");
+        }
+}
+
+static void
+display_config(void)
+{
+	display_device_config();
+
+	display_port_config();
+
+	display_queue_config();
+}
+
+/* Load imbalance is detected if, among ports with identical queue links
+ * (including priority), the port with the most dequeued events has at least
+ * a factor of LOAD_IMBALANCE_THRESHOLD more events than the port with the
+ * fewest dequeued events.
+ */
+#define LOAD_IMBALANCE_THRESHOLD 1.5f
+
+static bool
+detect_load_imbalance(unsigned int port_id)
+{
+	unsigned int i, j, k, num_links;
+	uint64_t rx_ok_high, rx_ok_low;
+
+	rx_ok_high = port_xstat_vals[port_id][RX_OK];
+	rx_ok_low = port_xstat_vals[port_id][RX_OK];
+
+	num_links = port_links[port_id].num_links;
+
+	for (i = 0; i < num_ports; i++) {
+		if (i == port_id)
+			continue;
+
+		if (port_links[i].num_links != (int)num_links)
+			continue;
+
+		/* Check if ports port_id and i have the same queue links */
+		for (j = 0; j < num_links; j++) {
+			for (k = 0; k < num_links; k++) {
+				if ((port_links[i].queues[j] ==
+				     port_links[port_id].queues[k]) &&
+				    (port_links[i].priorities[j] ==
+				     port_links[port_id].priorities[k]))
+					break;
+			}
+
+			/* Break early if no match found */
+			if (k == num_links)
+				break;
+		}
+
+		if (j < num_links)
+			continue;
+
+		rx_ok_high = RTE_MAX(rx_ok_high, port_xstat_vals[i][RX_OK]);
+		rx_ok_low = RTE_MIN(rx_ok_low, port_xstat_vals[i][RX_OK]);
+	}
+
+	return rx_ok_low * LOAD_IMBALANCE_THRESHOLD < rx_ok_high;
+}
+
+static void
+display_device_stats(void)
+{
+	float ldb_sched_throughput, dir_sched_throughput;
+	uint64_t events_inflight, nb_events_limit;
+	uint64_t total = 0;
+	unsigned int i;
+
+	events_inflight = dev_xstat_vals[DEV_INFL_EVENTS];
+	nb_events_limit = dev_xstat_vals[DEV_NB_EVENTS_LIMIT];
+
+	ldb_sched_throughput = 0.0f;
+	dir_sched_throughput = 0.0f;
+
+	for (i = 0; i < num_ports; i++) {
+		if (port_xstat_vals[i][PORT_IS_LOAD_BALANCED]) {
+			ldb_sched_throughput +=
+				(port_xstat_vals[i][RX_OK] -
+				prev_sched_throughput[i]);
+		} else {
+			dir_sched_throughput +=
+				(port_xstat_vals[i][RX_OK] -
+				prev_sched_throughput[i]);
+		}
+
+		total += (port_xstat_vals[i][RX_OK] -
+				prev_sched_throughput[i]);
+		prev_sched_throughput[i] = port_xstat_vals[i][RX_OK];
+	}
+
+	/* Throughput is displayed in millions of events per second, so no need
+	 * to convert microseconds to seconds.
+	 */
+	ldb_sched_throughput = ldb_sched_throughput / measure_time_us;
+	dir_sched_throughput = dir_sched_throughput / measure_time_us;
+
+	printf("                        Device stats\n");
+	printf("-----------------------------------------------------------\n");
+	printf("LDB scheduling throughput: %.2f ME/s (%.2f ME/s globally)\n",
+	       ldb_sched_throughput,
+	       (float)total / (float)measure_time_us);
+	printf("DIR scheduling throughput: %.2f ME/s (%.2f ME/s globally)\n",
+	       dir_sched_throughput,
+	       (float)total / (float)measure_time_us);
+	printf("Inflight events: %"PRIu64"/%"PRIu64"\n",
+	       events_inflight, nb_events_limit);
+	printf("\n");
+}
+
+static void
+display_port_dequeue_stats(void)
+{
+	unsigned int i;
+
+	printf("                               Port dequeue stats\n");
+	printf("-----------------------------------------------------------------------------\n");
+	printf("    |  Sched  |                |  Out-  |   Dequeued sched   |Avg evts| Zero|\n");
+	printf("    |throughpt|  Total events  |standing|   type percentage  |  per   | poll|\n");
+	printf("Port|  (ME/s) |    dequeued    |Releases| ATM  PAR  ORD  DIR |dequeue |  pct|\n");
+	printf("----|---------|----------------|--------|--------------------|--------|-----|\n");
+
+	bool print_load_imbalance_warning = false;
+	bool print_batch_size_warning = false;
+
+	for (i = 0; i < num_ports; i++) {
+		float sched_tput, atm_pct, par_pct, ord_pct, zero_poll_pct;
+		char atm_str[PCT_STR_LEN], par_str[PCT_STR_LEN];
+		char ord_str[PCT_STR_LEN], dir_str[PCT_STR_LEN];
+		uint64_t rx_ok, total_polls, zero_polls;
+		char zero_poll_str[PCT_STR_LEN];
+		bool low_batch_size, imbalance;
+		float dir_pct, avg_deq_size;
+
+		rx_ok = port_xstat_vals[i][RX_OK];
+		total_polls = port_xstat_vals[i][TOTAL_POLLS];
+		zero_polls = port_xstat_vals[i][ZERO_POLLS];
+
+		if (skip_zero && rx_ok == 0)
+			continue;
+
+		zero_poll_pct = (zero_polls * 100.0f) / total_polls;
+
+		if (total_polls == 0)
+			zero_poll_pct = 0.0f;
+
+		format_percent_str(zero_poll_pct, zero_poll_str);
+
+		atm_pct = port_xstat_vals[i][RX_SCHED_ATOMIC];
+		atm_pct = (atm_pct * 100.0f) / rx_ok;
+
+		ord_pct = port_xstat_vals[i][RX_SCHED_ORDERED];
+		ord_pct = (ord_pct * 100.0f) / rx_ok;
+
+		par_pct = port_xstat_vals[i][RX_SCHED_UNORDERED];
+		par_pct = (par_pct * 100.0f) / rx_ok;
+
+		dir_pct = 0.0f;
+
+		if (!port_xstat_vals[i][PORT_IS_LOAD_BALANCED]) {
+			atm_pct = 0.0f;
+			ord_pct = 0.0f;
+			par_pct = 0.0f;
+			dir_pct = rx_ok > 0 ? 100.0f : 0.0f;
+		}
+
+		if (rx_ok == 0) {
+			atm_pct = 0.0f;
+			ord_pct = 0.0f;
+			par_pct = 0.0f;
+			dir_pct = 0.0f;
+		}
+
+		format_percent_str(atm_pct, atm_str);
+		format_percent_str(ord_pct, ord_str);
+		format_percent_str(par_pct, par_str);
+		format_percent_str(dir_pct, dir_str);
+
+		if (total_polls == 0)
+			avg_deq_size = 0.0f;
+		else
+			avg_deq_size = (float)rx_ok / (float)total_polls;
+
+		/* Throughput is displayed in millions of events per second, so
+		 * no need to convert microseconds to seconds.
+		 */
+		sched_tput = (float)(port_xstat_vals[i][RX_OK] -
+				     prev_sched_throughput[i]) /
+				  (float)measure_time_us;
+
+		low_batch_size = false;
+		if (avg_deq_size > 0.0f &&
+		    avg_deq_size <= port_confs[i].dequeue_depth / 8)
+			low_batch_size = true;
+
+		print_batch_size_warning |= low_batch_size;
+
+		imbalance = detect_load_imbalance(i);
+		print_load_imbalance_warning |= imbalance;
+
+		printf("%3u |  %6.2f |%s%16"PRIu64"%s| %3"PRIu64"/%-3"PRIu64"|%s %s %s %s | %s%6.2f%s | %s|\n",
+			i, sched_tput,
+			imbalance ? COL_RED : "", port_xstat_vals[i][RX_OK],
+			COL_RESET,
+			port_xstat_vals[i][OUTSTANDING_RELEASES],
+			port_xstat_vals[i][MAX_OUTSTANDING_RELEASES],
+			atm_str, par_str, ord_str, dir_str,
+			low_batch_size ? COL_RED : "", avg_deq_size, COL_RESET,
+			zero_poll_str);
+	}
+
+	printf("-----------------------------------------------------------------------------\n");
+
+	if (print_batch_size_warning)
+		printf("\nDequeue batch size: when the average dequeue size is much lower than the port's\n"
+		       "		    dequeue depth, likely either dequeue burst is called with a\n"
+		       "		    small nb_events argument, or event producers are supplying\n"
+		       "		    the port at a slower rate than it can dequeue events.\n");
+	if (print_load_imbalance_warning)
+		printf("\nLoad imbalance: the total events dequeued by certain ports with identical queue\n"
+		       "		links differs by at least %3.1fx. This can be caused by using a\n"
+		       "		small number of atomic flows, or if one core is being preempted\n"
+		       "		more frequently than another.\n",
+		       LOAD_IMBALANCE_THRESHOLD);
+	printf("\n");
+}
+
+static void
+display_port_enqueue_stats(void)
+{
+	bool print_new_rel_warning = false;
+	bool print_ldb_deadlock = false;
+	bool print_dir_deadlock = false;
+	bool print_comb_deadlock = false;
+	bool print_ldb = false;
+	bool print_dir = false;
+	bool print_comb = false;
+	bool print_net = false;
+	bool print_inf = false;
+	bool print_swc = false;
+	unsigned int i;
+
+	printf("                                 Port enqueue stats\n");
+	printf("-----------------------------------------------------------------------------------------------\n");
+	printf("    |                |   Enqueued sched   |     Enqueued op    |    %% of enqueue attempts     |\n");
+	printf("    |  Total events  |   type percentage  |   type percentage  |          backpressured       |\n");
+	if(dev_xstat_vals[DEV_HW_VERSION] == DLB2_HW_V2) {
+		printf("Port|    enqueued    | ATM  PAR  ORD  DIR | NEW  FWD  REL      |  LDB   DIR   NET   INF   SWC |\n");
+	}
+	else if(dev_xstat_vals[DEV_HW_VERSION] == DLB2_HW_V2_5) {	
+		printf("Port|    enqueued    | ATM  PAR  ORD  DIR | NEW  FWD  REL      |  COMB    NET    INF    SWC   |\n");
+	}
+	printf("----|----------------|--------------------|--------------------|------------------------------|\n");
+	
+	for (i = 0; i < num_ports; i++) {
+		char ldb_bp_str[PCT_STR_LEN], dir_bp_str[PCT_STR_LEN];
+		char comb_bp_str[PCT_STR_LEN];
+		char net_bp_str[PCT_STR_LEN], inf_bp_str[PCT_STR_LEN];
+		char new_str[PCT_STR_LEN], fwd_str[PCT_STR_LEN];
+		char rel_str[PCT_STR_LEN];
+		char atm_str[PCT_STR_LEN], par_str[PCT_STR_LEN];
+		char ord_str[PCT_STR_LEN], dir_str[PCT_STR_LEN];
+		float ldb_bp_pct, dir_bp_pct, comb_bp_pct, net_bp_pct;
+		float atm_pct, par_pct, ord_pct, dir_pct;
+		float new_pct, fwd_pct, rel_pct;
+		float inf_bp_pct, swc_bp_pct;
+		char swc_bp_str[PCT_STR_LEN];
+		uint64_t tx_ok, total_tx;
+		bool new_rel_issue;
+
+		tx_ok = port_xstat_vals[i][TX_OK];
+
+		if (skip_zero && tx_ok == 0)
+			continue;
+
+		atm_pct = port_xstat_vals[i][TX_SCHED_ATOMIC];
+		atm_pct = (atm_pct * 100.0f) / tx_ok;
+
+		ord_pct = port_xstat_vals[i][TX_SCHED_ORDERED];
+		ord_pct = (ord_pct * 100.0f) / tx_ok;
+
+		par_pct = port_xstat_vals[i][TX_SCHED_UNORDERED];
+		par_pct = (par_pct * 100.0f) / tx_ok;
+
+		dir_pct = port_xstat_vals[i][TX_SCHED_DIRECTED];
+		dir_pct = (dir_pct * 100.0f) / tx_ok;
+
+		new_pct = port_xstat_vals[i][TX_NEW];
+		new_pct = (new_pct * 100.0f) / tx_ok;
+
+		fwd_pct = port_xstat_vals[i][TX_FWD];
+		fwd_pct = (fwd_pct * 100.0f) / tx_ok;
+
+		rel_pct = port_xstat_vals[i][TX_REL];
+		rel_pct = (rel_pct * 100.0f) / tx_ok;
+
+		if (tx_ok == 0) {
+			atm_pct = 0.0f;
+			ord_pct = 0.0f;
+			par_pct = 0.0f;
+			dir_pct = 0.0f;
+			new_pct = 0.0f;
+			fwd_pct = 0.0f;
+			rel_pct = 0.0f;
+		}
+
+		format_percent_str(atm_pct, atm_str);
+		format_percent_str(ord_pct, ord_str);
+		format_percent_str(par_pct, par_str);
+		format_percent_str(dir_pct, dir_str);
+		format_percent_str(new_pct, new_str);
+		format_percent_str(fwd_pct, fwd_str);
+		format_percent_str(rel_pct, rel_str);
+
+		new_rel_issue = new_pct >= 40.0f && rel_pct >= 40.0f;
+
+		print_new_rel_warning |= new_rel_issue;
+
+		total_tx = port_xstat_vals[i][TX_NOSPC_LDB_HW_CREDITS] +
+			port_xstat_vals[i][TX_NOSPC_DIR_HW_CREDITS] +
+			port_xstat_vals[i][TX_NOSPC_HW_CREDITS] +
+			port_xstat_vals[i][TX_NOSPC_INFL_MAX] +
+			port_xstat_vals[i][TX_NOSPC_NEW_EVENT_LIM] +
+			port_xstat_vals[i][TX_NOSPC_INFL_CREDITS] +
+			tx_ok;
+
+		ldb_bp_pct = port_xstat_vals[i][TX_NOSPC_LDB_HW_CREDITS];
+		ldb_bp_pct = (ldb_bp_pct * 100.0f) / total_tx;
+
+		dir_bp_pct = port_xstat_vals[i][TX_NOSPC_DIR_HW_CREDITS];
+		dir_bp_pct = (dir_bp_pct * 100.0f) / total_tx;
+
+		comb_bp_pct = port_xstat_vals[i][TX_NOSPC_HW_CREDITS];
+		comb_bp_pct = (comb_bp_pct * 100.0f) / total_tx;
+
+		net_bp_pct = port_xstat_vals[i][TX_NOSPC_INFL_MAX];
+		net_bp_pct = (net_bp_pct * 100.0f) / total_tx;
+
+		inf_bp_pct = port_xstat_vals[i][TX_NOSPC_NEW_EVENT_LIM];
+		inf_bp_pct = (inf_bp_pct * 100.0f) / total_tx;
+
+		swc_bp_pct = port_xstat_vals[i][TX_NOSPC_INFL_CREDITS];
+		swc_bp_pct = (swc_bp_pct * 100.0f) / total_tx;
+
+		if (total_tx == 0) {
+			ldb_bp_pct = 0.0f;
+			dir_bp_pct = 0.0f;
+			comb_bp_pct = 0.0f;
+			net_bp_pct = 0.0f;
+			inf_bp_pct = 0.0f;
+			swc_bp_pct = 0.0f;
+		}
+
+		print_ldb_deadlock |= ldb_bp_pct > 90.0f;
+		print_dir_deadlock |= dir_bp_pct > 90.0f;
+		print_comb_deadlock |= comb_bp_pct > 90.0f;
+		print_ldb |= ldb_bp_pct > 0.0f;
+		print_dir |= dir_bp_pct > 0.0f;
+		print_comb |= comb_bp_pct > 0.0f;
+		print_net |= net_bp_pct > 0.0f;
+		print_inf |= inf_bp_pct > 0.0f;
+		print_swc |= swc_bp_pct > 0.0f;
+
+		format_percent_str(ldb_bp_pct, ldb_bp_str);
+		format_percent_str(dir_bp_pct, dir_bp_str);
+		format_percent_str(comb_bp_pct, comb_bp_str);
+		format_percent_str(net_bp_pct, net_bp_str);
+		format_percent_str(inf_bp_pct, inf_bp_str);
+		format_percent_str(swc_bp_pct, swc_bp_str);
+		
+		if(dev_xstat_vals[DEV_HW_VERSION] == DLB2_HW_V2) {
+			printf("%3u |%16"PRIu64"|%s %s %s %s |%s%s%s %s %s%s%s      | %s  %s  %s%s%s  %s  %s |\n",
+				i,
+				tx_ok,
+				atm_str, par_str, ord_str, dir_str,
+				new_rel_issue ? COL_RED : "", new_str, COL_RESET,
+				fwd_str,
+				new_rel_issue ? COL_RED : "", rel_str, COL_RESET,
+				ldb_bp_str,
+				dir_bp_str,
+				net_bp_pct > 25.0f ? COL_RED : "", net_bp_str,
+				COL_RESET,
+				inf_bp_str,
+				swc_bp_str);
+		}
+		else if(dev_xstat_vals[DEV_HW_VERSION] == DLB2_HW_V2_5) {			
+			printf("%3u |%16"PRIu64"|%s %s %s %s |%s%s%s %s %s%s%s      | %s    %s%s%s   %s   %s   |\n",
+				i,
+				tx_ok,
+				atm_str, par_str, ord_str, dir_str,
+				new_rel_issue ? COL_RED : "", new_str, COL_RESET,
+				fwd_str,
+				new_rel_issue ? COL_RED : "", rel_str, COL_RESET,
+				comb_bp_str,
+				net_bp_pct > 25.0f ? COL_RED : "", net_bp_str,
+				COL_RESET,
+				inf_bp_str,
+				swc_bp_str);	
+		}
+	}
+
+	printf("-----------------------------------------------------------------------------------------------\n");
+
+	if(dev_xstat_vals[DEV_HW_VERSION] == DLB2_HW_V2) {
+	
+		if (print_ldb)
+			printf("\nLDB backpressure: insufficient load-balanced hardware credits. Can occur\n"
+		       "                  occasionally when enqueueing faster than DLB refills credits.\n");
+		if (print_ldb_deadlock)
+			printf("\n                  A high LDB%% may be caused by credit deadlock. Threads should\n"
+		       "                  retry enqueue with a retry limit, and drop any unsent events\n"
+		       "                  if the limit is reached to release credits.\n");
+		if (print_dir)
+			printf("\nDIR backpressure: insufficient directed hardware credits. Can occur\n"
+		       "                  occasionally when enqueueing faster than DLB refills credits.\n");
+		if (print_dir_deadlock)
+			printf("\n                  A high DIR%% may be caused by credit deadlock. Threads should\n"
+		       "                  retry enqueue with a retry limit, and drop any unsent events\n"
+		       "                  if the limit is reached to release credits.\n");
+	}
+	else if(dev_xstat_vals[DEV_HW_VERSION] == DLB2_HW_V2_5) {
+		if (print_comb)
+			printf("\nCOMB backpressure: insufficient hardware credits. Can occur\n"
+		       "                  occasionally when enqueueing faster than DLB refills credits.\n");
+		if (print_comb_deadlock)
+			printf("\n                  A high COMB%% may be caused by credit deadlock. Threads should\n"
+		       "                  retry enqueue with a retry limit, and drop any unsent events\n"
+		       "                  if the limit is reached to release credits.\n");
+	}
+	if (print_net)
+		printf("\nNET backpressure: unable to enqueue NEW events because inflight events exceeded\n"
+		       "                  the port's New Event Threshold. Indicates the events are being\n"
+		       "                  processed and released slower than the enqueue rate.\n");
+	if (print_inf)
+		printf("\nINF backpressure: unable to enqueue because device-maximum events are inflight.\n");
+	if (print_swc)
+		printf("\nSWC backpressure: insufficient software credits.\n");
+	if (print_new_rel_warning)
+		printf("\n%sNEW + REL vs FWD%s: It is more efficient to send a FORWARD event vs. a NEW event\n"
+		       "                  followed by a RELEASE event. Unless you are intentionally\n"
+		       "		  releasing events early, FORWARD events are strongly recommended.\n",
+		       COL_RED, COL_RESET);
+	printf("\n");
+}
+
+static void
+display_queue_stats(void)
+{
+	float total_queue_depth_pct = 0.0f;
+	bool print_queue_warning = false;
+	int queue_warning_id = 0;
+	unsigned int i;
+
+	printf("                                  Queue stats\n");
+	printf("-------------------------------------------------------------------------------\n");
+	printf("Queue|Type|Depth|                        Depth (%%)                            |\n");
+	printf("-----|----|-----|-------------------------------------------------------------|\n");
+
+#define MAX_DEPTH_STRING_LEN 62
+	for (i = 0; i < num_queues; i++) {
+		uint64_t depth, max_depth, depth_pct;
+		char str[MAX_DEPTH_STRING_LEN + 1];
+		bool is_ldb, high_queue_depth;
+		int j;
+
+		is_ldb = queue_xstat_vals[i][QUEUE_IS_LOAD_BALANCED];
+
+		max_depth = is_ldb ? dev_xstat_vals[DEV_LDB_POOL_SIZE] :
+				     dev_xstat_vals[DEV_DIR_POOL_SIZE];
+
+		depth = queue_xstat_vals[i][CURRENT_DEPTH];
+
+		if (max_depth == 0) {  /* DLB 2.5 uses combined credit pool */
+			max_depth = dev_xstat_vals[DEV_POOL_SIZE];
+			depth = queue_xstat_vals[i][CURRENT_DEPTH];
+		}
+
+		if (skip_zero && depth == 0)
+			continue;
+
+		/* Normalize depth_pct to MAX_DEPTH_STRING_LEN */
+		depth_pct = (MAX_DEPTH_STRING_LEN * depth) / max_depth;
+		total_queue_depth_pct += (100.0f * depth) / max_depth;
+
+		high_queue_depth = false;
+
+		/* Flag the queue's depth if it is using at least 50% of the
+		 * available credits.
+		 */
+		if (((100.0f * depth) / max_depth) >= 50.0f) {
+			high_queue_depth = true;
+			queue_warning_id = i;
+		}
+
+		print_queue_warning |= high_queue_depth;
+
+		/* (max depth string len - 1: reserve one entry for the '>') */
+		for (j = 0; j < MAX_DEPTH_STRING_LEN - 1; j++) {
+			if (depth_pct > 0 && (unsigned int)j <= depth_pct - 1)
+				str[j] = '=';
+			else
+				str[j] = ' ';
+		}
+
+		if ((depth_pct > 0 || depth > 0) &&
+		    (depth_pct < MAX_DEPTH_STRING_LEN))
+			str[depth_pct] = '>';
+
+		str[MAX_DEPTH_STRING_LEN] = '\0';
+
+		printf(" %3u |%s|%s%5"PRIu64"%s|%s%s%s|\n",
+		       i,
+		       is_ldb ? " LDB" : " DIR",
+		       high_queue_depth ? COL_RED : "", depth, COL_RESET,
+		       high_queue_depth ? COL_RED : "", str, COL_RESET);
+	}
+
+	printf("-------------------------------------------------------------------------------\n");
+        if (dev_xstat_vals[DEV_POOL_SIZE] > 8192) /* DIR enqueue depth MSB is not accessible */
+        {
+                printf("WARNING: DIR Depth only shows lower 12 bits. \n If current depth is > 8192, displayed value will be incorrect\n");
+	printf("-------------------------------------------------------------------------------\n");
+        }
+
+	if (total_queue_depth_pct < 10.0f)
+		printf("\nLow queue depth: the queues are under-utilized, likely caused by a low event injection rate.\n");
+	if (print_queue_warning)
+		printf("\nHigh queue depth: queue %u's depth is very high. The port(s) servicing it are\n"
+		       "                  likely processing slower than the event enqueue rate.\n",
+		       queue_warning_id);
+
+	printf("\n");
+}
+
+static void
+display_stats(void)
+{
+	display_port_dequeue_stats();
+
+	display_port_enqueue_stats();
+
+	display_queue_stats();
+
+	display_device_stats();
+
+	printf("Note: scheduling throughput measured over a duration of %us. All other stats are instantaneous samples.\n",
+	       measure_time_us / US_PER_S);
+	printf("\n");
+}
+
+int
+main(int argc, char **argv)
+{
+	int i, diag, cnt;
+	char c_flag[] = "-c1";
+	char n_flag[] = "-n4";
+	char mp_flag[] = "--proc-type=secondary";
+	char *argp[argc + 3];
+
+	argp[0] = argv[0];
+	argp[1] = c_flag;
+	argp[2] = n_flag;
+	argp[3] = mp_flag;
+
+	for (i = 1; i < argc; i++)
+		argp[i + 3] = argv[i];
+
+	argc += 3;
+
+	diag = rte_eal_init(argc, argp);
+	if (diag < 0)
+		rte_panic("Cannot init EAL\n");
+
+	argc -= diag;
+	argv += (diag - 3);
+
+	/* Parse cli options*/
+	parse_app_args(argc, argv);
+
+	const uint8_t ndevs = rte_event_dev_count();
+	if (ndevs == 0)
+		rte_panic("No event devs found. Missing --vdev flag?\n");
+	if (ndevs <= dev_id)
+		rte_panic("Invalid eventdev ID %d (%d devices available)\n",
+			  dev_id, ndevs);
+
+	printf("\n");
+
+	/* Get and output any stats requested on the command line */
+	collect_config();
+
+	display_config();
+
+	cnt = 0;
+
+	do {
+		collect_stats();
+
+		if (do_watch)
+			printf("Sample #%d\n", cnt++);
+
+		if (skip_zero)
+			printf("Skipping ports and queues with zero stats\n");
+
+		display_stats();
+
+		if (do_reset) {
+			rte_event_dev_xstats_reset(dev_id,
+						   RTE_EVENT_DEV_XSTATS_DEVICE,
+						   -1, NULL, 0);
+			rte_event_dev_xstats_reset(dev_id,
+						   RTE_EVENT_DEV_XSTATS_PORT,
+						   -1, NULL, 0);
+			rte_event_dev_xstats_reset(dev_id,
+						   RTE_EVENT_DEV_XSTATS_QUEUE,
+						   -1, NULL, 0);
+			for (i = 0; i < MAX_PORTS_QUEUES; i++)
+				prev_sched_throughput[i] = 0;
+		}
+		/* Wait while the eventdev application executes */
+		rte_delay_us_sleep(measure_time_us);
+
+		do_watch = rte_eal_primary_proc_alive(NULL);
+	} while (do_watch);
+
+	return 0;
+}
diff --git a/app/dlb_monitor/meson.build b/app/dlb_monitor/meson.build
new file mode 100644
index 0000000..d7177ad
--- /dev/null
+++ b/app/dlb_monitor/meson.build
@@ -0,0 +1,13 @@
+# SPDX-License-Identifier: BSD-3-Clause
+# Copyright(c) 2019-2021 Intel Corporation
+
+allow_experimental_apis = true
+
+if not dpdk_conf.has('RTE_EVENT_DLB2')
+	message('DLB PMD not present; skipping dlb_monitor build')
+	build = false
+	subdir_done()
+endif
+
+sources = files('main.c')
+deps += ['eventdev', 'event_dlb2']
diff --git a/app/eventdev_dump/main.c b/app/eventdev_dump/main.c
new file mode 100644
index 0000000..7e1ac08
--- /dev/null
+++ b/app/eventdev_dump/main.c
@@ -0,0 +1,289 @@
+/* SPDX-License-Identifier: BSD-3-Clause
+ * Copyright(c) 2018-2019 Intel Corporation
+ */
+
+#include <getopt.h>
+#include <inttypes.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <stdbool.h>
+#include <unistd.h>
+
+#include <rte_eal.h>
+#include <rte_debug.h>
+#include <rte_eventdev.h>
+
+/* Note - port_queue_id in xstats APIs is 8 bits,   so we have a maximum of
+ * 256 ports and queues
+ */
+#define MAX_PORTS_QUEUES 256
+static uint32_t num_ports;
+uint8_t ports[MAX_PORTS_QUEUES];
+static uint32_t num_queues;
+uint8_t queues[MAX_PORTS_QUEUES];
+
+int evdev_id;
+bool do_dump;
+bool do_device_stats;
+bool do_all_ports;
+bool do_all_queues;
+bool do_reset;
+
+/* No required options */
+static struct option long_options[] = {
+	{0, 0, 0, 0}
+};
+
+static void
+usage(void)
+{
+	const char *usage_str =
+		"Usage: eventdev_dump [options]\n"
+		"Options:\n"
+		" -i <dev_id>		Eventdev id, default is 0\n"
+		" -D			Dump\n"
+		" -P			Get port stats for all ports\n"
+		" -p <port num>		Get port stats for specified port\n"
+		" -Q			Get queue stats for all configured queues\n"
+		" -q <queue num>	Get queue stats for specified queue\n"
+		" -r			Reset stats after reading them\n"
+		"\n";
+
+	printf("%s\n", usage_str);
+	exit(1);
+}
+
+static void
+parse_app_args(int argc, char **argv)
+{
+	/* Parse cli options*/
+	int option_index;
+	int c;
+	opterr = 0;
+
+	for (;;) {
+		c = getopt_long(argc, argv, "dDi:p:Pq:Qr", long_options,
+				&option_index);
+		if (c == -1)
+			break;
+
+		switch (c) {
+		case 'd':
+			do_device_stats = true;
+			break;
+		case 'D':
+			do_dump = true;
+			break;
+		case 'i':
+			evdev_id = atoi(optarg);
+			break;
+		case 'p':
+			ports[num_ports] = atoi(optarg);
+			num_ports++;
+			break;
+		case 'P':
+			do_all_ports = true;
+			break;
+		case 'q':
+			queues[num_queues] = atoi(optarg);
+			num_queues++;
+			break;
+		case 'Q':
+			do_all_queues = true;
+			break;
+		case 'r':
+			do_reset = true;
+			break;
+		default:
+			usage();
+		}
+	}
+}
+
+static int
+dump_all(int evdev_id)
+{
+	int ret = 0;
+
+	ret = rte_event_dev_dump(evdev_id, stdout);
+	return ret;
+}
+
+static void
+get_stats(uint8_t dev_id,
+	  enum rte_event_dev_xstats_mode mode,
+	  uint8_t queue_port_id,
+	  bool reset)
+{
+	int ret;
+	struct rte_event_dev_xstats_name *xstats_names;
+	uint64_t *ids;
+	unsigned int size;
+	int i;
+
+
+	/* Get amount of storage required */
+	ret = rte_event_dev_xstats_names_get(dev_id,
+					     mode,
+					     queue_port_id,
+					     NULL, /* names */
+					     NULL, /* ids */
+					     0);   /* num */
+
+	if (ret < 0)
+		rte_panic("rte_event_dev_xstats_names_get err %d\n", ret);
+
+	if (ret == 0) {
+		printf(
+		"No stats available for this item, mode=%d, queue_port_id=%d\n",
+			mode, queue_port_id);
+		return;
+	}
+
+	size = (unsigned int)ret; /* number of names */
+
+	/* Get memory to hold stat names, IDs, and values */
+
+	xstats_names = malloc(sizeof(struct rte_event_dev_xstats_name) * size);
+	ids = malloc(sizeof(uint64_t) * size);
+
+
+	if (!xstats_names || !ids)
+		rte_panic("unable to alloc memory for stats retrieval\n");
+
+	ret = rte_event_dev_xstats_names_get(dev_id, mode, queue_port_id,
+					     xstats_names, ids,
+					     size);
+	if (ret != (int)size)
+		rte_panic("rte_event_dev_xstats_names_get err %d\n", ret);
+
+	if (!reset) {
+		uint64_t *values;
+
+		values = malloc(sizeof(uint64_t) * size);
+		if (!values)
+			rte_panic("unable to alloc memory for stats retrieval\n");
+
+		ret = rte_event_dev_xstats_get(dev_id, mode, queue_port_id,
+					       ids, values, size);
+
+		if (ret != (int)size)
+			rte_panic("rte_event_dev_xstats_get err %d\n", ret);
+
+		for (i = 0; i < (int)size; i++) {
+			printf("id (%"PRIu64") %s = %"PRIu64"\n",
+				ids[i], &xstats_names[i].name[0], values[i]);
+		}
+
+		free(values);
+	} else
+		rte_event_dev_xstats_reset(dev_id, mode, queue_port_id,
+					   ids, size);
+
+	free(xstats_names);
+	free(ids);
+}
+
+static void
+process_stats(bool reset)
+{
+	unsigned int i;
+
+	if (do_device_stats) {
+		get_stats(evdev_id,
+			  RTE_EVENT_DEV_XSTATS_DEVICE,
+			  0,
+			  reset);
+	}
+
+	if (do_all_ports) {
+		for (i = 0; i < MAX_PORTS_QUEUES; i++) {
+			get_stats(evdev_id,
+				  RTE_EVENT_DEV_XSTATS_PORT,
+				  i,
+				  reset);
+		}
+	} else {
+		for (i = 0; i < num_ports; i++) {
+			get_stats(evdev_id,
+				  RTE_EVENT_DEV_XSTATS_PORT,
+				  ports[i],
+				  reset);
+		}
+	}
+
+	if (do_all_queues) {
+		uint32_t num_configured_queues;
+
+		if (rte_event_dev_attr_get(evdev_id, RTE_EVENT_DEV_ATTR_QUEUE_COUNT, &num_configured_queues))
+			rte_panic("Failed to get the device's queue count\n");
+
+		for (i = 0; i < num_configured_queues; i++) {
+			get_stats(evdev_id,
+				  RTE_EVENT_DEV_XSTATS_QUEUE,
+				  i,
+				  reset);
+		}
+	} else {
+		for (i = 0; i < num_queues; i++) {
+			get_stats(evdev_id,
+				  RTE_EVENT_DEV_XSTATS_QUEUE,
+				  queues[i],
+				  reset);
+		}
+	}
+}
+
+int
+main(int argc, char **argv)
+{
+	int diag;
+	int ret;
+	int i;
+	char c_flag[] = "-c1";
+	char n_flag[] = "-n4";
+	char mp_flag[] = "--proc-type=secondary";
+	char *argp[argc + 3];
+
+	argp[0] = argv[0];
+	argp[1] = c_flag;
+	argp[2] = n_flag;
+	argp[3] = mp_flag;
+
+	for (i = 1; i < argc; i++)
+		argp[i + 3] = argv[i];
+
+	argc += 3;
+
+	diag = rte_eal_init(argc, argp);
+	if (diag < 0)
+		rte_exit(EXIT_FAILURE, "Cannot init EAL\n");
+
+	argc -= diag;
+	argv += (diag - 3);
+
+	/* Parse cli options*/
+	parse_app_args(argc, argv);
+
+	const uint8_t ndevs = rte_event_dev_count();
+	if (ndevs == 0)
+		rte_panic("No event devs found. Do you need"
+			  " to pass in a --vdev flag?\n");
+	if (ndevs > 1)
+		printf("Warning: More than one event dev, but using idx 0\n");
+
+	if (do_dump) {
+		ret = dump_all(evdev_id);
+		if (ret)
+			rte_panic("dump failed with err=%d\n", ret);
+	}
+
+	/* Get and output any stats requested on the command line */
+	process_stats(false);
+
+	/* Reset the stats we just output? */
+	if (do_reset)
+		process_stats(true);
+
+	return 0;
+}
diff --git a/app/eventdev_dump/meson.build b/app/eventdev_dump/meson.build
new file mode 100644
index 0000000..fad085f
--- /dev/null
+++ b/app/eventdev_dump/meson.build
@@ -0,0 +1,6 @@
+# SPDX-License-Identifier: BSD-3-Clause
+# Copyright(c) 2020 Intel Corporation
+
+allow_experimental_apis = true
+sources = files('main.c')
+deps += ['eventdev']
diff --git a/app/test-eventdev/evt_common.h b/app/test-eventdev/evt_common.h
index 15e9c34..69a70fa 100644
--- a/app/test-eventdev/evt_common.h
+++ b/app/test-eventdev/evt_common.h
@@ -35,6 +35,7 @@
 #define EVT_MAX_STAGES           64
 #define EVT_MAX_PORTS            256
 #define EVT_MAX_QUEUES           256
+#define EVT_MAX_PRODUCERS        8
 
 enum evt_prod_type {
 	EVT_PROD_TYPE_NONE,
@@ -50,6 +51,9 @@ struct evt_options {
 	char test_name[EVT_TEST_NAME_MAX_LEN];
 	bool plcores[RTE_MAX_LCORE];
 	bool wlcores[RTE_MAX_LCORE];
+	uint8_t nb_dir_queues;
+	uint8_t dir_queue_ids[RTE_EVENT_MAX_QUEUES_PER_DEV];
+	uint8_t lb_queue_ids[RTE_EVENT_MAX_QUEUES_PER_DEV];
 	int pool_sz;
 	int socket_id;
 	int nb_stages;
@@ -191,7 +195,7 @@ evt_configure_eventdev(struct evt_options *opt, uint8_t nb_queues,
 			.dequeue_timeout_ns = opt->deq_tmo_nsec,
 			.nb_event_queues = nb_queues,
 			.nb_event_ports = nb_ports,
-			.nb_single_link_event_port_queues = 0,
+			.nb_single_link_event_port_queues = opt->nb_dir_queues,
 			.nb_events_limit  = info.max_num_events,
 			.nb_event_queue_flows = opt->nb_flows,
 			.nb_event_port_dequeue_depth =
@@ -203,4 +207,49 @@ evt_configure_eventdev(struct evt_options *opt, uint8_t nb_queues,
 	return rte_event_dev_configure(opt->dev_id, &config);
 }
 
+/* Function evt_configure_eventdev() with additional parameter
+ * nb_single_link passed. Used in pipeline_queue test.
+ */
+static inline int
+evt_configure_eventdev_with_adapter(struct evt_options *opt, uint8_t nb_queues,
+		uint8_t nb_ports, uint16_t nb_single_link)
+{
+	struct rte_event_dev_info info;
+	int ret;
+
+	memset(&info, 0, sizeof(struct rte_event_dev_info));
+	ret = rte_event_dev_info_get(opt->dev_id, &info);
+	if (ret) {
+		evt_err("failed to get eventdev info %d", opt->dev_id);
+		return ret;
+	}
+
+	if (opt->deq_tmo_nsec) {
+		if (opt->deq_tmo_nsec < info.min_dequeue_timeout_ns) {
+			opt->deq_tmo_nsec = info.min_dequeue_timeout_ns;
+			evt_info("dequeue_timeout_ns too low, using %d",
+					opt->deq_tmo_nsec);
+		}
+		if (opt->deq_tmo_nsec > info.max_dequeue_timeout_ns) {
+			opt->deq_tmo_nsec = info.max_dequeue_timeout_ns;
+			evt_info("dequeue_timeout_ns too high, using %d",
+					opt->deq_tmo_nsec);
+		}
+	}
+
+	struct rte_event_dev_config config = {
+			.dequeue_timeout_ns = opt->deq_tmo_nsec,
+			.nb_event_queues = nb_queues,
+			.nb_event_ports = nb_ports,
+			.nb_single_link_event_port_queues = nb_single_link,
+			.nb_events_limit  = info.max_num_events,
+			.nb_event_queue_flows = opt->nb_flows,
+			.nb_event_port_dequeue_depth =
+				info.max_event_port_dequeue_depth,
+			.nb_event_port_enqueue_depth =
+				info.max_event_port_enqueue_depth,
+	};
+	config.nb_event_ports += config.nb_single_link_event_port_queues;
+	return rte_event_dev_configure(opt->dev_id, &config);
+}
 #endif /*  _EVT_COMMON_*/
diff --git a/app/test-eventdev/evt_main.c b/app/test-eventdev/evt_main.c
index 5c7ac2c..6628224 100644
--- a/app/test-eventdev/evt_main.c
+++ b/app/test-eventdev/evt_main.c
@@ -16,6 +16,7 @@
 
 struct evt_options opt;
 struct evt_test *test;
+static rte_atomic32_t evt_dest = RTE_ATOMIC32_INIT(0);
 
 static void
 signal_handler(int signum)
@@ -29,6 +30,9 @@ signal_handler(int signum)
 			*(int *)test->test_priv = true;
 			rte_wmb();
 		}
+	} else if (signum == SIGALRM && test->ops.eventdev_destroy &&
+		   rte_atomic32_test_and_set(&evt_dest)) {
+		test->ops.eventdev_destroy(test, &opt);
 	}
 }
 
@@ -48,6 +52,7 @@ main(int argc, char **argv)
 
 	signal(SIGINT, signal_handler);
 	signal(SIGTERM, signal_handler);
+	signal(SIGALRM, signal_handler);
 
 	ret = rte_eal_init(argc, argv);
 	if (ret < 0)
@@ -160,6 +165,13 @@ main(int argc, char **argv)
 	if (test->ops.ethdev_rx_stop)
 		test->ops.ethdev_rx_stop(test, &opt);
 
+	/*
+	 * Worker threads may get stuck waiting for events in interrupt mode.
+	 * Setting alarm to stop eventdev after 1 sec to wake up workers and
+	 * terminate.
+	 */
+	alarm(1);
+
 	rte_eal_mp_wait_lcore();
 
 	if (test->ops.test_result)
@@ -168,7 +180,8 @@ main(int argc, char **argv)
 	if (test->ops.ethdev_destroy)
 		test->ops.ethdev_destroy(test, &opt);
 
-	if (test->ops.eventdev_destroy)
+	/* Destroy eventdev if not already done by alarm handler*/
+	if (test->ops.eventdev_destroy && rte_atomic32_test_and_set(&evt_dest))
 		test->ops.eventdev_destroy(test, &opt);
 
 	if (test->ops.cryptodev_destroy)
diff --git a/app/test-eventdev/test_order_common.c b/app/test-eventdev/test_order_common.c
index 6f00f24..10032e0 100644
--- a/app/test-eventdev/test_order_common.c
+++ b/app/test-eventdev/test_order_common.c
@@ -342,6 +342,7 @@ order_event_dev_port_setup(struct evt_test *test, struct evt_options *opt,
 			.dequeue_depth = opt->wkr_deq_dep,
 			.enqueue_depth = dev_info.max_event_port_dequeue_depth,
 			.new_event_threshold = dev_info.max_num_events,
+			.event_port_cfg = 0,
 	};
 
 	/* setup one port per worker, linking to all queues */
diff --git a/app/test-eventdev/test_perf_atq.c b/app/test-eventdev/test_perf_atq.c
index 9d30081..47e9aa6 100644
--- a/app/test-eventdev/test_perf_atq.c
+++ b/app/test-eventdev/test_perf_atq.c
@@ -215,6 +215,7 @@ perf_atq_eventdev_setup(struct evt_test *test, struct evt_options *opt)
 			.dequeue_depth = opt->wkr_deq_dep,
 			.enqueue_depth = dev_info.max_event_port_dequeue_depth,
 			.new_event_threshold = dev_info.max_num_events,
+			.event_port_cfg = 0,
 	};
 
 	ret = perf_event_dev_port_setup(test, opt, 1 /* stride */, nb_queues,
diff --git a/app/test-eventdev/test_perf_common.c b/app/test-eventdev/test_perf_common.c
index 140c0c2..cbd331e 100644
--- a/app/test-eventdev/test_perf_common.c
+++ b/app/test-eventdev/test_perf_common.c
@@ -179,11 +179,10 @@ perf_producer_burst(void *arg)
 	uint32_t flow_counter = 0;
 	uint16_t enq = 0;
 	uint64_t count = 0;
-	struct perf_elt *m[MAX_PROD_ENQ_BURST_SIZE + 1];
+	struct perf_elt *m[MAX_PROD_ENQ_BURST_SIZE + 1] = {NULL};
 	struct rte_event ev[MAX_PROD_ENQ_BURST_SIZE + 1];
 	uint32_t burst_size = opt->prod_enq_burst_sz;
 
-	memset(m, 0, sizeof(*m) * (MAX_PROD_ENQ_BURST_SIZE + 1));
 	rte_event_dev_info_get(dev_id, &dev_info);
 	if (dev_info.max_event_port_enqueue_depth < burst_size)
 		burst_size = dev_info.max_event_port_enqueue_depth;
@@ -210,18 +209,18 @@ perf_producer_burst(void *arg)
 			ev[i].event_ptr = m[i];
 			m[i]->timestamp = timestamp;
 		}
-		enq = rte_event_enqueue_burst(dev_id, port, ev, burst_size);
-		while (enq < burst_size) {
-			enq += rte_event_enqueue_burst(dev_id, port,
-							ev + enq,
+                enq = rte_event_enqueue_burst(dev_id, port, ev, burst_size);
+                while (enq < burst_size) {
+                        enq += rte_event_enqueue_burst(dev_id, port,
+                                                        ev + enq,
 							burst_size - enq);
-			if (t->done)
-				break;
+                        if (t->done)
+                                break;
 			rte_pause();
 			timestamp = rte_get_timer_cycles();
 			for (i = enq; i < burst_size; i++)
 				m[i]->timestamp = timestamp;
-		}
+                }
 		count += burst_size;
 	}
 	return 0;
@@ -560,29 +559,33 @@ perf_producer_wrapper(void *arg)
 	struct prod_data *p  = arg;
 	struct test_perf *t = p->t;
 	bool burst = evt_has_burst_mode(p->dev_id);
+	int ret = 0;
 
 	/* In case of synthetic producer, launch perf_producer or
 	 * perf_producer_burst depending on producer enqueue burst size
 	 */
 	if (t->opt->prod_type == EVT_PROD_TYPE_SYNT &&
 			t->opt->prod_enq_burst_sz == 1)
-		return perf_producer(arg);
+		ret =  perf_producer(arg);
 	else if (t->opt->prod_type == EVT_PROD_TYPE_SYNT &&
 			t->opt->prod_enq_burst_sz > 1) {
 		if (!burst)
 			evt_err("This event device does not support burst mode");
 		else
-			return perf_producer_burst(arg);
+			ret = perf_producer_burst(arg);
 	}
 	else if (t->opt->prod_type == EVT_PROD_TYPE_EVENT_TIMER_ADPTR &&
 			!t->opt->timdev_use_burst)
-		return perf_event_timer_producer(arg);
+		ret =  perf_event_timer_producer(arg);
 	else if (t->opt->prod_type == EVT_PROD_TYPE_EVENT_TIMER_ADPTR &&
 			t->opt->timdev_use_burst)
-		return perf_event_timer_producer_burst(arg);
+		ret =  perf_event_timer_producer_burst(arg);
 	else if (t->opt->prod_type == EVT_PROD_TYPE_EVENT_CRYPTO_ADPTR)
-		return perf_event_crypto_producer(arg);
-	return 0;
+		ret =  perf_event_crypto_producer(arg);
+
+	/* Unlink port to release any acquired HW resources*/
+	rte_event_port_unlink(p->dev_id, p->port_id, &p->queue_id, 1);
+	return ret;
 }
 
 static inline uint64_t
@@ -946,8 +949,18 @@ perf_event_dev_port_setup(struct evt_test *test, struct evt_options *opt,
 			return ret;
 		}
 
-		ret = rte_event_port_link(opt->dev_id, port, NULL, NULL, 0);
-		if (ret != nb_queues) {
+		if (opt->nb_dir_queues) {
+			uint8_t queues[EVT_MAX_PRODUCERS] = {0};
+			uint8_t prio[EVT_MAX_PRODUCERS] = {0};
+			uint8_t i;
+
+			for (i = 0; i < nb_queues - opt->nb_dir_queues; ++i)
+				queues[i] = opt->lb_queue_ids[i];
+			ret = rte_event_port_link(opt->dev_id, port, queues, prio, i);
+		} else {
+			ret = rte_event_port_link(opt->dev_id, port, NULL, NULL, 0);
+		}
+		if (ret != nb_queues - opt->nb_dir_queues) {
 			evt_err("failed to link all queues to port %d", port);
 			return -EINVAL;
 		}
@@ -1084,12 +1097,32 @@ perf_event_dev_port_setup(struct evt_test *test, struct evt_options *opt,
 			conf.event_port_cfg |=
 				RTE_EVENT_PORT_CFG_HINT_PRODUCER |
 				RTE_EVENT_PORT_CFG_HINT_CONSUMER;
+			if (opt->nb_dir_queues)
+				conf.event_port_cfg |= RTE_EVENT_PORT_CFG_SINGLE_LINK;
 
 			ret = rte_event_port_setup(opt->dev_id, port, &conf);
 			if (ret) {
 				evt_err("failed to setup port %d", port);
 				return ret;
 			}
+			if (opt->nb_dir_queues) {
+				uint8_t prio = 0;
+				
+				/*
+				 * For KW errors
+				 */
+				port = port % RTE_EVENT_MAX_PORTS_PER_DEV;
+				prod = prod % RTE_EVENT_MAX_PORTS_PER_DEV;
+
+				if (rte_event_port_link(opt->dev_id, port,
+							&opt->dir_queue_ids[prod],
+							&prio, 1) != 1) {
+					evt_err("failed to link dir queue %d "
+						"to port %d",
+						opt->dir_queue_ids[prod], port);
+					return -EINVAL;
+				}
+			}
 			prod++;
 		}
 	}
diff --git a/app/test-eventdev/test_perf_queue.c b/app/test-eventdev/test_perf_queue.c
index 69ef0eb..2cab3a4 100644
--- a/app/test-eventdev/test_perf_queue.c
+++ b/app/test-eventdev/test_perf_queue.c
@@ -12,7 +12,13 @@ perf_queue_nb_event_queues(struct evt_options *opt)
 	/* nb_queues = number of producers * number of stages */
 	uint8_t nb_prod = opt->prod_type == EVT_PROD_TYPE_ETH_RX_ADPTR ?
 		rte_eth_dev_count_avail() : evt_nr_active_lcores(opt->plcores);
-	return nb_prod * opt->nb_stages;
+	/* Directed ports need to be linked to directed queues. So increase the
+	 * number of queues by number of producers.
+	 */
+	if (opt->nb_dir_queues)
+		return nb_prod * (opt->nb_stages + 1);
+	else
+		return nb_prod * opt->nb_stages;
 }
 
 static __rte_always_inline void
@@ -178,9 +184,11 @@ perf_queue_eventdev_setup(struct evt_test *test, struct evt_options *opt)
 	struct test_perf *t = evt_test_priv(test);
 
 	nb_ports = evt_nr_active_lcores(opt->wlcores);
-	nb_ports += opt->prod_type == EVT_PROD_TYPE_ETH_RX_ADPTR ||
-		opt->prod_type == EVT_PROD_TYPE_EVENT_TIMER_ADPTR ? 0 :
-		evt_nr_active_lcores(opt->plcores);
+	if (opt->prod_type != EVT_PROD_TYPE_ETH_RX_ADPTR &&
+	    opt->prod_type != EVT_PROD_TYPE_EVENT_TIMER_ADPTR) {
+		nb_ports += evt_nr_active_lcores(opt->plcores);
+		opt->nb_dir_queues = evt_nr_active_lcores(opt->plcores);
+	}
 
 	nb_queues = perf_queue_nb_event_queues(opt);
 
@@ -201,22 +209,28 @@ perf_queue_eventdev_setup(struct evt_test *test, struct evt_options *opt)
 			.nb_atomic_flows = opt->nb_flows,
 			.nb_atomic_order_sequences = opt->nb_flows,
 	};
+	uint8_t num_lb_q = 0, num_dir_q = 0;
 	/* queue configurations */
 	for (queue = 0; queue < nb_queues; queue++) {
-		q_conf.schedule_type =
-			(opt->sched_type_list[queue % nb_stages]);
-
-		if (opt->q_priority) {
-			uint8_t stage_pos = queue % nb_stages;
-			/* Configure event queues(stage 0 to stage n) with
-			 * RTE_EVENT_DEV_PRIORITY_LOWEST to
-			 * RTE_EVENT_DEV_PRIORITY_HIGHEST.
-			 */
-			uint8_t step = RTE_EVENT_DEV_PRIORITY_LOWEST /
-					(nb_stages - 1);
-			/* Higher prio for the queues closer to last stage */
-			q_conf.priority = RTE_EVENT_DEV_PRIORITY_LOWEST -
-					(step * stage_pos);
+		if (queue >= (nb_queues - opt->nb_dir_queues)) {
+			q_conf.event_queue_cfg = RTE_EVENT_QUEUE_CFG_SINGLE_LINK;
+			opt->dir_queue_ids[num_dir_q++] = queue;
+		} else {
+			q_conf.schedule_type =
+				(opt->sched_type_list[queue % nb_stages]);
+			if (opt->q_priority) {
+				uint8_t stage_pos = queue % nb_stages;
+				/* Configure event queues(stage 0 to stage n) with
+				 * RTE_EVENT_DEV_PRIORITY_LOWEST to
+				 * RTE_EVENT_DEV_PRIORITY_HIGHEST.
+				 */
+				uint8_t step = RTE_EVENT_DEV_PRIORITY_LOWEST /
+							   (nb_stages - 1);
+				/* Higher prio for the queues closer to last stage */
+				q_conf.priority = RTE_EVENT_DEV_PRIORITY_LOWEST -
+								  (step * stage_pos);
+			}
+			opt->lb_queue_ids[num_lb_q++] = queue;
 		}
 		ret = rte_event_queue_setup(opt->dev_id, queue, &q_conf);
 		if (ret) {
@@ -233,6 +247,7 @@ perf_queue_eventdev_setup(struct evt_test *test, struct evt_options *opt)
 			.dequeue_depth = opt->wkr_deq_dep,
 			.enqueue_depth = dev_info.max_event_port_dequeue_depth,
 			.new_event_threshold = dev_info.max_num_events,
+			.event_port_cfg = 0,
 	};
 
 	ret = perf_event_dev_port_setup(test, opt, nb_stages /* stride */,
diff --git a/app/test-eventdev/test_pipeline_queue.c b/app/test-eventdev/test_pipeline_queue.c
index e989396..d7365eb 100644
--- a/app/test-eventdev/test_pipeline_queue.c
+++ b/app/test-eventdev/test_pipeline_queue.c
@@ -638,7 +638,11 @@ pipeline_queue_eventdev_setup(struct evt_test *test, struct evt_options *opt)
 	memset(queue_arr, 0, sizeof(uint8_t) * RTE_EVENT_MAX_QUEUES_PER_DEV);
 
 	rte_event_dev_info_get(opt->dev_id, &info);
-	ret = evt_configure_eventdev(opt, nb_queues, nb_ports);
+
+	/* Pass ethdev count to eventdev configuration to set single-link
+	 * port count. 1 single-link port to be configured per ethdev.
+	 */
+	ret = evt_configure_eventdev_with_adapter(opt, nb_queues, nb_ports, rte_eth_dev_count_avail());
 	if (ret) {
 		evt_err("failed to configure eventdev %d", opt->dev_id);
 		return ret;
@@ -679,7 +683,7 @@ pipeline_queue_eventdev_setup(struct evt_test *test, struct evt_options *opt)
 		opt->wkr_deq_dep = info.max_event_port_dequeue_depth;
 
 	/* port configuration */
-	const struct rte_event_port_conf p_conf = {
+	struct rte_event_port_conf p_conf = {
 			.dequeue_depth = opt->wkr_deq_dep,
 			.enqueue_depth = info.max_event_port_dequeue_depth,
 			.new_event_threshold = info.max_num_events,
@@ -715,6 +719,7 @@ pipeline_queue_eventdev_setup(struct evt_test *test, struct evt_options *opt)
 	if (ret)
 		return ret;
 
+	p_conf.event_port_cfg = RTE_EVENT_PORT_CFG_SINGLE_LINK;
 	ret = pipeline_event_tx_adapter_setup(opt, p_conf);
 	if (ret)
 		return ret;
diff --git a/doc/guides/sample_app_ug/eventdev_cpu_loopback.rst b/doc/guides/sample_app_ug/eventdev_cpu_loopback.rst
new file mode 100644
index 0000000..77fd8b6
--- /dev/null
+++ b/doc/guides/sample_app_ug/eventdev_cpu_loopback.rst
@@ -0,0 +1,92 @@
+..  SPDX-License-Identifier: BSD-3-Clause
+    Copyright(c) 2018 Intel Corporation.
+
+Eventdev CPU Loopback Sample Application
+===========================================
+
+The eventdev_cpu_loopback example is a simple event pipeline application that
+uses CPU-generated traffic to test the IHQM PMD.
+
+Compiling the Application
+-------------------------
+
+To compile the application:
+
+#.  Go to the sample application directory:
+
+    .. code-block:: console
+
+        export RTE_SDK=/path/to/rte_sdk
+        cd ${RTE_SDK}/examples/eventdev_cpu_loopback
+
+#.  Set the target (a default target is used if not specified). For example:
+
+    .. code-block:: console
+
+        export RTE_TARGET=x86_64-native-linuxapp-gcc
+
+    See the *DPDK Getting Started Guide* for possible RTE_TARGET values.
+
+#.  Build the application:
+
+    .. code-block:: console
+
+        make
+
+Running the Application
+-----------------------
+
+The application consists of an event producer, one or more worker threads, and
+an event consumer.
+
+The event producer continuously enqueues events to a load-balanced queue via its
+directed port. These events can be subjected to atomic, ordered, or unordered
+scheduling. The workers continuously dequeue and forward these events to a
+directed queue, from which the event consumer continuously dequeues. The
+producer, workers, and consumer all run in separate threads.
+
+Before running the application, the HQM kernel driver must be loaded:
+
+.. code-block:: console
+
+    insmod /path/to/hqm.ko
+
+An example invocation of eventdev_cpu_loopback using these settings is shown
+below:
+
+ * ``-w 1``: Number of worker threads
+ * ``-q``: Minimize printed output
+ * ``-n 64``: Send 64 packets
+
+.. code-block:: console
+
+    ./build/eventdev_cpu_loopback --vdev event_ihqm,dir_port_ids=0:1,dir_queue_ids=0:1 -- -w 1 -q -n 64
+
+Note that since the producer and consumer threads use directed ports, those
+must be specified in the vdev arguments. See the IHQM Event Device Driver guide
+for more details on its supported vdev arguments.
+
+Expected Output:
+
+.. code-block:: console
+
+  Consumer done! RX=[-n argument]
+  Producer thread done! TX=[-n argument] across [-f argument, default 16] flows
+
+
+Running the Application (DM)
+----------------------------
+
+The application also supports a simple DM test with the '-D' argument. The DM
+test uses two eventdevs, one for the producer and one for the consumer, and
+each uses one load-balanced port. The producer enqueues (via DM) to an event
+queue in the consumer's eventdev, to which the consumer port is connected.
+
+The test is single-threaded and does one event at a time: send one event, wait
+to receive it, then repeat.
+
+To run the DM test:
+
+.. code-block:: console
+
+    ./build/eventdev_cpu_loopback --vdev="event_ihqm,dm_enabled=true,domain_name=dm0" --vdev="event_ihqm1,dm_enabled=true,domain_name=dm1" -- -w 1 -q -n 100 -D
diff --git a/doc/guides/sample_app_ug/index.rst b/doc/guides/sample_app_ug/index.rst
index 6e1e83d..e384faf 100644
--- a/doc/guides/sample_app_ug/index.rst
+++ b/doc/guides/sample_app_ug/index.rst
@@ -55,3 +55,4 @@ Sample Applications User Guides
     ipsec_secgw
     bbdev_app
     ntb
+    eventdev_cpu_loopback
diff --git a/drivers/event/dlb2/bifurcated/dlb2_ioctl.h b/drivers/event/dlb2/bifurcated/dlb2_ioctl.h
new file mode 100644
index 0000000..e40384f
--- /dev/null
+++ b/drivers/event/dlb2/bifurcated/dlb2_ioctl.h
@@ -0,0 +1,71 @@
+/* SPDX-License-Identifier: BSD-3-Clause
+ * Copyright(c) 2016-2020 Intel Corporation
+ */
+
+#ifndef _DLB2_IOCTL_H_
+#define _DLB2_IOCTL_H_
+
+#include <stdbool.h>
+#include <stdint.h>
+#include <rte_debug.h>
+#include <rte_bus_pci.h>
+#include <rte_log.h>
+#include <rte_dev.h>
+#include <rte_mbuf.h>
+#include <rte_ring.h>
+#include <rte_errno.h>
+#include <rte_kvargs.h>
+#include <rte_malloc.h>
+#include <rte_cycles.h>
+#include <rte_io.h>
+#include <rte_eventdev.h>
+#include <eventdev_pmd.h>
+
+#include "../dlb2_priv.h"
+
+/*
+ * Note: The following operate on device_id, rather than domain_id:
+ * 1) dlb2_ioctl_get_device_version
+ * 2) dlb2_ioctl_sched_domain_create
+ * 3) dlb2_ioctl_get_num_resources
+ */
+int dlb2_ioctl_get_device_version(struct dlb2_hw_dev *handle,
+				  uint8_t *revision,
+				  uint8_t *version);
+int dlb2_ioctl_get_num_resources(struct dlb2_hw_dev *handle,
+				 struct dlb2_get_num_resources_args *rsrcs);
+int dlb2_ioctl_sched_domain_create(struct dlb2_hw_dev *handle,
+				   struct dlb2_create_sched_domain_args *args);
+int dlb2_ioctl_ldb_queue_create(struct dlb2_hw_dev *handle,
+				struct dlb2_create_ldb_queue_args *cfg);
+int dlb2_ioctl_dir_queue_create(struct dlb2_hw_dev *handle,
+				struct dlb2_create_dir_queue_args *cfg);
+int dlb2_ioctl_ldb_port_create(struct dlb2_hw_dev *handle,
+			       struct dlb2_create_ldb_port_args *cfg,
+			       enum dlb2_cq_poll_modes poll_mode,
+			       uint8_t evdev_id);
+int dlb2_ioctl_dir_port_create(struct dlb2_hw_dev *handle,
+			       struct dlb2_create_dir_port_args *cfg,
+			       enum dlb2_cq_poll_modes poll_mode,
+			       uint8_t evdev_id);
+int dlb2_ioctl_map_qid(struct dlb2_hw_dev *handle,
+		       struct dlb2_map_qid_args *cfg);
+int dlb2_ioctl_unmap_qid(struct dlb2_hw_dev *handle,
+			 struct dlb2_unmap_qid_args *cfg);
+int dlb2_ioctl_sched_domain_start(struct dlb2_hw_dev *handle,
+				  struct dlb2_start_domain_args *cfg);
+int dlb2_ioctl_sched_domain_stop(struct dlb2_hw_dev *handle,
+				 struct dlb2_stop_domain_args *cfg);
+int dlb2_ioctl_block_on_cq_interrupt(struct dlb2_hw_dev *handle,
+				     int port_id,
+				     bool is_ldb,
+				     volatile void *cq_va,
+				     uint8_t cq_gen,
+				     bool arm);
+int dlb2_ioctl_pending_port_unmaps(struct dlb2_hw_dev *handle,
+				   struct dlb2_pending_port_unmaps_args *args);
+int dlb2_ioctl_get_ldb_queue_depth(struct dlb2_hw_dev *handle,
+				   struct dlb2_get_ldb_queue_depth_args *args);
+int dlb2_ioctl_get_dir_queue_depth(struct dlb2_hw_dev *handle,
+				   struct dlb2_get_dir_queue_depth_args *args);
+#endif /* _DLB2_IOCTL_H_ */
diff --git a/drivers/event/dlb2/bifurcated/dlb2_vdev.c b/drivers/event/dlb2/bifurcated/dlb2_vdev.c
new file mode 100644
index 0000000..d30d281
--- /dev/null
+++ b/drivers/event/dlb2/bifurcated/dlb2_vdev.c
@@ -0,0 +1,1683 @@
+/* SPDX-License-Identifier: BSD-3-Clause
+ * Copyright(c) 2016-2020 Intel Corporation
+ */
+
+#include <stdint.h>
+#include <stdbool.h>
+#include <stdio.h>
+#include <sys/mman.h>
+#include <sys/fcntl.h>
+#include <sys/ioctl.h>
+#include <sys/syscall.h>
+#include <linux/perf_event.h>
+#include <errno.h>
+#include <assert.h>
+#include <unistd.h>
+#include <string.h>
+#include <pthread.h>
+#include <dirent.h>
+
+#include <rte_debug.h>
+#include <rte_bus_vdev.h>
+#include <rte_log.h>
+#include <rte_dev.h>
+#include <rte_mbuf.h>
+#include <rte_ring.h>
+#include <rte_errno.h>
+#include <rte_kvargs.h>
+#include <rte_malloc.h>
+#include <rte_cycles.h>
+#include <rte_io.h>
+#include <rte_eventdev.h>
+#include <eventdev_pmd.h>
+#include <eventdev_pmd_vdev.h>
+#include <rte_string_fns.h>
+#include "bus_pci_driver.h"
+
+#include "dlb2_ioctl.h"
+#include "../dlb2_priv.h"
+#include "../dlb2_iface.h"
+#include "dlb2_vdev.h"
+#include "../dlb2_inline_fns.h"
+
+#if !defined RTE_ARCH_X86_64
+#error "This implementation only supports RTE_ARCH_X86_64 architecture."
+#endif
+
+/*
+ * Process-local globals are required for multiprocess support.
+ * Note that a single process could potentially have more than one DLB2
+ * device/domain open, thus the extra array dimension.
+ */
+
+static int dlb2_device_fd[RTE_EVENT_MAX_DEVS];
+static rte_spinlock_t dlb2_domain_fd_lock;
+static int dlb2_domain_fd[RTE_EVENT_MAX_DEVS][DLB2_MAX_NUM_DOMAINS];
+static bool ll_init_done;
+
+static volatile bool eventdev_reset_all_comms_complete;
+
+/* RTE MP IPC strings */
+#define DLB2_MP_SECONDARY_RESET_REQUEST "dlb2_mp_secondary_reset_request"
+#define DLB2_MP_DO_RESET "dlb2_mp_do_reset"
+#define DLB2_MP_RESET_COMPLETE "dlb2_mp_reset_complete"
+#define DLB2_MP_DOMAIN_FD_REQUEST "dlb2_mp_domain_fd_request"
+#define DLB2_MP_DOMAIN_FD_PUBLISH "dlb2_mp_domain_fd_publish"
+
+#define PCI_DEVICE_ID_INTEL_DLB2_PF 0x2710
+#define PCI_DEVICE_ID_INTEL_DLB2_VF 0x2711
+#define PCI_DEVICE_ID_INTEL_DLB2_5_PF 0x2714
+#define PCI_DEVICE_ID_INTEL_DLB2_5_VF 0x2715
+#define PCI_VENDOR_ID_INTEL 0x8086
+
+static int
+dlb2_send_mp_sync_request(const char *msg_string,
+			  void *param,
+			  size_t len_param,
+			  int fd,
+			  struct rte_mp_reply *mp_reply);
+
+static int
+dlb2_get_sched_domain_fd(struct dlb2_hw_dev *handle)
+{
+	struct rte_mp_reply mp_reply;
+	int fd, ret;
+	uint8_t id[2];
+
+	id[0] = handle->domain_id;
+	id[1] = handle->device_id;
+
+	ret = dlb2_send_mp_sync_request(DLB2_MP_DOMAIN_FD_REQUEST,
+					&id, sizeof(id), -1, &mp_reply);
+	if (ret)
+		return ret;
+
+	fd = mp_reply.msgs[0].fds[0];
+
+	free(mp_reply.msgs);
+
+	return fd;
+}
+
+static int
+dlb2_domain_open(struct dlb2_hw_dev *handle)
+{
+	int fd, ret = 0;
+
+	rte_spinlock_lock(&dlb2_domain_fd_lock);
+
+	if (!handle->domain_id_valid) {
+		DLB2_LOG_ERR("domain not created yet\n");
+		ret = -EINVAL;
+		goto cleanup;
+	}
+
+	/* Do nothing if already open? */
+	if (dlb2_domain_fd[handle->device_id][handle->domain_id] < 0) {
+		DLB2_LOG_DBG("Open DLB2 domain %u\n", handle->domain_id);
+
+		fd = dlb2_get_sched_domain_fd(handle);
+		if (fd < 0) {
+			ret = fd;
+			DLB2_LOG_ERR("open failed: domain %u (ret: %d)\n",
+				     handle->domain_id, ret);
+			goto cleanup;
+		}
+
+		dlb2_domain_fd[handle->device_id][handle->domain_id] = fd;
+	}
+#ifdef DLB2_DEBUG
+	else
+		DLB2_LOG_DBG("domain file %s already open\n",
+			     handle->domain_device_path);
+#endif
+
+cleanup:
+	rte_spinlock_unlock(&dlb2_domain_fd_lock);
+
+	return ret;
+}
+
+static int
+dlb2_ioctl_get_cq_poll_mode(struct dlb2_hw_dev *handle,
+			    enum dlb2_cq_poll_modes *mode)
+{
+	struct dlb2_query_cq_poll_mode_args args = {0};
+	int ret;
+
+	ret = ioctl(dlb2_device_fd[handle->device_id],
+		    DLB2_IOC_QUERY_CQ_POLL_MODE,
+		    (unsigned long)&args);
+
+	*mode = args.response.id;
+
+	return (ret != 0) ? -errno : 0;
+}
+
+static void
+dlb2_ioctl_hardware_init(struct dlb2_hw_dev *handle)
+{
+	RTE_SET_USED(handle);
+
+	/* Intentionally left blank. Only pf pmd inits hw */
+}
+
+static int dlb2_ioctl_get_ldb_port_pp_fd(int fd, int port_id);
+static int dlb2_ioctl_get_ldb_port_cq_fd(int fd, int port_id);
+static int dlb2_ioctl_get_dir_port_pp_fd(int fd, int port_id);
+static int dlb2_ioctl_get_dir_port_cq_fd(int fd, int port_id);
+
+static int
+dlb2_map_cq(struct dlb2_port *qm_port)
+{
+	int ret, fd, domain_fd;
+	void *mmap_addr;
+
+	if (qm_port->config_state != DLB2_CONFIGURED) {
+		DLB2_LOG_ERR("port %d not set up\n", qm_port->id);
+		return -EINVAL;
+	}
+
+	if (dlb2_port[qm_port->evdev_id][qm_port->id]
+		     [PORT_TYPE(qm_port)].cq_base != NULL)
+		return 0; /* already mapped */
+
+	ret = dlb2_domain_open(&qm_port->dlb2->qm_instance);
+	if (ret < 0) {
+		DLB2_LOG_ERR("dlb2: open domain device file failed\n");
+		return ret;
+	}
+
+	domain_fd = dlb2_domain_fd[qm_port->dlb2->qm_instance.device_id]
+				  [qm_port->dlb2->qm_instance.domain_id];
+
+	if (PORT_TYPE(qm_port) == DLB2_LDB_PORT)
+		fd = dlb2_ioctl_get_ldb_port_cq_fd(domain_fd, qm_port->id);
+	else
+		fd = dlb2_ioctl_get_dir_port_cq_fd(domain_fd, qm_port->id);
+
+	if (fd < 0) {
+		DLB2_LOG_ERR("dlb2: open port %u's CQ file failed (ret: %d)\n",
+			     qm_port->id, fd);
+		return fd;
+	}
+
+	mmap_addr = mmap(NULL, DLB2_CQ_SIZE, PROT_WRITE, MAP_SHARED, fd, 0);
+
+
+	if (mmap_addr == (void *)-1) {
+		perror("mmap(): ");
+		close(fd);
+		return -errno;
+	}
+
+	if (close(fd) < 0)
+		DLB2_LOG_ERR("close port fd %d failed with error = %d\n",
+			     fd, errno);
+
+	DLB2_LOG_DBG("mmap OK for port %d, addr=%p\n",
+		     qm_port->id, mmap_addr);
+
+	dlb2_port[qm_port->evdev_id][qm_port->id]
+		 [PORT_TYPE(qm_port)].cq_base = mmap_addr;
+	return 0;
+}
+
+static int
+dlb2_map_pp(struct dlb2_port *qm_port)
+{
+	int ret, fd, domain_fd;
+	uint64_t *mmap_addr;
+	struct process_local_port_data *port_data;
+
+	if (qm_port->config_state != DLB2_CONFIGURED) {
+		DLB2_LOG_ERR("port_%d not set up\n",
+			     qm_port->id);
+		return -EINVAL;
+	}
+	if (dlb2_port[qm_port->evdev_id][qm_port->id]
+		     [PORT_TYPE(qm_port)].pp_addr != NULL)
+		return 0; /* already mapped */
+	ret = dlb2_domain_open(&qm_port->dlb2->qm_instance);
+	if (ret < 0) {
+		DLB2_LOG_ERR("dlb2: open domain device file failed\n");
+		return ret;
+	}
+	domain_fd = dlb2_domain_fd[qm_port->dlb2->qm_instance.device_id]
+				  [qm_port->dlb2->qm_instance.domain_id];
+
+	if (PORT_TYPE(qm_port) == DLB2_LDB_PORT)
+		fd = dlb2_ioctl_get_ldb_port_pp_fd(domain_fd, qm_port->id);
+	else
+		fd = dlb2_ioctl_get_dir_port_pp_fd(domain_fd, qm_port->id);
+
+	if (fd < 0) {
+		DLB2_LOG_ERR("dlb2: open port %u's PP file failed (ret: %d)\n",
+			     qm_port->id, fd);
+		return fd;
+	}
+	mmap_addr = mmap(NULL, DLB2_PP_SIZE, PROT_WRITE, MAP_SHARED, fd, 0);
+
+	if (mmap_addr == (void *)-1) {
+		perror("mmap(): ");
+		close(fd);
+		return -errno;
+	}
+
+	if (close(fd) < 0) {
+		DLB2_LOG_ERR("close port fd %d failed with error = %d\n",
+			     fd, errno);
+	}
+
+	DLB2_LOG_DBG("mmap OK for port %d, addr=%p\n",
+		     qm_port->id, mmap_addr);
+
+	port_data = &dlb2_port[qm_port->evdev_id][qm_port->id]
+			      [PORT_TYPE(qm_port)];
+	port_data->pp_addr = port_data->autopop_addr = mmap_addr;
+
+	if (port_data->use_ded_autopop_cl)
+		port_data->autopop_addr += DLB2_NUM_BYTES_PER_CACHE_LINE;
+
+	return 0;
+}
+
+static void
+dlb2_mmap_all(struct dlb2_port *qm_port)
+{
+	struct process_local_port_data *port_data;
+	struct dlb2_eventdev *dlb2;
+
+	if (!qm_port)
+		rte_panic("%s called with NULL port pointer\n", __func__);
+
+	dlb2 = qm_port->dlb2;
+
+	port_data = &dlb2_port[qm_port->evdev_id][qm_port->id]
+			      [PORT_TYPE(qm_port)];
+
+	rte_spinlock_lock(&dlb2->qm_instance.resource_lock);
+
+	if (dlb2_map_pp(qm_port))
+		rte_panic("%s could not map producer port for port_%d\n",
+			  __func__, qm_port->id);
+
+	if (dlb2_map_cq(qm_port))
+		rte_panic("%s could not map consumer queue for port_%d\n",
+			  __func__, qm_port->id);
+
+	rte_spinlock_unlock(&dlb2->qm_instance.resource_lock);
+
+	port_data->mmaped = true;
+}
+
+/* Setup low level port io just-in-time mmap jhooks. */
+static void
+dlb2_low_level_io_init(void)
+{
+	int i, j, evdev_id;
+
+	if (ll_init_done)
+		return;
+
+	/* init process-local just-in-time (JIT) memory mapping array */
+	for (evdev_id = 0; evdev_id < RTE_EVENT_MAX_DEVS; evdev_id++) {
+		for (i = 0; i < DLB2_MAX_NUM_PORTS_ALL; i++) {
+			for (j = 0; j < DLB2_NUM_PORT_TYPES; j++) {
+				dlb2_port[evdev_id][i][j].pp_addr = NULL;
+				dlb2_port[evdev_id][i][j].cq_base = NULL;
+			}
+		}
+	}
+
+	/* init domain fd array to -1 (invalid fd) */
+
+	for (evdev_id = 0; evdev_id < RTE_EVENT_MAX_DEVS; evdev_id++) {
+		for (i = 0; i < DLB2_MAX_NUM_DOMAINS; i++)
+			dlb2_domain_fd[evdev_id][i] = -1;
+	}
+
+	/* init device fd array to -1 (invalid fd) */
+
+	for (evdev_id = 0; evdev_id < RTE_EVENT_MAX_DEVS; evdev_id++)
+		dlb2_device_fd[evdev_id] = -1;
+
+	ll_init_done = true;
+}
+
+static void
+dlb2_unmap_producer_port(struct process_local_port_data *dlb2_port)
+{
+	if (dlb2_port->pp_addr) {
+		if (munmap(dlb2_port->pp_addr, DLB2_PP_SIZE) < 0) {
+			DLB2_LOG_ERR("munmap of pp mem failed, errno = %d\n",
+				     errno);
+		}
+	}
+
+	dlb2_port->pp_addr = NULL;
+}
+
+static void
+dlb2_unmap_cq(struct process_local_port_data *dlb2_port)
+{
+	if (dlb2_port->cq_base) {
+		if (munmap(dlb2_port->cq_base, DLB2_CQ_SIZE) < 0) {
+			DLB2_LOG_ERR("munmap of CQ mem failed, errno = %d\n",
+				     errno);
+		}
+	}
+
+	dlb2_port->cq_base = NULL;
+}
+
+static void
+dlb2_unmap_all(struct dlb2_eventdev *dlb2)
+{
+	int i;
+
+	/* Unmap PPs and CQ memory memory */
+	for (i = 0; i < dlb2->num_ports; i++) {
+		struct process_local_port_data *port_data;
+		struct dlb2_port *qm_port;
+
+		qm_port = &dlb2->ev_ports[i].qm_port;
+
+		port_data = &dlb2_port[qm_port->evdev_id][qm_port->id]
+				      [PORT_TYPE(qm_port)];
+
+		dlb2_unmap_cq(port_data);
+		dlb2_unmap_producer_port(port_data);
+
+		port_data->mmaped = false;
+	}
+}
+/* VDEV requires multiprocess communication */
+
+static inline struct dlb2_eventdev *
+dlb2_mp_msg_to_dlb2_ptr(const struct rte_mp_msg *msg)
+{
+	/* cast away const qualifier */
+	return *(struct dlb2_eventdev **)(uintptr_t)msg->param;
+}
+
+#define DLB2_MP_SYNC_TIMEOUT 10
+static int
+dlb2_send_mp_sync_request(const char *msg_string,
+			  void *param,
+			  size_t len_param,
+			  int fd,
+			  struct rte_mp_reply *mp_reply)
+{
+	struct rte_mp_msg msg;
+	struct timespec ts;
+	int ret;
+
+	rte_strlcpy(msg.name, msg_string, RTE_MP_MAX_NAME_LEN);
+
+	msg.num_fds = (fd == -1) ? 0 : 1;
+	msg.fds[0] = fd;
+	msg.len_param = len_param;
+
+	if (len_param > RTE_MP_MAX_PARAM_LEN)
+		return -EINVAL;
+
+	memcpy(msg.param, param, len_param);
+
+	ts.tv_sec = DLB2_MP_SYNC_TIMEOUT;
+	ts.tv_nsec = 0;
+
+	ret = rte_mp_request_sync(&msg, mp_reply, &ts);
+	if (ret < 0) {
+		DLB2_LOG_ERR("rte_mp_sendmsg rte_errno = %d\n",
+			     rte_errno);
+		ret = -rte_errno;
+	}
+
+	return ret;
+}
+
+static void
+dlb2_domain_close(struct dlb2_hw_dev *handle)
+{
+	/* Close domain FD */
+	rte_spinlock_lock(&dlb2_domain_fd_lock);
+	if (dlb2_domain_fd[handle->device_id][handle->domain_id] != -1) {
+		if (close(dlb2_domain_fd[handle->device_id]
+					[handle->domain_id]) < 0) {
+			DLB2_LOG_ERR("close domain %d failed with error = %d\n",
+				     handle->domain_id, errno);
+		}
+		dlb2_domain_fd[handle->device_id][handle->domain_id] = -1;
+	}
+	rte_spinlock_unlock(&dlb2_domain_fd_lock);
+}
+
+/* This temporary thread handles the case where a secondary process requested
+ * the domain reset.
+ */
+static void *
+__pri_reset_ctl_fn(void *__args)
+{
+	struct dlb2_eventdev *dlb2 = __args;
+	struct rte_mp_reply mp_reply;
+	int ret;
+
+	/* Notify all secondaries that RESET processing is commencing, and wait
+	 * for a response to ensure that they have all seen and acted on the
+	 * message. Upon successful return from the sync request, all
+	 * secondaries will have unmapped their mappings and closed their
+	 * domain file descriptors.
+	 */
+	ret = dlb2_send_mp_sync_request(DLB2_MP_DO_RESET,
+					&dlb2, sizeof(&dlb2), -1, &mp_reply);
+	if (ret)
+		rte_panic("%s: send mp_sync DLB2_MP_DO_RESET rte_errno=%d\n",
+			  __func__, rte_errno);
+
+	free(mp_reply.msgs);
+
+	/* Unmap all the eventdev's MMIO regions */
+	dlb2_unmap_all(dlb2);
+
+	/* Close the process's domain fd */
+	dlb2_domain_close(&dlb2->qm_instance);
+
+	/* Close processing is complete! */
+	ret = dlb2_send_mp_sync_request(DLB2_MP_RESET_COMPLETE,
+					&dlb2, sizeof(&dlb2), -1, &mp_reply);
+	if (ret)
+		rte_panic("%s: mp_sync DLB2_MP_RESET_COMPLETE rte_errno=%d\n",
+			  __func__, rte_errno);
+
+	free(mp_reply.msgs);
+
+	return NULL;
+}
+
+/* This callback is executed on the primary when a secondary calls
+ * dlb2_iface_domain_reset(). The primary then notifies all secondaries that
+ * RESET has been requested.
+ */
+static int
+dlb2_secondary_reset_request_cb(const struct rte_mp_msg *msg, const void *peer)
+{
+	struct dlb2_eventdev *dlb2 = dlb2_mp_msg_to_dlb2_ptr(msg);
+	struct rte_mp_msg mp_resp;
+	int ret;
+	pthread_t pri_reset_ctl_thread;
+
+	/* ACK the mp ipc msg */
+
+	rte_strlcpy(mp_resp.name, msg->name, RTE_MP_MAX_NAME_LEN);
+
+	mp_resp.len_param = 0;
+	mp_resp.num_fds = 0;
+	if (rte_mp_reply(&mp_resp, peer) < 0) {
+		DLB2_LOG_ERR("failed to send mp reply, rte_errno=%d\n",
+			     rte_errno);
+		return -rte_errno;
+	}
+
+	/* Spin off a short lived thread that handles the primary process
+	 * actions of a secondary-requested reset. The thread will exit
+	 * upon completion of this request.
+	 */
+	ret = pthread_create(&pri_reset_ctl_thread, NULL,
+			     __pri_reset_ctl_fn, (void *)dlb2);
+	if (ret)
+		rte_panic("Could not create primary reset_ctl thread, err=%d\n",
+			  ret);
+	return 0;
+}
+
+/* This callback is executed on the primary when a secondary calls
+ * dlb2_get_sched_domain_fd(). The primary then responds with the scheduling
+ * domain fd.
+ */
+static int
+dlb2_domain_fd_request_cb(const struct rte_mp_msg *msg, const void *peer)
+{
+	struct rte_mp_msg mp_resp;
+	uint8_t domain_id;
+	uint8_t device_id;
+
+	if (msg->num_fds != 0 || msg->len_param != 2)
+		return -EINVAL;
+
+	domain_id = msg->param[0];
+	device_id = msg->param[1];
+
+	if (domain_id >= DLB2_MAX_NUM_DOMAINS) {
+		DLB2_LOG_ERR("Invalid secondary-requested domain %u\n",
+			     domain_id);
+		return -EINVAL;
+	}
+
+	rte_strlcpy(mp_resp.name, msg->name, RTE_MP_MAX_NAME_LEN);
+
+	mp_resp.len_param = 0;
+	mp_resp.num_fds = 1;
+
+	rte_spinlock_lock(&dlb2_domain_fd_lock);
+
+	mp_resp.fds[0] = dlb2_domain_fd[device_id][domain_id];
+
+	rte_spinlock_unlock(&dlb2_domain_fd_lock);
+
+	if (rte_mp_reply(&mp_resp, peer) < 0) {
+		DLB2_LOG_ERR("failed to send mp reply, rte_errno=%d\n",
+			     rte_errno);
+		return -rte_errno;
+	}
+
+	return 0;
+}
+
+/* This callback is executed on the primary when a secondary calls
+ * dlb2_ioctl_sched_domain_create().
+ */
+static int
+dlb2_domain_fd_publish_cb(const struct rte_mp_msg *msg, const void *peer)
+{
+	struct rte_mp_msg mp_resp;
+	uint8_t domain_id, device_id;
+
+	if (msg->num_fds != 1 || msg->len_param != 2)
+		return -EINVAL;
+
+	domain_id = msg->param[0];
+	device_id = msg->param[1];
+
+	if (domain_id >= DLB2_MAX_NUM_DOMAINS) {
+		DLB2_LOG_ERR("Invalid secondary-published domain %u\n",
+			     domain_id);
+		return -EINVAL;
+	}
+
+	rte_spinlock_lock(&dlb2_domain_fd_lock);
+
+	dlb2_domain_fd[device_id][domain_id] = msg->fds[0];
+
+	rte_spinlock_unlock(&dlb2_domain_fd_lock);
+
+	rte_strlcpy(mp_resp.name, msg->name, RTE_MP_MAX_NAME_LEN);
+
+	mp_resp.len_param = 0;
+	mp_resp.num_fds = 0;
+
+	if (rte_mp_reply(&mp_resp, peer) < 0) {
+		DLB2_LOG_ERR("failed to send mp reply, rte_errno=%d\n",
+			     rte_errno);
+		return -rte_errno;
+	}
+
+	return 0;
+}
+
+/* This callback is executed on secondary processes. */
+static int
+dlb2_do_reset_cb(const struct rte_mp_msg *msg, const void *peer)
+{
+	struct dlb2_eventdev *dlb2 = dlb2_mp_msg_to_dlb2_ptr(msg);
+	struct rte_mp_msg mp_resp;
+
+	/* Do our reset related work before replying */
+	dlb2_unmap_all(dlb2);
+
+	dlb2_domain_close(&dlb2->qm_instance);
+
+	/* ACK the mp ipc msg */
+	rte_strlcpy(mp_resp.name, msg->name, RTE_MP_MAX_NAME_LEN);
+
+	mp_resp.len_param = 0;
+	mp_resp.num_fds = 0;
+	if (rte_mp_reply(&mp_resp, peer) < 0) {
+		DLB2_LOG_ERR("failed to send mp reply, rte_errno=%d\n",
+			     rte_errno);
+		return -rte_errno;
+	}
+
+	return 0;
+}
+
+/* This callback is executed on secondary processes */
+static int
+dlb2_reset_complete_cb(const struct rte_mp_msg *msg, const void *peer)
+{
+	struct rte_mp_msg mp_resp;
+
+	/* ACK the mp ipc msg */
+
+	rte_strlcpy(mp_resp.name, msg->name, RTE_MP_MAX_NAME_LEN);
+
+	mp_resp.len_param = 0;
+	mp_resp.num_fds = 0;
+	if (rte_mp_reply(&mp_resp, peer) < 0) {
+		DLB2_LOG_ERR("failed to send mp reply, rte_errno=%d\n",
+			     rte_errno);
+		return -rte_errno;
+	}
+
+	eventdev_reset_all_comms_complete = true;
+
+	return 0;
+}
+
+static void
+dlb2_do_secondary_initiated_reset(struct dlb2_eventdev *dlb2)
+{
+	struct timespec endtime, currtime;
+	struct rte_mp_reply mp_reply;
+	int ret;
+
+	eventdev_reset_all_comms_complete = false; /* clear flag */
+
+	ret = dlb2_send_mp_sync_request(DLB2_MP_SECONDARY_RESET_REQUEST,
+					&dlb2, sizeof(&dlb2), -1, &mp_reply);
+	if (ret)
+		rte_panic("%s: DLB2_MP_SECONDARY_RESET_REQUEST err=%d\n",
+			  __func__, ret);
+
+	free(mp_reply.msgs);
+
+	/* Wait ~10S for primary-controlled reset to fully complete */
+	if (clock_gettime(CLOCK_MONOTONIC, &endtime))
+		rte_panic("%s: clock_gettime() failed  err=%d\n",
+			  __func__, errno);
+
+	endtime.tv_sec += 10;
+
+	while (!eventdev_reset_all_comms_complete) {
+		if (clock_gettime(CLOCK_MONOTONIC, &currtime))
+			rte_panic("%s: clock_gettime() failed  err=%d\n",
+			  __func__, errno);
+
+		if (currtime.tv_sec >= endtime.tv_sec) {
+			rte_panic("%s: Secondary initiated reset timeout\n",
+				  __func__);
+		}
+		rte_pause();
+	}
+}
+
+static int
+dlb2_device_reset(struct dlb2_eventdev *dlb2)
+{
+	struct rte_mp_reply mp_reply;
+	int ret;
+
+	if (dlb2->run_state != DLB2_RUN_STATE_STOPPED) {
+		DLB2_LOG_ERR("Internal error: bad state %d for %s\n",
+			     (int)dlb2->run_state, __func__);
+		return DLB2_ST_DOMAIN_IN_USE;
+	}
+
+	/* Make sure domain is open - NOOP if already open */
+	ret = dlb2_domain_open(&dlb2->qm_instance);
+	if (ret < 0) {
+		DLB2_LOG_ERR("dlb2: open domain device file failed\n");
+		return ret;
+	}
+
+	if (rte_eal_process_type() == RTE_PROC_SECONDARY) {
+		/* dlb2_iface_domain_reset called by *secondary* process.
+		 * Primary will forward request to other secondaries.
+		 */
+
+		dlb2_do_secondary_initiated_reset(dlb2);
+		return 0;
+	}
+
+	/* dlb2_iface_domain_reset called by *primary* process.
+	 * Notify all secondaries that RESET processing is commencing, and wait
+	 * for a response to ensure that they have all seen and acted on the
+	 * message.
+	 */
+	ret = dlb2_send_mp_sync_request(DLB2_MP_DO_RESET,
+					&dlb2, sizeof(&dlb2), -1, &mp_reply);
+	if (ret)
+		rte_panic("%s: primary shut down event dev, err=%d\n",
+			  __func__, ret);
+
+	free(mp_reply.msgs);
+
+	/* Unmap all the eventdev's MMIO regions */
+	dlb2_unmap_all(dlb2);
+
+	/* Close the primary process's domain fd */
+	dlb2_domain_close(&dlb2->qm_instance);
+
+	/* Notify any secondaries that RESET processing is complete. */
+	ret = dlb2_send_mp_sync_request(DLB2_MP_RESET_COMPLETE,
+					&dlb2, sizeof(&dlb2), -1, &mp_reply);
+	if (ret)
+		rte_panic("%s: DLB2_MP_RESET_COMPLETE err=%d\n",
+			  __func__, ret);
+
+	free(mp_reply.msgs);
+
+	return 0;
+}
+
+int
+dlb2_register_pri_mp_callbacks(void)
+{
+	int ret;
+
+	ret = rte_mp_action_register(DLB2_MP_SECONDARY_RESET_REQUEST,
+				     dlb2_secondary_reset_request_cb);
+	if (ret && rte_errno != EEXIST)
+		return -rte_errno;
+
+	ret = rte_mp_action_register(DLB2_MP_DOMAIN_FD_REQUEST,
+				     dlb2_domain_fd_request_cb);
+	if (ret && rte_errno != EEXIST)
+		return -rte_errno;
+
+	ret = rte_mp_action_register(DLB2_MP_DOMAIN_FD_PUBLISH,
+				     dlb2_domain_fd_publish_cb);
+	if (ret && rte_errno != EEXIST)
+		return -rte_errno;
+
+	return 0;
+}
+
+int
+dlb2_register_sec_mp_callbacks(void)
+{
+	int ret;
+
+	ret = rte_mp_action_register(DLB2_MP_DO_RESET,
+				     dlb2_do_reset_cb);
+	if (ret && rte_errno != EEXIST)
+		return -rte_errno;
+
+	ret = rte_mp_action_register(DLB2_MP_RESET_COMPLETE,
+				     dlb2_reset_complete_cb);
+	if (ret && rte_errno != EEXIST)
+		goto dlb2_reset_complete_cb_failed;
+
+	return 0;
+
+dlb2_reset_complete_cb_failed:
+	rte_mp_action_unregister(DLB2_MP_DO_RESET);
+
+	return -rte_errno;
+}
+
+static int
+dlb2_get_pmu_type(int dlb_id)
+{
+	char path[PATH_MAX];
+	unsigned long pmu_type;
+
+	snprintf(path, sizeof(path),
+		 EVENT_SOURCE_DEV_PATH "%d/type", dlb_id);
+
+	if (eal_parse_sysfs_value(path, &pmu_type) < 0)
+		return -1;
+
+	return pmu_type;
+}
+
+static int
+dlb2_read_perf_sched_idle_counts(int pmu_type,
+                                 struct dlb2_sched_idle_counts *data)
+{
+	uint64_t counter_id[DLB2_MAX_NUM_CNTRS] = {0};
+	int fd[DLB2_MAX_NUM_CNTRS], ret;
+	struct perf_event_attr attr;
+	struct read_format *rf;
+	uint64_t index, val;
+	char buf[4096];
+	uint8_t i;
+
+	rf = (struct read_format *) buf;
+	memset(&attr, 0, sizeof(struct perf_event_attr));
+
+	/* First counter is designated as perf events group leader
+	 * and its file descriptor is passed in perf_event_open()
+	 * syscall of remaining counters.
+	 */
+	attr.type = pmu_type;
+	attr.size = sizeof(struct perf_event_attr);
+	/* Event group requires group leader starting counter in
+	 * disabled state. Child events must set disabled bit as 0.
+	 */
+	attr.disabled = 1;
+	attr.read_format = PERF_FORMAT_GROUP | PERF_FORMAT_ID;
+	attr.exclude_kernel = 0;
+	attr.exclude_hv = 0;
+	attr.config = 0; /* First counter == group leader */
+	fd[0] = syscall(__NR_perf_event_open, &attr, -1, 0, -1, 0);
+	ret = ioctl(fd[0], PERF_EVENT_IOC_ID, &counter_id[0]);
+	if (ret != 0) {
+		DLB2_LOG_ERR("dlb2: perf grp event id ioctl error\n");
+		return -errno;
+	}
+
+	/* Remaining counters (child events) */
+	for (i = 1; i < DLB2_MAX_NUM_CNTRS; i++) {
+		attr.disabled = 0;
+		attr.config = i;
+		fd[i] = syscall(__NR_perf_event_open, &attr, -1, 0,
+				fd[0], 0);
+		ret = ioctl(fd[i], PERF_EVENT_IOC_ID, &counter_id[i]);
+		if (ret != 0) {
+			DLB2_LOG_ERR("dlb2: perf event id ioctl error\n");
+			return -errno;
+		}
+	}
+
+	/* Reset and enable the counters */
+	ret = ioctl(fd[0], PERF_EVENT_IOC_RESET, PERF_IOC_FLAG_GROUP);
+	if (ret != 0) {
+		DLB2_LOG_ERR("dlb2: perf reset ioctl error\n");
+		return -errno;
+	}
+	ret = ioctl(fd[0], PERF_EVENT_IOC_ENABLE, PERF_IOC_FLAG_GROUP);
+	if (ret != 0) {
+		DLB2_LOG_ERR("dlb2: perf enable ioctl error\n");
+		return -errno;
+	}
+
+	/* Time interval when counters are tracked */
+	rte_delay_ms(DLB2_IDLE_CNT_INTERVAL);
+
+	/* Read the diff of counters after the interval */
+	ret = read(fd[0], buf, sizeof(buf));
+	if (ret < 0)
+		return -errno;
+
+	/* read_format structure updated after read() */
+	for (i = 0; i < rf->num_counters; i++) {
+		index = rf->values[i].counter_index;
+		val = rf->values[i].counter_value;
+		if (index == counter_id[DLB2_SCHED_CNT])
+			data->ldb_perf_sched_cnt = val;
+		else if (index == counter_id[DLB2_NO_WORK_CNT])
+			data->ldb_perf_nowork_idle_cnt = val;
+		else if (index == counter_id[DLB2_NO_SPACE_CNT])
+			data->ldb_perf_nospace_idle_cnt = val;
+		else if (index == counter_id[DLB2_PFRICTION_CNT])
+			data->ldb_perf_pfriction_idle_cnt = val;
+		else if (index == counter_id[DLB2_IFLIMIT_CNT])
+			data->ldb_perf_iflimit_idle_cnt = val;
+		else if (index == counter_id[DLB2_FIDLIMIT_CNT])
+			data->ldb_perf_fidlimit_idle_cnt = val;
+		else if (index == counter_id[DLB2_PROC_ON_CNT])
+			data->perf_proc_on_cnt = val;
+		else if (index == counter_id[DLB2_CLK_ON_CNT])
+			data->perf_clk_on_cnt = val;
+		else if (index == counter_id[DLB2_HCW_ERR_CNT])
+			data->hcw_err_cnt = val;
+	}
+
+	ret = ioctl(fd[0], PERF_EVENT_IOC_DISABLE, PERF_IOC_FLAG_GROUP);
+	if (ret != 0) {
+		DLB2_LOG_ERR("dlb2: perf disable ioctl error\n");
+		return -errno;
+	}
+
+	return 0;
+}
+
+static int
+dlb2_perf_get_sched_idle_counts(struct dlb2_hw_dev *handle,
+				void *idle_counts)
+{
+	struct dlb2_sched_idle_counts *data = idle_counts;
+	int pmu_type;
+	int ret = 0;
+
+	/* DLB2 Perf PMU value that is initialized during perf init
+	 * is read from sysfs filesystem.
+	 */
+	pmu_type = dlb2_get_pmu_type(handle->device_id);
+	if (pmu_type <= 0) {
+		DLB2_LOG_ERR("dlb2: perf pmu not supported\n");
+		return -1;
+	}
+
+	/* Counters read with perf ioctls */
+	ret = dlb2_read_perf_sched_idle_counts(pmu_type,
+					       data);
+	return ret;
+}
+
+/* Begin IOCTLS that take a device fd */
+
+int
+dlb2_ioctl_get_device_version(struct dlb2_hw_dev *handle,
+			      uint8_t *revision,
+			      uint8_t *version)
+{
+	struct dlb2_get_device_version_args ioctl_args = {0};
+
+	int ret = ioctl(dlb2_device_fd[handle->device_id],
+			DLB2_IOC_GET_DEVICE_VERSION,
+			(unsigned long)&ioctl_args);
+	if (!ret) {
+		*revision = DLB2_DEVICE_REVISION(ioctl_args.response.id);
+		*version = DLB2_DEVICE_VERSION(ioctl_args.response.id);
+		*version -= 2; /* Mapping from kernel vals to PMD vals */
+	}
+	return (ret != 0) ? -errno : 0;
+}
+
+int
+dlb2_ioctl_get_num_resources(struct dlb2_hw_dev *handle,
+			     struct dlb2_get_num_resources_args *rsrcs)
+{
+	int ret = ioctl(dlb2_device_fd[handle->device_id],
+			DLB2_IOC_GET_NUM_RESOURCES,
+			(unsigned long)rsrcs);
+
+	return (ret != 0) ? -errno : 0;
+}
+
+int
+dlb2_ioctl_sched_domain_create(struct dlb2_hw_dev *handle,
+			       struct dlb2_create_sched_domain_args *args)
+{
+	int ret = ioctl(dlb2_device_fd[handle->device_id],
+			DLB2_IOC_CREATE_SCHED_DOMAIN, (unsigned long)args);
+
+	if (ret == 0)
+		dlb2_domain_fd[handle->device_id][args->response.id] =
+			args->domain_fd;
+
+	if (rte_eal_process_type() == RTE_PROC_SECONDARY) {
+		struct rte_mp_reply mp_reply;
+		int fd, ret;
+		uint8_t id[2];
+
+		id[0] = args->response.id;
+		id[1] = handle->device_id;
+		fd = args->domain_fd;
+
+		ret = dlb2_send_mp_sync_request(DLB2_MP_DOMAIN_FD_PUBLISH,
+						&id, sizeof(id), fd, &mp_reply);
+		if (ret)
+			return ret;
+
+		free(mp_reply.msgs);
+	}
+
+	return (ret != 0) ? -errno : 0;
+}
+
+/* Begin IOCTLS that take a domain fd */
+int
+dlb2_ioctl_ldb_queue_create(struct dlb2_hw_dev *handle,
+			    struct dlb2_create_ldb_queue_args *args)
+{
+	int ret = dlb2_domain_open(handle);
+	if (ret < 0) {
+		DLB2_LOG_ERR("dlb2: open domain device file failed\n");
+		return ret;
+	}
+
+	ret = ioctl(dlb2_domain_fd[handle->device_id][handle->domain_id],
+		    DLB2_IOC_CREATE_LDB_QUEUE,
+		    (unsigned long)args);
+
+	return (ret != 0) ? -errno : 0;
+}
+
+int
+dlb2_ioctl_dir_queue_create(struct dlb2_hw_dev *handle,
+			    struct dlb2_create_dir_queue_args *args)
+{
+	int ret = dlb2_domain_open(handle);
+	if (ret < 0) {
+		DLB2_LOG_ERR("dlb2: open domain device file failed\n");
+		return ret;
+	}
+
+	ret = ioctl(dlb2_domain_fd[handle->device_id][handle->domain_id],
+		    DLB2_IOC_CREATE_DIR_QUEUE,
+		    (unsigned long)args);
+
+	return (ret != 0) ? -errno : 0;
+}
+
+int
+dlb2_ioctl_ldb_port_create(struct dlb2_hw_dev *handle,
+			   struct dlb2_create_ldb_port_args *args,
+			   enum dlb2_cq_poll_modes poll_mode __rte_unused,
+			   uint8_t evdev_id)
+{
+	RTE_SET_USED(evdev_id);
+	int ret = dlb2_domain_open(handle);
+	if (ret < 0) {
+		DLB2_LOG_ERR("dlb2: open domain device file failed\n");
+		return ret;
+	}
+
+	ret = ioctl(dlb2_domain_fd[handle->device_id][handle->domain_id],
+		    DLB2_IOC_CREATE_LDB_PORT, (unsigned long)args);
+
+	return (ret != 0) ? -errno : 0;
+}
+
+int
+dlb2_ioctl_dir_port_create(struct dlb2_hw_dev *handle,
+			   struct dlb2_create_dir_port_args *args,
+			   enum dlb2_cq_poll_modes poll_mode __rte_unused,
+			   uint8_t evdev_id)
+{
+	RTE_SET_USED(evdev_id);
+	int ret = dlb2_domain_open(handle);
+	if (ret < 0) {
+		DLB2_LOG_ERR("dlb2: open domain device file failed\n");
+		return ret;
+	}
+
+	ret = ioctl(dlb2_domain_fd[handle->device_id][handle->domain_id],
+		    DLB2_IOC_CREATE_DIR_PORT, (unsigned long)args);
+
+	return (ret != 0) ? -errno : 0;
+}
+
+int
+dlb2_ioctl_map_qid(struct dlb2_hw_dev *handle,
+		   struct dlb2_map_qid_args *args)
+{
+	int ret = dlb2_domain_open(handle);
+	if (ret < 0) {
+		DLB2_LOG_ERR("dlb2: open domain device file failed\n");
+		return ret;
+	}
+
+	ret = ioctl(dlb2_domain_fd[handle->device_id][handle->domain_id],
+		    DLB2_IOC_MAP_QID, (unsigned long)args);
+
+	return (ret != 0) ? -errno : 0;
+}
+
+int
+dlb2_ioctl_unmap_qid(struct dlb2_hw_dev *handle,
+		     struct dlb2_unmap_qid_args *args)
+{
+	int ret = dlb2_domain_open(handle);
+	if (ret < 0) {
+		DLB2_LOG_ERR("dlb2: open domain device file failed\n");
+		return ret;
+	}
+
+	ret = ioctl(dlb2_domain_fd[handle->device_id][handle->domain_id],
+		    DLB2_IOC_UNMAP_QID, (unsigned long)args);
+
+	return (ret != 0) ? -errno : 0;
+}
+
+int
+dlb2_ioctl_sched_domain_start(struct dlb2_hw_dev *handle,
+			      struct dlb2_start_domain_args *args)
+{
+	int ret = dlb2_domain_open(handle);
+	if (ret < 0) {
+		DLB2_LOG_ERR("dlb2: open domain device file failed\n");
+		return ret;
+	}
+
+	ret = ioctl(dlb2_domain_fd[handle->device_id][handle->domain_id],
+		    DLB2_IOC_START_DOMAIN, (unsigned long)args);
+
+	return (ret != 0) ? -errno : 0;
+}
+
+int dlb2_ioctl_sched_domain_stop(struct dlb2_hw_dev *handle,
+				 struct dlb2_stop_domain_args *args) {
+	int ret = dlb2_domain_open(handle);
+	if (ret < 0) {
+		DLB2_LOG_ERR("dlb2: open domain device file failed\n");
+		return ret;
+	}
+
+	ret = ioctl(dlb2_domain_fd[handle->device_id][handle->domain_id],
+		    DLB2_IOC_STOP_DOMAIN, (unsigned long)args);
+
+	return (ret != 0) ? -errno : 0;
+}
+
+int
+dlb2_ioctl_block_on_cq_interrupt(struct dlb2_hw_dev *handle,
+				 int port_id,
+				 bool is_ldb,
+				 volatile void *cq_va,
+				 uint8_t cq_gen,
+				 bool arm)
+{
+	struct dlb2_block_on_cq_interrupt_args ioctl_args = {0};
+
+	ioctl_args.port_id = port_id;
+	ioctl_args.is_ldb = is_ldb;
+	ioctl_args.cq_va = (uintptr_t)cq_va;
+	ioctl_args.cq_gen = cq_gen;
+	ioctl_args.arm = arm;
+
+	int ret = dlb2_domain_open(handle);
+	if (ret < 0) {
+		DLB2_LOG_ERR("dlb2: open domain device file failed\n");
+		return ret;
+	}
+
+	ret = ioctl(dlb2_domain_fd[handle->device_id][handle->domain_id],
+		    DLB2_IOC_BLOCK_ON_CQ_INTERRUPT,
+		    (unsigned long)&ioctl_args);
+
+	return (ret != 0) ? -errno : 0;
+}
+
+int
+dlb2_ioctl_get_ldb_queue_depth(struct dlb2_hw_dev *handle,
+			       struct dlb2_get_ldb_queue_depth_args *args)
+{
+	int ret = dlb2_domain_open(handle);
+	if (ret < 0) {
+		DLB2_LOG_ERR("dlb2: open domain device file failed\n");
+		return ret;
+	}
+
+	ret = ioctl(dlb2_domain_fd[handle->device_id][handle->domain_id],
+		    DLB2_IOC_GET_LDB_QUEUE_DEPTH, (unsigned long)args);
+
+	return (ret != 0) ? -errno : 0;
+}
+
+int
+dlb2_ioctl_pending_port_unmaps(struct dlb2_hw_dev *handle,
+			       struct dlb2_pending_port_unmaps_args *args)
+{
+	int ret = dlb2_domain_open(handle);
+	if (ret < 0) {
+		DLB2_LOG_ERR("dlb2: open domain device file failed\n");
+		return ret;
+	}
+
+	ret = ioctl(dlb2_domain_fd[handle->device_id][handle->domain_id],
+		    DLB2_IOC_PENDING_PORT_UNMAPS, (unsigned long)args);
+
+	return (ret != 0) ? -errno : 0;
+}
+
+int
+dlb2_ioctl_get_dir_queue_depth(struct dlb2_hw_dev *handle,
+			       struct dlb2_get_dir_queue_depth_args *args)
+{
+	int ret = dlb2_domain_open(handle);
+	if (ret < 0) {
+		DLB2_LOG_ERR("dlb2: open domain device file failed\n");
+		return ret;
+	}
+
+	ret = ioctl(dlb2_domain_fd[handle->device_id][handle->domain_id],
+		    DLB2_IOC_GET_DIR_QUEUE_DEPTH, (unsigned long)args);
+
+	return (ret != 0) ? -errno : 0;
+}
+
+static int
+dlb2_ioctl_get_sn_allocation(struct dlb2_hw_dev *handle,
+			     struct dlb2_get_sn_allocation_args *args)
+{
+	int ret = ioctl(dlb2_device_fd[handle->device_id],
+			DLB2_IOC_GET_SN_ALLOCATION, (unsigned long)args);
+
+	return (ret != 0) ? -errno : 0;
+}
+
+static int
+dlb2_ioctl_set_sn_allocation(struct dlb2_hw_dev *handle,
+			     struct dlb2_set_sn_allocation_args *args)
+{
+	int ret = ioctl(dlb2_device_fd[handle->device_id],
+			DLB2_IOC_SET_SN_ALLOCATION, (unsigned long)args);
+
+	return (ret != 0) ? -errno : 0;
+}
+
+static int
+dlb2_ioctl_get_sn_occupancy(struct dlb2_hw_dev *handle,
+			    struct dlb2_get_sn_occupancy_args *args)
+{
+	int ret = ioctl(dlb2_device_fd[handle->device_id],
+			DLB2_IOC_GET_SN_OCCUPANCY, (unsigned long)args);
+
+	return (ret != 0) ? -errno : 0;
+}
+
+static int dlb2_ioctl_get_port_fd(int fd, int port_id, uint32_t ioc)
+{
+	struct dlb2_get_port_fd_args ioctl_args = {0};
+	int ret;
+
+	ioctl_args.port_id = port_id;
+
+	ret = ioctl(fd, ioc, (unsigned long)&ioctl_args);
+
+	return (ret != 0) ? -errno : (int)ioctl_args.response.id;
+}
+
+static int dlb2_ioctl_port_ctrl(struct dlb2_port *qm_port, bool enable)
+{
+	int domain_fd, ret;
+
+	domain_fd = dlb2_domain_fd[qm_port->dlb2->qm_instance.device_id]
+				  [qm_port->dlb2->qm_instance.domain_id];
+
+	if (PORT_TYPE(qm_port) == DLB2_LDB_PORT) {
+		if (enable) {
+			struct dlb2_enable_ldb_port_args args = {.port_id = qm_port->id};
+
+			ret = ioctl(domain_fd, DLB2_IOC_ENABLE_LDB_PORT, (unsigned long)&args);
+		} else {
+			struct dlb2_disable_ldb_port_args args = {.port_id = qm_port->id};
+
+			ret = ioctl(domain_fd, DLB2_IOC_DISABLE_LDB_PORT, (unsigned long)&args);
+		}
+	} else {
+		if (enable) {
+			struct dlb2_enable_dir_port_args args = {.port_id = qm_port->id};
+
+			ret = ioctl(domain_fd, DLB2_IOC_ENABLE_DIR_PORT, (unsigned long)&args);
+		} else {
+			struct dlb2_disable_dir_port_args args = {.port_id = qm_port->id};
+
+			ret = ioctl(domain_fd, DLB2_IOC_DISABLE_DIR_PORT, (unsigned long)&args);
+		}
+	}
+
+	return (ret != 0) ? -errno : 0;
+}
+
+static int dlb2_ioctl_get_ldb_port_pp_fd(int fd, int port_id)
+{
+	return dlb2_ioctl_get_port_fd(fd, port_id, DLB2_IOC_GET_LDB_PORT_PP_FD);
+}
+
+static int dlb2_ioctl_get_ldb_port_cq_fd(int fd, int port_id)
+{
+	return dlb2_ioctl_get_port_fd(fd, port_id, DLB2_IOC_GET_LDB_PORT_CQ_FD);
+}
+
+static int dlb2_ioctl_get_dir_port_pp_fd(int fd, int port_id)
+{
+	return dlb2_ioctl_get_port_fd(fd, port_id, DLB2_IOC_GET_DIR_PORT_PP_FD);
+}
+
+static int dlb2_ioctl_get_dir_port_cq_fd(int fd, int port_id)
+{
+	return dlb2_ioctl_get_port_fd(fd, port_id, DLB2_IOC_GET_DIR_PORT_CQ_FD);
+}
+
+static int
+dlb2_eventdev_name_to_dev_names(const char *name, char *dev_path,
+				int dev_path_id, char *dev_name, int *id)
+{
+	int len, ret;
+
+	/* Expected name is of the form qm_dlb2X or qm_dlb2XX */
+	len = strnlen(name, DLB2_MAX_DEVICE_PATH);
+
+	if (len == 10) {
+		*id = 0;
+	} else if (len >= 11 && len <= 12) {
+		ret = dlb2_string_to_int(id, &name[10]);
+		if (ret < 0)
+			return ret;
+
+		if (*id >= DLB2_MAX_NUM_DOMAINS) {
+			DLB2_LOG_ERR("Bad eventdev id %d >= %d\n",
+				     *id, DLB2_MAX_NUM_DOMAINS);
+			return -EINVAL;
+		}
+	} else {
+		DLB2_LOG_ERR("Bad eventdev name %s - unsafe\n",
+			     name);
+		return -EINVAL;
+	}
+
+	snprintf(dev_path,
+		 DLB2_MAX_DEVICE_PATH,
+		 "/dev/dlb%d",
+		 dev_path_id);
+
+	snprintf(dev_name,
+		 DLB2_MAX_DEVICE_PATH,
+		 "dlb2%d",
+		 *id);
+
+	return 0;
+}
+
+static int
+dlb2_hw_open(struct dlb2_hw_dev *handle, const char *name)
+{
+	if (name == NULL)
+		return -EINVAL;
+
+	if (dlb2_eventdev_name_to_dev_names(name,
+					    &handle->device_path[0],
+					    handle->device_path_id,
+				 &handle->device_name[0],
+				 &handle->device_id)) {
+		DLB2_LOG_ERR("dlb2: could not derive device path for %s\n",
+			     name);
+		return -EINVAL;
+	}
+
+	/* Do nothing if already open */
+	if (dlb2_device_fd[handle->device_id] < 0) {
+		DLB2_LOG_DBG("Open DLB2 device %s\n", handle->device_path);
+
+		dlb2_device_fd[handle->device_id] = open(handle->device_path,
+							 O_RDWR);
+
+		if (dlb2_device_fd[handle->device_id] < 0) {
+			DLB2_LOG_ERR("open failed: device_path %s\n",
+				     handle->device_path);
+			perror("open(): ");
+			return -EINVAL;
+		}
+	}
+#ifdef DLB2_DEBUG
+	else
+		DLB2_LOG_DBG("%s already open\n", handle->device_path);
+#endif
+
+	return 0;
+}
+
+static int
+dlb2_hwdev_open(struct dlb2_hw_dev *handle,
+		const char *name)
+{
+	int socket_id = rte_socket_id();
+
+	if (name == NULL)
+		return -EINVAL;
+
+	RTE_LOG(INFO, PMD, "Initialising DLB2 %s on NUMA node %d\n", name,
+		socket_id);
+
+	/* First verify that we can open the driver */
+	if (dlb2_hw_open(handle, name) != 0) {
+		DLB2_LOG_ERR("could not open driver %s\n", name);
+		return -EINVAL;
+	}
+
+	handle->info.socket_id = socket_id;
+
+	return 0;
+}
+
+static int
+dlb2_ioctl_enable_cq_weight(struct dlb2_hw_dev *handle,
+		     struct dlb2_enable_cq_weight_args *args)
+{
+	int ret = dlb2_domain_open(handle);
+	if (ret < 0) {
+		DLB2_LOG_ERR("dlb2: open domain device file failed\n");
+		return ret;
+	}
+
+	ret = ioctl(dlb2_domain_fd[handle->device_id][handle->domain_id],
+		    DLB2_IOC_ENABLE_CQ_WEIGHT, (unsigned long)args);
+
+	return (ret != 0) ? -errno : 0;
+}
+
+static int
+dlb2_ioctl_set_cq_inflight_ctrl(struct dlb2_hw_dev *handle,
+		     struct dlb2_cq_inflight_ctrl_args *args)
+{
+	int ret = dlb2_domain_open(handle);
+	if (ret < 0) {
+		DLB2_LOG_ERR("dlb2: open domain device file failed\n");
+		return ret;
+	}
+
+	ret = ioctl(dlb2_domain_fd[handle->device_id][handle->domain_id],
+		    DLB2_IOC_SET_CQ_INFLIGHT_CTRL, (unsigned long)args);
+
+	return (ret != 0) ? -errno : 0;
+}
+
+static void
+dlb2_iface_fn_ptrs_init(void)
+{
+	dlb2_iface_low_level_io_init = dlb2_low_level_io_init;
+	dlb2_iface_open = dlb2_hwdev_open;
+	dlb2_iface_domain_reset = dlb2_device_reset;
+	dlb2_iface_get_device_version = dlb2_ioctl_get_device_version;
+	dlb2_iface_get_num_resources = dlb2_ioctl_get_num_resources;
+	dlb2_iface_sched_domain_create = dlb2_ioctl_sched_domain_create;
+	dlb2_iface_ldb_queue_create = dlb2_ioctl_ldb_queue_create;
+	dlb2_iface_dir_queue_create = dlb2_ioctl_dir_queue_create;
+	dlb2_iface_ldb_port_create = dlb2_ioctl_ldb_port_create;
+	dlb2_iface_dir_port_create = dlb2_ioctl_dir_port_create;
+	dlb2_iface_map_qid = dlb2_ioctl_map_qid;
+	dlb2_iface_unmap_qid = dlb2_ioctl_unmap_qid;
+	dlb2_iface_sched_domain_start = dlb2_ioctl_sched_domain_start;
+	dlb2_iface_sched_domain_stop = dlb2_ioctl_sched_domain_stop;
+	dlb2_iface_block_on_cq_interrupt = dlb2_ioctl_block_on_cq_interrupt;
+	dlb2_iface_pending_port_unmaps = dlb2_ioctl_pending_port_unmaps;
+	dlb2_iface_get_ldb_queue_depth = dlb2_ioctl_get_ldb_queue_depth;
+	dlb2_iface_get_dir_queue_depth = dlb2_ioctl_get_dir_queue_depth;
+	dlb2_iface_get_sn_allocation = dlb2_ioctl_get_sn_allocation;
+	dlb2_iface_set_sn_allocation = dlb2_ioctl_set_sn_allocation;
+	dlb2_iface_get_sn_occupancy = dlb2_ioctl_get_sn_occupancy;
+	dlb2_iface_get_cq_poll_mode = dlb2_ioctl_get_cq_poll_mode;
+	dlb2_iface_hardware_init = dlb2_ioctl_hardware_init;
+	dlb2_iface_port_mmap = dlb2_mmap_all;
+	dlb2_iface_get_sched_idle_counts = dlb2_perf_get_sched_idle_counts;
+	dlb2_iface_enable_cq_weight = dlb2_ioctl_enable_cq_weight;
+	dlb2_iface_set_cq_inflight_ctrl = dlb2_ioctl_set_cq_inflight_ctrl;
+	dlb2_iface_port_ctrl = dlb2_ioctl_port_ctrl;
+}
+
+static int
+check_pci_addr(const char *buf, int bufsize)
+{
+	union splitaddr {
+		struct {
+			char *domain;
+			char *bus;
+			char *devid;
+			char *function;
+		};
+		char *str[PCI_FMT_NVAL];
+	} splitaddr;
+
+	char *buf_copy = strndup(buf, bufsize);
+	if (buf_copy == NULL)
+		return -1;
+
+	if (rte_strsplit(buf_copy, bufsize, splitaddr.str, PCI_FMT_NVAL, ':')
+			!= PCI_FMT_NVAL - 1)
+		goto error;
+
+	splitaddr.function = strchr(splitaddr.devid, '.');
+	if (splitaddr.function == NULL)
+		goto error;
+
+	free(buf_copy);
+	return 0;
+error:
+	free(buf_copy);
+	return -1;
+}
+
+static int
+dlb2_get_dev_version(void)
+{
+	DIR *dir;
+	char dirname[PATH_MAX];
+	char filename[PATH_MAX];
+	unsigned long devId, vendorId;
+	struct dirent *e;
+
+	dir = opendir(rte_pci_get_sysfs_path());
+	if (dir == NULL) {
+		RTE_LOG(ERR, EAL, "%s(): opendir failed: %s\n",
+			__func__, strerror(errno));
+		return -1;
+	}
+
+	while ((e = readdir(dir)) != NULL) {
+		if (check_pci_addr(e->d_name, sizeof(e->d_name)))
+			continue;
+
+		snprintf(dirname, sizeof(dirname), "%s/%s",
+			 rte_pci_get_sysfs_path(), e->d_name);
+
+		snprintf(filename, sizeof(filename), "%s/vendor", dirname);
+		if (eal_parse_sysfs_value(filename, &vendorId) < 0)
+			return -1;
+
+		snprintf(filename, sizeof(filename), "%s/device", dirname);
+		if (eal_parse_sysfs_value(filename, &devId) < 0)
+			return -1;
+
+		if (((devId == PCI_DEVICE_ID_INTEL_DLB2_5_PF)  ||
+		     (devId == PCI_DEVICE_ID_INTEL_DLB2_5_VF)) &&
+		    (vendorId == PCI_VENDOR_ID_INTEL))
+			return DLB2_HW_V2_5;
+		else if (((devId == PCI_DEVICE_ID_INTEL_DLB2_PF)  ||
+			  (devId == PCI_DEVICE_ID_INTEL_DLB2_VF)) &&
+		    (vendorId == PCI_VENDOR_ID_INTEL))
+			return DLB2_HW_V2;
+	}
+
+	closedir(dir);
+
+	return -1;
+}
+
+static int
+event_dlb2_vdev_probe(struct rte_vdev_device *vdev)
+{
+	struct rte_eventdev *dev;
+	const char *name;
+	int ret;
+	int q;
+	struct dlb2_devargs dlb2_args = {
+		.socket_id = rte_socket_id(),
+		.max_num_events = DLB2_MAX_NUM_LDB_CREDITS,
+		.num_dir_credits_override = -1,
+		.hwdev_id = 0,
+		.qid_depth_thresholds = { {0} },
+		.producer_coremask = {'\0'},
+		.sw_credit_quanta = {DLB2_SW_CREDIT_QUANTA_DEFAULT,
+			DLB2_SW_CREDIT_P_QUANTA_DEFAULT,
+			DLB2_SW_CREDIT_C_QUANTA_DEFAULT},
+		.hw_credit_quanta = {DLB2_SW_CREDIT_BATCH_SZ,
+			DLB2_SW_CREDIT_P_BATCH_SZ,
+			DLB2_SW_CREDIT_C_BATCH_SZ},
+		.max_cq_depth = DLB2_DEFAULT_CQ_DEPTH,
+		.max_enq_depth = DLB2_MAX_ENQUEUE_DEPTH,
+		.use_default_hl = true,
+		.alloc_hl_entries = 0
+	};
+
+	for (q = 0; q < DLB2_MAX_NUM_PORTS_ALL; q++)
+		dlb2_args.port_cos.cos_id[q] = DLB2_COS_DEFAULT;
+
+	/* runtime init of globals */
+	dlb2_low_level_io_init();
+
+	name = rte_vdev_device_name(vdev);
+
+	if (name == NULL) {
+		DLB2_LOG_ERR("rte_vdev_device_name failed for secondary");
+		return -EFAULT;
+	}
+
+	if (rte_eal_process_type() == RTE_PROC_PRIMARY) {
+		const char *params;
+		int version = dlb2_get_dev_version();
+
+		if (version == -1) {
+			DLB2_LOG_ERR("failed to get vdev device version\n");
+			return -EINVAL;
+		}
+
+		if (version == DLB2_HW_V2_5)
+			dlb2_args.max_num_events =
+				DLB2_MAX_NUM_CREDITS(version);
+
+		params = rte_vdev_device_args(vdev);
+
+		DLB2_LOG_DBG("%s : %s\n", name, params);
+
+		ret = dlb2_parse_params(params, name, &dlb2_args, version);
+		if (ret) {
+			DLB2_LOG_ERR("failed to parse vdev args");
+			return -EINVAL;
+		}
+	}
+
+	dev = rte_event_pmd_vdev_init(name,
+				      sizeof(struct dlb2_eventdev),
+				      dlb2_args.socket_id);
+	if (dev == NULL) {
+		DLB2_LOG_ERR("eventdev vdev init() failed");
+		return -EFAULT;
+	}
+
+	dlb2_iface_fn_ptrs_init();
+
+	rte_spinlock_init(&dlb2_domain_fd_lock);
+
+	if (rte_eal_process_type() == RTE_PROC_PRIMARY)
+		ret = dlb2_primary_eventdev_probe(dev,
+						  name,
+						  &dlb2_args,
+						  DLB2_IS_VDEV);
+	else
+		ret = dlb2_secondary_eventdev_probe(dev,
+						    name,
+						    DLB2_IS_VDEV);
+	if (ret)
+		return ret;
+
+	event_dev_probing_finish(dev);
+
+	return 0;
+}
+
+static int
+event_dlb2_vdev_remove(struct rte_vdev_device *vdev)
+{
+	const char *name;
+	int ret;
+
+	name = rte_vdev_device_name(vdev);
+	if (name == NULL)
+		return -EINVAL;
+
+	RTE_LOG(INFO, PMD, "Closing eventdev dlb2 device %s\n", name);
+
+	ret = dlb2_uninit(name);
+
+	return ret;
+}
+
+static struct rte_vdev_driver vdev_eventdev_dlb2_pmd = {
+	.probe = event_dlb2_vdev_probe,
+	.remove = event_dlb2_vdev_remove,
+};
+
+RTE_PMD_REGISTER_VDEV(EVDEV_DLB2_NAME_PMD, vdev_eventdev_dlb2_pmd);
+RTE_PMD_REGISTER_PARAM_STRING(EVDEV_DLB2_NAME_PMD,
+	NUMA_NODE_ARG "=<int> "
+	DLB2_MAX_NUM_EVENTS "=<int> "
+	DLB2_NUM_DIR_CREDITS "=<int> "
+	DLB2_NUM_ORDERED_QUEUES_0 "=<int> "
+	DLB2_NUM_ORDERED_QUEUES_1 "=<int> "
+	HWDEV_ID_ARG "=<int> "
+);
diff --git a/drivers/event/dlb2/bifurcated/dlb2_vdev.h b/drivers/event/dlb2/bifurcated/dlb2_vdev.h
new file mode 100644
index 0000000..7b1d3b8
--- /dev/null
+++ b/drivers/event/dlb2/bifurcated/dlb2_vdev.h
@@ -0,0 +1,19 @@
+/* SPDX-License-Identifier: BSD-3-Clause
+ * Copyright(c) 2016-2020 Intel Corporation
+ */
+
+#ifndef _DLB2_VDEV_H_
+#define _DLB2_VDEV_H_
+
+/* vdev specific functions */
+int dlb2_register_pri_mp_callbacks(void);
+int dlb2_register_sec_mp_callbacks(void);
+
+extern struct
+process_local_port_data dlb2_port[RTE_EVENT_MAX_DEVS]
+				 [DLB2_MAX_NUM_PORTS_ALL]
+				 [DLB2_NUM_PORT_TYPES];
+
+extern int eal_parse_sysfs_value(const char *filename, unsigned long *val);
+
+#endif /* _DLB2_VDEV_H_ */
diff --git a/drivers/event/dlb2/dlb2.c b/drivers/event/dlb2/dlb2.c
index 60c5cd4..1703e42 100644
--- a/drivers/event/dlb2/dlb2.c
+++ b/drivers/event/dlb2/dlb2.c
@@ -1,5 +1,5 @@
 /* SPDX-License-Identifier: BSD-3-Clause
- * Copyright(c) 2016-2022 Intel Corporation
+ * Copyright(c) 2016-2023 Intel Corporation
  */
 
 #include <assert.h>
@@ -17,7 +17,7 @@
 #include <rte_config.h>
 #include <rte_cycles.h>
 #include <rte_debug.h>
-#include <dev_driver.h>
+#include <rte_dev.h>
 #include <rte_errno.h>
 #include <rte_eventdev.h>
 #include <eventdev_pmd.h>
@@ -30,10 +30,12 @@
 #include <rte_prefetch.h>
 #include <rte_ring.h>
 #include <rte_string_fns.h>
+#include <dirent.h>
 
 #include "dlb2_priv.h"
 #include "dlb2_iface.h"
 #include "dlb2_inline_fns.h"
+#include "bifurcated/dlb2_vdev.h"
 
 /*
  * Bypass memory fencing instructions when port is of Producer type.
@@ -43,7 +45,60 @@
  * to DLB can go ahead of relevant application writes like updates to buffers
  * being sent with event
  */
+#ifndef DLB2_BYPASS_FENCE_ON_PP
 #define DLB2_BYPASS_FENCE_ON_PP 0  /* 1 == Bypass fence, 0 == do not bypass */
+#endif
+/*
+ * Optimization switches for improving driver performance.
+ * WARNING: Do not change any of the below switches without first
+ * consulting with DLB2 software development team.
+ *
+ * HW credit checks can only be turned off for DLB2 device if following
+ * is true for each created eventdev
+ * LDB credits <= DIR credits + minimum CQ Depth
+ * (CQ Depth is minimum of all ports configured within eventdev)
+ * This needs to be true for all eventdevs created on any DLB2 device
+ * managed by this driver.
+ * DLB2.5 does not any such restriction as it has single credit pool
+ */
+#ifndef DLB_HW_CREDITS_CHECKS
+#define DLB_HW_CREDITS_CHECKS 1
+#endif
+
+/*
+ * SW credit checks can only be turned off if application has a way to
+ * limit input events to the eventdev below assigned credit limit
+ */
+#ifndef DLB_SW_CREDITS_CHECKS
+#define DLB_SW_CREDITS_CHECKS 1
+#endif
+
+/*
+ * Once application is fully validated, type check can be turned off.
+ * HW will continue checking for correct type and generate alarm on mismatch
+ */
+#ifndef DLB_TYPE_CHECK
+#define DLB_TYPE_CHECK 1
+#endif
+#define DLB_TYPE_MACRO 0x010002
+
+/*
+ * To avoid deadlock situations, by default, per port new_event_threshold
+ * check is disabled. nb_events_limit is still checked while allocating
+ * new event credits.
+ */
+#define ENABLE_PORT_THRES_CHECK 1
+/*
+ * To avoid deadlock, ports holding to credits will release them after these
+ * many consecutive zero dequeues
+ */
+#define DLB2_ZERO_DEQ_CREDIT_RETURN_THRES 16384
+
+/*
+ * To avoid deadlock, ports holding to credits will release them after these
+ * many consecutive enqueue failures
+ */
+#define DLB2_ENQ_FAIL_CREDIT_RETURN_THRES 100
 
 /*
  * Resources exposed to eventdev. Some values overridden at runtime using
@@ -52,41 +107,26 @@
 #if (RTE_EVENT_MAX_QUEUES_PER_DEV > UINT8_MAX)
 #error "RTE_EVENT_MAX_QUEUES_PER_DEV cannot fit in member max_event_queues"
 #endif
-static struct rte_event_dev_info evdev_dlb2_default_info = {
-	.driver_name = "", /* probe will set */
-	.min_dequeue_timeout_ns = DLB2_MIN_DEQUEUE_TIMEOUT_NS,
-	.max_dequeue_timeout_ns = DLB2_MAX_DEQUEUE_TIMEOUT_NS,
-#if (RTE_EVENT_MAX_QUEUES_PER_DEV < DLB2_MAX_NUM_LDB_QUEUES)
-	.max_event_queues = RTE_EVENT_MAX_QUEUES_PER_DEV,
-#else
-	.max_event_queues = DLB2_MAX_NUM_LDB_QUEUES,
-#endif
-	.max_event_queue_flows = DLB2_MAX_NUM_FLOWS,
-	.max_event_queue_priority_levels = DLB2_QID_PRIORITIES,
-	.max_event_priority_levels = DLB2_QID_PRIORITIES,
-	.max_event_ports = DLB2_MAX_NUM_LDB_PORTS,
-	.max_event_port_dequeue_depth = DLB2_DEFAULT_CQ_DEPTH,
-	.max_event_port_enqueue_depth = DLB2_MAX_ENQUEUE_DEPTH,
-	.max_event_port_links = DLB2_MAX_NUM_QIDS_PER_LDB_CQ,
-	.max_num_events = DLB2_MAX_NUM_LDB_CREDITS,
-	.max_single_link_event_port_queue_pairs =
-		DLB2_MAX_NUM_DIR_PORTS(DLB2_HW_V2),
-	.event_dev_cap = (RTE_EVENT_DEV_CAP_EVENT_QOS |
-			  RTE_EVENT_DEV_CAP_DISTRIBUTED_SCHED |
-			  RTE_EVENT_DEV_CAP_QUEUE_ALL_TYPES |
-			  RTE_EVENT_DEV_CAP_BURST_MODE |
-			  RTE_EVENT_DEV_CAP_IMPLICIT_RELEASE_DISABLE |
-			  RTE_EVENT_DEV_CAP_RUNTIME_PORT_LINK |
-			  RTE_EVENT_DEV_CAP_MULTIPLE_QUEUE_PORT |
-			  RTE_EVENT_DEV_CAP_MAINTENANCE_FREE),
-};
 
+/*
+ * Use dlb2_movdir64b_single() for token return in
+ * dlb2_consume_qe_immediate(struct dlb2_port *qm_port, int num)
+ */
+#define USE_MOVDIR64B_SINGLE 1
+
+
+/* These functions will vary based on processor capabilities */
+static struct dlb2_port_low_level_io_functions qm_mmio_fns;
+
+/* Contains process-local mapped hardware I/O addresses */
 struct process_local_port_data
-dlb2_port[DLB2_MAX_NUM_PORTS_ALL][DLB2_NUM_PORT_TYPES];
+dlb2_port[RTE_EVENT_MAX_DEVS][DLB2_MAX_NUM_PORTS_ALL][DLB2_NUM_PORT_TYPES];
 
 static void
 dlb2_free_qe_mem(struct dlb2_port *qm_port)
 {
+	const struct rte_memzone *mz;
+
 	if (qm_port == NULL)
 		return;
 
@@ -99,8 +139,9 @@ dlb2_free_qe_mem(struct dlb2_port *qm_port)
 	rte_free(qm_port->consume_qe);
 	qm_port->consume_qe = NULL;
 
-	rte_memzone_free(dlb2_port[qm_port->id][PORT_TYPE(qm_port)].mz);
-	dlb2_port[qm_port->id][PORT_TYPE(qm_port)].mz = NULL;
+	mz = dlb2_port[qm_port->evdev_id][qm_port->id][PORT_TYPE(qm_port)].mz;
+	rte_memzone_free(mz);
+	dlb2_port[qm_port->evdev_id][qm_port->id][PORT_TYPE(qm_port)].mz = NULL;
 }
 
 /* override defaults with value(s) provided on command line */
@@ -119,59 +160,75 @@ dlb2_init_queue_depth_thresholds(struct dlb2_eventdev *dlb2,
 
 /* override defaults with value(s) provided on command line */
 static void
-dlb2_init_cq_weight(struct dlb2_eventdev *dlb2, int *cq_weight)
+dlb2_init_quanta_credits(struct dlb2_eventdev *dlb2,
+			 int *sw_credit_quanta,
+			 int *hw_credit_quanta)
 {
-	int q;
+	int i;
+
+	for (i = 0; i < DLB2_CREDIT_QUANTA_ARRAY_SZ; i++) {
+		dlb2->sw_credit_quanta[i] = sw_credit_quanta[i];
+		dlb2->hw_credit_quanta[i] = hw_credit_quanta[i];
+	}
 
-	for (q = 0; q < DLB2_MAX_NUM_PORTS_ALL; q++)
-		dlb2->ev_ports[q].cq_weight = cq_weight[q];
 }
 
+/* override defaults with value(s) provided on command line */
 static int
-set_cq_weight(const char *key __rte_unused,
-	      const char *value,
-	      void *opaque)
+dlb2_init_port_dequeue_wait(struct dlb2_eventdev *dlb2,
+			    enum dlb2_port_dequeue_wait_types
+				*port_dequeue_wait_modes)
 {
-	struct dlb2_cq_weight *cq_weight = opaque;
-	int first, last, weight, i;
-
-	if (value == NULL || opaque == NULL) {
-		DLB2_LOG_ERR("NULL pointer\n");
-		return -EINVAL;
-	}
-
-	/* command line override may take one of the following 3 forms:
-	 * qid_depth_thresh=all:<threshold_value> ... all queues
-	 * qid_depth_thresh=qidA-qidB:<threshold_value> ... a range of queues
-	 * qid_depth_thresh=qid:<threshold_value> ... just one queue
-	 */
-	if (sscanf(value, "all:%d", &weight) == 1) {
-		first = 0;
-		last = DLB2_MAX_NUM_PORTS_ALL - 1;
-	} else if (sscanf(value, "%d-%d:%d", &first, &last, &weight) == 3) {
-		/* we have everything we need */
-	} else if (sscanf(value, "%d:%d", &first, &weight) == 2) {
-		last = first;
-	} else {
-		DLB2_LOG_ERR("Error parsing ldb port qe weight devarg. Should be all:val, qid-qid:val, or qid:val\n");
-		return -EINVAL;
-	}
+	int p;
 
-	if (first > last || first < 0 ||
-		last >= DLB2_MAX_NUM_PORTS_ALL) {
-		DLB2_LOG_ERR("Error parsing ldb port qe weight arg, invalid port value\n");
-		return -EINVAL;
-	}
-
-	if (weight < 0 || weight > DLB2_MAX_CQ_DEPTH_OVERRIDE) {
-		DLB2_LOG_ERR("Error parsing ldb port qe weight devarg, must be < cq depth\n");
-		return -EINVAL;
+	for (p = 0; p < DLB2_MAX_NUM_PORTS(dlb2->version); p++) {
+		if (port_dequeue_wait_modes[p] == DLB2_PORT_DEQUEUE_WAIT_INTERRUPT &&
+		    dlb2->qm_instance.pf_dev) {
+			DLB2_LOG_ERR(": Interrupt mode not supported in PF PMD\n");
+			return -EINVAL;
+		}
+		if (port_dequeue_wait_modes[p] != 0)
+			dlb2->ev_ports[p].qm_port.dequeue_wait =
+				port_dequeue_wait_modes[p];
 	}
+	return 0;
+}
 
-	for (i = first; i <= last; i++)
-		cq_weight->limit[i] = weight; /* indexed by qid */
+static void dlb2_init_evdev_rsrcs(struct dlb2_eventdev *dlb2, const char *name)
+{
+	static struct rte_event_dev_info evdev_dlb2_default_info = {
+		.driver_name = "", /* probe will set */
+		.min_dequeue_timeout_ns = DLB2_MIN_DEQUEUE_TIMEOUT_NS,
+		.max_dequeue_timeout_ns = DLB2_MAX_DEQUEUE_TIMEOUT_NS,
+#if (RTE_EVENT_MAX_QUEUES_PER_DEV < DLB2_MAX_NUM_LDB_QUEUES)
+		.max_event_queues = RTE_EVENT_MAX_QUEUES_PER_DEV,
+#else
+		.max_event_queues = DLB2_MAX_NUM_LDB_QUEUES,
+#endif
+		.max_event_queue_flows = DLB2_MAX_NUM_FLOWS,
+		.max_event_queue_priority_levels = DLB2_QID_PRIORITIES,
+		.max_event_priority_levels = DLB2_QID_PRIORITIES,
+		.max_event_ports = DLB2_MAX_NUM_LDB_PORTS,
+		.max_event_port_dequeue_depth = DLB2_DEFAULT_CQ_DEPTH,
+		.max_event_port_enqueue_depth = DLB2_MAX_ENQUEUE_DEPTH,
+		.max_event_port_links = DLB2_MAX_NUM_QIDS_PER_LDB_CQ,
+		.max_num_events = DLB2_MAX_NUM_LDB_CREDITS,
+		.max_single_link_event_port_queue_pairs =
+			DLB2_MAX_NUM_DIR_PORTS(DLB2_HW_V2),
+		.event_dev_cap = (RTE_EVENT_DEV_CAP_EVENT_QOS |
+				  RTE_EVENT_DEV_CAP_BURST_MODE |
+				  RTE_EVENT_DEV_CAP_DISTRIBUTED_SCHED |
+				  RTE_EVENT_DEV_CAP_IMPLICIT_RELEASE_DISABLE |
+				  RTE_EVENT_DEV_CAP_MULTIPLE_QUEUE_PORT |
+				  RTE_EVENT_DEV_CAP_NONSEQ_MODE |
+				  RTE_EVENT_DEV_CAP_MAINTENANCE_FREE |
+					RTE_EVENT_DEV_CAP_QUEUE_ALL_TYPES |
+				  RTE_EVENT_DEV_CAP_RUNTIME_PORT_LINK),
+	};
+	struct dlb2_hw_dev *handle = &dlb2->qm_instance;
 
-	return 0;
+	handle->evdev_rsrcs = evdev_dlb2_default_info;
+	handle->evdev_rsrcs.driver_name = name;
 }
 
 /* override defaults with value(s) provided on command line */
@@ -233,23 +290,25 @@ dlb2_hw_query_resources(struct dlb2_eventdev *dlb2)
 	 * The capabilities (CAPs) were set at compile time.
 	 */
 
-	if (dlb2->max_cq_depth != DLB2_DEFAULT_CQ_DEPTH)
-		num_ldb_ports = DLB2_MAX_HL_ENTRIES / dlb2->max_cq_depth;
-	else
-		num_ldb_ports = dlb2->hw_rsrc_query_results.num_ldb_ports;
+	num_ldb_ports = dlb2->hw_rsrc_query_results.num_ldb_ports;
 
-	evdev_dlb2_default_info.max_event_queues =
+	handle->evdev_rsrcs.max_event_queues =
 		dlb2->hw_rsrc_query_results.num_ldb_queues;
 
-	evdev_dlb2_default_info.max_event_ports = num_ldb_ports;
+	handle->evdev_rsrcs.max_event_ports = num_ldb_ports;
+
+	handle->evdev_rsrcs.max_single_link_event_port_queue_pairs =
+		dlb2->hw_rsrc_query_results.num_dir_ports + /* num available */
+		dlb2->num_dir_ports;      /* num in-use */
 
 	if (dlb2->version == DLB2_HW_V2_5) {
-		evdev_dlb2_default_info.max_num_events =
+		handle->evdev_rsrcs.max_num_events =
 			dlb2->hw_rsrc_query_results.num_credits;
 	} else {
-		evdev_dlb2_default_info.max_num_events =
+		handle->evdev_rsrcs.max_num_events =
 			dlb2->hw_rsrc_query_results.num_ldb_credits;
 	}
+
 	/* Save off values used when creating the scheduling domain. */
 
 	handle->info.num_sched_domains =
@@ -277,6 +336,21 @@ dlb2_hw_query_resources(struct dlb2_eventdev *dlb2)
 	handle->info.hw_rsrc_max.reorder_window_size =
 		dlb2->hw_rsrc_query_results.num_hist_list_entries;
 
+	if (handle->num_ordered_queues_0 == 0 &&
+		handle->num_ordered_queues_1 == 0) {
+		/* User did not override, so use all available */
+		handle->req_ordered_queues_0 =
+			dlb2->hw_rsrc_query_results.num_sn_slots[0];
+		handle->req_ordered_queues_1 =
+			dlb2->hw_rsrc_query_results.num_sn_slots[1];
+	} else {
+		/* Use command line overrides */
+		handle->req_ordered_queues_0 =
+			handle->num_ordered_queues_0;
+		handle->req_ordered_queues_1 =
+			handle->num_ordered_queues_1;
+	}
+
 	rte_memcpy(dlb2_info, &handle->info.hw_rsrc_max, sizeof(*dlb2_info));
 
 	return 0;
@@ -284,7 +358,7 @@ dlb2_hw_query_resources(struct dlb2_eventdev *dlb2)
 
 #define DLB2_BASE_10 10
 
-static int
+int
 dlb2_string_to_int(int *result, const char *str)
 {
 	long ret;
@@ -306,19 +380,13 @@ dlb2_string_to_int(int *result, const char *str)
 	return 0;
 }
 
-static int
-set_producer_coremask(const char *key __rte_unused,
-		      const char *value,
-		      void *opaque)
+int
+dlb2_uninit(const char *name)
 {
-	const char **mask_str = opaque;
-
-	if (value == NULL || opaque == NULL) {
-		DLB2_LOG_ERR("NULL pointer\n");
+	if (name == NULL)
 		return -EINVAL;
-	}
 
-	*mask_str = value;
+	RTE_LOG(INFO, PMD, "name=%s\n", name);
 
 	return 0;
 }
@@ -359,7 +427,7 @@ set_max_cq_depth(const char *key __rte_unused,
 	if (*max_cq_depth < DLB2_MIN_CQ_DEPTH_OVERRIDE ||
 	    *max_cq_depth > DLB2_MAX_CQ_DEPTH_OVERRIDE ||
 	    !rte_is_power_of_2(*max_cq_depth)) {
-		DLB2_LOG_ERR("dlb2: max_cq_depth %d and %d and a power of 2\n",
+		DLB2_LOG_ERR("dlb2: Allowed max_cq_depth range %d - %d and should be power of 2\n",
 			     DLB2_MIN_CQ_DEPTH_OVERRIDE,
 			     DLB2_MAX_CQ_DEPTH_OVERRIDE);
 		return -EINVAL;
@@ -370,8 +438,8 @@ set_max_cq_depth(const char *key __rte_unused,
 
 static int
 set_max_enq_depth(const char *key __rte_unused,
-		  const char *value,
-		  void *opaque)
+		 const char *value,
+		 void *opaque)
 {
 	int *max_enq_depth = opaque;
 	int ret;
@@ -389,15 +457,13 @@ set_max_enq_depth(const char *key __rte_unused,
 	    *max_enq_depth > DLB2_MAX_ENQ_DEPTH_OVERRIDE ||
 	    !rte_is_power_of_2(*max_enq_depth)) {
 		DLB2_LOG_ERR("dlb2: max_enq_depth %d and %d and a power of 2\n",
-		DLB2_MIN_ENQ_DEPTH_OVERRIDE,
-		DLB2_MAX_ENQ_DEPTH_OVERRIDE);
+			     DLB2_MIN_ENQ_DEPTH_OVERRIDE,
+			     DLB2_MAX_ENQ_DEPTH_OVERRIDE);
 		return -EINVAL;
 	}
 
 	return 0;
 }
-
-
 static int
 set_max_num_events(const char *key __rte_unused,
 		   const char *value,
@@ -426,6 +492,33 @@ set_max_num_events(const char *key __rte_unused,
 }
 
 static int
+set_max_num_events_v2_5(const char *key __rte_unused,
+			const char *value,
+			void *opaque)
+{
+	int *max_num_events = opaque;
+	int ret;
+
+	if (value == NULL || opaque == NULL) {
+		DLB2_LOG_ERR("NULL pointer\n");
+		return -EINVAL;
+	}
+
+	ret = dlb2_string_to_int(max_num_events, value);
+	if (ret < 0)
+		return ret;
+
+	if (*max_num_events < 0 || *max_num_events >
+			DLB2_MAX_NUM_CREDITS(DLB2_HW_V2_5)) {
+		DLB2_LOG_ERR("dlb2: max_num_events must be between 0 and %d\n",
+			     DLB2_MAX_NUM_CREDITS(DLB2_HW_V2_5));
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int
 set_num_dir_credits(const char *key __rte_unused,
 		    const char *value,
 		    void *opaque)
@@ -453,22 +546,40 @@ set_num_dir_credits(const char *key __rte_unused,
 }
 
 static int
-set_dev_id(const char *key __rte_unused,
-	   const char *value,
-	   void *opaque)
+set_hwdev_id(const char *key __rte_unused,
+	     const char *value,
+	     void *opaque)
 {
-	int *dev_id = opaque;
+	int *hwdev_id = opaque;
 	int ret;
+	DIR *check_dir;
+	char hw_dev_path[DLB2_MAX_DEVICE_PATH] = {0};
+	char *hw_devp = &hw_dev_path[0];
 
 	if (value == NULL || opaque == NULL) {
 		DLB2_LOG_ERR("NULL pointer\n");
 		return -EINVAL;
 	}
 
-	ret = dlb2_string_to_int(dev_id, value);
+	ret = dlb2_string_to_int(hwdev_id, value);
 	if (ret < 0)
 		return ret;
 
+	/* HQM-514: check for device availability */
+	snprintf(hw_devp,
+		 DLB2_MAX_DEVICE_PATH,
+		 "/sys/class/dlb2/dlb%d",
+		 *hwdev_id);
+
+	check_dir = opendir(hw_devp);
+
+	if (check_dir == NULL) {
+		DLB2_LOG_ERR("Invalid hwdev_id\n");
+		return -EINVAL;
+	}
+
+	closedir(check_dir);
+
 	return 0;
 }
 
@@ -562,37 +673,115 @@ set_cos_bw(const char *key __rte_unused,
 	return 0;
 }
 
+int
+dlb2_credit_quanta_parse_params(const char *value,
+				int *quanta_credit,
+				int type)
+{
+	int ret = 0;
+	int worker, producer, consumer, total_quanta;
+	int sw_credit = 0;
+	int hw_credit = 1;
+	/* command line override may take one of the following 2 forms:
+	 * sw_credit_quanta=val or hw_credit_quanta=val, value for worker only,
+	 * rest will be default-P:256, C:256.
+	 * sw_credit_quanta=valw:valp:valc ... value for worker,
+	 * producer and consumer respectively.
+	 */
+
+	if (sscanf(value, "%d:%d:%d", &worker, &producer, &consumer) == 3) {
+
+		if (worker <= 0 || producer <= 0 || consumer <= 0
+				|| worker > DLB2_CREDIT_QUANTA_LIMIT
+				|| producer > DLB2_CREDIT_QUANTA_LIMIT
+				|| consumer > DLB2_CREDIT_QUANTA_LIMIT) {
+			DLB2_LOG_ERR("Error parsing credit quanta, values should be > 0 or < 1024\n");
+			return -EINVAL;
+		}
+
+		total_quanta = worker + producer + consumer;
+
+		if (total_quanta > DLB2_CREDIT_QUANTA_LIMIT) {
+			DLB2_LOG_ERR("Error parsing credit quanta, total quantas exceeded 1024\n");
+			return -EINVAL;
+		}
+
+		quanta_credit[DLB2_CQ_WORKER_INDEX] = worker;
+		quanta_credit[DLB2_CQ_PRODUCER_INDEX] = producer;
+		quanta_credit[DLB2_CQ_CONSUMER_INDEX] = consumer;
+	} else if (sscanf(value, "%d:%d:%d", &worker, &producer, &consumer) == 1) {
+
+		if (worker <= 0 || worker > DLB2_CREDIT_QUANTA_LIMIT) {
+			DLB2_LOG_ERR("Error parsing credit quanta, value should be > 0 or < 1024\n");
+			return -EINVAL;
+		}
+
+		quanta_credit[DLB2_CQ_WORKER_INDEX] = worker;
+
+		if (type == sw_credit) {
+			quanta_credit[DLB2_CQ_PRODUCER_INDEX] = DLB2_SW_CREDIT_P_QUANTA_DEFAULT;
+			quanta_credit[DLB2_CQ_CONSUMER_INDEX] = DLB2_SW_CREDIT_C_QUANTA_DEFAULT;
+		} else if (type == hw_credit) {
+			quanta_credit[DLB2_CQ_PRODUCER_INDEX] = DLB2_SW_CREDIT_P_BATCH_SZ;
+			quanta_credit[DLB2_CQ_CONSUMER_INDEX] = DLB2_SW_CREDIT_C_BATCH_SZ;
+		}
+
+	} else {
+
+		DLB2_LOG_ERR("Error parsing credit quanta, enter hw/sw_credit_quanta:val or hw/sw_credit_quanta=valw:valp:valc\n");
+		return -EINVAL;
+
+	}
+
+	return ret;
+
+}
+
 static int
 set_sw_credit_quanta(const char *key __rte_unused,
 	const char *value,
 	void *opaque)
 {
-	int *sw_credit_quanta = opaque;
-	int ret;
+	int *quanta_credit = opaque;
+	int ret = 0;
+	int type = 0;
 
 	if (value == NULL || opaque == NULL) {
 		DLB2_LOG_ERR("NULL pointer\n");
 		return -EINVAL;
 	}
 
-	ret = dlb2_string_to_int(sw_credit_quanta, value);
-	if (ret < 0)
-		return ret;
+	ret = dlb2_credit_quanta_parse_params(value, quanta_credit, type);
 
-	if (*sw_credit_quanta <= 0) {
-		DLB2_LOG_ERR("sw_credit_quanta must be > 0\n");
+	return ret;
+}
+
+static int
+set_hw_credit_quanta(const char *key __rte_unused,
+	const char *value,
+	void *opaque)
+{
+	int *quanta_credit = opaque;
+	int ret = 0;
+	int type = 1;
+
+	if (value == NULL || opaque == NULL) {
+		DLB2_LOG_ERR("NULL pointer\n");
 		return -EINVAL;
 	}
 
-	return 0;
+	ret = dlb2_credit_quanta_parse_params(value, quanta_credit, type);
+
+	return ret;
 }
 
 static int
-set_hw_credit_quanta(const char *key __rte_unused,
+set_num_ordered_queues(const char *key __rte_unused,
 	const char *value,
 	void *opaque)
 {
-	int *hw_credit_quanta = opaque;
+	int *num_ordered_queues = opaque;
+	int x = 0;
 	int ret;
 
 	if (value == NULL || opaque == NULL) {
@@ -600,10 +789,38 @@ set_hw_credit_quanta(const char *key __rte_unused,
 		return -EINVAL;
 	}
 
-	ret = dlb2_string_to_int(hw_credit_quanta, value);
+	ret = dlb2_string_to_int(&x, value);
 	if (ret < 0)
 		return ret;
 
+	if (x < 0 || x > 16) {
+		DLB2_LOG_ERR(
+			"num_ordered_queues_x %d out of range, must be <= 16\n",
+			x);
+		return -EINVAL;
+	}
+
+	*num_ordered_queues = x;
+
+	return 0;
+}
+
+static int
+set_producer_coremask(const char *key __rte_unused,
+	const char *value,
+	void *opaque)
+{
+	char *mask_str = opaque;
+
+	if (value == NULL || opaque == NULL) {
+		DLB2_LOG_ERR("NULL pointer\n");
+		return -EINVAL;
+	}
+	if (rte_strscpy(mask_str, value, DLB2_COREMASK_LEN) < 0) {
+		DLB2_LOG_ERR("Core mask too long.\n");
+		return -EINVAL;
+	}
+
 	return 0;
 }
 
@@ -628,11 +845,11 @@ set_default_depth_thresh(const char *key __rte_unused,
 }
 
 static int
-set_vector_opts_enab(const char *key __rte_unused,
-	const char *value,
-	void *opaque)
+set_vector_opts_disab(const char *key __rte_unused,
+		      const char *value,
+		      void *opaque)
 {
-	bool *dlb2_vector_opts_enabled = opaque;
+	bool *dlb2_vector_opts_disabled = opaque;
 
 	if (value == NULL || opaque == NULL) {
 		DLB2_LOG_ERR("NULL pointer\n");
@@ -640,9 +857,9 @@ set_vector_opts_enab(const char *key __rte_unused,
 	}
 
 	if ((*value == 'y') || (*value == 'Y'))
-		*dlb2_vector_opts_enabled = true;
+		*dlb2_vector_opts_disabled = true;
 	else
-		*dlb2_vector_opts_enabled = false;
+		*dlb2_vector_opts_disabled = false;
 
 	return 0;
 }
@@ -668,6 +885,68 @@ set_default_ldb_port_allocation(const char *key __rte_unused,
 }
 
 static int
+set_enable_cq_weight(const char *key __rte_unused,
+		      const char *value,
+		      void *opaque)
+{
+	bool *enable_cq_weight = opaque;
+
+	if (value == NULL || opaque == NULL) {
+		DLB2_LOG_ERR("NULL pointer\n");
+		return -EINVAL;
+	}
+
+	if ((*value == 'y') || (*value == 'Y'))
+		*enable_cq_weight = true;
+	else
+		*enable_cq_weight = false;
+
+	return 0;
+}
+
+static int set_hl_override(const char *key __rte_unused, const char *value,
+			   void *opaque) {
+	bool *default_hl = opaque;
+
+	if (value == NULL || opaque == NULL) {
+		DLB2_LOG_ERR("NULL pointer\n");
+		return -EINVAL;
+	}
+
+	if ((*value == 'n') || (*value == 'N') || (*value == '0'))
+		*default_hl = false;
+	else
+		*default_hl = true;
+
+	return 0;
+}
+
+static int set_hl_entries(const char *key __rte_unused, const char *value,
+			  void *opaque) {
+	int hl_entries;
+	int ret;
+
+	if (value == NULL || opaque == NULL) {
+		DLB2_LOG_ERR("NULL pointer\n");
+		return -EINVAL;
+	}
+
+	ret = dlb2_string_to_int(&hl_entries, value);
+	if (ret < 0)
+		return ret;
+
+	if ((uint32_t)hl_entries > DLB2_MAX_HL_ENTRIES) {
+		DLB2_LOG_ERR(
+		    "alloc_hl_entries %u out of range, must be in [1 - %d]\n",
+		    hl_entries, DLB2_MAX_HL_ENTRIES);
+		return -EINVAL;
+	}
+	*(uint32_t *)opaque = hl_entries;
+
+	return 0;
+}
+
+static int
 set_qid_depth_thresh(const char *key __rte_unused,
 		     const char *value,
 		     void *opaque)
@@ -763,11 +1042,117 @@ set_qid_depth_thresh_v2_5(const char *key __rte_unused,
 	return 0;
 }
 
+static int
+set_port_dequeue_wait_ver(const char *key __rte_unused,
+			  const char *value,
+			  void *opaque,
+			  int version)
+{
+	struct dlb2_port_dequeue_wait *dequeue_wait = opaque;
+	int first, last;
+	enum dlb2_port_dequeue_wait_types wait;
+	const char *valp = value;
+	bool port_list[DLB2_MAX_NUM_PORTS_ALL] = {false};
+	int lmax = DLB2_MAX_NUM_PORTS(version);
+	int len;
+	int lc;
+
+	if (value == NULL || opaque == NULL) {
+		DLB2_LOG_ERR("NULL pointer\n");
+		return -EINVAL;
+	}
+
+	/* command line override may take a combination of the following forms:
+	 * port_dequeue_wait=all:<wait_mode> ... all ports
+	 * port_dequeue_wait=portA-portB:<wait_mode> ... a range of ports
+	 * port_dequeue_wait=portA:<wait_mode> ... just one port
+	 */
+
+	do {
+		do {
+			if (strncmp(valp, "all", 3) == 0) {
+				for (lc = 0; lc < lmax; lc++)
+					port_list[lc] = true;
+				valp += 3;
+			} else if (sscanf(valp, "%d-%d%n",
+					  &first,
+					  &last,
+					  &len) == 2) {
+				if ((first < 0) ||
+				    (last >= lmax) ||
+				    (first > last)) {
+					DLB2_LOG_ERR("Invalid portId\n");
+					return -EINVAL;
+				}
+				for (lc = first; lc <= last; lc++)
+					port_list[lc] = true;
+				valp += len;
+			} else if (sscanf(valp, "%d%n",
+					  &first,
+					  &len) == 1) {
+				if ((first < 0) ||
+				    (first >= lmax)) {
+					DLB2_LOG_ERR("Invalid portId\n");
+					return -EINVAL;
+				}
+				port_list[first] = true;
+				valp += len;
+			}
+		} while (strncmp(valp, "+", 1) == 0);
+
+		if (strncmp(valp++, ":", 1) == 0) {
+			if (strncmp(valp, "interrupt", 9) == 0) {
+				wait = DLB2_PORT_DEQUEUE_WAIT_INTERRUPT;
+				len = 9;
+			} else if (strncmp(valp, "polling", 7) == 0) {
+				wait = DLB2_PORT_DEQUEUE_WAIT_POLLING;
+				len = 7;
+			} else if (strncmp(valp, "umwait", 6) == 0) {
+				wait = DLB2_PORT_DEQUEUE_WAIT_UMWAIT;
+				len = 6;
+			} else {
+				DLB2_LOG_ERR("Error parsing port wait mode devarg, invalid mode\n");
+				return -EINVAL;
+			}
+
+			valp += len;
+
+			for (lc = 0; lc < lmax; lc++)
+				if (port_list[lc]) {
+					dequeue_wait->val[lc] = wait;
+					port_list[lc] = false;
+				}
+		} else {
+			DLB2_LOG_ERR("Error parsing port wait mode devarg. Should be all:val, portId-portId:val, or portId:val\n");
+			return -EINVAL;
+		}
+	} while (strncmp(valp++, "_", 1) == 0);
+
+	return 0;
+}
+
+static int
+set_port_dequeue_wait(const char *key __rte_unused,
+		      const char *value,
+		      void *opaque)
+{
+	return set_port_dequeue_wait_ver(key, value, opaque, DLB2_HW_V2);
+}
+
+static int
+set_port_dequeue_wait_v2_5(const char *key __rte_unused,
+		      const char *value,
+		      void *opaque)
+{
+	return set_port_dequeue_wait_ver(key, value, opaque, DLB2_HW_V2_5);
+}
+
 static void
 dlb2_eventdev_info_get(struct rte_eventdev *dev,
 		       struct rte_event_dev_info *dev_info)
 {
 	struct dlb2_eventdev *dlb2 = dlb2_pmd_priv(dev);
+	struct dlb2_hw_dev *handle = &dlb2->qm_instance;
 	int ret;
 
 	ret = dlb2_hw_query_resources(dlb2);
@@ -786,24 +1171,24 @@ dlb2_eventdev_info_get(struct rte_eventdev *dev,
 	 * to the application recalling eventdev_configure to *reconfigure* the
 	 * domain.
 	 */
-	evdev_dlb2_default_info.max_event_ports += dlb2->num_ldb_ports;
-	evdev_dlb2_default_info.max_event_queues += dlb2->num_ldb_queues;
+	handle->evdev_rsrcs.max_event_ports += dlb2->num_ldb_ports;
+	handle->evdev_rsrcs.max_event_queues += dlb2->num_ldb_queues;
 	if (dlb2->version == DLB2_HW_V2_5) {
-		evdev_dlb2_default_info.max_num_events +=
+		handle->evdev_rsrcs.max_num_events +=
 			dlb2->max_credits;
 	} else {
-		evdev_dlb2_default_info.max_num_events +=
+		handle->evdev_rsrcs.max_num_events +=
 			dlb2->max_ldb_credits;
 	}
-	evdev_dlb2_default_info.max_event_queues =
-		RTE_MIN(evdev_dlb2_default_info.max_event_queues,
+	handle->evdev_rsrcs.max_event_queues =
+		RTE_MIN(handle->evdev_rsrcs.max_event_queues,
 			RTE_EVENT_MAX_QUEUES_PER_DEV);
 
-	evdev_dlb2_default_info.max_num_events =
-		RTE_MIN(evdev_dlb2_default_info.max_num_events,
+	handle->evdev_rsrcs.max_num_events =
+		RTE_MIN(handle->evdev_rsrcs.max_num_events,
 			dlb2->max_num_events_override);
 
-	*dev_info = evdev_dlb2_default_info;
+	*dev_info = handle->evdev_rsrcs;
 }
 
 static int
@@ -864,26 +1249,73 @@ dlb2_hw_create_sched_domain(struct dlb2_eventdev *dlb2,
 		DLB2_NUM_ATOMIC_INFLIGHTS_PER_QUEUE *
 		cfg->num_ldb_queues;
 
-	cfg->num_hist_list_entries = resources_asked->num_ldb_ports *
-		evdev_dlb2_default_info.max_event_port_dequeue_depth;
+	/* If hl_entries is non-zero then user specified command line option.
+	 * Else compute using default_port_hl that has been set earlier based
+	 * on use_default_hl option
+	 */
+	if (dlb2->hl_entries)
+		cfg->num_hist_list_entries = dlb2->hl_entries;
+	else
+		cfg->num_hist_list_entries =
+		    resources_asked->num_ldb_ports * dlb2->default_port_hl;
+
+	if (handle->req_ordered_queues_0 >
+			dlb2->hw_rsrc_query_results.num_sn_slots[0] ||
+		handle->req_ordered_queues_1 >
+			dlb2->hw_rsrc_query_results.num_sn_slots[1]) {
+		DLB2_LOG_ERR("dlb2: domain create failed, cmd line requested %d,%d ordered queues, but only %d,%d slots available\n",
+		    handle->req_ordered_queues_0, handle->req_ordered_queues_1,
+		    dlb2->hw_rsrc_query_results.num_sn_slots[0],
+		    dlb2->hw_rsrc_query_results.num_sn_slots[1]);
+		ret = -ENOSPC;
+		goto error_exit;
+	}
+	if (handle->num_ordered_queues_0 == 0 &&
+		handle->num_ordered_queues_1 == 0) {
+		/*
+		 * number of ordered queues_X was not specified, so allocate
+		 * just enough to cover the number of load balanced queues,
+		 * assuming they're all ordered.
+		 */
+		if (cfg->num_ldb_queues <= handle->req_ordered_queues_0) {
+			cfg->num_sn_slots[0] = cfg->num_ldb_queues;
+			cfg->num_sn_slots[1] = 0;
+		} else {
+			cfg->num_sn_slots[0] = handle->req_ordered_queues_0;
+			cfg->num_sn_slots[1] = cfg->num_ldb_queues -
+					handle->req_ordered_queues_0;
+		}
+	} else {
+		/*
+		 * Use exact number specified, Note that the cmdline specified
+		 * num_ordered == req_ordered, so below is just more readable
+		 * and instructive.
+		 */
+		cfg->num_sn_slots[0] = handle->num_ordered_queues_0;
+		cfg->num_sn_slots[1] = handle->num_ordered_queues_1;
+	}
 
 	if (device_version == DLB2_HW_V2_5) {
-		DLB2_LOG_DBG("sched domain create - ldb_qs=%d, ldb_ports=%d, dir_ports=%d, atomic_inflights=%d, hist_list_entries=%d, credits=%d\n",
+		DLB2_LOG_DBG("sched domain create - ldb_qs=%d, ldb_ports=%d, dir_ports=%d, atomic_inflights=%d, hist_list_entries=%d, credits=%d, sn=%d,%d\n",
 			     cfg->num_ldb_queues,
 			     resources_asked->num_ldb_ports,
 			     cfg->num_dir_ports,
 			     cfg->num_atomic_inflights,
 			     cfg->num_hist_list_entries,
-			     cfg->num_credits);
+			     cfg->num_credits,
+			     cfg->num_sn_slots[0],
+			     cfg->num_sn_slots[1]);
 	} else {
-		DLB2_LOG_DBG("sched domain create - ldb_qs=%d, ldb_ports=%d, dir_ports=%d, atomic_inflights=%d, hist_list_entries=%d, ldb_credits=%d, dir_credits=%d\n",
+		DLB2_LOG_DBG("sched domain create - ldb_qs=%d, ldb_ports=%d, dir_ports=%d, atomic_inflights=%d, hist_list_entries=%d, ldb_credits=%d, dir_credits=%d, sn=%d,%d\n",
 			     cfg->num_ldb_queues,
 			     resources_asked->num_ldb_ports,
 			     cfg->num_dir_ports,
 			     cfg->num_atomic_inflights,
 			     cfg->num_hist_list_entries,
 			     cfg->num_ldb_credits,
-			     cfg->num_dir_credits);
+			     cfg->num_dir_credits,
+			     cfg->num_sn_slots[0],
+			     cfg->num_sn_slots[1]);
 	}
 
 	/* Configure the QM */
@@ -898,6 +1330,7 @@ dlb2_hw_create_sched_domain(struct dlb2_eventdev *dlb2,
 	}
 
 	handle->domain_id = cfg->response.id;
+	handle->domain_id_valid = 1;
 	handle->cfg.configured = true;
 
 error_exit:
@@ -905,18 +1338,29 @@ dlb2_hw_create_sched_domain(struct dlb2_eventdev *dlb2,
 	return ret;
 }
 
-static void
+static int
 dlb2_hw_reset_sched_domain(const struct rte_eventdev *dev, bool reconfig)
 {
 	struct dlb2_eventdev *dlb2 = dlb2_pmd_priv(dev);
 	enum dlb2_configuration_state config_state;
-	int i, j;
+	enum dlb2_configuration_state prev_config_state[DLB2_MAX_NUM_PORTS_ALL];
+	int i, j, ret;
 
-	dlb2_iface_domain_reset(dlb2);
+	ret = dlb2_iface_domain_reset(dlb2);
+	if (ret) {
+		DLB2_LOG_ERR("dlb2_hw_reset_domain err %d", ret);
+		return ret;
+	}
 
 	/* Free all dynamically allocated port memory */
-	for (i = 0; i < dlb2->num_ports; i++)
+	for (i = 0; i < dlb2->num_ports; i++) {
+		prev_config_state[i] = dlb2->ev_ports[i].qm_port.config_state;
 		dlb2_free_qe_mem(&dlb2->ev_ports[i].qm_port);
+		if (!reconfig) {
+			dlb2->ev_ports[i].qm_port.enable_inflight_ctrl = 0;
+			dlb2->ev_ports[i].qm_port.token_pop_mode = 0;
+		}
+	}
 
 	/* If reconfiguring, mark the device's queues and ports as "previously
 	 * configured." If the user doesn't reconfigure them, the PMD will
@@ -926,7 +1370,10 @@ dlb2_hw_reset_sched_domain(const struct rte_eventdev *dev, bool reconfig)
 		DLB2_NOT_CONFIGURED;
 
 	for (i = 0; i < dlb2->num_ports; i++) {
-		dlb2->ev_ports[i].qm_port.config_state = config_state;
+		/* Retain configuration status before reconfig */
+		dlb2->ev_ports[i].qm_port.config_state =
+			(prev_config_state[i] == DLB2_NOT_CONFIGURED) ?
+			DLB2_NOT_CONFIGURED : config_state;
 		/* Reset setup_done so ports can be reconfigured */
 		dlb2->ev_ports[i].setup_done = false;
 		for (j = 0; j < DLB2_MAX_NUM_QIDS_PER_LDB_CQ; j++)
@@ -945,7 +1392,18 @@ dlb2_hw_reset_sched_domain(const struct rte_eventdev *dev, bool reconfig)
 	dlb2->num_queues = 0;
 	dlb2->num_ldb_queues = 0;
 	dlb2->num_dir_queues = 0;
+	if (dlb2->version == DLB2_HW_V2_5) {
+		dlb2->num_credits = 0;
+		dlb2->max_credits = 0;
+	} else {
+		dlb2->num_ldb_credits = 0;
+		dlb2->num_dir_credits = 0;
+		dlb2->max_ldb_credits = 0;
+		dlb2->max_dir_credits = 0;
+	}
 	dlb2->configured = false;
+
+	return 0;
 }
 
 /* Note: 1 QM instance per QM device, QM instance/device == event device */
@@ -963,7 +1421,9 @@ dlb2_eventdev_configure(const struct rte_eventdev *dev)
 	 * scheduling domain before attempting to configure a new one.
 	 */
 	if (dlb2->configured) {
-		dlb2_hw_reset_sched_domain(dev, true);
+		ret = dlb2_hw_reset_sched_domain(dev, true);
+		if (ret)
+			return ret;
 		ret = dlb2_hw_query_resources(dlb2);
 		if (ret) {
 			DLB2_LOG_ERR("get resources err=%d, devid=%d\n",
@@ -1007,8 +1467,15 @@ dlb2_eventdev_configure(const struct rte_eventdev *dev)
 	}
 
 	/* Does this platform support umonitor/umwait? */
-	if (rte_cpu_get_flag_enabled(RTE_CPUFLAG_WAITPKG))
+	if (rte_cpu_get_flag_enabled(RTE_CPUFLAG_WAITPKG)) {
+		if (RTE_LIBRTE_PMD_DLB2_UMWAIT_CTL_STATE != 0 &&
+		    RTE_LIBRTE_PMD_DLB2_UMWAIT_CTL_STATE != 1) {
+			DLB2_LOG_ERR("invalid value (%d) for RTE_LIBRTE_PMD_DLB2_UMWAIT_CTL_STATE, must be 0 or 1.\n",
+				     RTE_LIBRTE_PMD_DLB2_UMWAIT_CTL_STATE);
+			return -EINVAL;
+		}
 		dlb2->umwait_allowed = true;
+	}
 
 	rsrcs->num_dir_ports = config->nb_single_link_event_port_queues;
 	rsrcs->num_ldb_ports  = config->nb_event_ports - rsrcs->num_dir_ports;
@@ -1053,13 +1520,17 @@ dlb2_eventdev_configure(const struct rte_eventdev *dev)
 	if (dlb2->version == DLB2_HW_V2_5) {
 		dlb2->credit_pool = rsrcs->num_credits;
 		dlb2->max_credits = rsrcs->num_credits;
+		dlb2->num_credits = rsrcs->num_credits;
 	} else {
 		dlb2->ldb_credit_pool = rsrcs->num_ldb_credits;
 		dlb2->max_ldb_credits = rsrcs->num_ldb_credits;
+		dlb2->num_ldb_credits = rsrcs->num_ldb_credits;
 		dlb2->dir_credit_pool = rsrcs->num_dir_credits;
 		dlb2->max_dir_credits = rsrcs->num_dir_credits;
+		dlb2->num_dir_credits = rsrcs->num_dir_credits;
 	}
 
+	dlb2->evdev_id = dev->data->dev_id;
 	dlb2->configured = true;
 
 	return 0;
@@ -1074,8 +1545,8 @@ dlb2_eventdev_port_default_conf_get(struct rte_eventdev *dev,
 	struct dlb2_eventdev *dlb2 = dlb2_pmd_priv(dev);
 
 	port_conf->new_event_threshold = dlb2->new_event_limit;
-	port_conf->dequeue_depth = 32;
-	port_conf->enqueue_depth = DLB2_MAX_ENQUEUE_DEPTH;
+	port_conf->dequeue_depth = dlb2->default_port_hl / 2;
+	port_conf->enqueue_depth = dlb2->qm_instance.evdev_rsrcs.max_event_port_enqueue_depth;
 	port_conf->event_port_cfg = 0;
 }
 
@@ -1094,7 +1565,7 @@ dlb2_eventdev_queue_default_conf_get(struct rte_eventdev *dev,
 }
 
 static int32_t
-dlb2_get_sn_allocation(struct dlb2_eventdev *dlb2, int group)
+dlb2_get_sn_allocation(struct dlb2_eventdev *dlb2, uint32_t group)
 {
 	struct dlb2_hw_dev *handle = &dlb2->qm_instance;
 	struct dlb2_get_sn_allocation_args cfg;
@@ -1113,7 +1584,7 @@ dlb2_get_sn_allocation(struct dlb2_eventdev *dlb2, int group)
 }
 
 static int
-dlb2_set_sn_allocation(struct dlb2_eventdev *dlb2, int group, int num)
+dlb2_set_sn_allocation(struct dlb2_eventdev *dlb2, uint32_t group, uint32_t num)
 {
 	struct dlb2_hw_dev *handle = &dlb2->qm_instance;
 	struct dlb2_set_sn_allocation_args cfg;
@@ -1162,9 +1633,9 @@ dlb2_program_sn_allocation(struct dlb2_eventdev *dlb2,
 {
 	int grp_occupancy[DLB2_NUM_SN_GROUPS];
 	int grp_alloc[DLB2_NUM_SN_GROUPS];
-	int i, sequence_numbers;
+	uint32_t i, sequence_numbers;
 
-	sequence_numbers = (int)queue_conf->nb_atomic_order_sequences;
+	sequence_numbers = queue_conf->nb_atomic_order_sequences;
 
 	for (i = 0; i < DLB2_NUM_SN_GROUPS; i++) {
 		int total_slots;
@@ -1182,7 +1653,7 @@ dlb2_program_sn_allocation(struct dlb2_eventdev *dlb2,
 		/* DLB has at least one available slot for the requested
 		 * sequence numbers, so no further configuration required.
 		 */
-		if (grp_alloc[i] == sequence_numbers &&
+		if (grp_alloc[i] == (int)sequence_numbers &&
 		    grp_occupancy[i] < total_slots)
 			return;
 	}
@@ -1221,21 +1692,23 @@ dlb2_hw_create_ldb_queue(struct dlb2_eventdev *dlb2,
 	uint32_t qm_qid;
 	int sched_type = -1;
 
-	if (evq_conf == NULL)
+	if (evq_conf == NULL) 
 		return -EINVAL;
 
-	if (evq_conf->event_queue_cfg & RTE_EVENT_QUEUE_CFG_ALL_TYPES) {
-		if (evq_conf->nb_atomic_order_sequences != 0)
-			sched_type = RTE_SCHED_TYPE_ORDERED;
-		else
-			sched_type = RTE_SCHED_TYPE_PARALLEL;
-	} else
-		sched_type = evq_conf->schedule_type;
+  if (evq_conf->event_queue_cfg & RTE_EVENT_QUEUE_CFG_ALL_TYPES) {
+    if (evq_conf->nb_atomic_order_sequences != 0)
+      sched_type = RTE_SCHED_TYPE_ORDERED;
+    else
+      sched_type = RTE_SCHED_TYPE_PARALLEL;
+  } else
+    sched_type = evq_conf->schedule_type;
 
+	/* Atomic events are allowed on any queue type */
 	cfg.num_atomic_inflights = DLB2_NUM_ATOMIC_INFLIGHTS_PER_QUEUE;
 	cfg.num_sequence_numbers = evq_conf->nb_atomic_order_sequences;
 	cfg.num_qid_inflights = evq_conf->nb_atomic_order_sequences;
 
+  /* For atomic and un-ordered queues, QID inflight = 2048 and sn = 0 */
 	if (sched_type != RTE_SCHED_TYPE_ORDERED) {
 		cfg.num_sequence_numbers = 0;
 		cfg.num_qid_inflights = 2048;
@@ -1264,9 +1737,8 @@ dlb2_hw_create_ldb_queue(struct dlb2_eventdev *dlb2,
 	}
 
 	if (ev_queue->depth_threshold == 0) {
-		cfg.depth_threshold = dlb2->default_depth_thresh;
-		ev_queue->depth_threshold =
-			dlb2->default_depth_thresh;
+		cfg.depth_threshold = RTE_PMD_DLB2_DEFAULT_DEPTH_THRESH;
+		ev_queue->depth_threshold = RTE_PMD_DLB2_DEFAULT_DEPTH_THRESH;
 	} else
 		cfg.depth_threshold = ev_queue->depth_threshold;
 
@@ -1303,7 +1775,7 @@ dlb2_eventdev_ldb_queue_setup(struct rte_eventdev *dev,
 	struct dlb2_eventdev *dlb2 = dlb2_pmd_priv(dev);
 	int32_t qm_qid;
 
-	if (queue_conf->nb_atomic_order_sequences)
+	if (queue_conf->nb_atomic_order_sequences && dlb2->is_vdev == false)
 		dlb2_program_sn_allocation(dlb2, queue_conf);
 
 	qm_qid = dlb2_hw_create_ldb_queue(dlb2, ev_queue, queue_conf);
@@ -1536,7 +2008,7 @@ dlb2_hw_cq_bitmask_init(struct dlb2_port *qm_port, uint32_t cq_depth)
 		return; /* need to fall back to scalar code */
 
 	/*
-	 * all 1's in first u64, all zeros in second is correct bit pattern to
+	 * all 1's in first u64, all zeros in 2nd is correct bit pattern to
 	 * start. Special casing == 64 easier than adapting complex loop logic.
 	 */
 	if (cq_depth == 64) {
@@ -1555,18 +2027,15 @@ dlb2_hw_cq_bitmask_init(struct dlb2_port *qm_port, uint32_t cq_depth)
 static int
 dlb2_hw_create_ldb_port(struct dlb2_eventdev *dlb2,
 			struct dlb2_eventdev_port *ev_port,
-			uint32_t dequeue_depth,
-			uint32_t enqueue_depth)
+			uint32_t dequeue_depth)
 {
 	struct dlb2_hw_dev *handle = &dlb2->qm_instance;
 	struct dlb2_create_ldb_port_args cfg = { {0} };
-	int ret;
-	struct dlb2_port *qm_port = NULL;
+	struct dlb2_port *qm_port = &ev_port->qm_port;
+	struct process_local_port_data *port_data;
 	char mz_name[RTE_MEMZONE_NAMESIZE];
 	uint32_t qm_port_id;
-	uint16_t ldb_credit_high_watermark = 0;
-	uint16_t dir_credit_high_watermark = 0;
-	uint16_t credit_high_watermark = 0;
+	int ret;
 
 	if (handle == NULL)
 		return -EINVAL;
@@ -1577,43 +2046,33 @@ dlb2_hw_create_ldb_port(struct dlb2_eventdev *dlb2,
 		return -EINVAL;
 	}
 
-	if (dlb2->version == DLB2_HW_V2 && ev_port->cq_weight != 0 &&
-	    ev_port->cq_weight > dequeue_depth) {
-		DLB2_LOG_ERR("dlb2: invalid cq dequeue depth %d, must be >= cq weight %d\n",
-			     dequeue_depth, ev_port->cq_weight);
-		return -EINVAL;
-	}
-
 	rte_spinlock_lock(&handle->resource_lock);
 
 	/* We round up to the next power of 2 if necessary */
 	cfg.cq_depth = rte_align32pow2(dequeue_depth);
 	cfg.cq_depth_threshold = 1;
 
-	cfg.cq_history_list_size = cfg.cq_depth;
+	if (dlb2->version == DLB2_HW_V2_5 && qm_port->enable_inflight_ctrl) {
+		cfg.enable_inflight_ctrl = 1;
+		cfg.inflight_threshold = qm_port->inflight_threshold;
+		if (!qm_port->hist_list)
+			qm_port->hist_list = cfg.cq_depth;
+	}
+
+	if (qm_port->hist_list)
+		cfg.cq_history_list_size = qm_port->hist_list;
+	else if (dlb2->default_port_hl == DLB2_FIXED_CQ_HL_SIZE)
+		cfg.cq_history_list_size = DLB2_FIXED_CQ_HL_SIZE;
+	else
+		cfg.cq_history_list_size = cfg.cq_depth * 2;
 
 	cfg.cos_id = ev_port->cos_id;
 	cfg.cos_strict = 0;/* best effots */
 
-	/* User controls the LDB high watermark via enqueue depth. The DIR high
-	 * watermark is equal, unless the directed credit pool is too small.
-	 */
-	if (dlb2->version == DLB2_HW_V2) {
-		ldb_credit_high_watermark = enqueue_depth;
-		/* If there are no directed ports, the kernel driver will
-		 * ignore this port's directed credit settings. Don't use
-		 * enqueue_depth if it would require more directed credits
-		 * than are available.
-		 */
-		dir_credit_high_watermark =
-			RTE_MIN(enqueue_depth,
-				handle->cfg.num_dir_credits / dlb2->num_ports);
-	} else
-		credit_high_watermark = enqueue_depth;
-
 	/* Per QM values */
 
-	ret = dlb2_iface_ldb_port_create(handle, &cfg,  dlb2->poll_mode);
+	ret = dlb2_iface_ldb_port_create(handle, &cfg,  dlb2->poll_mode,
+					 dlb2->evdev_id);
 	if (ret < 0) {
 		DLB2_LOG_ERR("dlb2: dlb2_ldb_port_create error, ret=%d (driver status: %s)\n",
 			     ret, dlb2_error_strings[cfg.response.status]);
@@ -1625,7 +2084,7 @@ dlb2_hw_create_ldb_port(struct dlb2_eventdev *dlb2,
 	DLB2_LOG_DBG("dlb2: ev_port %d uses qm LB port %d <<<<<\n",
 		     ev_port->id, qm_port_id);
 
-	qm_port = &ev_port->qm_port;
+	qm_port->evdev_id = dlb2->evdev_id;
 	qm_port->ev_port = ev_port; /* back ptr */
 	qm_port->dlb2 = dlb2; /* back ptr */
 	/*
@@ -1647,12 +2106,15 @@ dlb2_hw_create_ldb_port(struct dlb2_eventdev *dlb2,
 	if (dlb2->version == DLB2_HW_V2) {
 		qm_port->cached_ldb_credits = 0;
 		qm_port->cached_dir_credits = 0;
-		if (ev_port->cq_weight) {
+	} else
+		qm_port->cached_credits = 0;
+
+	if (dlb2->version == DLB2_HW_V2_5 && (dlb2->enable_cq_weight == true)) {
 			struct dlb2_enable_cq_weight_args
 				cq_weight_args = { {0} };
 
 			cq_weight_args.port_id = qm_port->id;
-			cq_weight_args.limit = ev_port->cq_weight;
+			cq_weight_args.limit = dequeue_depth;
 			ret = dlb2_iface_enable_cq_weight(handle, &cq_weight_args);
 			if (ret < 0) {
 				DLB2_LOG_ERR("dlb2: dlb2_dir_port_create error, ret=%d (driver status: %s)\n",
@@ -1660,11 +2122,6 @@ dlb2_hw_create_ldb_port(struct dlb2_eventdev *dlb2,
 					dlb2_error_strings[cfg.response.  status]);
 				goto error_exit;
 			}
-		}
-		qm_port->cq_weight = ev_port->cq_weight;
-	} else {
-		qm_port->cached_credits = 0;
-		qm_port->cq_weight = 0;
 	}
 
 	/* CQs with depth < 8 use an 8-entry queue, but withhold credits so
@@ -1709,34 +2166,46 @@ dlb2_hw_create_ldb_port(struct dlb2_eventdev *dlb2,
 	qm_port->owed_tokens = 0;
 	qm_port->issued_releases = 0;
 
+	port_data = &dlb2_port[qm_port->evdev_id][qm_port->id]
+			      [PORT_TYPE(qm_port)];
+
 	/* Save config message too. */
 	rte_memcpy(&qm_port->cfg.ldb, &cfg, sizeof(qm_port->cfg.ldb));
 
 	/* update state */
 	qm_port->state = PORT_STARTED; /* enabled at create time */
 	qm_port->config_state = DLB2_CONFIGURED;
+	port_data->use_ded_autopop_cl = false;
 
 	if (dlb2->version == DLB2_HW_V2) {
-		qm_port->dir_credits = dir_credit_high_watermark;
-		qm_port->ldb_credits = ldb_credit_high_watermark;
 		qm_port->credit_pool[DLB2_DIR_QUEUE] = &dlb2->dir_credit_pool;
 		qm_port->credit_pool[DLB2_LDB_QUEUE] = &dlb2->ldb_credit_pool;
 
-		DLB2_LOG_DBG("dlb2: created ldb port %d, depth = %d, ldb credits=%d, dir credits=%d\n",
+		DLB2_LOG_DBG("dlb2: created ldb port %d, depth = %d\n",
 			     qm_port_id,
-			     dequeue_depth,
-			     qm_port->ldb_credits,
-			     qm_port->dir_credits);
+			     dequeue_depth);
 	} else {
-		qm_port->credits = credit_high_watermark;
 		qm_port->credit_pool[DLB2_COMBINED_POOL] = &dlb2->credit_pool;
 
-		DLB2_LOG_DBG("dlb2: created ldb port %d, depth = %d, credits=%d\n",
+		DLB2_LOG_DBG("dlb2: created ldb port %d, depth = %d\n",
 			     qm_port_id,
-			     dequeue_depth,
-			     qm_port->credits);
+			     dequeue_depth);
+
+		if (qm_port->token_pop_mode == AUTO_POP)
+			port_data->use_ded_autopop_cl = true;
+
 	}
 
+	/*
+	 * NPF-2161: For PF PMD. Will be overwritten for bifurcated mode when
+	 * port is mmapped
+	 */
+	if (port_data->use_ded_autopop_cl)
+		port_data->autopop_addr = port_data->pp_addr +
+			DLB2_NUM_BYTES_PER_CACHE_LINE;
+	else
+		port_data->autopop_addr = port_data->pp_addr;
+
 	qm_port->use_scalar = false;
 
 #if (!defined RTE_ARCH_X86_64)
@@ -1744,7 +2213,7 @@ dlb2_hw_create_ldb_port(struct dlb2_eventdev *dlb2,
 #else
 	if ((qm_port->cq_depth > 64) ||
 	    (!rte_is_power_of_2(qm_port->cq_depth)) ||
-	    (dlb2->vector_opts_enabled == false))
+	    (dlb2->vector_opts_disab == true))
 		qm_port->use_scalar = true;
 #endif
 
@@ -1754,8 +2223,7 @@ dlb2_hw_create_ldb_port(struct dlb2_eventdev *dlb2,
 
 error_exit:
 
-	if (qm_port)
-		dlb2_free_qe_mem(qm_port);
+	dlb2_free_qe_mem(qm_port);
 
 	rte_spinlock_unlock(&handle->resource_lock);
 
@@ -1793,11 +2261,9 @@ dlb2_hw_create_dir_port(struct dlb2_eventdev *dlb2,
 	struct dlb2_create_dir_port_args cfg = { {0} };
 	int ret;
 	struct dlb2_port *qm_port = NULL;
+	struct process_local_port_data *port_data;
 	char mz_name[RTE_MEMZONE_NAMESIZE];
 	uint32_t qm_port_id;
-	uint16_t ldb_credit_high_watermark = 0;
-	uint16_t dir_credit_high_watermark = 0;
-	uint16_t credit_high_watermark = 0;
 
 	if (dlb2 == NULL || handle == NULL)
 		return -EINVAL;
@@ -1823,26 +2289,12 @@ dlb2_hw_create_dir_port(struct dlb2_eventdev *dlb2,
 	cfg.cq_depth = rte_align32pow2(dequeue_depth);
 	cfg.cq_depth_threshold = 1;
 
-	/* User controls the LDB high watermark via enqueue depth. The DIR high
-	 * watermark is equal, unless the directed credit pool is too small.
-	 */
-	if (dlb2->version == DLB2_HW_V2) {
-		ldb_credit_high_watermark = enqueue_depth;
-		/* Don't use enqueue_depth if it would require more directed
-		 * credits than are available.
-		 */
-		dir_credit_high_watermark =
-			RTE_MIN(enqueue_depth,
-				handle->cfg.num_dir_credits / dlb2->num_ports);
-	} else
-		credit_high_watermark = enqueue_depth;
-
 	if (ev_port->conf.event_port_cfg & RTE_EVENT_PORT_CFG_HINT_PRODUCER)
 		cfg.is_producer = 1;
-
 	/* Per QM values */
 
-	ret = dlb2_iface_dir_port_create(handle, &cfg,  dlb2->poll_mode);
+	ret = dlb2_iface_dir_port_create(handle, &cfg,  dlb2->poll_mode,
+					 dlb2->evdev_id);
 	if (ret < 0) {
 		DLB2_LOG_ERR("dlb2: dlb2_dir_port_create error, ret=%d (driver status: %s)\n",
 			     ret, dlb2_error_strings[cfg.response.status]);
@@ -1855,6 +2307,7 @@ dlb2_hw_create_dir_port(struct dlb2_eventdev *dlb2,
 		     ev_port->id, qm_port_id);
 
 	qm_port = &ev_port->qm_port;
+	qm_port->evdev_id = dlb2->evdev_id;
 	qm_port->ev_port = ev_port; /* back ptr */
 	qm_port->dlb2 = dlb2;  /* back ptr */
 
@@ -1910,40 +2363,52 @@ dlb2_hw_create_dir_port(struct dlb2_eventdev *dlb2,
 	qm_port->owed_tokens = 0;
 	qm_port->issued_releases = 0;
 
+	port_data = &dlb2_port[qm_port->evdev_id][qm_port->id]
+			      [PORT_TYPE(qm_port)];
+
 	/* Save config message too. */
 	rte_memcpy(&qm_port->cfg.dir, &cfg, sizeof(qm_port->cfg.dir));
 
 	/* update state */
 	qm_port->state = PORT_STARTED; /* enabled at create time */
 	qm_port->config_state = DLB2_CONFIGURED;
+	port_data->use_ded_autopop_cl = false;
 
 	if (dlb2->version == DLB2_HW_V2) {
-		qm_port->dir_credits = dir_credit_high_watermark;
-		qm_port->ldb_credits = ldb_credit_high_watermark;
 		qm_port->credit_pool[DLB2_DIR_QUEUE] = &dlb2->dir_credit_pool;
 		qm_port->credit_pool[DLB2_LDB_QUEUE] = &dlb2->ldb_credit_pool;
 
-		DLB2_LOG_DBG("dlb2: created dir port %d, depth = %d cr=%d,%d\n",
+		DLB2_LOG_DBG("dlb2: created dir port %d, depth = %d\n",
 			     qm_port_id,
-			     dequeue_depth,
-			     dir_credit_high_watermark,
-			     ldb_credit_high_watermark);
+			     dequeue_depth);
 	} else {
-		qm_port->credits = credit_high_watermark;
 		qm_port->credit_pool[DLB2_COMBINED_POOL] = &dlb2->credit_pool;
 
-		DLB2_LOG_DBG("dlb2: created dir port %d, depth = %d cr=%d\n",
+		DLB2_LOG_DBG("dlb2: created dir port %d, depth = %d\n",
 			     qm_port_id,
-			     dequeue_depth,
-			     credit_high_watermark);
+			     dequeue_depth);
+		if (qm_port->token_pop_mode == AUTO_POP)
+			port_data->use_ded_autopop_cl = true;
 	}
 
+	/*
+	 * NPF-2161: For PF PMD. Will be overwritten for bifurcated mode when
+	 * port is mmapped
+	 */
+	if (port_data->use_ded_autopop_cl)
+		port_data->autopop_addr = port_data->pp_addr +
+			DLB2_NUM_BYTES_PER_CACHE_LINE;
+	else
+		port_data->autopop_addr = port_data->pp_addr;
+
+	qm_port->use_scalar = false;
+
 #if (!defined RTE_ARCH_X86_64)
 	qm_port->use_scalar = true;
 #else
 	if ((qm_port->cq_depth > 64) ||
 	    (!rte_is_power_of_2(qm_port->cq_depth)) ||
-	    (dlb2->vector_opts_enabled == false))
+	    (dlb2->vector_opts_disab == true))
 		qm_port->use_scalar = true;
 #endif
 
@@ -1970,8 +2435,9 @@ dlb2_eventdev_port_setup(struct rte_eventdev *dev,
 {
 	struct dlb2_eventdev *dlb2;
 	struct dlb2_eventdev_port *ev_port;
-	uint32_t hw_credit_quanta, sw_credit_quanta;
+	struct dlb2_hw_dev *handle;
 	int ret;
+	uint32_t hw_credit_quanta, sw_credit_quanta;
 
 	if (dev == NULL || port_conf == NULL) {
 		DLB2_LOG_ERR("Null parameter\n");
@@ -1979,14 +2445,15 @@ dlb2_eventdev_port_setup(struct rte_eventdev *dev,
 	}
 
 	dlb2 = dlb2_pmd_priv(dev);
+	handle = &dlb2->qm_instance;
 
 	if (ev_port_id >= DLB2_MAX_NUM_PORTS(dlb2->version))
 		return -EINVAL;
 
 	if (port_conf->dequeue_depth >
-		evdev_dlb2_default_info.max_event_port_dequeue_depth ||
+		handle->evdev_rsrcs.max_event_port_dequeue_depth ||
 	    port_conf->enqueue_depth >
-		evdev_dlb2_default_info.max_event_port_enqueue_depth)
+		handle->evdev_rsrcs.max_event_port_enqueue_depth)
 		return -EINVAL;
 
 	ev_port = &dlb2->ev_ports[ev_port_id];
@@ -1996,55 +2463,45 @@ dlb2_eventdev_port_setup(struct rte_eventdev *dev,
 		return -EINVAL;
 	}
 
-	/* Default for worker ports */
-	sw_credit_quanta = dlb2->sw_credit_quanta;
-	hw_credit_quanta = dlb2->hw_credit_quanta;
-
-	ev_port->qm_port.is_producer = false;
 	ev_port->qm_port.is_directed = port_conf->event_port_cfg &
 		RTE_EVENT_PORT_CFG_SINGLE_LINK;
 
+	/*
+	 * Validate credit config before creating port
+	 */
+
+	/* Default for worker ports */
+	sw_credit_quanta = dlb2->sw_credit_quanta[DLB2_CQ_WORKER_INDEX];
+	hw_credit_quanta = dlb2->hw_credit_quanta[DLB2_CQ_WORKER_INDEX];
+	ev_port->qm_port.is_producer = false;
+
 	if (port_conf->event_port_cfg & RTE_EVENT_PORT_CFG_HINT_PRODUCER) {
 		/* Producer type ports. Mostly enqueue */
-		sw_credit_quanta = DLB2_SW_CREDIT_P_QUANTA_DEFAULT;
-		hw_credit_quanta = DLB2_SW_CREDIT_P_BATCH_SZ;
+		sw_credit_quanta = dlb2->sw_credit_quanta[DLB2_CQ_PRODUCER_INDEX];
+		hw_credit_quanta = dlb2->hw_credit_quanta[DLB2_CQ_PRODUCER_INDEX];
 		ev_port->qm_port.is_producer = true;
 	}
 	if (port_conf->event_port_cfg & RTE_EVENT_PORT_CFG_HINT_CONSUMER) {
 		/* Consumer type ports. Mostly dequeue */
-		sw_credit_quanta = DLB2_SW_CREDIT_C_QUANTA_DEFAULT;
-		hw_credit_quanta = DLB2_SW_CREDIT_C_BATCH_SZ;
+		sw_credit_quanta = dlb2->sw_credit_quanta[DLB2_CQ_CONSUMER_INDEX];
+		hw_credit_quanta = dlb2->hw_credit_quanta[DLB2_CQ_CONSUMER_INDEX];
 	}
 	ev_port->credit_update_quanta = sw_credit_quanta;
 	ev_port->qm_port.hw_credit_quanta = hw_credit_quanta;
 
+	ev_port->enq_retries = port_conf->enqueue_depth; /* / sw_credit_quanta; */
+
 	/*
-	 * Validate credit config before creating port
+	 * Create port
 	 */
 
-	if (port_conf->enqueue_depth > sw_credit_quanta ||
-	    port_conf->enqueue_depth > hw_credit_quanta) {
-		DLB2_LOG_ERR("Invalid port config. Enqueue depth %d must be <= credit quanta %d and batch size %d\n",
-			     port_conf->enqueue_depth,
-			     sw_credit_quanta,
-			     hw_credit_quanta);
-		return -EINVAL;
-	}
-	ev_port->enq_retries = port_conf->enqueue_depth / sw_credit_quanta;
-
 	/* Save off port config for reconfig */
 	ev_port->conf = *port_conf;
 
-
-	/*
-	 * Create port
-	 */
-
 	if (!ev_port->qm_port.is_directed) {
 		ret = dlb2_hw_create_ldb_port(dlb2,
 					      ev_port,
-					      port_conf->dequeue_depth,
-					      port_conf->enqueue_depth);
+					      port_conf->dequeue_depth);
 		if (ret < 0) {
 			DLB2_LOG_ERR("Failed to create the lB port ve portId=%d\n",
 				     ev_port_id);
@@ -2065,6 +2522,7 @@ dlb2_eventdev_port_setup(struct rte_eventdev *dev,
 	ev_port->id = ev_port_id;
 	ev_port->enq_configured = true;
 	ev_port->setup_done = true;
+	ev_port->qm_port.pending_frags = 0;
 	ev_port->inflight_max = port_conf->new_event_threshold;
 	ev_port->implicit_release = !(port_conf->event_port_cfg &
 		  RTE_EVENT_PORT_CFG_DISABLE_IMPL_REL);
@@ -2072,36 +2530,12 @@ dlb2_eventdev_port_setup(struct rte_eventdev *dev,
 	ev_port->inflight_credits = 0;
 	ev_port->dlb2 = dlb2; /* reverse link */
 
-	/* Default for worker ports */
-	sw_credit_quanta = dlb2->sw_credit_quanta;
-	hw_credit_quanta = dlb2->hw_credit_quanta;
-
-	if (port_conf->event_port_cfg & RTE_EVENT_PORT_CFG_HINT_PRODUCER) {
-		/* Producer type ports. Mostly enqueue */
-		sw_credit_quanta = DLB2_SW_CREDIT_P_QUANTA_DEFAULT;
-		hw_credit_quanta = DLB2_SW_CREDIT_P_BATCH_SZ;
-	}
-	if (port_conf->event_port_cfg & RTE_EVENT_PORT_CFG_HINT_CONSUMER) {
-		/* Consumer type ports. Mostly dequeue */
-		sw_credit_quanta = DLB2_SW_CREDIT_C_QUANTA_DEFAULT;
-		hw_credit_quanta = DLB2_SW_CREDIT_C_BATCH_SZ;
-	}
-	ev_port->credit_update_quanta = sw_credit_quanta;
-	ev_port->qm_port.hw_credit_quanta = hw_credit_quanta;
-
-
 	/* Tear down pre-existing port->queue links */
 	if (dlb2->run_state == DLB2_RUN_STATE_STOPPED)
 		dlb2_port_link_teardown(dlb2, &dlb2->ev_ports[ev_port_id]);
 
 	dev->data->ports[ev_port_id] = &dlb2->ev_ports[ev_port_id];
 
-	if (rte_cpu_get_flag_enabled(RTE_CPUFLAG_AVX512VL) &&
-	    rte_vect_get_max_simd_bitwidth() >= RTE_VECT_SIMD_512)
-		ev_port->qm_port.use_avx512 = true;
-	else
-		ev_port->qm_port.use_avx512 = false;
-
 	return 0;
 }
 
@@ -2188,9 +2622,8 @@ dlb2_hw_create_dir_queue(struct dlb2_eventdev *dlb2,
 	cfg.port_id = qm_port_id;
 
 	if (ev_queue->depth_threshold == 0) {
-		cfg.depth_threshold = dlb2->default_depth_thresh;
-		ev_queue->depth_threshold =
-			dlb2->default_depth_thresh;
+		cfg.depth_threshold = RTE_PMD_DLB2_DEFAULT_DEPTH_THRESH;
+		ev_queue->depth_threshold = RTE_PMD_DLB2_DEFAULT_DEPTH_THRESH;
 	} else
 		cfg.depth_threshold = ev_queue->depth_threshold;
 
@@ -2447,7 +2880,7 @@ dlb2_event_queue_detach_ldb(struct dlb2_eventdev *dlb2,
 	}
 
 	/* This is expected with eventdev API!
-	 * It blindly attempts to unmap all queues.
+	 * It blindly attemmpts to unmap all queues.
 	 */
 	if (i == DLB2_MAX_NUM_QIDS_PER_LDB_CQ) {
 		DLB2_LOG_DBG("dlb2: ignoring LB QID %d not mapped for qm_port %d.\n",
@@ -2465,6 +2898,61 @@ dlb2_event_queue_detach_ldb(struct dlb2_eventdev *dlb2,
 	return ret;
 }
 
+static inline void
+dlb2_port_credits_return(struct dlb2_port *qm_port)
+{
+	/* Return all port credits */
+	if (qm_port->dlb2->version == DLB2_HW_V2_5) {
+		if (qm_port->cached_credits) {
+			__atomic_fetch_add(qm_port->credit_pool[DLB2_COMBINED_POOL],
+					   qm_port->cached_credits, __ATOMIC_SEQ_CST);
+			qm_port->cached_credits = 0;
+		}
+	} else {
+		if (qm_port->cached_ldb_credits) {
+			__atomic_fetch_add(qm_port->credit_pool[DLB2_LDB_QUEUE],
+					   qm_port->cached_ldb_credits, __ATOMIC_SEQ_CST);
+			qm_port->cached_ldb_credits = 0;
+		}
+		if (qm_port->cached_dir_credits) {
+			__atomic_fetch_add(qm_port->credit_pool[DLB2_DIR_QUEUE],
+					   qm_port->cached_dir_credits, __ATOMIC_SEQ_CST);
+			qm_port->cached_dir_credits = 0;
+		}
+	}
+}
+
+static inline void
+dlb2_release_sw_credits(struct dlb2_eventdev *dlb2,
+			struct dlb2_eventdev_port *ev_port, uint16_t val)
+{
+	if (ev_port->inflight_credits) {
+		__atomic_fetch_sub(&dlb2->inflights, val, __ATOMIC_SEQ_CST);
+		ev_port->inflight_credits -= val;
+	}
+}
+
+static void dlb2_check_and_return_credits(struct dlb2_eventdev_port *ev_port,
+					  bool cond, uint32_t threshold)
+{
+#if DLB_SW_CREDITS_CHECKS || DLB_HW_CREDITS_CHECKS
+	if (cond) {
+		if (++ev_port->credit_return_count > threshold) {
+#if DLB_SW_CREDITS_CHECKS
+			dlb2_release_sw_credits(ev_port->dlb2, ev_port,
+						ev_port->inflight_credits);
+#endif
+#if DLB_HW_CREDITS_CHECKS
+			dlb2_port_credits_return(&ev_port->qm_port);
+#endif
+			ev_port->credit_return_count = 0;
+		}
+	} else {
+		ev_port->credit_return_count = 0;
+	}
+#endif
+}
+
 static int
 dlb2_eventdev_port_unlink(struct rte_eventdev *dev, void *event_port,
 			  uint8_t queues[], uint16_t nb_unlinks)
@@ -2484,14 +2972,15 @@ dlb2_eventdev_port_unlink(struct rte_eventdev *dev, void *event_port,
 
 	if (queues == NULL || nb_unlinks == 0) {
 		DLB2_LOG_DBG("dlb2: queues is NULL or nb_unlinks is 0\n");
-		return 0; /* Ignore and return success */
+		nb_unlinks = 0; /* Ignore and return success */
+		goto ret_credits;
 	}
 
 	if (ev_port->qm_port.is_directed) {
 		DLB2_LOG_DBG("dlb2: ignore unlink from dir port %d\n",
 			     ev_port->id);
 		rte_errno = 0;
-		return nb_unlinks; /* as if success */
+		goto ret_credits;
 	}
 
 	dlb2 = ev_port->dlb2;
@@ -2530,6 +3019,10 @@ dlb2_eventdev_port_unlink(struct rte_eventdev *dev, void *event_port,
 		ev_queue->num_links--;
 	}
 
+ret_credits:
+	if (ev_port->inflight_credits)
+		dlb2_check_and_return_credits(ev_port, true, 0);
+
 	return nb_unlinks;
 }
 
@@ -2642,6 +3135,27 @@ dlb2_eventdev_apply_port_links(struct rte_eventdev *dev)
 }
 
 static int
+dlb2_set_port_ctrl(struct dlb2_eventdev_port *ev_port, bool enable)
+{
+	const char *err_str = enable ? "enabled" : "disabled";
+
+	if (!ev_port->setup_done)
+		return 0;
+
+	if (!(ev_port->enq_configured ^ enable)) {
+		DLB2_LOG_INFO("dlb2: ev_port %d already %s\n", ev_port->id, err_str);
+		return 0;
+	}
+	if (dlb2_iface_port_ctrl(&ev_port->qm_port, enable)) {
+		DLB2_LOG_ERR("dlb2: ev_port %d could not be %s\n", ev_port->id, err_str);
+		return -EFAULT;
+	}
+	ev_port->enq_configured = enable;
+
+	return 0;
+}
+
+static int
 dlb2_eventdev_start(struct rte_eventdev *dev)
 {
 	struct dlb2_eventdev *dlb2 = dlb2_pmd_priv(dev);
@@ -2672,10 +3186,14 @@ dlb2_eventdev_start(struct rte_eventdev *dev)
 		return ret;
 
 	for (i = 0; i < dlb2->num_ports; i++) {
-		if (!dlb2->ev_ports[i].setup_done) {
+		struct dlb2_eventdev_port *ev_port = &dlb2->ev_ports[i];
+
+		if (!ev_port->setup_done && ev_port->qm_port.config_state != DLB2_NOT_CONFIGURED) {
 			DLB2_LOG_ERR("dlb2: port %d not setup", i);
 			return -ESTALE;
 		}
+		if (dlb2_set_port_ctrl(ev_port, true))
+			return -EFAULT;
 	}
 
 	for (i = 0; i < dlb2->num_queues; i++) {
@@ -2698,12 +3216,28 @@ dlb2_eventdev_start(struct rte_eventdev *dev)
 	return 0;
 }
 
+static uint8_t cmd_byte_map[DLB2_NUM_PORT_TYPES][DLB2_NUM_HW_SCHED_TYPES] = {
+	{
+		/* Load-balanced cmd bytes */
+		[RTE_EVENT_OP_NEW] = DLB2_NEW_CMD_BYTE,
+		[RTE_EVENT_OP_FORWARD] = DLB2_FWD_CMD_BYTE,
+		[RTE_EVENT_OP_RELEASE] = DLB2_COMP_CMD_BYTE,
+		[RTE_EVENT_DLB2_OP_FRAG] = DLB2_FRAG_CMD_BYTE,
+	},
+	{
+		/* Directed cmd bytes */
+		[RTE_EVENT_OP_NEW] = DLB2_NEW_CMD_BYTE,
+		[RTE_EVENT_OP_FORWARD] = DLB2_NEW_CMD_BYTE,
+		[RTE_EVENT_OP_RELEASE] = DLB2_NOOP_CMD_BYTE,
+		[RTE_EVENT_DLB2_OP_FRAG] = DLB2_NEW_CMD_BYTE,
+	},
+};
+
 static inline uint32_t
 dlb2_port_credits_get(struct dlb2_port *qm_port,
 		      enum dlb2_hw_queue_types type)
 {
 	uint32_t credits = *qm_port->credit_pool[type];
-	/* By default hw_credit_quanta is DLB2_SW_CREDIT_BATCH_SZ */
 	uint32_t batch_size = qm_port->hw_credit_quanta;
 
 	if (unlikely(credits < batch_size))
@@ -2729,8 +3263,7 @@ dlb2_replenish_sw_credits(struct dlb2_eventdev *dlb2,
 		/* Replenish credits, saving one quanta for enqueues */
 		uint16_t val = ev_port->inflight_credits - quanta;
 
-		__atomic_fetch_sub(&dlb2->inflights, val, __ATOMIC_SEQ_CST);
-		ev_port->inflight_credits -= val;
+		dlb2_release_sw_credits(dlb2, ev_port, val);
 	}
 }
 
@@ -2742,11 +3275,13 @@ dlb2_check_enqueue_sw_credits(struct dlb2_eventdev *dlb2,
 						__ATOMIC_SEQ_CST);
 	const int num = 1;
 
+#if ENABLE_PORT_THRES_CHECK
 	if (unlikely(ev_port->inflight_max < sw_inflights)) {
 		DLB2_INC_STAT(ev_port->stats.traffic.tx_nospc_inflight_max, 1);
 		rte_errno = -ENOSPC;
 		return 1;
 	}
+#endif
 
 	if (ev_port->inflight_credits < num) {
 		/* check if event enqueue brings ev_port over max threshold */
@@ -2760,10 +3295,14 @@ dlb2_check_enqueue_sw_credits(struct dlb2_eventdev *dlb2,
 			rte_errno = -ENOSPC;
 			return 1;
 		}
-
-		__atomic_fetch_add(&dlb2->inflights, credit_update_quanta,
-				   __ATOMIC_SEQ_CST);
-		ev_port->inflight_credits += (credit_update_quanta);
+		/* Application will retry if this attempt fails due to contention */
+		if(__atomic_compare_exchange_n(&dlb2->inflights, &sw_inflights, (sw_inflights+credit_update_quanta), false, __ATOMIC_SEQ_CST, __ATOMIC_SEQ_CST) )
+			ev_port->inflight_credits += (credit_update_quanta);
+		else
+		{
+			rte_errno = -ENOSPC;
+			return 1;
+		}
 
 		if (ev_port->inflight_credits < num) {
 			DLB2_INC_STAT(
@@ -2834,10 +3373,35 @@ dlb2_check_enqueue_hw_credits(struct dlb2_port *qm_port)
 }
 
 static __rte_always_inline void
-dlb2_pp_write(struct dlb2_enqueue_qe *qe4,
-	      struct process_local_port_data *port_data)
+dlb2_pp_write(struct process_local_port_data *port_data,
+	      struct dlb2_enqueue_qe *qe4)
 {
-	dlb2_movdir64b(port_data->pp_addr, qe4);
+	qm_mmio_fns.pp_enqueue_four(port_data->pp_addr, qe4);
+}
+
+static __rte_always_inline void
+dlb2_pp_write_reorder(struct process_local_port_data *port_data,
+	      struct dlb2_enqueue_qe *qe4)
+{
+	for (uint8_t i = 0; i < 4; i++) {
+		if (qe4[i].cmd_byte != DLB2_NOOP_CMD_BYTE) {
+			qm_mmio_fns.pp_enqueue_four(port_data->pp_addr, qe4);
+			return;
+		}
+	}
+}
+
+static __rte_always_inline int
+dlb2_pp_check4_write(struct process_local_port_data *port_data,
+	      struct dlb2_enqueue_qe *qe4)
+{
+	for (uint8_t i = 0; i < DLB2_NUM_QES_PER_CACHE_LINE; i++)
+		if (((uint64_t *)&qe4[i])[1] == 0)
+			return 0;
+	
+	qm_mmio_fns.pp_enqueue_four(port_data->pp_addr, qe4);
+	memset(qe4, 0 , DLB2_NUM_QES_PER_CACHE_LINE * sizeof(struct dlb2_enqueue_qe));
+	return DLB2_NUM_QES_PER_CACHE_LINE;
 }
 
 static inline int
@@ -2855,9 +3419,14 @@ dlb2_consume_qe_immediate(struct dlb2_port *qm_port, int num)
 	/* No store fence needed since no pointer is being sent, and CQ token
 	 * pops can be safely reordered with other HCWs.
 	 */
-	port_data = &dlb2_port[qm_port->id][PORT_TYPE(qm_port)];
+	port_data = &dlb2_port[qm_port->evdev_id][qm_port->id]
+			      [PORT_TYPE(qm_port)];
 
-	dlb2_movntdq_single(port_data->pp_addr, qe);
+#if USE_MOVDIR64B_SINGLE
+	dlb2_movdir64b_single(port_data->autopop_addr, qe);
+#else
+	dlb2_movntdq_single(port_data->autopop_addr, qe);
+#endif
 
 	DLB2_LOG_DBG("dlb2: consume immediate - %d QEs\n", num);
 
@@ -2877,7 +3446,7 @@ dlb2_hw_do_enqueue(struct dlb2_port *qm_port,
 	if (do_sfence)
 		rte_wmb();
 
-	dlb2_pp_write(qm_port->qe4, port_data);
+	dlb2_pp_write(port_data, (struct dlb2_enqueue_qe *)qm_port->qe4);
 }
 
 static inline void
@@ -2892,6 +3461,239 @@ dlb2_construct_token_pop_qe(struct dlb2_port *qm_port, int idx)
 	qm_port->owed_tokens = 0;
 }
 
+static inline void
+dlb2_event_build_hcws(struct dlb2_port *qm_port,
+		      const struct rte_event ev[],
+		      int num,
+		      uint8_t *sched_type,
+		      uint8_t *queue_id)
+{
+	struct dlb2_enqueue_qe *qe;
+	uint16_t sched_word[4];
+	__m128i sse_qe[2];
+	int i;
+
+	qe = qm_port->qe4;
+
+	sse_qe[0] = _mm_setzero_si128();
+	sse_qe[1] = _mm_setzero_si128();
+
+	switch (num) {
+	case 4:
+		/* Construct the metadata portion of two HCWs in one 128b SSE
+		 * register. HCW metadata is constructed in the SSE registers
+		 * like so:
+		 * sse_qe[0][63:0]:   qe[0]'s metadata
+		 * sse_qe[0][127:64]: qe[1]'s metadata
+		 * sse_qe[1][63:0]:   qe[2]'s metadata
+		 * sse_qe[1][127:64]: qe[3]'s metadata
+		 */
+
+		/* Convert the event operation into a command byte and store it
+		 * in the metadata:
+		 * sse_qe[0][63:56]   = cmd_byte_map[is_directed][ev[0].op]
+		 * sse_qe[0][127:120] = cmd_byte_map[is_directed][ev[1].op]
+		 * sse_qe[1][63:56]   = cmd_byte_map[is_directed][ev[2].op]
+		 * sse_qe[1][127:120] = cmd_byte_map[is_directed][ev[3].op]
+		 */
+#define DLB2_QE_CMD_BYTE 7
+		sse_qe[0] = _mm_insert_epi8(sse_qe[0],
+				cmd_byte_map[qm_port->is_directed][ev[0].op],
+				DLB2_QE_CMD_BYTE);
+		sse_qe[0] = _mm_insert_epi8(sse_qe[0],
+				cmd_byte_map[qm_port->is_directed][ev[1].op],
+				DLB2_QE_CMD_BYTE + 8);
+		sse_qe[1] = _mm_insert_epi8(sse_qe[1],
+				cmd_byte_map[qm_port->is_directed][ev[2].op],
+				DLB2_QE_CMD_BYTE);
+		sse_qe[1] = _mm_insert_epi8(sse_qe[1],
+				cmd_byte_map[qm_port->is_directed][ev[3].op],
+				DLB2_QE_CMD_BYTE + 8);
+
+		/* Store priority, scheduling type, and queue ID in the sched
+		 * word array because these values are re-used when the
+		 * destination is a directed queue.
+		 */
+		sched_word[0] = EV_TO_DLB2_PRIO(ev[0].priority) << 10 |
+				sched_type[0] << 8 |
+				queue_id[0];
+		sched_word[1] = EV_TO_DLB2_PRIO(ev[1].priority) << 10 |
+				sched_type[1] << 8 |
+				queue_id[1];
+		sched_word[2] = EV_TO_DLB2_PRIO(ev[2].priority) << 10 |
+				sched_type[2] << 8 |
+				queue_id[2];
+		sched_word[3] = EV_TO_DLB2_PRIO(ev[3].priority) << 10 |
+				sched_type[3] << 8 |
+				queue_id[3];
+
+		/* Store the event priority, scheduling type, and queue ID in
+		 * the metadata:
+		 * sse_qe[0][31:16] = sched_word[0]
+		 * sse_qe[0][95:80] = sched_word[1]
+		 * sse_qe[1][31:16] = sched_word[2]
+		 * sse_qe[1][95:80] = sched_word[3]
+		 */
+#define DLB2_QE_QID_SCHED_WORD 1
+		sse_qe[0] = _mm_insert_epi16(sse_qe[0],
+					     sched_word[0],
+					     DLB2_QE_QID_SCHED_WORD);
+		sse_qe[0] = _mm_insert_epi16(sse_qe[0],
+					     sched_word[1],
+					     DLB2_QE_QID_SCHED_WORD + 4);
+		sse_qe[1] = _mm_insert_epi16(sse_qe[1],
+					     sched_word[2],
+					     DLB2_QE_QID_SCHED_WORD);
+		sse_qe[1] = _mm_insert_epi16(sse_qe[1],
+					     sched_word[3],
+					     DLB2_QE_QID_SCHED_WORD + 4);
+
+		/* If the destination is a load-balanced queue, store the lock
+		 * ID. If it is a directed queue, DLB places this field in
+		 * bytes 10-11 of the received QE, so we format it accordingly:
+		 * sse_qe[0][47:32]  = dir queue ? sched_word[0] : flow_id[0]
+		 * sse_qe[0][111:96] = dir queue ? sched_word[1] : flow_id[1]
+		 * sse_qe[1][47:32]  = dir queue ? sched_word[2] : flow_id[2]
+		 * sse_qe[1][111:96] = dir queue ? sched_word[3] : flow_id[3]
+		 */
+#define DLB2_QE_LOCK_ID_WORD 2
+		sse_qe[0] = _mm_insert_epi16(sse_qe[0],
+				(sched_type[0] == DLB2_SCHED_DIRECTED) ?
+					sched_word[0] : ev[0].flow_id,
+				DLB2_QE_LOCK_ID_WORD);
+		sse_qe[0] = _mm_insert_epi16(sse_qe[0],
+				(sched_type[1] == DLB2_SCHED_DIRECTED) ?
+					sched_word[1] : ev[1].flow_id,
+				DLB2_QE_LOCK_ID_WORD + 4);
+		sse_qe[1] = _mm_insert_epi16(sse_qe[1],
+				(sched_type[2] == DLB2_SCHED_DIRECTED) ?
+					sched_word[2] : ev[2].flow_id,
+				DLB2_QE_LOCK_ID_WORD);
+		sse_qe[1] = _mm_insert_epi16(sse_qe[1],
+				(sched_type[3] == DLB2_SCHED_DIRECTED) ?
+					sched_word[3] : ev[3].flow_id,
+				DLB2_QE_LOCK_ID_WORD + 4);
+
+		/* Store the event type and sub event type in the metadata:
+		 * sse_qe[0][15:0]  = flow_id[0]
+		 * sse_qe[0][79:64] = flow_id[1]
+		 * sse_qe[1][15:0]  = flow_id[2]
+		 * sse_qe[1][79:64] = flow_id[3]
+		 */
+#define DLB2_QE_EV_TYPE_WORD 0
+		sse_qe[0] = _mm_insert_epi16(sse_qe[0],
+					     ev[0].sub_event_type << 4 |
+						ev[0].event_type << 12,
+					     DLB2_QE_EV_TYPE_WORD);
+		sse_qe[0] = _mm_insert_epi16(sse_qe[0],
+					     ev[1].sub_event_type << 4 |
+						ev[1].event_type << 12,
+					     DLB2_QE_EV_TYPE_WORD + 4);
+		sse_qe[1] = _mm_insert_epi16(sse_qe[1],
+					     ev[2].sub_event_type << 4 |
+						ev[2].event_type << 12,
+					     DLB2_QE_EV_TYPE_WORD);
+		sse_qe[1] = _mm_insert_epi16(sse_qe[1],
+					     ev[3].sub_event_type << 4 |
+						ev[3].event_type << 12,
+					     DLB2_QE_EV_TYPE_WORD + 4);
+#ifdef __AVX512VL__
+    /*
+    1) Build avx512 QE store and build each QE individualy as XMM register
+    2) Merge the 4 XMM registers/QEs into single AVX512 register
+    3) Store single avx512 register to &qe[0] (4x QEs stored in 1x store)
+    */
+
+      __m128i v_qe0 = _mm_setzero_si128();
+      uint64_t meta = _mm_extract_epi64(sse_qe[0], 0);
+      v_qe0 = _mm_insert_epi64(v_qe0, ev[0].u64, 0);
+      v_qe0 = _mm_insert_epi64(v_qe0, meta, 1);
+
+      __m128i v_qe1 = _mm_setzero_si128();
+      meta = _mm_extract_epi64(sse_qe[0], 1);
+      v_qe1 = _mm_insert_epi64(v_qe1, ev[1].u64, 0);
+      v_qe1 = _mm_insert_epi64(v_qe1, meta, 1);
+
+     __m128i v_qe2 = _mm_setzero_si128();
+      meta = _mm_extract_epi64(sse_qe[1], 0);
+      v_qe2 = _mm_insert_epi64(v_qe2, ev[2].u64, 0);
+      v_qe2 = _mm_insert_epi64(v_qe2, meta, 1);
+
+      __m128i v_qe3 = _mm_setzero_si128();
+      meta = _mm_extract_epi64(sse_qe[1], 1);
+      v_qe3 = _mm_insert_epi64(v_qe3, ev[3].u64, 0);
+      v_qe3 = _mm_insert_epi64(v_qe3, meta, 1);
+
+      /* we have 4x XMM registers, one per QE. */
+      __m512i v_all_qes = _mm512_setzero_si512();
+      v_all_qes = _mm512_inserti32x4 (v_all_qes, v_qe0, 0);
+      v_all_qes = _mm512_inserti32x4 (v_all_qes, v_qe1, 1);
+      v_all_qes = _mm512_inserti32x4 (v_all_qes, v_qe2, 2);
+      v_all_qes = _mm512_inserti32x4 (v_all_qes, v_qe3, 3);
+
+     /* store the 4x QEs in a single register to the scratch space of the PMD */
+      _mm512_store_si512(&qe[0], v_all_qes);
+#else
+		/* Store the metadata to memory (use the double-precision
+		 * _mm_storeh_pd because there is no integer function for
+		 * storing the upper 64b):
+		 * qe[0] metadata = sse_qe[0][63:0]
+		 * qe[1] metadata = sse_qe[0][127:64]
+		 * qe[2] metadata = sse_qe[1][63:0]
+		 * qe[3] metadata = sse_qe[1][127:64]
+		 */
+		_mm_storel_epi64((__m128i *)&qe[0].u.opaque_data, sse_qe[0]);
+		_mm_storeh_pd((double *)&qe[1].u.opaque_data,
+			      (__m128d)sse_qe[0]);
+		_mm_storel_epi64((__m128i *)&qe[2].u.opaque_data, sse_qe[1]);
+		_mm_storeh_pd((double *)&qe[3].u.opaque_data,
+			      (__m128d)sse_qe[1]);
+
+		qe[0].data = ev[0].u64;
+		qe[1].data = ev[1].u64;
+		qe[2].data = ev[2].u64;
+		qe[3].data = ev[3].u64;
+
+#endif
+		/* will only be set for DLB 2.5 + */
+		if (qm_port->dlb2->enable_cq_weight) {
+			qe[0].weight = DLB2_GET_QE_WEIGHT(&ev[0]);
+			qe[1].weight = DLB2_GET_QE_WEIGHT(&ev[1]);
+			qe[2].weight = DLB2_GET_QE_WEIGHT(&ev[2]);
+			qe[3].weight = DLB2_GET_QE_WEIGHT(&ev[3]);
+		}
+
+		break;
+	case 3:
+	case 2:
+	case 1:
+		for (i = 0; i < num; i++) {
+			qe[i].cmd_byte =
+				cmd_byte_map[qm_port->is_directed][ev[i].op];
+			qe[i].sched_type = sched_type[i];
+			qe[i].data = ev[i].u64;
+			qe[i].qid = queue_id[i];
+			qe[i].priority = EV_TO_DLB2_PRIO(ev[i].priority);
+			qe[i].lock_id = ev[i].flow_id;
+			if (sched_type[i] == DLB2_SCHED_DIRECTED) {
+				struct dlb2_msg_info *info =
+					(struct dlb2_msg_info *)&qe[i].lock_id;
+
+				info->qid = queue_id[i];
+				info->sched_type = DLB2_SCHED_DIRECTED;
+				info->priority = qe[i].priority;
+			}
+			qe[i].u.event_type.major = ev[i].event_type;
+			qe[i].u.event_type.sub = ev[i].sub_event_type;
+			qe[i].weight = DLB2_GET_QE_WEIGHT(&ev[i]);
+		}
+		break;
+	case 0:
+		break;
+	}
+}
+
+
 static inline int
 dlb2_event_enqueue_prep(struct dlb2_eventdev_port *ev_port,
 			struct dlb2_port *qm_port,
@@ -2901,7 +3703,9 @@ dlb2_event_enqueue_prep(struct dlb2_eventdev_port *ev_port,
 {
 	struct dlb2_eventdev *dlb2 = ev_port->dlb2;
 	struct dlb2_eventdev_queue *ev_queue;
+#if DLB_HW_CREDITS_CHECKS
 	uint16_t *cached_credits = NULL;
+#endif
 	struct dlb2_queue *qm_queue;
 
 	ev_queue = &dlb2->ev_queues[ev->queue_id];
@@ -2913,6 +3717,7 @@ dlb2_event_enqueue_prep(struct dlb2_eventdev_port *ev_port,
 		goto op_check;
 
 	if (!qm_queue->is_directed) {
+#if DLB_HW_CREDITS_CHECKS
 		/* Load balanced destination queue */
 
 		if (dlb2->version == DLB2_HW_V2) {
@@ -2928,6 +3733,8 @@ dlb2_event_enqueue_prep(struct dlb2_eventdev_port *ev_port,
 			}
 			cached_credits = &qm_port->cached_credits;
 		}
+#endif
+#if DLB_TYPE_CHECK
 		switch (ev->sched_type) {
 		case RTE_SCHED_TYPE_ORDERED:
 			DLB2_LOG_DBG("dlb2: put_qe: RTE_SCHED_TYPE_ORDERED\n");
@@ -2941,13 +3748,15 @@ dlb2_event_enqueue_prep(struct dlb2_eventdev_port *ev_port,
 			break;
 		case RTE_SCHED_TYPE_ATOMIC:
 			DLB2_LOG_DBG("dlb2: put_qe: RTE_SCHED_TYPE_ATOMIC\n");
+			/* No check as Atomic events are allowed on any queue type */
 			*sched_type = DLB2_SCHED_ATOMIC;
 			break;
 		case RTE_SCHED_TYPE_PARALLEL:
 			DLB2_LOG_DBG("dlb2: put_qe: RTE_SCHED_TYPE_PARALLEL\n");
-			if (qm_queue->sched_type == RTE_SCHED_TYPE_ORDERED)
+			/* Allow Parallel events on ordered queue to support CFG_ALL_TYPEs */
+			if (qm_queue->sched_type == RTE_SCHED_TYPE_ORDERED) 
 				*sched_type = DLB2_SCHED_ORDERED;
-			else
+		  else	
 				*sched_type = DLB2_SCHED_UNORDERED;
 			break;
 		default:
@@ -2956,9 +3765,20 @@ dlb2_event_enqueue_prep(struct dlb2_eventdev_port *ev_port,
 			rte_errno = -EINVAL;
 			return 1;
 		}
+#else
+#if (RTE_SCHED_TYPE_PARALLEL != 2) || (RTE_SCHED_TYPE_ATOMIC != 1)
+#error "ERROR: RTE event schedule type values changed. Needs a code change"
+#endif
+		/* Map RTE eventdev schedule type to DLB HW schedule type */
+		if (qm_queue->sched_type != RTE_SCHED_TYPE_ORDERED)
+			/* RTE-Parallel -> DLB-UnOrd 2->1, RTE-Atm -> DLB-Atm 1->0 */
+			*sched_type = ev->sched_type - 1;
+		else /* To support CFG_ALL_TYPEs */
+			*sched_type = DLB2_SCHED_ORDERED; /* RTE-Ord -> DLB-Ord 0->2 */
+#endif
 	} else {
 		/* Directed destination queue */
-
+#if DLB_HW_CREDITS_CHECKS
 		if (dlb2->version == DLB2_HW_V2) {
 			if (dlb2_check_enqueue_hw_dir_credits(qm_port)) {
 				rte_errno = -ENOSPC;
@@ -2972,6 +3792,7 @@ dlb2_event_enqueue_prep(struct dlb2_eventdev_port *ev_port,
 			}
 			cached_credits = &qm_port->cached_credits;
 		}
+#endif
 		DLB2_LOG_DBG("dlb2: put_qe: RTE_SCHED_TYPE_DIRECTED\n");
 
 		*sched_type = DLB2_SCHED_DIRECTED;
@@ -2980,13 +3801,17 @@ dlb2_event_enqueue_prep(struct dlb2_eventdev_port *ev_port,
 op_check:
 	switch (ev->op) {
 	case RTE_EVENT_OP_NEW:
+#if DLB_SW_CREDITS_CHECKS
 		/* Check that a sw credit is available */
 		if (dlb2_check_enqueue_sw_credits(dlb2, ev_port)) {
 			rte_errno = -ENOSPC;
 			return 1;
 		}
 		ev_port->inflight_credits--;
+#endif
+#if DLB_HW_CREDITS_CHECKS
 		(*cached_credits)--;
+#endif
 		break;
 	case RTE_EVENT_OP_FORWARD:
 		/* Check for outstanding_releases underflow. If this occurs,
@@ -2995,12 +3820,25 @@ dlb2_event_enqueue_prep(struct dlb2_eventdev_port *ev_port,
 		 * dequeued.
 		 */
 		RTE_ASSERT(ev_port->outstanding_releases > 0);
+		if (dlb2->version == DLB2_HW_V2_5 &&
+		    !(ev_port->conf.event_port_cfg & RTE_EVENT_PORT_CFG_RESTORE_DEQ_ORDER)) {
+			if (unlikely(qm_port->pending_frags == DLB2_MAX_FRAGS)) {
+				rte_errno = -EINVAL;
+				DLB2_LOG_ERR("dlb2: >16 frags/port\n");
+				return 1;
+			}
+			qm_port->pending_frags = 0;
+		}
 		ev_port->outstanding_releases--;
 		qm_port->issued_releases++;
+#if DLB_HW_CREDITS_CHECKS
 		(*cached_credits)--;
+#endif
 		break;
 	case RTE_EVENT_OP_RELEASE:
+#if DLB_SW_CREDITS_CHECKS
 		ev_port->inflight_credits++;
+#endif
 		/* Check for outstanding_releases underflow. If this occurs,
 		 * the application is not using the EVENT_OPs correctly; for
 		 * example, forwarding or releasing events that were not
@@ -3010,15 +3848,45 @@ dlb2_event_enqueue_prep(struct dlb2_eventdev_port *ev_port,
 		ev_port->outstanding_releases--;
 		qm_port->issued_releases++;
 
+#if DLB_SW_CREDITS_CHECKS
 		/* Replenish s/w credits if enough are cached */
 		dlb2_replenish_sw_credits(dlb2, ev_port);
+#endif
+		break;
+	/* Fragments not supported in the API, but left here for
+	 * possible future use.
+	 */
+	case RTE_EVENT_DLB2_OP_FRAG:
+		/* not supported in 2.0 */
+		RTE_ASSERT(dlb2->version == DLB2_HW_V2_5);
+#if DLB_SW_CREDITS_CHECKS
+		/* Check that a sw credit is available */
+		if (dlb2_check_enqueue_sw_credits(dlb2, ev_port)) {
+			rte_errno = -ENOSPC;
+			return 1;
+		}
+#endif
+		if (!(ev_port->conf.event_port_cfg & RTE_EVENT_PORT_CFG_RESTORE_DEQ_ORDER)) {
+			if (unlikely(qm_port->pending_frags == DLB2_MAX_FRAGS)) {
+				rte_errno = -EINVAL;
+				DLB2_LOG_ERR("dlb2: >16 frags/port\n");
+				return 1;
+			}
+			qm_port->pending_frags++;
+		}
+#if DLB_SW_CREDITS_CHECKS
+		ev_port->inflight_credits--;
+#endif
+#if DLB_HW_CREDITS_CHECKS
+		(*cached_credits)--;
+#endif
 		break;
 	}
 
 	DLB2_INC_STAT(ev_port->stats.tx_op_cnt[ev->op], 1);
 	DLB2_INC_STAT(ev_port->stats.traffic.tx_ok, 1);
 
-#ifndef RTE_LIBRTE_PMD_DLB_QUELL_STATS
+#ifndef RTE_LIBRTE_PMD_DLB2_QUELL_STATS
 	if (ev->op != RTE_EVENT_OP_RELEASE) {
 		DLB2_INC_STAT(ev_port->stats.queue[ev->queue_id].enq_ok, 1);
 		DLB2_INC_STAT(ev_port->stats.tx_sched_cnt[*sched_type], 1);
@@ -3028,6 +3896,196 @@ dlb2_event_enqueue_prep(struct dlb2_eventdev_port *ev_port,
 	return 0;
 }
 
+static inline __m128i
+dlb2_event_to_qe(const struct rte_event *ev, uint8_t cmd, uint8_t sched_type, uint8_t qid)
+{
+	__m128i dlb2_to_qe_shuffle = _mm_set_epi8(
+	    0xFF, 0xFF,			 /* zero out cmd word */
+	    1, 0,			 /* low 16-bits of flow id */
+	    0xFF, 0xFF, /* zero QID, sched_type etc fields to be filled later */
+	    3, 2,			 /* top of flow id, event type and subtype */
+	    15, 14, 13, 12, 11, 10, 9, 8 /* data from end of event goes at start */
+	);
+
+	/* event may not be 16 byte aligned. Use 16 byte unaligned load */
+	__m128i tmp = _mm_lddqu_si128((const __m128i *)ev);
+	__m128i qe = _mm_shuffle_epi8(tmp, dlb2_to_qe_shuffle);
+	struct dlb2_enqueue_qe *dq = (struct dlb2_enqueue_qe *)&qe;
+	/* set the cmd field */
+	qe = _mm_insert_epi8(qe, cmd, 15);
+	/* insert missing 16-bits with qid, sched_type and priority */
+	uint16_t qid_stype_prio =
+	    qid | (uint16_t)sched_type << 8 | ((uint16_t)ev->priority & 0xE0) << 5;
+	qe = _mm_insert_epi16(qe, qid_stype_prio, 5);
+	dq->weight = DLB2_GET_QE_WEIGHT(ev);
+	return qe;
+}
+
+static inline uint16_t
+__dlb2_event_enqueue_burst_reorder(void *event_port,
+		const struct rte_event events[],
+		uint16_t num,
+		bool use_delayed)
+{
+	struct dlb2_eventdev_port *ev_port = event_port;
+	struct dlb2_port *qm_port = &ev_port->qm_port;
+	struct process_local_port_data *port_data;
+	bool is_directed = qm_port->is_directed;
+	uint8_t n = qm_port->next_to_enqueue;
+	int retries = ev_port->enq_retries;
+	uint8_t frag = 0, p_cnt = 0;
+	__m128i new_qes[4], *from;
+	int num_new = 0;
+	int num_tx;
+	int i;
+
+	port_data = &dlb2_port[qm_port->evdev_id][qm_port->id]
+			      [PORT_TYPE(qm_port)];
+
+	if (!port_data->mmaped)
+		dlb2_iface_port_mmap(qm_port);
+
+	num_tx = RTE_MIN(num, ev_port->conf.enqueue_depth);
+#if DLB2_BYPASS_FENCE_ON_PP == 1
+	if (!qm_port->is_producer) /* Call memory fense once at the start */
+		rte_wmb();	   /*  calls _mm_sfence() */
+#else
+	rte_wmb(); /*  calls _mm_sfence() */
+#endif
+	for (i = 0; i < num_tx; i++) {
+		uint8_t sched_type = 0;
+		uint8_t reorder_idx = events[i].impl_opaque;
+		int16_t thresh = qm_port->token_pop_thresh;
+		uint8_t qid = 0;
+		int ret;
+
+		while ((ret = dlb2_event_enqueue_prep(ev_port, qm_port, &events[i],
+						      &sched_type, &qid)) != 0 &&
+		       rte_errno == -ENOSPC && --retries > 0)
+			rte_pause();
+
+		if (ret != 0) /* Either there is error or retires exceeded */
+			break;
+
+		switch (events[i].op) {
+		case RTE_EVENT_OP_NEW:
+			new_qes[num_new++] = dlb2_event_to_qe(
+			    &events[i], DLB2_NEW_CMD_BYTE, sched_type, qid);
+			if (num_new == RTE_DIM(new_qes)) {
+				dlb2_pp_write(port_data, (struct dlb2_enqueue_qe *)&new_qes);
+				num_new = 0;
+			}
+			break;
+		case RTE_EVENT_OP_FORWARD: {
+			qm_port->enq_reorder[reorder_idx].m128 = dlb2_event_to_qe(
+			    &events[i], is_directed ? DLB2_NEW_CMD_BYTE : DLB2_FWD_CMD_BYTE,
+			    sched_type, qid);
+			if (!qm_port->pending_frags)
+				n += dlb2_pp_check4_write(port_data, &qm_port->enq_reorder[n].qe);
+			break;
+		}
+		case RTE_EVENT_DLB2_OP_FRAG: {
+			uint8_t frag_index = qm_port->frag_reorder[reorder_idx].num_frags++;
+
+			if (unlikely(frag_index >= DLB2_MAX_FRAGS - 1)) {
+				rte_errno = -EINVAL;
+				DLB2_LOG_ERR("dlb2: >16 frags/event\n");
+				printf("dlb2: >16 frags/event\n");
+				break;
+			}
+			qm_port->frag_reorder[reorder_idx].enq[frag_index].m128 = dlb2_event_to_qe(
+			    &events[i], DLB2_FRAG_CMD_BYTE, sched_type, qid);
+			qm_port->pending_frags++;
+			break;
+		}
+		case RTE_EVENT_OP_RELEASE: {
+			qm_port->enq_reorder[reorder_idx].m128 = dlb2_event_to_qe(
+			    &events[i], is_directed ? DLB2_NOOP_CMD_BYTE : DLB2_COMP_CMD_BYTE,
+			    sched_type, 0xFF);
+			break;
+		}
+		}
+
+		if (use_delayed && qm_port->token_pop_mode == DELAYED_POP &&
+		    (events[i].op == RTE_EVENT_OP_FORWARD ||
+		     events[i].op == RTE_EVENT_OP_RELEASE) &&
+		    qm_port->issued_releases >= thresh - 1) {
+
+			dlb2_consume_qe_immediate(qm_port, qm_port->owed_tokens);
+
+			/* Reset the releases for the next QE batch */
+			qm_port->issued_releases -= thresh;
+
+			/* When using delayed token pop mode, the
+			 * initial token threshold is the full CQ
+			 * depth. After the first token pop, we need to
+			 * reset it to the dequeue_depth.
+			 */
+			qm_port->token_pop_thresh =
+			    qm_port->dequeue_depth;
+		}
+	}
+	while (qm_port->enq_reorder[n].u64[1] != 0) {
+		bool p_frags = !!qm_port->pending_frags;
+		__m128i tmp[4] = {0}, *send;
+
+		if (!p_frags || !qm_port->frag_reorder[n].num_frags) {
+			bool enq;
+			if (!p_cnt)
+				from = &qm_port->enq_reorder[n].m128;
+			frag = 0;
+			p_cnt++;
+			n++;
+			/* Send current accumulated events if any of these is true:
+			 *  - going to wraparound
+			 *  - 4 consecutive events have been accumulated
+			 *  - last event
+			 *  - if frags are pending and next event has frags
+			 */
+			enq = !n || p_cnt == 4 || !qm_port->enq_reorder[n].u64[1] ||
+			      (p_frags && qm_port->frag_reorder[n].num_frags);
+			if (!enq)
+				continue;
+		} else {
+			p_cnt = RTE_MIN(qm_port->frag_reorder[n].num_frags, 4);
+			from = &qm_port->frag_reorder[n].enq[frag].m128;
+			frag += p_cnt;
+			qm_port->frag_reorder[n].num_frags -= p_cnt;
+			qm_port->pending_frags -= p_cnt;
+		}
+
+		if (p_cnt < 4) {
+			memcpy(tmp, from, p_cnt * sizeof(struct dlb2_enqueue_qe));
+			send = tmp;
+		} else {
+			send  = from;
+		}
+		if (is_directed)
+			dlb2_pp_write_reorder(port_data, (struct dlb2_enqueue_qe *)send);
+		else
+			dlb2_pp_write(port_data, (struct dlb2_enqueue_qe *)send);
+		memset(from, 0, p_cnt * sizeof(struct dlb2_enqueue_qe));
+		p_cnt = 0;
+	}
+	qm_port->next_to_enqueue = n;
+
+	if (num_new > 0) {
+		switch (num_new) {
+		case 1:
+			new_qes[1] = _mm_setzero_si128(); /* fall-through */
+		case 2:
+			new_qes[2] = _mm_setzero_si128(); /* fall-through */
+		case 3:
+			new_qes[3] = _mm_setzero_si128();
+		}
+		dlb2_pp_write(port_data, (struct dlb2_enqueue_qe *)&new_qes);
+		num_new = 0;
+	}
+
+	dlb2_check_and_return_credits(ev_port, !i, DLB2_ENQ_FAIL_CREDIT_RETURN_THRES);
+	return i;
+}
+
 static inline uint16_t
 __dlb2_event_enqueue_burst(void *event_port,
 			   const struct rte_event events[],
@@ -3044,9 +4102,17 @@ __dlb2_event_enqueue_burst(void *event_port,
 	RTE_ASSERT(ev_port->enq_configured);
 	RTE_ASSERT(events != NULL);
 
+	if (ev_port->conf.event_port_cfg & RTE_EVENT_PORT_CFG_RESTORE_DEQ_ORDER)
+		return __dlb2_event_enqueue_burst_reorder(event_port, events, num, use_delayed);
+
 	i = 0;
 
-	port_data = &dlb2_port[qm_port->id][PORT_TYPE(qm_port)];
+	port_data = &dlb2_port[qm_port->evdev_id][qm_port->id]
+			      [PORT_TYPE(qm_port)];
+
+	if (!port_data->mmaped)
+		dlb2_iface_port_mmap(qm_port);
+
 	num_tx = RTE_MIN(num, ev_port->conf.enqueue_depth);
 	while (i < num_tx) {
 		uint8_t sched_types[DLB2_NUM_QES_PER_CACHE_LINE];
@@ -3079,6 +4145,14 @@ __dlb2_event_enqueue_burst(void *event_port,
 				/* Reset the releases for the next QE batch */
 				qm_port->issued_releases -= thresh;
 
+				/* When using delayed token pop mode, the
+				 * initial token threshold is the full CQ
+				 * depth. After the first token pop, we need to
+				 * reset it to the dequeue_depth.
+				 */
+				qm_port->token_pop_thresh =
+					qm_port->dequeue_depth;
+
 				pop_offs = 1;
 				j++;
 				break;
@@ -3104,12 +4178,11 @@ __dlb2_event_enqueue_burst(void *event_port,
 
 		dlb2_event_build_hcws(qm_port, &events[i], j - pop_offs,
 				      sched_types, queue_ids);
-
 #if DLB2_BYPASS_FENCE_ON_PP == 1
-		/* Bypass fence instruction for producer ports */
-		dlb2_hw_do_enqueue(qm_port, i == 0 && !qm_port->is_producer, port_data);
+                /* Bypass fence instruction for producer ports */
+                dlb2_hw_do_enqueue(qm_port, i == 0 && !qm_port->is_producer, port_data);
 #else
-		dlb2_hw_do_enqueue(qm_port, i == 0, port_data);
+                dlb2_hw_do_enqueue(qm_port, i == 0, port_data);
 #endif
 
 		/* Don't include the token pop QE in the enqueue count */
@@ -3122,6 +4195,8 @@ __dlb2_event_enqueue_burst(void *event_port,
 			break;
 	}
 
+	dlb2_check_and_return_credits(ev_port, !i,
+				      DLB2_ENQ_FAIL_CREDIT_RETURN_THRES);
 	return i;
 }
 
@@ -3206,7 +4281,8 @@ dlb2_event_release(struct dlb2_eventdev *dlb2,
 
 	ev_port = &dlb2->ev_ports[port_id];
 	qm_port = &ev_port->qm_port;
-	port_data = &dlb2_port[qm_port->id][PORT_TYPE(qm_port)];
+	port_data = &dlb2_port[qm_port->evdev_id][qm_port->id]
+			      [PORT_TYPE(qm_port)];
 
 	i = 0;
 
@@ -3225,7 +4301,6 @@ dlb2_event_release(struct dlb2_eventdev *dlb2,
 		_mm_storeu_si128((void *)&qm_port->qe4[2], _mm_setzero_si128());
 		_mm_storeu_si128((void *)&qm_port->qe4[3], _mm_setzero_si128());
 
-
 		for (; j < DLB2_NUM_QES_PER_CACHE_LINE && (i + j) < n; j++) {
 			int16_t thresh = qm_port->token_pop_thresh;
 
@@ -3237,6 +4312,14 @@ dlb2_event_release(struct dlb2_eventdev *dlb2,
 				/* Reset the releases for the next QE batch */
 				qm_port->issued_releases -= thresh;
 
+				/* When using delayed token pop mode, the
+				 * initial token threshold is the full CQ
+				 * depth. After the first token pop, we need to
+				 * reset it to the dequeue_depth.
+				 */
+				qm_port->token_pop_thresh =
+					qm_port->dequeue_depth;
+
 				pop_offs = 1;
 				j++;
 				break;
@@ -3260,62 +4343,110 @@ dlb2_event_release(struct dlb2_eventdev *dlb2,
 		return;
 	}
 	ev_port->outstanding_releases -= i;
+#if DLB_SW_CREDITS_CHECKS
 	ev_port->inflight_credits += i;
 
 	/* Replenish s/w credits if enough releases are performed */
 	dlb2_replenish_sw_credits(dlb2, ev_port);
+#endif
 }
 
 static inline void
 dlb2_port_credits_inc(struct dlb2_port *qm_port, int num)
 {
 	uint32_t batch_size = qm_port->hw_credit_quanta;
+	int val;
 
 	/* increment port credits, and return to pool if exceeds threshold */
-	if (!qm_port->is_directed) {
-		if (qm_port->dlb2->version == DLB2_HW_V2) {
-			qm_port->cached_ldb_credits += num;
-			if (qm_port->cached_ldb_credits >= 2 * batch_size) {
-				__atomic_fetch_add(
-					qm_port->credit_pool[DLB2_LDB_QUEUE],
-					batch_size, __ATOMIC_SEQ_CST);
-				qm_port->cached_ldb_credits -= batch_size;
-			}
-		} else {
-			qm_port->cached_credits += num;
-			if (qm_port->cached_credits >= 2 * batch_size) {
-				__atomic_fetch_add(
-				      qm_port->credit_pool[DLB2_COMBINED_POOL],
-				      batch_size, __ATOMIC_SEQ_CST);
-				qm_port->cached_credits -= batch_size;
-			}
+	if (qm_port->dlb2->version == DLB2_HW_V2_5) {
+		qm_port->cached_credits += num;
+		if (qm_port->cached_credits >= 2 * batch_size) {
+			val = qm_port->cached_credits - batch_size;
+			__atomic_fetch_add(
+			    qm_port->credit_pool[DLB2_COMBINED_POOL], val,
+			    __ATOMIC_SEQ_CST);
+			qm_port->cached_credits -= val;
+		}
+	} else if (!qm_port->is_directed) {
+		qm_port->cached_ldb_credits += num;
+		if (qm_port->cached_ldb_credits >= 2 * batch_size) {
+			val = qm_port->cached_ldb_credits - batch_size;
+			__atomic_fetch_add(qm_port->credit_pool[DLB2_LDB_QUEUE],
+					   val, __ATOMIC_SEQ_CST);
+			qm_port->cached_ldb_credits -= val;
 		}
 	} else {
-		if (qm_port->dlb2->version == DLB2_HW_V2) {
-			qm_port->cached_dir_credits += num;
-			if (qm_port->cached_dir_credits >= 2 * batch_size) {
-				__atomic_fetch_add(
-					qm_port->credit_pool[DLB2_DIR_QUEUE],
-					batch_size, __ATOMIC_SEQ_CST);
-				qm_port->cached_dir_credits -= batch_size;
-			}
-		} else {
-			qm_port->cached_credits += num;
-			if (qm_port->cached_credits >= 2 * batch_size) {
-				__atomic_fetch_add(
-				      qm_port->credit_pool[DLB2_COMBINED_POOL],
-				      batch_size, __ATOMIC_SEQ_CST);
-				qm_port->cached_credits -= batch_size;
-			}
+		qm_port->cached_dir_credits += num;
+		if (qm_port->cached_dir_credits >= 2 * batch_size) {
+			val = qm_port->cached_dir_credits - batch_size;
+			__atomic_fetch_add(qm_port->credit_pool[DLB2_DIR_QUEUE],
+					   val, __ATOMIC_SEQ_CST);
+			qm_port->cached_dir_credits -= val;
 		}
 	}
 }
 
+static inline void
+dlb2_issue_int_arm_hcw(struct dlb2_port *qm_port)
+{
+	struct process_local_port_data *port_data;
+	struct dlb2_enqueue_qe *qe;
+
+	RTE_ASSERT(qm_port->config_state == DLB2_CONFIGURED);
+
+	qe = qm_port->int_arm_qe;
+
+	/* No store fence needed since no pointer is being sent */
+
+	port_data = &dlb2_port[qm_port->evdev_id][qm_port->id]
+			      [PORT_TYPE(qm_port)];
+
+	dlb2_pp_write(port_data, qe);
+
+	DLB2_LOG_DBG("dlb2: issued interrupt_arm QE\n");
+
+	qm_port->int_armed = true;
+}
+
+static inline int
+dlb2_block_on_cq_interrupt(struct dlb2_hw_dev *handle,
+			   struct dlb2_port *qm_port,
+			   struct dlb2_eventdev_port *ev_port)
+{
+	struct process_local_port_data *port_data;
+	int ret;
+
+	if (!qm_port->int_armed)
+		dlb2_issue_int_arm_hcw(qm_port);
+
+	port_data = &dlb2_port[qm_port->evdev_id][qm_port->id]
+			      [PORT_TYPE(qm_port)];
+
+	/* Note: it's safe to access the per-process cq_base address here,
+	 * since the PMD won't block on the CQ until after attempting at least
+	 * one CQ dequeue.
+	 */
+	ret = dlb2_iface_block_on_cq_interrupt(handle,
+					qm_port->id,
+					!ev_port->qm_port.is_directed,
+					&port_data->cq_base[qm_port->cq_idx],
+					qm_port->gen_bit,
+					false);
+
+	if (ret == EPERM)
+		DLB2_LOG_DBG("dlb2: Not enough interupts available (for VF)\n");
+
+	/* If the CQ int ioctl was unsuccessful, the interrupt remains armed */
+	qm_port->int_armed = (ret != 0);
+
+	return ret;
+}
+
 #define CLB_MASK_IDX 0
 #define CLB_VAL_IDX 1
 static int
 dlb2_monitor_callback(const uint64_t val,
-		const uint64_t opaque[RTE_POWER_MONITOR_OPAQUE_SZ])
+		      const uint64_t opaque[RTE_POWER_MONITOR_OPAQUE_SZ])
 {
 	/* abort if the value matches */
 	return (val & opaque[CLB_MASK_IDX]) == opaque[CLB_VAL_IDX] ? -1 : 0;
@@ -3328,19 +4459,47 @@ dlb2_dequeue_wait(struct dlb2_eventdev *dlb2,
 		  uint64_t timeout,
 		  uint64_t start_ticks)
 {
+	struct dlb2_hw_dev *handle = &dlb2->qm_instance;
 	struct process_local_port_data *port_data;
 	uint64_t elapsed_ticks;
 
-	port_data = &dlb2_port[qm_port->id][PORT_TYPE(qm_port)];
+	port_data = &dlb2_port[qm_port->evdev_id][qm_port->id]
+			      [PORT_TYPE(qm_port)];
 
 	elapsed_ticks = rte_get_timer_cycles() - start_ticks;
 
 	/* Wait/poll time expired */
 	if (elapsed_ticks >= timeout) {
+		if (dlb2->run_state == DLB2_RUN_STATE_STARTED &&
+		    qm_port->dequeue_wait == DLB2_PORT_DEQUEUE_WAIT_INTERRUPT) {
+			/* Return all credits before blocking if remaining credits in
+			 * system is less than quanta.
+			 */
+			uint32_t sw_inflights = __atomic_load_n(&dlb2->inflights, __ATOMIC_SEQ_CST);
+			uint32_t quanta = ev_port->credit_update_quanta;
+
+			if (dlb2->new_event_limit - sw_inflights < quanta)
+				dlb2_check_and_return_credits(ev_port, true, 0);
+			/* Block on CQ interrupt. */
+			int ret = dlb2_block_on_cq_interrupt(handle,
+							     qm_port,
+							     ev_port);
+			if (ret != 0) {
+				DLB2_LOG_DBG("dlb2: wait for interrupt ret=%d\n",
+					     ret);
+				rte_errno = ret;
+				return 1;
+			}
+
+			DLB2_INC_STAT(ev_port->stats.traffic.rx_interrupt_wait,
+				      1);
+			return 0;
+		}
 		return 1;
-	} else if (dlb2->umwait_allowed) {
-		struct rte_power_monitor_cond pmc;
+	} else if (dlb2->umwait_allowed &&
+		   (qm_port->dequeue_wait == DLB2_PORT_DEQUEUE_WAIT_UMWAIT)) {
 		volatile struct dlb2_dequeue_qe *cq_base;
+		struct rte_power_monitor_cond pmc;
 		union {
 			uint64_t raw_qe[2];
 			struct dlb2_dequeue_qe qe;
@@ -3396,7 +4555,7 @@ dlb2_process_dequeue_qes(struct dlb2_eventdev_port *ev_port,
 
 	for (i = 0, num = 0; i < cnt; i++) {
 		struct dlb2_dequeue_qe *qe = &qes[i];
-		int sched_type_map[DLB2_NUM_HW_SCHED_TYPES] = {
+		static int sched_type_map[DLB2_NUM_HW_SCHED_TYPES] = {
 			[DLB2_SCHED_ATOMIC] = RTE_SCHED_TYPE_ATOMIC,
 			[DLB2_SCHED_UNORDERED] = RTE_SCHED_TYPE_PARALLEL,
 			[DLB2_SCHED_ORDERED] = RTE_SCHED_TYPE_ORDERED,
@@ -3421,7 +4580,8 @@ dlb2_process_dequeue_qes(struct dlb2_eventdev_port *ev_port,
 		events[num].event_type = qe->u.event_type.major;
 		events[num].sub_event_type = qe->u.event_type.sub;
 		events[num].sched_type = sched_type_map[qe->sched_type];
-		events[num].impl_opaque = qe->qid_depth;
+		events[num].impl_opaque = qm_port->reorder_id++;
+		DLB2_SET_QID_DEPTH(&events[num], qe->qid_depth);
 
 		/* qid not preserved for directed queues */
 		if (qm_port->is_directed)
@@ -3448,7 +4608,7 @@ dlb2_process_dequeue_four_qes(struct dlb2_eventdev_port *ev_port,
 			      struct rte_event *events,
 			      struct dlb2_dequeue_qe *qes)
 {
-	int sched_type_map[] = {
+	static int sched_type_map[] = {
 		[DLB2_SCHED_ATOMIC] = RTE_SCHED_TYPE_ATOMIC,
 		[DLB2_SCHED_UNORDERED] = RTE_SCHED_TYPE_PARALLEL,
 		[DLB2_SCHED_ORDERED] = RTE_SCHED_TYPE_ORDERED,
@@ -3456,7 +4616,6 @@ dlb2_process_dequeue_four_qes(struct dlb2_eventdev_port *ev_port,
 	};
 	const int num_events = DLB2_NUM_QES_PER_CACHE_LINE;
 	uint8_t *qid_mappings = qm_port->qid_mappings;
-	__m128i sse_evt[2];
 
 	/* In the unlikely case that any of the QE error bits are set, process
 	 * them one at a time.
@@ -3466,180 +4625,48 @@ dlb2_process_dequeue_four_qes(struct dlb2_eventdev_port *ev_port,
 		return dlb2_process_dequeue_qes(ev_port, qm_port, events,
 						 qes, num_events);
 
-	events[0].u64 = qes[0].data;
-	events[1].u64 = qes[1].data;
-	events[2].u64 = qes[2].data;
-	events[3].u64 = qes[3].data;
-
-	/* Construct the metadata portion of two struct rte_events
-	 * in one 128b SSE register. Event metadata is constructed in the SSE
-	 * registers like so:
-	 * sse_evt[0][63:0]:   event[0]'s metadata
-	 * sse_evt[0][127:64]: event[1]'s metadata
-	 * sse_evt[1][63:0]:   event[2]'s metadata
-	 * sse_evt[1][127:64]: event[3]'s metadata
-	 */
-	sse_evt[0] = _mm_setzero_si128();
-	sse_evt[1] = _mm_setzero_si128();
-
-	/* Convert the hardware queue ID to an event queue ID and store it in
-	 * the metadata:
-	 * sse_evt[0][47:40]   = qid_mappings[qes[0].qid]
-	 * sse_evt[0][111:104] = qid_mappings[qes[1].qid]
-	 * sse_evt[1][47:40]   = qid_mappings[qes[2].qid]
-	 * sse_evt[1][111:104] = qid_mappings[qes[3].qid]
-	 */
-#define DLB_EVENT_QUEUE_ID_BYTE 5
-	sse_evt[0] = _mm_insert_epi8(sse_evt[0],
-				     qid_mappings[qes[0].qid],
-				     DLB_EVENT_QUEUE_ID_BYTE);
-	sse_evt[0] = _mm_insert_epi8(sse_evt[0],
-				     qid_mappings[qes[1].qid],
-				     DLB_EVENT_QUEUE_ID_BYTE + 8);
-	sse_evt[1] = _mm_insert_epi8(sse_evt[1],
-				     qid_mappings[qes[2].qid],
-				     DLB_EVENT_QUEUE_ID_BYTE);
-	sse_evt[1] = _mm_insert_epi8(sse_evt[1],
-				     qid_mappings[qes[3].qid],
-				     DLB_EVENT_QUEUE_ID_BYTE + 8);
-
-	/* Convert the hardware priority to an event priority and store it in
-	 * the metadata, while also returning the queue depth status
-	 * value captured by the hardware, storing it in impl_opaque, which can
-	 * be read by the application but not modified
-	 * sse_evt[0][55:48]   = DLB2_TO_EV_PRIO(qes[0].priority)
-	 * sse_evt[0][63:56]   = qes[0].qid_depth
-	 * sse_evt[0][119:112] = DLB2_TO_EV_PRIO(qes[1].priority)
-	 * sse_evt[0][127:120] = qes[1].qid_depth
-	 * sse_evt[1][55:48]   = DLB2_TO_EV_PRIO(qes[2].priority)
-	 * sse_evt[1][63:56]   = qes[2].qid_depth
-	 * sse_evt[1][119:112] = DLB2_TO_EV_PRIO(qes[3].priority)
-	 * sse_evt[1][127:120] = qes[3].qid_depth
-	 */
-#define DLB_EVENT_PRIO_IMPL_OPAQUE_WORD 3
-#define DLB_BYTE_SHIFT 8
-	sse_evt[0] =
-		_mm_insert_epi16(sse_evt[0],
-			DLB2_TO_EV_PRIO((uint8_t)qes[0].priority) |
-			(qes[0].qid_depth << DLB_BYTE_SHIFT),
-			DLB_EVENT_PRIO_IMPL_OPAQUE_WORD);
-	sse_evt[0] =
-		_mm_insert_epi16(sse_evt[0],
-			DLB2_TO_EV_PRIO((uint8_t)qes[1].priority) |
-			(qes[1].qid_depth << DLB_BYTE_SHIFT),
-			DLB_EVENT_PRIO_IMPL_OPAQUE_WORD + 4);
-	sse_evt[1] =
-		_mm_insert_epi16(sse_evt[1],
-			DLB2_TO_EV_PRIO((uint8_t)qes[2].priority) |
-			(qes[2].qid_depth << DLB_BYTE_SHIFT),
-			DLB_EVENT_PRIO_IMPL_OPAQUE_WORD);
-	sse_evt[1] =
-		_mm_insert_epi16(sse_evt[1],
-			DLB2_TO_EV_PRIO((uint8_t)qes[3].priority) |
-			(qes[3].qid_depth << DLB_BYTE_SHIFT),
-			DLB_EVENT_PRIO_IMPL_OPAQUE_WORD + 4);
-
-	/* Write the event type, sub event type, and flow_id to the event
-	 * metadata.
-	 * sse_evt[0][31:0]   = qes[0].flow_id |
-	 *			qes[0].u.event_type.major << 28 |
-	 *			qes[0].u.event_type.sub << 20;
-	 * sse_evt[0][95:64]  = qes[1].flow_id |
-	 *			qes[1].u.event_type.major << 28 |
-	 *			qes[1].u.event_type.sub << 20;
-	 * sse_evt[1][31:0]   = qes[2].flow_id |
-	 *			qes[2].u.event_type.major << 28 |
-	 *			qes[2].u.event_type.sub << 20;
-	 * sse_evt[1][95:64]  = qes[3].flow_id |
-	 *			qes[3].u.event_type.major << 28 |
-	 *			qes[3].u.event_type.sub << 20;
-	 */
-#define DLB_EVENT_EV_TYPE_DW 0
-#define DLB_EVENT_EV_TYPE_SHIFT 28
-#define DLB_EVENT_SUB_EV_TYPE_SHIFT 20
-	sse_evt[0] = _mm_insert_epi32(sse_evt[0],
-			qes[0].flow_id |
-			qes[0].u.event_type.major << DLB_EVENT_EV_TYPE_SHIFT |
-			qes[0].u.event_type.sub <<  DLB_EVENT_SUB_EV_TYPE_SHIFT,
-			DLB_EVENT_EV_TYPE_DW);
-	sse_evt[0] = _mm_insert_epi32(sse_evt[0],
-			qes[1].flow_id |
-			qes[1].u.event_type.major << DLB_EVENT_EV_TYPE_SHIFT |
-			qes[1].u.event_type.sub <<  DLB_EVENT_SUB_EV_TYPE_SHIFT,
-			DLB_EVENT_EV_TYPE_DW + 2);
-	sse_evt[1] = _mm_insert_epi32(sse_evt[1],
-			qes[2].flow_id |
-			qes[2].u.event_type.major << DLB_EVENT_EV_TYPE_SHIFT |
-			qes[2].u.event_type.sub <<  DLB_EVENT_SUB_EV_TYPE_SHIFT,
-			DLB_EVENT_EV_TYPE_DW);
-	sse_evt[1] = _mm_insert_epi32(sse_evt[1],
-			qes[3].flow_id |
-			qes[3].u.event_type.major << DLB_EVENT_EV_TYPE_SHIFT  |
-			qes[3].u.event_type.sub << DLB_EVENT_SUB_EV_TYPE_SHIFT,
-			DLB_EVENT_EV_TYPE_DW + 2);
-
-	/* Write the sched type to the event metadata. 'op' and 'rsvd' are not
-	 * set:
-	 * sse_evt[0][39:32]  = sched_type_map[qes[0].sched_type] << 6
-	 * sse_evt[0][103:96] = sched_type_map[qes[1].sched_type] << 6
-	 * sse_evt[1][39:32]  = sched_type_map[qes[2].sched_type] << 6
-	 * sse_evt[1][103:96] = sched_type_map[qes[3].sched_type] << 6
-	 */
-#define DLB_EVENT_SCHED_TYPE_BYTE 4
-#define DLB_EVENT_SCHED_TYPE_SHIFT 6
-	sse_evt[0] = _mm_insert_epi8(sse_evt[0],
-		sched_type_map[qes[0].sched_type] << DLB_EVENT_SCHED_TYPE_SHIFT,
-		DLB_EVENT_SCHED_TYPE_BYTE);
-	sse_evt[0] = _mm_insert_epi8(sse_evt[0],
-		sched_type_map[qes[1].sched_type] << DLB_EVENT_SCHED_TYPE_SHIFT,
-		DLB_EVENT_SCHED_TYPE_BYTE + 8);
-	sse_evt[1] = _mm_insert_epi8(sse_evt[1],
-		sched_type_map[qes[2].sched_type] << DLB_EVENT_SCHED_TYPE_SHIFT,
-		DLB_EVENT_SCHED_TYPE_BYTE);
-	sse_evt[1] = _mm_insert_epi8(sse_evt[1],
-		sched_type_map[qes[3].sched_type] << DLB_EVENT_SCHED_TYPE_SHIFT,
-		DLB_EVENT_SCHED_TYPE_BYTE + 8);
-
-	/* Store the metadata to the event (use the double-precision
-	 * _mm_storeh_pd because there is no integer function for storing the
-	 * upper 64b):
-	 * events[0].event = sse_evt[0][63:0]
-	 * events[1].event = sse_evt[0][127:64]
-	 * events[2].event = sse_evt[1][63:0]
-	 * events[3].event = sse_evt[1][127:64]
-	 */
-	_mm_storel_epi64((__m128i *)&events[0].event, sse_evt[0]);
-	_mm_storeh_pd((double *)&events[1].event, (__m128d) sse_evt[0]);
-	_mm_storel_epi64((__m128i *)&events[2].event, sse_evt[1]);
-	_mm_storeh_pd((double *)&events[3].event, (__m128d) sse_evt[1]);
+	const __m128i qe_to_ev_shuffle =
+	    _mm_set_epi8(7, 6, 5, 4, 3, 2, 1, 0, /* last 8-bytes = data from first 8 */
+			 0xFF, 0xFF, 0xFF, 0xFF, /* fill in later as 32-bit value*/
+			 9, 8,			 /* event type and sub-event, + 4 zero bits */
+			 13, 12 /* flow id, 16 bits */);
+	for (int i = 0; i < 4; i++) {
+		const __m128i hw_qe = _mm_load_si128((void *)&qes[i]);
+		const __m128i event = _mm_shuffle_epi8(hw_qe, qe_to_ev_shuffle);
+		/* prepare missing 32-bits for op, sched_type, QID, Priority and
+		 * sequence number in impl_opaque
+		 */
+		const uint16_t qid_sched_prio = _mm_extract_epi16(hw_qe, 5);
+		const uint32_t qid =  (qm_port->is_directed) ? ev_port->link[0].queue_id : 
+					qid_mappings[(uint8_t)qid_sched_prio];
+		const uint32_t sched_type = sched_type_map[(qid_sched_prio >> 8) & 0x3];
+		const uint32_t priority = (qid_sched_prio >> 5) & 0xE0;
+
+		const uint32_t dword1 =
+		    sched_type << 6 | qid << 8 | priority << 16 | (qm_port->reorder_id + i) << 24;
+
+		/* events[] may not be 16 byte aligned. So use separate load and store */
+		const __m128i tmpEv = _mm_insert_epi32(event, dword1, 1);
+		_mm_storeu_si128((__m128i *) &events[i], tmpEv);
+	}
+	qm_port->reorder_id += 4;
 
 	DLB2_INC_STAT(ev_port->stats.rx_sched_cnt[qes[0].sched_type], 1);
 	DLB2_INC_STAT(ev_port->stats.rx_sched_cnt[qes[1].sched_type], 1);
 	DLB2_INC_STAT(ev_port->stats.rx_sched_cnt[qes[2].sched_type], 1);
 	DLB2_INC_STAT(ev_port->stats.rx_sched_cnt[qes[3].sched_type], 1);
 
-	DLB2_INC_STAT(
-		ev_port->stats.queue[events[0].queue_id].
-			qid_depth[qes[0].qid_depth],
-		1);
-	DLB2_INC_STAT(
-		ev_port->stats.queue[events[1].queue_id].
-			qid_depth[qes[1].qid_depth],
-		1);
-	DLB2_INC_STAT(
-		ev_port->stats.queue[events[2].queue_id].
-			qid_depth[qes[2].qid_depth],
-		1);
-	DLB2_INC_STAT(
-		ev_port->stats.queue[events[3].queue_id].
-			qid_depth[qes[3].qid_depth],
-		1);
+	DLB2_INC_STAT(ev_port->stats.queue[events[0].queue_id].qid_depth[qes[0].qid_depth], 1);
+	DLB2_INC_STAT(ev_port->stats.queue[events[1].queue_id].qid_depth[qes[1].qid_depth], 1);
+	DLB2_INC_STAT(ev_port->stats.queue[events[2].queue_id].qid_depth[qes[2].qid_depth], 1);
+	DLB2_INC_STAT(ev_port->stats.queue[events[3].queue_id].qid_depth[qes[3].qid_depth], 1);
 
 	DLB2_INC_STAT(ev_port->stats.traffic.rx_ok, num_events);
 
 	return num_events;
 }
 
+
 static __rte_always_inline int
 dlb2_recv_qe_sparse(struct dlb2_port *qm_port, struct dlb2_dequeue_qe *qe)
 {
@@ -3651,9 +4678,10 @@ dlb2_recv_qe_sparse(struct dlb2_port *qm_port, struct dlb2_dequeue_qe *qe)
 	uintptr_t addr[4];
 	uint16_t idx;
 
-	cq_addr = dlb2_port[qm_port->id][PORT_TYPE(qm_port)].cq_base;
-
+	cq_addr = dlb2_port[qm_port->evdev_id][qm_port->id]
+			   [PORT_TYPE(qm_port)].cq_base;
 	idx = qm_port->cq_idx_unmasked & qm_port->cq_depth_mask;
+
 	/* Load the next 4 QEs */
 	addr[0] = (uintptr_t)&cq_addr[idx];
 	addr[1] = (uintptr_t)&cq_addr[(idx +  4) & qm_port->cq_depth_mask];
@@ -3715,7 +4743,7 @@ _process_deq_qes_vec_impl(struct dlb2_port *qm_port,
 	 * Each v_qe_[0-4] is just a 16-byte load of the whole QE. It is
 	 * passed along in registers as the QE data is required later.
 	 *
-	 * v_qe_meta is an u32 unpack of all 4x QEs. A.k.a, it contains one
+	 * v_qe_meta is an u32 unpack of all 4x QEs. Aka, it contains one
 	 * 32-bit slice of each QE, so makes up a full SSE register. This
 	 * allows parallel processing of 4x QEs in a single register.
 	 */
@@ -3726,10 +4754,17 @@ _process_deq_qes_vec_impl(struct dlb2_port *qm_port,
 	int hw_qid2 = _mm_extract_epi8(v_qe_meta, 10);
 	int hw_qid3 = _mm_extract_epi8(v_qe_meta, 14);
 
-	int ev_qid0 = qm_port->qid_mappings[hw_qid0];
-	int ev_qid1 = qm_port->qid_mappings[hw_qid1];
-	int ev_qid2 = qm_port->qid_mappings[hw_qid2];
-	int ev_qid3 = qm_port->qid_mappings[hw_qid3];
+	int ev_qid0, ev_qid1, ev_qid2, ev_qid3;
+
+	if (qm_port->is_directed) {
+		ev_qid0 = ev_qid1 = ev_qid2 = ev_qid3 =
+			qm_port->ev_port->link[0].queue_id;
+	} else {
+		ev_qid0 = qm_port->qid_mappings[hw_qid0];
+		ev_qid1 = qm_port->qid_mappings[hw_qid1];
+		ev_qid2 = qm_port->qid_mappings[hw_qid2];
+		ev_qid3 = qm_port->qid_mappings[hw_qid3];
+	}
 
 	int hw_sched0 = _mm_extract_epi8(v_qe_meta, 3) & 3ul;
 	int hw_sched1 = _mm_extract_epi8(v_qe_meta, 7) & 3ul;
@@ -3743,7 +4778,7 @@ _process_deq_qes_vec_impl(struct dlb2_port *qm_port,
 
 	/* Schedule field remapping using byte shuffle
 	 * - Full byte containing sched field handled here (op, rsvd are zero)
-	 * - Note sanitizing the register requires two masking ANDs:
+	 * - Note santizing the register requires two masking ANDs:
 	 *   1) to strip prio/msg_type from byte for correct shuffle lookup
 	 *   2) to strip any non-sched-field lanes from any results to OR later
 	 * - Final byte result is >> 10 to another byte-lane inside the u32.
@@ -3889,7 +4924,8 @@ dlb2_recv_qe_sparse_vec(struct dlb2_port *qm_port, void *events,
 	uint16_t idx = qm_port->cq_idx_unmasked;
 	volatile struct dlb2_dequeue_qe *cq_addr;
 
-	cq_addr = dlb2_port[qm_port->id][PORT_TYPE(qm_port)].cq_base;
+	cq_addr = dlb2_port[qm_port->evdev_id][qm_port->id]
+			   [PORT_TYPE(qm_port)].cq_base;
 
 	uintptr_t qe_ptr_3 = (uintptr_t)&cq_addr[(idx + 12) &
 						 qm_port->cq_depth_mask];
@@ -3994,23 +5030,37 @@ dlb2_hw_dequeue_sparse(struct dlb2_eventdev *dlb2,
 		       uint16_t max_num,
 		       uint64_t dequeue_timeout_ticks)
 {
+	uint64_t timeout;
 	uint64_t start_ticks = 0ULL;
 	struct dlb2_port *qm_port;
 	int num = 0;
 	bool use_scalar;
-	uint64_t timeout;
 
 	qm_port = &ev_port->qm_port;
 	use_scalar = qm_port->use_scalar;
 
+	/* We have a special implementation for waiting. Wait can be:
+	 * 1) no waiting at all
+	 * 2) busy poll only
+	 * 3) wait for interrupt. If wakeup and poll time
+	 * has expired, then return to caller
+	 * 4) umonitor/umwait repeatedly up to poll time
+	 */
+
+	/* If configured for per dequeue wait, then use wait value provided
+	 * to this API. Otherwise we must use the global
+	 * value from eventdev config time.
+	 */
+
 	if (!dlb2->global_dequeue_wait)
 		timeout = dequeue_timeout_ticks;
 	else
 		timeout = dlb2->global_dequeue_wait_ticks;
 
-	start_ticks = rte_get_timer_cycles();
+	if (timeout)
+		start_ticks = rte_get_timer_cycles();
 
-	use_scalar = use_scalar || (max_num & 0x3);
+	use_scalar = true; /* Always use scaler - Fixme. use_scalar || (max_num & 0x3); */
 
 	while (num < max_num) {
 		struct dlb2_dequeue_qe qes[DLB2_NUM_QES_PER_CACHE_LINE];
@@ -4036,14 +5086,18 @@ dlb2_hw_dequeue_sparse(struct dlb2_eventdev *dlb2,
 								num_avail);
 			if (n_iter != 0) {
 				num += n_iter;
-				/* update rolling_mask for vector code support */
+				/* update vector code rolling masks */
 				m_rshift = qm_port->cq_rolling_mask >> n_iter;
-				m_lshift = qm_port->cq_rolling_mask << (64 - n_iter);
-				m2_rshift = qm_port->cq_rolling_mask_2 >> n_iter;
+				m_lshift = qm_port->cq_rolling_mask <<
+					(64 - n_iter);
+				m2_rshift = qm_port->cq_rolling_mask_2 >>
+					n_iter;
 				m2_lshift = qm_port->cq_rolling_mask_2 <<
 					(64 - n_iter);
-				qm_port->cq_rolling_mask = (m_rshift | m2_lshift);
-				qm_port->cq_rolling_mask_2 = (m2_rshift | m_lshift);
+				qm_port->cq_rolling_mask =
+					(m_rshift | m2_lshift);
+				qm_port->cq_rolling_mask_2 =
+					(m2_rshift | m_lshift);
 			}
 		} else { /* !use_scalar */
 			num_avail = dlb2_recv_qe_sparse_vec(qm_port,
@@ -4071,7 +5125,9 @@ dlb2_hw_dequeue_sparse(struct dlb2_eventdev *dlb2,
 
 		ev_port->outstanding_releases += num;
 
+#if DLB_HW_CREDITS_CHECKS
 		dlb2_port_credits_inc(qm_port, num);
+#endif
 	}
 
 	return num;
@@ -4089,7 +5145,8 @@ dlb2_recv_qe(struct dlb2_port *qm_port, struct dlb2_dequeue_qe *qe,
 	uint64_t *cache_line_base;
 	uint8_t gen_bits;
 
-	cq_addr = dlb2_port[qm_port->id][PORT_TYPE(qm_port)].cq_base;
+	cq_addr = dlb2_port[qm_port->evdev_id][qm_port->id]
+			   [PORT_TYPE(qm_port)].cq_base;
 	cq_addr = &cq_addr[qm_port->cq_idx];
 
 	cache_line_base = (void *)(((uintptr_t)cq_addr) & ~0x3F);
@@ -4156,7 +5213,8 @@ dlb2_hw_dequeue(struct dlb2_eventdev *dlb2,
 	else
 		timeout = dlb2->global_dequeue_wait_ticks;
 
-	start_ticks = rte_get_timer_cycles();
+	if (timeout)
+		start_ticks = rte_get_timer_cycles();
 
 	while (num < max_num) {
 		struct dlb2_dequeue_qe qes[DLB2_NUM_QES_PER_CACHE_LINE];
@@ -4198,7 +5256,9 @@ dlb2_hw_dequeue(struct dlb2_eventdev *dlb2,
 
 		ev_port->outstanding_releases += num;
 
+#if DLB_HW_CREDITS_CHECKS
 		dlb2_port_credits_inc(qm_port, num);
+#endif
 	}
 
 	return num;
@@ -4211,15 +5271,37 @@ dlb2_event_dequeue_burst(void *event_port, struct rte_event *ev, uint16_t num,
 	struct dlb2_eventdev_port *ev_port = event_port;
 	struct dlb2_port *qm_port = &ev_port->qm_port;
 	struct dlb2_eventdev *dlb2 = ev_port->dlb2;
+	struct process_local_port_data *port_data;
 	uint16_t cnt;
 
 	RTE_ASSERT(ev_port->setup_done);
 	RTE_ASSERT(ev != NULL);
 
+	port_data = &dlb2_port[qm_port->evdev_id][qm_port->id]
+			      [PORT_TYPE(qm_port)];
+
+	if (!port_data->mmaped)
+		dlb2_iface_port_mmap(qm_port);
+
 	if (ev_port->implicit_release && ev_port->outstanding_releases > 0) {
 		uint16_t out_rels = ev_port->outstanding_releases;
-
-		dlb2_event_release(dlb2, ev_port->id, out_rels);
+		if (ev_port->conf.event_port_cfg & RTE_EVENT_PORT_CFG_RESTORE_DEQ_ORDER) {
+			/* for directed, no-op command-byte = 0, but set dsi field */
+			/* for load-balanced, set COMP */
+			uint64_t release_u64 =
+			    qm_port->is_directed ? 0xFF : (uint64_t)DLB2_COMP_CMD_BYTE << 56;
+
+			/* go through reorder buffer looking for missing releases. */
+			/* TODO: copy implementation for sparse dequeue */
+			for (uint8_t i = qm_port->next_to_enqueue; i != qm_port->reorder_id; i++)
+				if (qm_port->enq_reorder[i].u64[1] == 0)
+					qm_port->enq_reorder[i].u64[1] = release_u64;
+
+			__dlb2_event_enqueue_burst_reorder(event_port, NULL, 0,
+						   qm_port->token_pop_mode == DELAYED_POP);
+		} else {
+			dlb2_event_release(dlb2, ev_port->id, out_rels);
+		}
 
 		DLB2_INC_STAT(ev_port->stats.tx_implicit_rel, out_rels);
 	}
@@ -4231,6 +5313,8 @@ dlb2_event_dequeue_burst(void *event_port, struct rte_event *ev, uint16_t num,
 
 	DLB2_INC_STAT(ev_port->stats.traffic.total_polls, 1);
 	DLB2_INC_STAT(ev_port->stats.traffic.zero_polls, ((cnt == 0) ? 1 : 0));
+	dlb2_check_and_return_credits(ev_port, !cnt,
+				      DLB2_ZERO_DEQ_CREDIT_RETURN_THRES);
 
 	return cnt;
 }
@@ -4248,16 +5332,50 @@ dlb2_event_dequeue_burst_sparse(void *event_port, struct rte_event *ev,
 	struct dlb2_eventdev_port *ev_port = event_port;
 	struct dlb2_port *qm_port = &ev_port->qm_port;
 	struct dlb2_eventdev *dlb2 = ev_port->dlb2;
+	struct process_local_port_data *port_data;
 	uint16_t cnt;
 
 	RTE_ASSERT(ev_port->setup_done);
 	RTE_ASSERT(ev != NULL);
 
+	port_data = &dlb2_port[qm_port->evdev_id][qm_port->id]
+			      [PORT_TYPE(qm_port)];
+
+	if (!port_data->mmaped)
+		dlb2_iface_port_mmap(qm_port);
+
 	if (ev_port->implicit_release && ev_port->outstanding_releases > 0) {
 		uint16_t out_rels = ev_port->outstanding_releases;
+		if (ev_port->conf.event_port_cfg & RTE_EVENT_PORT_CFG_RESTORE_DEQ_ORDER) {
+			struct rte_event release_burst[8];
+			int num_releases = 0;
+
+			/* go through reorder buffer looking for missing releases. */
+			for (uint8_t i = qm_port->next_to_enqueue; i != qm_port->reorder_id; i++) {
+				if (qm_port->enq_reorder[i].u64[1] == 0) {
+					release_burst[num_releases++] = (struct rte_event){
+						.op = RTE_EVENT_OP_RELEASE,
+							.impl_opaque = i,
+					};
+
+					if (num_releases == RTE_DIM(release_burst)) {
+						__dlb2_event_enqueue_burst_reorder(event_port, release_burst,
+							RTE_DIM(release_burst),
+							qm_port->token_pop_mode == DELAYED_POP);
+						num_releases = 0;
+					}
+				}
+			}
 
-		dlb2_event_release(dlb2, ev_port->id, out_rels);
+			if (num_releases)
+				__dlb2_event_enqueue_burst_reorder(event_port, release_burst
+					, num_releases, qm_port->token_pop_mode == DELAYED_POP);
+		} else {
+			dlb2_event_release(dlb2, ev_port->id, out_rels);
+		}
 
+		if (ev_port->outstanding_releases != 0)
+			rte_panic("Still outstanding releases %d\n", ev_port->outstanding_releases);
 		DLB2_INC_STAT(ev_port->stats.tx_implicit_rel, out_rels);
 	}
 
@@ -4268,6 +5386,9 @@ dlb2_event_dequeue_burst_sparse(void *event_port, struct rte_event *ev,
 
 	DLB2_INC_STAT(ev_port->stats.traffic.total_polls, 1);
 	DLB2_INC_STAT(ev_port->stats.traffic.zero_polls, ((cnt == 0) ? 1 : 0));
+	dlb2_check_and_return_credits(ev_port, !cnt,
+				      DLB2_ZERO_DEQ_CREDIT_RETURN_THRES);
+
 	return cnt;
 }
 
@@ -4282,12 +5403,16 @@ static void
 dlb2_flush_port(struct rte_eventdev *dev, int port_id)
 {
 	struct dlb2_eventdev *dlb2 = dlb2_pmd_priv(dev);
+	struct dlb2_eventdev_port *ev_port = &dlb2->ev_ports[port_id];
 	eventdev_stop_flush_t flush;
 	struct rte_event ev;
 	uint8_t dev_id;
 	void *arg;
 	int i;
 
+	if (!ev_port->setup_done)
+		return;
+
 	flush = dev->dev_ops->dev_stop_flush;
 	dev_id = dev->data->dev_id;
 	arg = dev->data->dev_stop_flush_arg;
@@ -4307,8 +5432,10 @@ dlb2_flush_port(struct rte_eventdev *dev, int port_id)
 	/* Enqueue any additional outstanding releases */
 	ev.op = RTE_EVENT_OP_RELEASE;
 
-	for (i = dlb2->ev_ports[port_id].outstanding_releases; i > 0; i--)
+	for (i = dlb2->ev_ports[port_id].outstanding_releases; i > 0; i--) {
+		ev.impl_opaque = dlb2->ev_ports[port_id].qm_port.next_to_enqueue;
 		rte_event_enqueue_burst(dev_id, port_id, &ev, 1);
+	}
 }
 
 static uint32_t
@@ -4323,8 +5450,11 @@ dlb2_get_ldb_queue_depth(struct dlb2_eventdev *dlb2,
 
 	ret = dlb2_iface_get_ldb_queue_depth(handle, &cfg);
 	if (ret < 0) {
+		int status;
+		status = cfg.response.status < RTE_DIM(dlb2_error_strings) ?
+			cfg.response.status : DLB2_ST_INVALID_CQ_DEPTH;
 		DLB2_LOG_ERR("dlb2: get_ldb_queue_depth ret=%d (driver status: %s)\n",
-			     ret, dlb2_error_strings[cfg.response.status]);
+			     ret, dlb2_error_strings[status]);
 		return ret;
 	}
 
@@ -4343,8 +5473,11 @@ dlb2_get_dir_queue_depth(struct dlb2_eventdev *dlb2,
 
 	ret = dlb2_iface_get_dir_queue_depth(handle, &cfg);
 	if (ret < 0) {
+		int status;
+		status = cfg.response.status < RTE_DIM(dlb2_error_strings) ?
+			cfg.response.status : DLB2_ST_INVALID_CQ_DEPTH;
 		DLB2_LOG_ERR("dlb2: get_dir_queue_depth ret=%d (driver status: %s)\n",
-			     ret, dlb2_error_strings[cfg.response.status]);
+			     ret, dlb2_error_strings[status]);
 		return ret;
 	}
 
@@ -4361,6 +5494,61 @@ dlb2_get_queue_depth(struct dlb2_eventdev *dlb2,
 		return dlb2_get_ldb_queue_depth(dlb2, queue);
 }
 
+#define RET_ERR(param, ret, err_str)                                                                         \
+	do {                                                                                                 \
+		if (!ret)                                                                                    \
+			ret = -EINVAL;                                                                       \
+		DLB2_LOG_ERR("dlb2: dlb2_set_port_param error, param=%lx ret=%d %s\n", param, ret, err_str); \
+		return ret;                                                                                  \
+	} while (0)
+
+int
+dlb2_set_port_param(struct dlb2_eventdev *dlb2,
+		    int port_id,
+		    uint64_t param_flags,
+		    void *param_val)
+{
+	struct dlb2_port_param *port_param = (struct dlb2_port_param *)param_val;
+	struct dlb2_port *port = &dlb2->ev_ports[port_id].qm_port;
+	struct dlb2_hw_dev *handle = &dlb2->qm_instance;
+	int ret = 0, bit = 0;
+
+	while (param_flags) {
+		uint64_t param = rte_bit_relaxed_test_and_clear64(bit++, &param_flags);
+
+		if (!param)
+			continue;
+		switch (param) {
+			case DLB2_FLOW_MIGRATION_THRESHOLD:
+				if (dlb2->version == DLB2_HW_V2_5) {
+					struct dlb2_cq_inflight_ctrl_args args = {0};
+
+					args.enable = true;
+					args.port_id = port->id;
+					args.threshold = port_param->inflight_threshold;
+					if (dlb2->ev_ports[port_id].setup_done)
+						ret = dlb2_iface_set_cq_inflight_ctrl(handle, &args);
+					if (ret)
+						RET_ERR(param, ret, "Failed to set inflight threshold");
+					port->enable_inflight_ctrl = true;
+					port->inflight_threshold = args.threshold;
+				} else {
+					RET_ERR(param, ret, "FLOW_MIGRATION_THRESHOLD is only supported for 2.5 HW");
+				}
+				break;
+			case DLB2_SET_PORT_HL:
+				if (dlb2->ev_ports[port_id].setup_done)
+					RET_ERR(param, ret, "DLB2_SET_PORT_HL must be called before setting up port");
+				port->hist_list = port_param->port_hl;
+				break;
+			default:
+				RET_ERR(param, ret, "Unsupported flag");
+		}
+	}
+
+	return ret;
+}
+
 static bool
 dlb2_queue_is_empty(struct dlb2_eventdev *dlb2,
 		    struct dlb2_eventdev_queue *queue)
@@ -4400,9 +5588,11 @@ static void
 dlb2_drain(struct rte_eventdev *dev)
 {
 	struct dlb2_eventdev *dlb2 = dlb2_pmd_priv(dev);
+	struct dlb2_hw_dev *handle = &dlb2->qm_instance;
 	struct dlb2_eventdev_port *ev_port = NULL;
+	struct dlb2_stop_domain_args cfg;
 	uint8_t dev_id;
-	int i;
+	int i, ret;
 
 	dev_id = dev->data->dev_id;
 
@@ -4420,7 +5610,7 @@ dlb2_drain(struct rte_eventdev *dev)
 
 	/* If the domain's queues are empty, we're done. */
 	if (dlb2_queues_empty(dlb2))
-		return;
+		goto domain_cleanup;
 
 	/* Else, there must be at least one unlinked load-balanced queue.
 	 * Select a load-balanced port with which to drain the unlinked
@@ -4449,7 +5639,6 @@ dlb2_drain(struct rte_eventdev *dev)
 
 	for (i = 0; i < dlb2->num_queues; i++) {
 		uint8_t qid, prio;
-		int ret;
 
 		if (dlb2_queue_is_empty(dlb2, &dlb2->ev_queues[i]))
 			continue;
@@ -4480,6 +5669,101 @@ dlb2_drain(struct rte_eventdev *dev)
 			return;
 		}
 	}
+
+domain_cleanup:
+	for (i = 0; i < dlb2->num_ports; i++)
+		dlb2_set_port_ctrl(&dlb2->ev_ports[i], false);
+
+	ret = dlb2_iface_sched_domain_stop(handle, &cfg);
+	if (ret < 0) {
+		DLB2_LOG_ERR("dlb2: sched_domain_stop ret=%d (driver status: %s)\n",
+			     ret, dlb2_error_strings[cfg.response.status]);
+		return;
+	}
+}
+
+int
+dlb2_get_sched_idle_counts(struct dlb2_eventdev *dlb2)
+{
+	uint64_t active_sched_cycles, total_idle_cycles, clk_gated_cycles;
+	struct dlb2_hw_dev *handle = &dlb2->qm_instance;
+	struct dlb2_sched_cycles_percent *dev_cycles;
+	uint64_t total_cycles, powered_on_cycles;
+	struct dlb2_sched_idle_counts *data;
+	int ret;
+
+	dev_cycles = &handle->dev_cycles_pct;
+	data = &handle->idle_counts;
+
+	memset(dev_cycles, 0, sizeof(*dev_cycles));
+	memset(data, 0, sizeof(*data));
+
+	if (dlb2 == NULL || handle == NULL)
+		return -EINVAL;
+
+	ret = dlb2_iface_get_sched_idle_counts(handle, data);
+	if (ret != 0) {
+		DLB2_LOG_ERR("Failed to read scheduler idle counters\n");
+		return ret;
+	}
+
+	/* Compute percentages of device cycles spent idle */
+	total_cycles = data->perf_clk_on_cnt;
+	powered_on_cycles = data->perf_proc_on_cnt;
+	clk_gated_cycles = total_cycles - powered_on_cycles;
+	active_sched_cycles = data->ldb_perf_sched_cnt * 8;
+	total_idle_cycles = data->ldb_perf_nowork_idle_cnt +
+			    data->ldb_perf_nospace_idle_cnt +
+			    data->ldb_perf_pfriction_idle_cnt +
+			    clk_gated_cycles;
+
+	/* In the unlikely scenario of total_cycles == 0,
+	 * to avoid divide by zero crash
+	 */
+	RTE_ASSERT(total_cycles != 0);
+
+	dev_cycles->total_sched_pct = (DLB2_PERF_MAX_PERCENT *
+				       active_sched_cycles) /
+				       total_cycles;
+
+	if (total_idle_cycles == 0) {
+		DLB2_LOG_ERR("No missed scheduling cycles due to no-work/no-CQ "
+			      "space/pipeline-friction\n");
+		return -EINVAL;
+	}
+
+	dev_cycles->total_idle_pct = DLB2_PERF_MAX_PERCENT -
+				     dev_cycles->total_sched_pct;
+
+	dev_cycles->clk_gated_pct = (DLB2_PERF_MAX_PERCENT -
+				     ((float)data->perf_proc_on_cnt *
+				     DLB2_PERF_MAX_PERCENT /
+				     total_cycles)) *
+				     dev_cycles->total_idle_pct;
+
+	dev_cycles->nowork_idle_pct = (dev_cycles->total_idle_pct *
+				       (data->ldb_perf_nowork_idle_cnt -
+				       data->ldb_perf_fidlimit_idle_cnt)) /
+				       total_idle_cycles;
+
+	dev_cycles->nospace_idle_pct = (dev_cycles->total_idle_pct *
+					(data->ldb_perf_nospace_idle_cnt -
+					data->ldb_perf_iflimit_idle_cnt)) /
+					total_idle_cycles;
+
+	dev_cycles->pfriction_idle_pct = (dev_cycles->total_idle_pct *
+					  data->ldb_perf_pfriction_idle_cnt) /
+					  total_idle_cycles;
+
+	dev_cycles->iflimit_idle_pct = (dev_cycles->total_idle_pct *
+					data->ldb_perf_iflimit_idle_cnt) /
+					total_idle_cycles;
+
+	dev_cycles->fidlimit_idle_pct = (dev_cycles->total_idle_pct *
+					 data->ldb_perf_fidlimit_idle_cnt) /
+					 total_idle_cycles;
+
+	return 0;
 }
 
 static void
@@ -4512,9 +5796,7 @@ dlb2_eventdev_stop(struct rte_eventdev *dev)
 static int
 dlb2_eventdev_close(struct rte_eventdev *dev)
 {
-	dlb2_hw_reset_sched_domain(dev, false);
-
-	return 0;
+	return dlb2_hw_reset_sched_domain(dev, false);
 }
 
 static void
@@ -4600,49 +5882,86 @@ dlb2_entry_points_init(struct rte_eventdev *dev)
 	}
 }
 
+static void
+dlb2_qm_mmio_fn_init(void)
+{
+	/* Process-local function pointers for performing low level port i/o */
+
+	if (rte_cpu_get_flag_enabled(RTE_CPUFLAG_MOVDIR64B))
+		qm_mmio_fns.pp_enqueue_four = dlb2_movdir64b;
+	else
+		qm_mmio_fns.pp_enqueue_four = dlb2_movntdq;
+}
+
+static int
+dlb2_init_producer_cores(struct dlb2_eventdev *dlb2, const struct dlb2_devargs *dev_args)
+{
+	struct dlb2_create_sched_domain_args *args = &dlb2->qm_instance.cfg.resources;
+	const char *mask = dev_args->producer_coremask;
+	int i, cores[RTE_MAX_LCORE];
+
+	if (strlen(mask) && rte_eal_parse_coremask(mask, cores)) {
+		DLB2_LOG_ERR("Invalid producer coremask=%s", mask);
+		return -1;
+	}
+
+	memset(args->pcore_bmp, 0, sizeof(args->pcore_bmp));
+	memset(args->core_bmp, 0, sizeof(args->core_bmp));
+
+	for (i = 0; i < RTE_MAX_LCORE; i++) {
+		if (rte_lcore_is_enabled(i))
+			args->core_bmp[i / __BITS_PER_LONG] |= 1ULL << (i % __BITS_PER_LONG);
+		if (strlen(mask) && cores[i] != -1)
+			args->pcore_bmp[i / __BITS_PER_LONG] |= 1ULL << (i % __BITS_PER_LONG);
+	}
+
+	return 0;
+}
+
 int
 dlb2_primary_eventdev_probe(struct rte_eventdev *dev,
 			    const char *name,
-			    struct dlb2_devargs *dlb2_args)
+			    struct dlb2_devargs *dlb2_args,
+			    bool is_vdev)
 {
 	struct dlb2_eventdev *dlb2;
 	int err, i;
 
 	dlb2 = dev->data->dev_private;
-
 	dlb2->event_dev = dev; /* backlink */
+	dlb2->is_vdev = is_vdev; /* backlink */
 
-	evdev_dlb2_default_info.driver_name = name;
+	dlb2_init_evdev_rsrcs(dlb2, name);
 
 	dlb2->max_num_events_override = dlb2_args->max_num_events;
 	dlb2->num_dir_credits_override = dlb2_args->num_dir_credits_override;
+	dlb2->qm_instance.device_path_id = dlb2_args->hwdev_id;
 	dlb2->poll_interval = dlb2_args->poll_interval;
-	dlb2->sw_credit_quanta = dlb2_args->sw_credit_quanta;
-	dlb2->hw_credit_quanta = dlb2_args->hw_credit_quanta;
 	dlb2->default_depth_thresh = dlb2_args->default_depth_thresh;
-	dlb2->vector_opts_enabled = dlb2_args->vector_opts_enabled;
-
+	dlb2->vector_opts_disab = dlb2_args->vector_opts_disab;
+	dlb2->enable_cq_weight = dlb2_args->enable_cq_weight;
+	dlb2->qm_instance.num_ordered_queues_0 =
+		dlb2_args->num_ordered_queues_0;
+	dlb2->qm_instance.num_ordered_queues_1 =
+		dlb2_args->num_ordered_queues_1;
 
 	if (dlb2_args->max_cq_depth != 0)
 		dlb2->max_cq_depth = dlb2_args->max_cq_depth;
 	else
 		dlb2->max_cq_depth = DLB2_DEFAULT_CQ_DEPTH;
 
-	evdev_dlb2_default_info.max_event_port_dequeue_depth = dlb2->max_cq_depth;
-
 	if (dlb2_args->max_enq_depth != 0)
-		dlb2->max_enq_depth = dlb2_args->max_enq_depth;
-	else
-		dlb2->max_enq_depth = DLB2_DEFAULT_CQ_DEPTH;
-
-	evdev_dlb2_default_info.max_event_port_enqueue_depth =
-		dlb2->max_enq_depth;
+		dlb2->qm_instance.evdev_rsrcs.max_event_port_enqueue_depth =
+			dlb2_args->max_enq_depth;
 
-	dlb2_init_queue_depth_thresholds(dlb2,
-					 dlb2_args->qid_depth_thresholds.val);
-
-	dlb2_init_cq_weight(dlb2,
-			    dlb2_args->cq_weight.limit);
+	dlb2->qm_instance.evdev_rsrcs.max_event_port_dequeue_depth = dlb2->max_cq_depth;
+	err = dlb2_init_producer_cores(dlb2, dlb2_args);
+	if (err < 0) {
+		DLB2_LOG_ERR("could not init pcores, err=%d\n",
+			     err);
+		rte_event_pmd_release(dev);
+		return err;
+	}
 
 	dlb2_init_port_cos(dlb2,
 			   dlb2_args->port_cos.cos_id);
@@ -4654,11 +5973,13 @@ dlb2_primary_eventdev_probe(struct rte_eventdev *dev,
 	if (err < 0) {
 		DLB2_LOG_ERR("could not open event hardware device, err=%d\n",
 			     err);
+		rte_event_pmd_release(dev);
 		return err;
 	}
 
 	err = dlb2_iface_get_device_version(&dlb2->qm_instance,
-					    &dlb2->revision);
+					    &dlb2->revision,
+					    &dlb2->version);
 	if (err < 0) {
 		DLB2_LOG_ERR("dlb2: failed to get the device version, err=%d\n",
 			     err);
@@ -4672,30 +5993,68 @@ dlb2_primary_eventdev_probe(struct rte_eventdev *dev,
 		return err;
 	}
 
-	dlb2_iface_hardware_init(&dlb2->qm_instance);
-
-	/* configure class of service */
-	{
-		struct dlb2_set_cos_bw_args
-			set_cos_bw_args = { {0} };
-		int id;
-		int ret = 0;
-
-		for (id = 0; id < DLB2_COS_NUM_VALS; id++) {
-			set_cos_bw_args.cos_id = id;
-			set_cos_bw_args.bandwidth = dlb2->cos_bw[id];
-			ret = dlb2_iface_set_cos_bw(&dlb2->qm_instance,
-						    &set_cos_bw_args);
-			if (ret != 0)
-				break;
+	if (dlb2_args->use_default_hl) {
+		dlb2->default_port_hl = DLB2_FIXED_CQ_HL_SIZE;
+		if (dlb2_args->alloc_hl_entries)
+			DLB2_LOG_ERR(": Ignoring 'alloc_hl_entries' and using "
+				     "default history list sizes for eventdev:"
+				     " %s\n", dev->data->name);
+                dlb2->hl_entries = 0;
+	} else {
+		dlb2->default_port_hl = 2 * DLB2_FIXED_CQ_HL_SIZE;
+
+		if (dlb2_args->alloc_hl_entries >
+		    dlb2->hw_rsrc_query_results.num_hist_list_entries) {
+			DLB2_LOG_ERR(": Insufficient HL entries asked=%d "
+				     "available=%d for eventdev: %s\n",
+				     dlb2->hl_entries,
+				     dlb2->hw_rsrc_query_results.num_hist_list_entries,
+				     dev->data->name);
+                        return -EINVAL;
 		}
-		if (ret) {
-			DLB2_LOG_ERR("dlb2: failed to configure class of service, err=%d\n",
-				     err);
+		dlb2->hl_entries = dlb2_args->alloc_hl_entries;
+	}
+
+	if (dlb2->is_vdev) {
+		err = dlb2_register_pri_mp_callbacks();
+		if (err) {
+			DLB2_LOG_ERR("dlb2_register_pri_mp_callbacks err=%d for %s\n",
+				     err, name);
 			return err;
 		}
 	}
 
+	dlb2_iface_hardware_init(&dlb2->qm_instance);
+
+	if (!dlb2->is_vdev) {
+
+		/* configure class of service */
+		{
+			struct dlb2_set_cos_bw_args
+				set_cos_bw_args = { {0} };
+			int id;
+			int ret = 0;
+
+			for (id = 0; id < DLB2_COS_NUM_VALS; id++) {
+				set_cos_bw_args.cos_id = id;
+				set_cos_bw_args.bandwidth = dlb2->cos_bw[id];
+				ret = dlb2_iface_set_cos_bw(&dlb2->qm_instance,
+								&set_cos_bw_args);
+				if (ret != 0)
+					break;
+			}
+			if (ret) {
+				DLB2_LOG_ERR("dlb2: failed to configure class of service, err=%d\n",
+						err);
+				return err;
+			}
+		}
+
+	} else {
+		DLB2_LOG_DBG("Port specific COS is not supported for bifurcated mode,"
+				"pls set COS through sysfs calls.\n");
+	}
+
 	err = dlb2_iface_get_cq_poll_mode(&dlb2->qm_instance, &dlb2->poll_mode);
 	if (err < 0) {
 		DLB2_LOG_ERR("dlb2: failed to get the poll mode, err=%d\n",
@@ -4716,31 +6075,58 @@ dlb2_primary_eventdev_probe(struct rte_eventdev *dev,
 
 	rte_spinlock_init(&dlb2->qm_instance.resource_lock);
 
+	dlb2_qm_mmio_fn_init();
+
 	dlb2_iface_low_level_io_init();
 
 	dlb2_entry_points_init(dev);
 
+	dlb2_init_queue_depth_thresholds(dlb2,
+					 dlb2_args->qid_depth_thresholds.val);
+
+	err = dlb2_init_port_dequeue_wait(dlb2,
+					  dlb2_args->port_dequeue_wait.val);
+	if (err) {
+		DLB2_LOG_ERR("dlb2: failed to init port_dequeue_wait, err=%d\n", err);
+		return err;
+	}
+
+	dlb2_init_quanta_credits(dlb2,
+				 dlb2_args->sw_credit_quanta,
+				 dlb2_args->hw_credit_quanta);
+
 	return 0;
 }
 
 int
 dlb2_secondary_eventdev_probe(struct rte_eventdev *dev,
-			      const char *name)
+			      const char *name,
+			      bool is_vdev)
 {
 	struct dlb2_eventdev *dlb2;
 	int err;
 
 	dlb2 = dev->data->dev_private;
 
-	evdev_dlb2_default_info.driver_name = name;
+	dlb2_init_evdev_rsrcs(dlb2, name);
 
 	err = dlb2_iface_open(&dlb2->qm_instance, name);
 	if (err < 0) {
 		DLB2_LOG_ERR("could not open event hardware device, err=%d\n",
 			     err);
+		rte_event_pmd_release(dev);
 		return err;
 	}
 
+	if (is_vdev) {
+		err = dlb2_register_sec_mp_callbacks();
+		if (err) {
+			DLB2_LOG_ERR("dlb2_register_sec_mp_callbacks err=%d for %s\n",
+				     err, name);
+			return err;
+		}
+	}
+
 	err = dlb2_hw_query_resources(dlb2);
 	if (err) {
 		DLB2_LOG_ERR("get resources err=%d for %s\n",
@@ -4748,6 +6134,8 @@ dlb2_secondary_eventdev_probe(struct rte_eventdev *dev,
 		return err;
 	}
 
+	dlb2_qm_mmio_fn_init();
+
 	dlb2_iface_low_level_io_init();
 
 	dlb2_entry_points_init(dev);
@@ -4765,29 +6153,37 @@ dlb2_parse_params(const char *params,
 	static const char * const args[] = { NUMA_NODE_ARG,
 					     DLB2_MAX_NUM_EVENTS,
 					     DLB2_NUM_DIR_CREDITS,
-					     DEV_ID_ARG,
+					     HWDEV_ID_ARG,
 					     DLB2_QID_DEPTH_THRESH_ARG,
+					     DLB2_VECTOR_OPTS_DISAB_ARG,
+					     DLB2_PORT_DEQUEUE_WAIT_ARG,
 					     DLB2_POLL_INTERVAL_ARG,
 					     DLB2_SW_CREDIT_QUANTA_ARG,
 					     DLB2_HW_CREDIT_QUANTA_ARG,
 					     DLB2_DEPTH_THRESH_ARG,
-					     DLB2_VECTOR_OPTS_ENAB_ARG,
+					     DLB2_NUM_ORDERED_QUEUES_0,
+					     DLB2_NUM_ORDERED_QUEUES_1,
 					     DLB2_MAX_CQ_DEPTH,
 					     DLB2_MAX_ENQ_DEPTH,
-					     DLB2_CQ_WEIGHT,
+					     DLB2_PRODUCER_COREMASK,
 					     DLB2_PORT_COS,
 					     DLB2_COS_BW,
-					     DLB2_PRODUCER_COREMASK,
 					     DLB2_DEFAULT_LDB_PORT_ALLOCATION_ARG,
+					     DLB2_ENABLE_CQ_WEIGHT_ARG,
+					     DLB2_USE_DEFAULT_HL,
+					     DLB2_ALLOC_HL_ENTRIES,
 					     NULL };
 
 	if (params != NULL && params[0] != '\0') {
 		struct rte_kvargs *kvlist = rte_kvargs_parse(params, args);
 
 		if (kvlist == NULL) {
-			RTE_LOG(INFO, PMD,
-				"Ignoring unsupported parameters when creating device '%s'\n",
+			DLB2_LOG_ERR("Unsupported parameters when creating device '%s'\n",
 				name);
+			DLB2_LOG_ERR("Supported params:");
+			for (int i = 0; i < (int) RTE_DIM(args) && args[i]; ++i)
+				rte_log(RTE_LOG_NOTICE, eventdev_dlb2_log_level, "\t%s\n", args[i]);
+			ret = -EINVAL;
 		} else {
 			int ret = rte_kvargs_process(kvlist, NUMA_NODE_ARG,
 						     set_numa_node,
@@ -4799,9 +6195,18 @@ dlb2_parse_params(const char *params,
 				return ret;
 			}
 
-			ret = rte_kvargs_process(kvlist, DLB2_MAX_NUM_EVENTS,
-						 set_max_num_events,
-						 &dlb2_args->max_num_events);
+			if (version == DLB2_HW_V2) {
+				ret = rte_kvargs_process(kvlist,
+						DLB2_MAX_NUM_EVENTS,
+						set_max_num_events,
+						&dlb2_args->max_num_events);
+			} else {
+				ret = rte_kvargs_process(kvlist,
+						DLB2_MAX_NUM_EVENTS,
+						set_max_num_events_v2_5,
+						&dlb2_args->max_num_events);
+			}
+
 			if (ret != 0) {
 				DLB2_LOG_ERR("%s: Error parsing max_num_events parameter",
 					     name);
@@ -4821,11 +6226,22 @@ dlb2_parse_params(const char *params,
 					return ret;
 				}
 			}
-			ret = rte_kvargs_process(kvlist, DEV_ID_ARG,
-						 set_dev_id,
-						 &dlb2_args->dev_id);
+			ret = rte_kvargs_process(kvlist, HWDEV_ID_ARG,
+						 set_hwdev_id,
+						 &dlb2_args->hwdev_id);
+			if (ret != 0) {
+				DLB2_LOG_ERR("%s: Error parsing hwdev_id parameter",
+					     name);
+				rte_kvargs_free(kvlist);
+				return ret;
+			}
+
+			ret = rte_kvargs_process(kvlist,
+						 DLB2_VECTOR_OPTS_DISAB_ARG,
+						 set_vector_opts_disab,
+						 &dlb2_args->vector_opts_disab);
 			if (ret != 0) {
-				DLB2_LOG_ERR("%s: Error parsing dev_id parameter",
+				DLB2_LOG_ERR("%s: Error parsing vector_opts_disab parameter",
 					     name);
 				rte_kvargs_free(kvlist);
 				return ret;
@@ -4851,6 +6267,26 @@ dlb2_parse_params(const char *params,
 				return ret;
 			}
 
+			if (version == DLB2_HW_V2) {
+				ret = rte_kvargs_process(
+					kvlist,
+					DLB2_PORT_DEQUEUE_WAIT_ARG,
+					set_port_dequeue_wait,
+					&dlb2_args->port_dequeue_wait);
+			} else {
+				ret = rte_kvargs_process(
+					kvlist,
+					DLB2_PORT_DEQUEUE_WAIT_ARG,
+					set_port_dequeue_wait_v2_5,
+					&dlb2_args->port_dequeue_wait);
+			}
+			if (ret != 0) {
+				DLB2_LOG_ERR("%s: Error parsing port_dequeue_wait parameter",
+					     name);
+				rte_kvargs_free(kvlist);
+				return ret;
+			}
+
 			ret = rte_kvargs_process(kvlist, DLB2_POLL_INTERVAL_ARG,
 						 set_poll_interval,
 						 &dlb2_args->poll_interval);
@@ -4861,6 +6297,16 @@ dlb2_parse_params(const char *params,
 				return ret;
 			}
 
+			ret = rte_kvargs_process(kvlist, DLB2_DEPTH_THRESH_ARG,
+						 set_default_depth_thresh,
+						 &dlb2_args->default_depth_thresh);
+			if (ret != 0) {
+				DLB2_LOG_ERR("%s: Error parsing set depth thresh parameter",
+					     name);
+				rte_kvargs_free(kvlist);
+				return ret;
+			}
+
 			ret = rte_kvargs_process(kvlist,
 						 DLB2_SW_CREDIT_QUANTA_ARG,
 						 set_sw_credit_quanta,
@@ -4883,33 +6329,34 @@ dlb2_parse_params(const char *params,
 				return ret;
 			}
 
-			ret = rte_kvargs_process(kvlist, DLB2_DEPTH_THRESH_ARG,
-					set_default_depth_thresh,
-					&dlb2_args->default_depth_thresh);
+			ret = rte_kvargs_process(kvlist,
+					 DLB2_NUM_ORDERED_QUEUES_0,
+					 set_num_ordered_queues,
+					 &dlb2_args->num_ordered_queues_0);
 			if (ret != 0) {
-				DLB2_LOG_ERR("%s: Error parsing set depth thresh parameter",
+				DLB2_LOG_ERR("%s: Error parsing num_ordered_queues_0 parameter",
 					     name);
 				rte_kvargs_free(kvlist);
 				return ret;
 			}
 
 			ret = rte_kvargs_process(kvlist,
-					DLB2_VECTOR_OPTS_ENAB_ARG,
-					set_vector_opts_enab,
-					&dlb2_args->vector_opts_enabled);
+					 DLB2_NUM_ORDERED_QUEUES_1,
+					 set_num_ordered_queues,
+					 &dlb2_args->num_ordered_queues_1);
 			if (ret != 0) {
-				DLB2_LOG_ERR("%s: Error parsing vector opts enabled",
+				DLB2_LOG_ERR("%s: Error parsing num_ordered_queues_1 parameter",
 					     name);
 				rte_kvargs_free(kvlist);
 				return ret;
 			}
 
 			ret = rte_kvargs_process(kvlist,
-					DLB2_MAX_CQ_DEPTH,
-					set_max_cq_depth,
-					&dlb2_args->max_cq_depth);
+						 DLB2_MAX_CQ_DEPTH,
+						 set_max_cq_depth,
+						 &dlb2_args->max_cq_depth);
 			if (ret != 0) {
-				DLB2_LOG_ERR("%s: Error parsing max cq depth",
+				DLB2_LOG_ERR("%s: Error parsing vector opts enabled",
 					     name);
 				rte_kvargs_free(kvlist);
 				return ret;
@@ -4927,11 +6374,11 @@ dlb2_parse_params(const char *params,
 			}
 
 			ret = rte_kvargs_process(kvlist,
-					DLB2_CQ_WEIGHT,
-					set_cq_weight,
-					&dlb2_args->cq_weight);
+					 DLB2_PRODUCER_COREMASK,
+					 set_producer_coremask,
+					 &dlb2_args->producer_coremask);
 			if (ret != 0) {
-				DLB2_LOG_ERR("%s: Error parsing cq weight on",
+				DLB2_LOG_ERR("%s: Error parsing producer coremask",
 					     name);
 				rte_kvargs_free(kvlist);
 				return ret;
@@ -4959,25 +6406,45 @@ dlb2_parse_params(const char *params,
 				return ret;
 			}
 
-
 			ret = rte_kvargs_process(kvlist,
-						 DLB2_PRODUCER_COREMASK,
-						 set_producer_coremask,
-						 &dlb2_args->producer_coremask);
+						 DLB2_DEFAULT_LDB_PORT_ALLOCATION_ARG,
+						 set_default_ldb_port_allocation,
+						 &dlb2_args->default_ldb_port_allocation);
 			if (ret != 0) {
-				DLB2_LOG_ERR(
-					"%s: Error parsing producer coremask",
-					name);
+				DLB2_LOG_ERR("%s: Error parsing ldb default port allocation arg",
+					     name);
 				rte_kvargs_free(kvlist);
 				return ret;
 			}
 
 			ret = rte_kvargs_process(kvlist,
-						 DLB2_DEFAULT_LDB_PORT_ALLOCATION_ARG,
-						 set_default_ldb_port_allocation,
-						 &dlb2_args->default_ldb_port_allocation);
+						 DLB2_ENABLE_CQ_WEIGHT_ARG,
+						 set_enable_cq_weight,
+						 &dlb2_args->enable_cq_weight);
 			if (ret != 0) {
-				DLB2_LOG_ERR("%s: Error parsing ldb default port allocation arg",
+				DLB2_LOG_ERR("%s: Error parsing enable_cq_weight arg",
+					     name);
+				rte_kvargs_free(kvlist);
+				return ret;
+			}
+			if (version == DLB2_HW_V2 && dlb2_args->enable_cq_weight)
+				DLB2_LOG_INFO("Ignoring 'enable_cq_weight=y'. Only supported for 2.5 HW onwards");
+
+			ret = rte_kvargs_process(kvlist, DLB2_USE_DEFAULT_HL,
+						 set_hl_override,
+						 &dlb2_args->use_default_hl);
+			if (ret != 0) {
+				DLB2_LOG_ERR("%s: Error parsing hl_override arg",
+					     name);
+				rte_kvargs_free(kvlist);
+				return ret;
+			}
+
+			ret = rte_kvargs_process(kvlist, DLB2_ALLOC_HL_ENTRIES,
+						 set_hl_entries,
+						 &dlb2_args->alloc_hl_entries);
+			if (ret != 0) {
+				DLB2_LOG_ERR("%s: Error parsing hl_override arg",
 					     name);
 				rte_kvargs_free(kvlist);
 				return ret;
@@ -4988,4 +6455,4 @@ dlb2_parse_params(const char *params,
 	}
 	return ret;
 }
-RTE_LOG_REGISTER_DEFAULT(eventdev_dlb2_log_level, NOTICE);
+RTE_LOG_REGISTER(eventdev_dlb2_log_level, pmd.event.dlb2, NOTICE);
diff --git a/drivers/event/dlb2/dlb2_frag.h b/drivers/event/dlb2/dlb2_frag.h
new file mode 100644
index 0000000..18163d3
--- /dev/null
+++ b/drivers/event/dlb2/dlb2_frag.h
@@ -0,0 +1,13 @@
+/* SPDX-License-Identifier: BSD-3-Clause
+ * Copyright(c) 2016-2017 Intel Corporation
+ */
+
+#ifndef _DLB2_FRAG_H_
+#define _DLB2_FRAG_H_
+
+/* Fragments/partials are not supported by the API, but the capability is in
+ * the PMD in case future support is added.
+ */
+#define RTE_EVENT_DLB2_OP_FRAG 3
+
+#endif	/* _DLB2_FRAG_H_ */
diff --git a/drivers/event/dlb2/dlb2_iface.c b/drivers/event/dlb2/dlb2_iface.c
index 100db43..77d2d08 100644
--- a/drivers/event/dlb2/dlb2_iface.c
+++ b/drivers/event/dlb2/dlb2_iface.c
@@ -17,10 +17,13 @@ void (*dlb2_iface_low_level_io_init)(void);
 int (*dlb2_iface_open)(struct dlb2_hw_dev *handle, const char *name);
 
 int (*dlb2_iface_get_device_version)(struct dlb2_hw_dev *handle,
-				     uint8_t *revision);
+				     uint8_t *revision,
+				     uint8_t *version);
 
 void (*dlb2_iface_hardware_init)(struct dlb2_hw_dev *handle);
 
+void (*dlb2_iface_port_mmap)(struct dlb2_port *qm_port);
+
 int (*dlb2_iface_get_cq_poll_mode)(struct dlb2_hw_dev *handle,
 				   enum dlb2_cq_poll_modes *mode);
 
@@ -30,7 +33,7 @@ int (*dlb2_iface_get_num_resources)(struct dlb2_hw_dev *handle,
 int (*dlb2_iface_sched_domain_create)(struct dlb2_hw_dev *handle,
 				struct dlb2_create_sched_domain_args *args);
 
-void (*dlb2_iface_domain_reset)(struct dlb2_eventdev *dlb2);
+int (*dlb2_iface_domain_reset)(struct dlb2_eventdev *dlb2);
 
 int (*dlb2_iface_ldb_queue_create)(struct dlb2_hw_dev *handle,
 				   struct dlb2_create_ldb_queue_args *cfg);
@@ -46,11 +49,13 @@ int (*dlb2_iface_get_sn_occupancy)(struct dlb2_hw_dev *handle,
 
 int (*dlb2_iface_ldb_port_create)(struct dlb2_hw_dev *handle,
 				  struct dlb2_create_ldb_port_args *cfg,
-				  enum dlb2_cq_poll_modes poll_mode);
+				  enum dlb2_cq_poll_modes poll_mode,
+				  uint8_t evdev_id);
 
 int (*dlb2_iface_dir_port_create)(struct dlb2_hw_dev *handle,
 				  struct dlb2_create_dir_port_args *cfg,
-				  enum dlb2_cq_poll_modes poll_mode);
+				  enum dlb2_cq_poll_modes poll_mode,
+				  uint8_t evdev_id);
 
 int (*dlb2_iface_dir_queue_create)(struct dlb2_hw_dev *handle,
 				   struct dlb2_create_dir_queue_args *cfg);
@@ -61,21 +66,36 @@ int (*dlb2_iface_map_qid)(struct dlb2_hw_dev *handle,
 int (*dlb2_iface_unmap_qid)(struct dlb2_hw_dev *handle,
 			    struct dlb2_unmap_qid_args *cfg);
 
+int (*dlb2_iface_block_on_cq_interrupt)(struct dlb2_hw_dev *handle,
+					int port_id, bool is_ldb,
+					volatile void *cq_va, uint8_t cq_gen,
+					bool arm);
+
 int (*dlb2_iface_pending_port_unmaps)(struct dlb2_hw_dev *handle,
 				struct dlb2_pending_port_unmaps_args *args);
 
 int (*dlb2_iface_sched_domain_start)(struct dlb2_hw_dev *handle,
 				     struct dlb2_start_domain_args *cfg);
 
+int (*dlb2_iface_sched_domain_stop)(struct dlb2_hw_dev *handle,
+				    struct dlb2_stop_domain_args *cfg);
+
 int (*dlb2_iface_get_ldb_queue_depth)(struct dlb2_hw_dev *handle,
 				struct dlb2_get_ldb_queue_depth_args *args);
 
 int (*dlb2_iface_get_dir_queue_depth)(struct dlb2_hw_dev *handle,
 				struct dlb2_get_dir_queue_depth_args *args);
 
+int (*dlb2_iface_get_sched_idle_counts)(struct dlb2_hw_dev *handle,
+					void *data);
 
 int (*dlb2_iface_enable_cq_weight)(struct dlb2_hw_dev *handle,
 				   struct dlb2_enable_cq_weight_args *args);
 
+int (*dlb2_iface_set_cq_inflight_ctrl)(struct dlb2_hw_dev *handle,
+				       struct dlb2_cq_inflight_ctrl_args *args);
+
 int (*dlb2_iface_set_cos_bw)(struct dlb2_hw_dev *handle,
 			     struct dlb2_set_cos_bw_args *args);
+
+int (*dlb2_iface_port_ctrl)(struct dlb2_port *qm_port, bool enable);
\ No newline at end of file
diff --git a/drivers/event/dlb2/dlb2_iface.h b/drivers/event/dlb2/dlb2_iface.h
index dc0c446..2241ff8 100644
--- a/drivers/event/dlb2/dlb2_iface.h
+++ b/drivers/event/dlb2/dlb2_iface.h
@@ -16,10 +16,13 @@ extern void (*dlb2_iface_low_level_io_init)(void);
 extern int (*dlb2_iface_open)(struct dlb2_hw_dev *handle, const char *name);
 
 extern int (*dlb2_iface_get_device_version)(struct dlb2_hw_dev *handle,
-					    uint8_t *revision);
+					    uint8_t *revision,
+					    uint8_t *version);
 
 extern void (*dlb2_iface_hardware_init)(struct dlb2_hw_dev *handle);
 
+extern void (*dlb2_iface_port_mmap)(struct dlb2_port *qm_port);
+
 extern int (*dlb2_iface_get_cq_poll_mode)(struct dlb2_hw_dev *handle,
 					  enum dlb2_cq_poll_modes *mode);
 
@@ -29,7 +32,7 @@ extern int (*dlb2_iface_get_num_resources)(struct dlb2_hw_dev *handle,
 extern int (*dlb2_iface_sched_domain_create)(struct dlb2_hw_dev *handle,
 				 struct dlb2_create_sched_domain_args *args);
 
-extern void (*dlb2_iface_domain_reset)(struct dlb2_eventdev *dlb2);
+extern int (*dlb2_iface_domain_reset)(struct dlb2_eventdev *dlb2);
 
 extern int (*dlb2_iface_ldb_queue_create)(struct dlb2_hw_dev *handle,
 				  struct dlb2_create_ldb_queue_args *cfg);
@@ -45,11 +48,13 @@ extern int (*dlb2_iface_get_sn_occupancy)(struct dlb2_hw_dev *handle,
 
 extern int (*dlb2_iface_ldb_port_create)(struct dlb2_hw_dev *handle,
 					 struct dlb2_create_ldb_port_args *cfg,
-					 enum dlb2_cq_poll_modes poll_mode);
+					 enum dlb2_cq_poll_modes poll_mode,
+					 uint8_t evdev_id);
 
 extern int (*dlb2_iface_dir_port_create)(struct dlb2_hw_dev *handle,
 					 struct dlb2_create_dir_port_args *cfg,
-					 enum dlb2_cq_poll_modes poll_mode);
+					 enum dlb2_cq_poll_modes poll_mode,
+					 uint8_t evdev_id);
 
 extern int (*dlb2_iface_dir_queue_create)(struct dlb2_hw_dev *handle,
 					struct dlb2_create_dir_queue_args *cfg);
@@ -60,23 +65,38 @@ extern int (*dlb2_iface_map_qid)(struct dlb2_hw_dev *handle,
 extern int (*dlb2_iface_unmap_qid)(struct dlb2_hw_dev *handle,
 				   struct dlb2_unmap_qid_args *cfg);
 
+extern int (*dlb2_iface_block_on_cq_interrupt)(struct dlb2_hw_dev *handle,
+					       int port_id, bool is_ldb,
+					       volatile void *cq_va,
+					       uint8_t cq_gen,
+					       bool arm);
+
 extern int (*dlb2_iface_pending_port_unmaps)(struct dlb2_hw_dev *handle,
 				struct dlb2_pending_port_unmaps_args *args);
 
 extern int (*dlb2_iface_sched_domain_start)(struct dlb2_hw_dev *handle,
 				struct dlb2_start_domain_args *cfg);
 
+extern int (*dlb2_iface_sched_domain_stop)(struct dlb2_hw_dev *handle,
+				struct dlb2_stop_domain_args *cfg);
+
 extern int (*dlb2_iface_get_ldb_queue_depth)(struct dlb2_hw_dev *handle,
 				struct dlb2_get_ldb_queue_depth_args *args);
 
 extern int (*dlb2_iface_get_dir_queue_depth)(struct dlb2_hw_dev *handle,
 				struct dlb2_get_dir_queue_depth_args *args);
 
+extern int (*dlb2_iface_get_sched_idle_counts)(struct dlb2_hw_dev *handle,
+					       void *data);
 
 extern int (*dlb2_iface_enable_cq_weight)(struct dlb2_hw_dev *handle,
 					  struct dlb2_enable_cq_weight_args *args);
 
+extern int (*dlb2_iface_set_cq_inflight_ctrl)(struct dlb2_hw_dev *handle,
+					      struct dlb2_cq_inflight_ctrl_args *args);
 extern int (*dlb2_iface_set_cos_bw)(struct dlb2_hw_dev *handle,
 				    struct dlb2_set_cos_bw_args *args);
 
+extern int (*dlb2_iface_port_ctrl)(struct dlb2_port *qm_port, bool enable);
+
 #endif /* _DLB2_IFACE_H_ */
diff --git a/drivers/event/dlb2/dlb2_inline_fns.h b/drivers/event/dlb2/dlb2_inline_fns.h
index 1429281..1842120 100644
--- a/drivers/event/dlb2/dlb2_inline_fns.h
+++ b/drivers/event/dlb2/dlb2_inline_fns.h
@@ -16,6 +16,27 @@ dlb2_pmd_priv(const struct rte_eventdev *eventdev)
 }
 
 static inline void
+dlb2_movntdq(void *pp_addr, void *qe4)
+{
+	/* Move entire 64B cache line of QEs, 128 bits (16B) at a time. */
+	long long *_qe  = (long long *)qe4;
+
+	__v2di src_data0 = (__v2di){_qe[0], _qe[1]};
+	__v2di src_data1 = (__v2di){_qe[2], _qe[3]};
+	__v2di src_data2 = (__v2di){_qe[4], _qe[5]};
+	__v2di src_data3 = (__v2di){_qe[6], _qe[7]};
+
+	__builtin_ia32_movntdq((__v2di *)pp_addr + 0, (__v2di)src_data0);
+	rte_wmb();
+	__builtin_ia32_movntdq((__v2di *)pp_addr + 1, (__v2di)src_data1);
+	rte_wmb();
+	__builtin_ia32_movntdq((__v2di *)pp_addr + 2, (__v2di)src_data2);
+	rte_wmb();
+	__builtin_ia32_movntdq((__v2di *)pp_addr + 3, (__v2di)src_data3);
+	rte_wmb();
+}
+
+static inline void
 dlb2_movntdq_single(void *pp_addr, void *qe4)
 {
 	long long *_qe  = (long long *)qe4;
@@ -25,6 +46,15 @@ dlb2_movntdq_single(void *pp_addr, void *qe4)
 }
 
 static inline void
+dlb2_movdir64b_single(void *pp_addr, void *qe4)
+{
+	asm volatile(".byte 0x66, 0x0f, 0x38, 0xf8, 0x02"
+		:
+	: "a" (pp_addr), "d" (qe4));
+}
+
+
+static inline void
 dlb2_movdir64b(void *dest, void *src)
 {
 	asm volatile(".byte 0x66, 0x0f, 0x38, 0xf8, 0x02"
diff --git a/drivers/event/dlb2/dlb2_priv.h b/drivers/event/dlb2/dlb2_priv.h
index f4b9e7f..a64050d 100644
--- a/drivers/event/dlb2/dlb2_priv.h
+++ b/drivers/event/dlb2/dlb2_priv.h
@@ -1,5 +1,5 @@
 /* SPDX-License-Identifier: BSD-3-Clause
- * Copyright(c) 2016-2022 Intel Corporation
+ * Copyright(c) 2016-2023 Intel Corporation
  */
 
 #ifndef _DLB2_PRIV_H_
@@ -13,7 +13,7 @@
 #include "dlb2_user.h"
 #include "dlb2_log.h"
 #include "rte_pmd_dlb2.h"
-
+#include <bus_pci_driver.h>
 #ifndef RTE_LIBRTE_PMD_DLB2_QUELL_STATS
 #define DLB2_INC_STAT(_stat, _incr_val) ((_stat) += _incr_val)
 #else
@@ -22,6 +22,11 @@
 
 #define EVDEV_DLB2_NAME_PMD dlb2_event
 
+/* DLB2 defines moved from rte_config.h */
+#define RTE_LIBRTE_PMD_DLB2_POLL_INTERVAL 1000
+#define RTE_LIBRTE_PMD_DLB2_UMWAIT_CTL_STATE  0
+#define RTE_PMD_DLB2_DEFAULT_DEPTH_THRESH 256
+
 /* Default values for command line devargs */
 #define DLB2_POLL_INTERVAL_DEFAULT 1000
 #define DLB2_SW_CREDIT_QUANTA_DEFAULT 32 /* Default = Worker */
@@ -33,25 +38,30 @@
 #define DLB2_MIN_ENQ_DEPTH_OVERRIDE 32
 #define DLB2_MAX_ENQ_DEPTH_OVERRIDE 1024
 
-
 /*  command line arg strings */
 #define NUMA_NODE_ARG "numa_node"
 #define DLB2_MAX_NUM_EVENTS "max_num_events"
 #define DLB2_NUM_DIR_CREDITS "num_dir_credits"
-#define DEV_ID_ARG "dev_id"
+#define HWDEV_ID_ARG "hwdev_id"
 #define DLB2_QID_DEPTH_THRESH_ARG "qid_depth_thresh"
+#define DLB2_PORT_DEQUEUE_WAIT_ARG "port_dequeue_wait"
 #define DLB2_POLL_INTERVAL_ARG "poll_interval"
 #define DLB2_SW_CREDIT_QUANTA_ARG "sw_credit_quanta"
 #define DLB2_HW_CREDIT_QUANTA_ARG "hw_credit_quanta"
 #define DLB2_DEPTH_THRESH_ARG "default_depth_thresh"
-#define DLB2_VECTOR_OPTS_ENAB_ARG "vector_opts_enable"
+#define DLB2_VECTOR_OPTS_DISAB_ARG "vector_opts_disable"
+#define DLB2_NUM_ORDERED_QUEUES_0 "num_ordered_queues_0"
+#define DLB2_NUM_ORDERED_QUEUES_1 "num_ordered_queues_1"
 #define DLB2_MAX_CQ_DEPTH "max_cq_depth"
-#define DLB2_MAX_ENQ_DEPTH "max_enqueue_depth"
-#define DLB2_CQ_WEIGHT "cq_weight"
+#define DLB2_MAX_ENQ_DEPTH "max_enq_depth"
+#define DLB2_PRODUCER_COREMASK "producer_coremask"
 #define DLB2_PORT_COS "port_cos"
 #define DLB2_COS_BW "cos_bw"
-#define DLB2_PRODUCER_COREMASK "producer_coremask"
-#define DLB2_DEFAULT_LDB_PORT_ALLOCATION_ARG "default_port_allocation"
+#define DLB2_DEFAULT_LDB_PORT_ALLOCATION_ARG "ldb_port_default"
+#define DLB2_ENABLE_CQ_WEIGHT_ARG "enable_cq_weight"
+#define DLB2_USE_DEFAULT_HL "use_default_hl"
+#define DLB2_ALLOC_HL_ENTRIES "alloc_hl_entries"
+
 
 /* Begin HW related defines and structs */
 
@@ -75,13 +85,14 @@
 #define DLB2_MAX_NUM_FLOWS (64 * 1024)
 #define DLB2_MAX_NUM_LDB_CREDITS (8 * 1024)
 #define DLB2_MAX_NUM_DIR_CREDITS(ver)		(ver == DLB2_HW_V2 ? 4096 : 0)
-#define DLB2_MAX_NUM_CREDITS(ver)		(ver == DLB2_HW_V2 ? \
-						 0 : DLB2_MAX_NUM_LDB_CREDITS)
+#define DLB2_MAX_NUM_CREDITS(ver)		(ver == DLB2_HW_V2 ? 0 : 16384)
 #define DLB2_MAX_NUM_LDB_CREDIT_POOLS 64
 #define DLB2_MAX_NUM_DIR_CREDIT_POOLS 64
 #define DLB2_MAX_NUM_HIST_LIST_ENTRIES 2048
+#define DLB2_MAX_NUM_AQOS_ENTRIES 2048
 #define DLB2_MAX_NUM_QIDS_PER_LDB_CQ 8
 #define DLB2_QID_PRIORITIES 8
+#define DLB2_MAX_FRAGS 16
 #define DLB2_MAX_DEVICE_PATH 32
 #define DLB2_MIN_DEQUEUE_TIMEOUT_NS 1
 /* Note: "- 1" here to support the timeout range check in eventdev_autotest */
@@ -89,10 +100,24 @@
 #define DLB2_SW_CREDIT_BATCH_SZ 32 /* Default - Worker */
 #define DLB2_SW_CREDIT_P_BATCH_SZ 256 /* Producer */
 #define DLB2_SW_CREDIT_C_BATCH_SZ 256 /* Consumer */
+#define DLB2_CREDIT_QUANTA_LIMIT 1024 /*Total W:P:C*/
+#define DLB2_CREDIT_QUANTA_ARRAY_SZ 3
+#define DLB2_CQ_WORKER_INDEX 0
+#define DLB2_CQ_PRODUCER_INDEX 1
+#define DLB2_CQ_CONSUMER_INDEX 2
 #define DLB2_NUM_SN_GROUPS 2
 #define DLB2_MAX_LDB_SN_ALLOC 1024
 #define DLB2_MAX_QUEUE_DEPTH_THRESHOLD 8191
+#define DLB2_WB_CNTL_RATE_LIMIT 3
 #define DLB2_MAX_NUM_LDB_PORTS_PER_COS (DLB2_MAX_NUM_LDB_PORTS/DLB2_COS_NUM_VALS)
+#define DLB2_COREMASK_LEN 36
+
+enum dlb2_port_dequeue_wait_types {
+	DLB2_PORT_DEQUEUE_WAIT_POLLING,
+	DLB2_PORT_DEQUEUE_WAIT_INTERRUPT,
+	DLB2_PORT_DEQUEUE_WAIT_UMWAIT,
+	DLB2_NUM_PORT_DEQUEUE_WAIT_TYPES /* Must be last */
+};
 
 /* 2048 total hist list entries and 64 total ldb ports, which
  * makes for 2048/64 == 32 hist list entries per port. However, CQ
@@ -101,7 +126,8 @@
  */
 #define DLB2_MAX_HL_ENTRIES 2048
 #define DLB2_MIN_CQ_DEPTH 1
-#define DLB2_DEFAULT_CQ_DEPTH 32
+#define DLB2_DEFAULT_CQ_DEPTH 128  /* Can be overridden using max_cq_depth command line parameter */
+#define DLB2_FIXED_CQ_HL_SIZE 32  /* Used when ENABLE_FIXED_HL_SIZE is true */
 #define DLB2_MIN_HARDWARE_CQ_DEPTH 8
 #define DLB2_NUM_HIST_LIST_ENTRIES_PER_LDB_PORT \
 	DLB2_DEFAULT_CQ_DEPTH
@@ -121,9 +147,12 @@
 				    DLB2_LDB_CQ_MAX_SIZE)
 #define PP_BASE(is_dir) ((is_dir) ? DLB2_DIR_PP_BASE : DLB2_LDB_PP_BASE)
 
+#define PAGE_SIZE (sysconf(_SC_PAGESIZE))
+
 #define DLB2_NUM_QES_PER_CACHE_LINE 4
+#define DLB2_NUM_BYTES_PER_CACHE_LINE 64
 
-#define DLB2_MAX_ENQUEUE_DEPTH 32
+#define DLB2_MAX_ENQUEUE_DEPTH 128
 #define DLB2_MIN_ENQUEUE_DEPTH 4
 
 #define DLB2_NAME_SIZE 64
@@ -140,11 +169,6 @@
 #define EV_TO_DLB2_PRIO(x) ((x) >> 5)
 #define DLB2_TO_EV_PRIO(x) ((x) << 5)
 
-enum dlb2_hw_ver {
-	DLB2_HW_VER_2,
-	DLB2_HW_VER_2_5,
-};
-
 enum dlb2_hw_port_types {
 	DLB2_LDB_PORT,
 	DLB2_DIR_PORT,
@@ -190,12 +214,18 @@ struct dlb2_hw_rsrcs {
 };
 
 struct dlb2_hw_resource_info {
-	/**> Max resources that can be provided */
+	/* Max resources that can be provided */
 	struct dlb2_hw_rsrcs hw_rsrc_max;
 	int num_sched_domains;
 	uint32_t socket_id;
 };
 
+/* Fragments/partials are not supported by the API, but the capability is in
+ * the PMD in case future support is added -- at which point this #define can
+ * be removed.
+ */
+#define RTE_EVENT_DLB2_OP_FRAG 3
+
 enum dlb2_enqueue_type {
 	/**>
 	 * New : Used to inject a new packet into the QM.
@@ -208,8 +238,16 @@ enum dlb2_enqueue_type {
 	 */
 	DLB2_ENQ_FWD,
 	/**>
+	 * Partial : Enqueue an ordered packet fragment. This can be
+	 * used for multicast transmit or IP fragmentation, for example. The
+	 * last fragment is indicated by a release operation, either with
+	 * DLB2_ENQ_DROP or DLB2_ENQ_FWD.
+	 */
+	DLB2_ENQ_PARTIAL,
+	/**>
 	 * Enqueue Drop : Release an inflight packet. Must be called with
-	 * event == NULL. Used to drop a packet.
+	 * event == NULL. Used to drop a packet or release a set of ordered
+	 * fragments for egress.
 	 *
 	 * Note that all packets dequeued from a load-balanced port must be
 	 * released, either with DLB2_ENQ_DROP or DLB2_ENQ_FWD.
@@ -223,9 +261,9 @@ enum dlb2_enqueue_type {
 /* hw-specific format - do not change */
 
 struct dlb2_event_type {
-	uint16_t major:4;
-	uint16_t unused:4;
-	uint16_t sub:8;
+	uint16_t flow_id_hi : 4;
+	uint16_t sub : 8;
+	uint16_t major : 4;
 };
 
 union dlb2_opaque_data {
@@ -243,6 +281,7 @@ struct dlb2_msg_info {
 #define DLB2_NEW_CMD_BYTE 0x08
 #define DLB2_FWD_CMD_BYTE 0x0A
 #define DLB2_COMP_CMD_BYTE 0x02
+#define DLB2_FRAG_CMD_BYTE 0x0C
 #define DLB2_POP_CMD_BYTE 0x01
 #define DLB2_NOOP_CMD_BYTE 0x00
 
@@ -344,7 +383,6 @@ struct dlb2_port {
 	uint32_t id;
 	bool is_directed;
 	bool gen_bit;
-	uint16_t dir_credits;
 	uint32_t dequeue_depth;
 	enum dlb2_token_pop_mode token_pop_mode;
 	union dlb2_port_config cfg;
@@ -352,12 +390,10 @@ struct dlb2_port {
 	union {
 		struct {
 			uint16_t cached_ldb_credits;
-			uint16_t ldb_credits;
 			uint16_t cached_dir_credits;
 		};
 		struct {
 			uint16_t cached_credits;
-			uint16_t credits;
 		};
 	};
 	bool int_armed;
@@ -384,20 +420,49 @@ struct dlb2_port {
 	struct dlb2_cq_pop_qe *consume_qe;
 	struct dlb2_eventdev *dlb2; /* back ptr */
 	struct dlb2_eventdev_port *ev_port; /* back ptr */
+	enum dlb2_port_dequeue_wait_types dequeue_wait;
 	bool use_scalar; /* force usage of scalar code */
+	uint8_t evdev_id; /* index into process-local multidev mapping array */
 	uint16_t hw_credit_quanta;
-	bool use_avx512;
-	uint32_t cq_weight;
+	bool enable_inflight_ctrl; /*DLB2.5 enable HW inflight control */
 	bool is_producer; /* True if port is of type producer */
+	uint16_t inflight_threshold; /* DLB2.5 HW inflight threshold */
+	uint16_t hist_list; /* Port history list */
+
+	/* a reorder buffer for events coming back in different order from dequeue
+	 * We use UINT8_MAX + 1 elements, but add on three no-ops to make movdirs easier at the end
+	 */
+	union {
+		__m128i m128;
+		struct dlb2_enqueue_qe qe;
+		uint64_t u64[2];
+	} enq_reorder[UINT8_MAX + 4];
+	/* id used for reordering events coming back into the scheduler */
+	uint8_t reorder_id;
+	/* id of the next entry in the reorder enqueue ring to send in */
+	uint8_t next_to_enqueue;
+	/* total frags pending across all events*/
+	uint16_t pending_frags;
+	/* Per event reorder buffer for frags.*/
+	struct {
+		union {
+			__m128i m128;
+			struct dlb2_enqueue_qe qe;
+			uint64_t u64[2];
+		} enq[DLB2_MAX_FRAGS];
+		uint8_t num_frags;
+	} frag_reorder[UINT8_MAX + 1];
 };
 
 /* Per-process per-port mmio and memory pointers */
 struct process_local_port_data {
 	uint64_t *pp_addr;
+	uint64_t *autopop_addr;
 	struct dlb2_dequeue_qe *cq_base;
 	const struct rte_memzone *mz;
 	bool mmaped;
-};
+	bool use_ded_autopop_cl;
+} __rte_cache_aligned;
 
 struct dlb2_eventdev;
 
@@ -429,11 +494,78 @@ enum dlb2_cos {
 	DLB2_COS_NUM_VALS
 };
 
+/* Time in milli seconds */
+#define DLB2_IDLE_CNT_INTERVAL 10
+
+#define DLB2_PERF_MAX_PERCENT 100
+
+struct dlb2_sched_cycles_percent {
+	uint64_t total_sched_pct;
+	uint64_t total_idle_pct;
+	uint64_t clk_gated_pct;
+	uint64_t nowork_idle_pct;
+	uint64_t nospace_idle_pct;
+	uint64_t pfriction_idle_pct;
+	uint64_t iflimit_idle_pct;
+	uint64_t fidlimit_idle_pct;
+};
+
+#define EVENT_SOURCE_DEV_PATH "/sys/bus/event_source/devices/dlb"
+
+struct dlb2_sched_idle_counts {
+	uint64_t ldb_perf_sched_cnt;
+	uint64_t ldb_perf_nowork_idle_cnt;
+	uint64_t ldb_perf_nospace_idle_cnt;
+	uint64_t ldb_perf_pfriction_idle_cnt;
+	uint64_t ldb_perf_iflimit_idle_cnt;
+	uint64_t ldb_perf_fidlimit_idle_cnt;
+	uint64_t perf_proc_on_cnt;
+	uint64_t perf_clk_on_cnt;
+	uint64_t hcw_err_cnt;
+};
+
+enum dlb2_cntrs {
+	DLB2_SCHED_CNT = 0,
+	DLB2_NO_WORK_CNT,
+	DLB2_NO_SPACE_CNT,
+	DLB2_PFRICTION_CNT,
+	DLB2_IFLIMIT_CNT,
+	DLB2_FIDLIMIT_CNT,
+	DLB2_PROC_ON_CNT,
+	DLB2_CLK_ON_CNT,
+	DLB2_HCW_ERR_CNT,
+	DLB2_MAX_NUM_CNTRS
+};
+
+/* Format of the data returned by read() on
+ * perf_event_open() file descriptor
+ */
+struct read_format {
+	uint8_t num_counters;
+	struct {
+		uint64_t counter_value;
+		uint64_t counter_index;
+	} values[];
+};
+
 struct dlb2_hw_dev {
+	char device_name[DLB2_NAME_SIZE];
+	char device_path[DLB2_MAX_DEVICE_PATH];
+	int device_path_id;
 	struct dlb2_config cfg;
 	struct dlb2_hw_resource_info info;
+	/* max resources from evdev perspective */
+	struct rte_event_dev_info evdev_rsrcs;
 	void *pf_dev; /* opaque pointer to PF PMD dev (struct dlb2_dev) */
+	int device_id;
 	uint32_t domain_id;
+	int domain_id_valid;
+	uint16_t num_ordered_queues_0; /* from command line */
+	uint16_t num_ordered_queues_1; /* from command line */
+	uint16_t req_ordered_queues_0; /* max num to request */
+	uint16_t req_ordered_queues_1; /* max num to request */
+	struct dlb2_sched_idle_counts idle_counts; /* device idle cycles */
+	struct dlb2_sched_cycles_percent dev_cycles_pct; /* idle cycle percent */
 	rte_spinlock_t resource_lock; /* for MP support */
 } __rte_cache_aligned;
 
@@ -521,6 +653,7 @@ struct dlb2_eventdev_port {
 	struct rte_event_port_conf conf; /* user-supplied configuration */
 	uint16_t inflight_credits; /* num credits this port has right now */
 	uint16_t credit_update_quanta;
+	uint32_t credit_return_count; /* count till the credit return condition is true */
 	struct dlb2_eventdev *dlb2; /* backlink optimization */
 	struct dlb2_port_stats stats __rte_cache_aligned;
 	struct dlb2_event_queue_link link[DLB2_MAX_NUM_QIDS_PER_LDB_CQ];
@@ -536,7 +669,7 @@ struct dlb2_eventdev_port {
 	bool setup_done;
 	/* enq_configured is set when the qm port is created */
 	bool enq_configured;
-	uint8_t implicit_release; /* release events before dequeuing */
+	uint8_t implicit_release; /* release events before dequeueing */
 	uint32_t cq_weight; /* DLB2.5 and above ldb ports only */
 	int cos_id; /*ldb port class of service */
 }  __rte_cache_aligned;
@@ -545,9 +678,9 @@ struct dlb2_queue {
 	uint32_t num_qid_inflights; /* User config */
 	uint32_t num_atm_inflights; /* User config */
 	enum dlb2_configuration_state config_state;
-	int  sched_type; /* LB queue only */
+	int sched_type; /* LB queue only */
 	uint8_t id;
-	bool	 is_directed;
+	bool is_directed;
 };
 
 struct dlb2_eventdev_queue {
@@ -566,6 +699,9 @@ enum dlb2_run_state {
 	DLB2_RUN_STATE_STARTED
 };
 
+#define DLB2_IS_VDEV true
+#define DLB2_NOT_VDEV false
+
 struct dlb2_eventdev {
 	struct dlb2_eventdev_port ev_ports[DLB2_MAX_NUM_PORTS_ALL];
 	struct dlb2_eventdev_queue ev_queues[DLB2_MAX_NUM_QUEUES_ALL];
@@ -590,9 +726,8 @@ struct dlb2_eventdev {
 	uint32_t new_event_limit;
 	int max_num_events_override;
 	int num_dir_credits_override;
-	bool vector_opts_enabled;
+	bool vector_opts_disab;
 	int max_cq_depth;
-	int max_enq_depth;
 	volatile enum dlb2_run_state run_state;
 	uint16_t num_dir_queues; /* total num of evdev dir queues requested */
 	union {
@@ -609,15 +744,17 @@ struct dlb2_eventdev {
 	uint16_t num_ports; /* total num of evdev ports requested */
 	uint16_t num_ldb_ports; /* total num of ldb ports requested */
 	uint16_t num_dir_ports; /* total num of dir ports requested */
+	bool is_vdev;
 	bool umwait_allowed;
 	bool global_dequeue_wait; /* Not using per dequeue wait if true */
 	enum dlb2_cq_poll_modes poll_mode;
 	int poll_interval;
-	int sw_credit_quanta;
-	int hw_credit_quanta;
 	int default_depth_thresh;
+	int sw_credit_quanta[DLB2_CREDIT_QUANTA_ARRAY_SZ];
+	int hw_credit_quanta[DLB2_CREDIT_QUANTA_ARRAY_SZ];
 	uint8_t revision;
 	uint8_t version;
+	uint8_t evdev_id; /* index into mappings array for multidev support */
 	bool configured;
 	union {
 		struct {
@@ -637,6 +774,9 @@ struct dlb2_eventdev {
 	uint32_t cos_ports[DLB2_COS_NUM_VALS]; /* total ldb ports in each class */
 	uint32_t cos_bw[DLB2_COS_NUM_VALS]; /* bandwidth per cos domain */
 	uint8_t max_cos_port; /* Max LDB port from any cos */
+	bool enable_cq_weight;
+	uint16_t hl_entries; /* Num HL entires to allocate for the domain */
+	int default_port_hl;  /* Fixed or dynamic (2*CQ Depth) HL assignment */
 };
 
 /* used for collecting and passing around the dev args */
@@ -644,8 +784,9 @@ struct dlb2_qid_depth_thresholds {
 	int val[DLB2_MAX_NUM_QUEUES_ALL];
 };
 
-struct dlb2_cq_weight {
-	int limit[DLB2_MAX_NUM_PORTS_ALL];
+/* used for collecting and passing around the dev args */
+struct dlb2_port_dequeue_wait {
+	enum dlb2_port_dequeue_wait_types val[DLB2_MAX_NUM_PORTS_ALL];
 };
 
 struct dlb2_port_cos {
@@ -660,26 +801,33 @@ struct dlb2_devargs {
 	int socket_id;
 	int max_num_events;
 	int num_dir_credits_override;
-	int dev_id;
+	int hwdev_id;
 	struct dlb2_qid_depth_thresholds qid_depth_thresholds;
+	struct dlb2_port_dequeue_wait port_dequeue_wait;
 	int poll_interval;
-	int sw_credit_quanta;
-	int hw_credit_quanta;
+	int sw_credit_quanta[DLB2_CREDIT_QUANTA_ARRAY_SZ];
+	int hw_credit_quanta[DLB2_CREDIT_QUANTA_ARRAY_SZ];
 	int default_depth_thresh;
-	bool vector_opts_enabled;
+	bool vector_opts_disab;
+	int num_ordered_queues_0;
+	int num_ordered_queues_1;
 	int max_cq_depth;
 	int max_enq_depth;
-	struct dlb2_cq_weight cq_weight;
+	char producer_coremask[DLB2_COREMASK_LEN];
 	struct dlb2_port_cos port_cos;
 	struct dlb2_cos_bw cos_bw;
-	const char *producer_coremask;
 	bool default_ldb_port_allocation;
+	bool enable_cq_weight;
+	bool use_default_hl;
+	uint32_t alloc_hl_entries;
 };
 
 /* End Eventdev related defines and structs */
 
 /* Forwards for non-inlined functions */
 
+int dlb2_uninit(const char *name);
+
 void dlb2_eventdev_dump(struct rte_eventdev *dev, FILE *f);
 
 int dlb2_xstats_init(struct dlb2_eventdev *dlb2);
@@ -704,33 +852,38 @@ int dlb2_eventdev_xstats_reset(struct rte_eventdev *dev,
 		const uint64_t ids[],
 		uint32_t nb_ids);
 
+int dlb2_get_sched_idle_counts(struct dlb2_eventdev *dlb2);
+
 int test_dlb2_eventdev(void);
 
 int dlb2_primary_eventdev_probe(struct rte_eventdev *dev,
 				const char *name,
-				struct dlb2_devargs *dlb2_args);
+				struct dlb2_devargs *dlb2_args,
+				bool is_vdev);
 
 int dlb2_secondary_eventdev_probe(struct rte_eventdev *dev,
-				  const char *name);
+				  const char *name,
+				  bool is_vdev);
 
 uint32_t dlb2_get_queue_depth(struct dlb2_eventdev *dlb2,
 			      struct dlb2_eventdev_queue *queue);
+int dlb2_set_port_param(struct dlb2_eventdev *dlb2, int port_id,
+			uint64_t flags, void *val);
 
 int dlb2_parse_params(const char *params,
 		      const char *name,
 		      struct dlb2_devargs *dlb2_args,
 		      uint8_t version);
 
-void dlb2_event_build_hcws(struct dlb2_port *qm_port,
-			   const struct rte_event ev[],
-			   int num,
-			   uint8_t *sched_type,
-			   uint8_t *queue_id);
+int dlb2_string_to_int(int *result, const char *str);
 
-/* Extern functions */
-extern int rte_eal_parse_coremask(const char *coremask, int *cores);
+int dlb2_credit_quanta_parse_params(const char *value,
+				    int *quanta_credit,
+				    int type);
 
 /* Extern globals */
-extern struct process_local_port_data dlb2_port[][DLB2_NUM_PORT_TYPES];
+extern struct process_local_port_data dlb2_port[RTE_EVENT_MAX_DEVS]
+					       [DLB2_MAX_NUM_PORTS_ALL]
+					       [DLB2_NUM_PORT_TYPES];
 
 #endif	/* _DLB2_PRIV_H_ */
diff --git a/drivers/event/dlb2/dlb2_selftest.c b/drivers/event/dlb2/dlb2_selftest.c
index 1863ffe..1232254 100644
--- a/drivers/event/dlb2/dlb2_selftest.c
+++ b/drivers/event/dlb2/dlb2_selftest.c
@@ -20,6 +20,7 @@
 #include <rte_cycles.h>
 #include <rte_eventdev.h>
 #include <rte_pause.h>
+#include <dev_driver.h>
 
 #include "dlb2_priv.h"
 #include "rte_pmd_dlb2.h"
@@ -223,7 +224,7 @@ test_stop_flush(struct test *t) /* test to check we can properly flush events */
 				    0,
 				    RTE_EVENT_PORT_ATTR_DEQ_DEPTH,
 				    &dequeue_depth)) {
-		printf("%d: Error retrieving dequeue depth\n", __LINE__);
+		printf("%d: Error retrieveing dequeue depth\n", __LINE__);
 		goto err;
 	}
 
@@ -1354,7 +1355,7 @@ test_delayed_pop(void)
 	}
 
 	/* Release one more event. This will trigger the token pop, and
-	 * dequeue_depth more events will be scheduled to the device.
+	 * dequeue_depth - 1 more events will be scheduled to the device.
 	 */
 	ev.op = RTE_EVENT_OP_RELEASE;
 
@@ -1480,7 +1481,6 @@ test_dlb2_eventdev(void)
 	int i, ret = 0;
 	int found = 0, skipped = 0, passed = 0, failed = 0;
 	struct rte_event_dev_info info;
-
 	for (i = 0; found + skipped < num_evdevs && i < RTE_EVENT_MAX_DEVS;
 	     i++) {
 		ret = rte_event_dev_info_get(i, &info);
@@ -1494,7 +1494,13 @@ test_dlb2_eventdev(void)
 			continue;
 		}
 
-		evdev = rte_event_dev_get_dev_id(info.driver_name);
+		if (info.dev)
+			/* PF PMD Mode */
+			evdev = rte_event_dev_get_dev_id(info.dev->driver->name);
+		else  /* Bifurcated PMD Mode */
+			evdev = rte_event_dev_get_dev_id(info.driver_name);
+
+
 		if (evdev < 0) {
 			printf("Could not get dev_id for eventdev with name %s, i=%d\n",
 			       info.driver_name, i);
diff --git a/drivers/event/dlb2/dlb2_user.h b/drivers/event/dlb2/dlb2_user.h
index 8739e2a..be64ff3 100644
--- a/drivers/event/dlb2/dlb2_user.h
+++ b/drivers/event/dlb2/dlb2_user.h
@@ -18,7 +18,6 @@ enum dlb2_error {
 	DLB2_ST_LDB_QUEUES_UNAVAILABLE,
 	DLB2_ST_LDB_CREDITS_UNAVAILABLE,
 	DLB2_ST_DIR_CREDITS_UNAVAILABLE,
-	DLB2_ST_CREDITS_UNAVAILABLE,
 	DLB2_ST_SEQUENCE_NUMBERS_UNAVAILABLE,
 	DLB2_ST_INVALID_DOMAIN_ID,
 	DLB2_ST_INVALID_QID_INFLIGHT_ALLOCATION,
@@ -48,6 +47,7 @@ enum dlb2_error {
 	DLB2_ST_INVALID_LOCK_ID_COMP_LEVEL,
 	DLB2_ST_INVALID_COS_ID,
 	DLB2_ST_INVALID_CQ_WEIGHT_LIMIT,
+	DLB2_ST_SN_SLOTS_UNAVAILABLE,
 	DLB2_ST_FEATURE_UNAVAILABLE,
 };
 
@@ -60,7 +60,6 @@ static const char dlb2_error_strings[][128] = {
 	"DLB2_ST_LDB_QUEUES_UNAVAILABLE",
 	"DLB2_ST_LDB_CREDITS_UNAVAILABLE",
 	"DLB2_ST_DIR_CREDITS_UNAVAILABLE",
-	"DLB2_ST_CREDITS_UNAVAILABLE",
 	"DLB2_ST_SEQUENCE_NUMBERS_UNAVAILABLE",
 	"DLB2_ST_INVALID_DOMAIN_ID",
 	"DLB2_ST_INVALID_QID_INFLIGHT_ALLOCATION",
@@ -90,6 +89,7 @@ static const char dlb2_error_strings[][128] = {
 	"DLB2_ST_INVALID_LOCK_ID_COMP_LEVEL",
 	"DLB2_ST_INVALID_COS_ID",
 	"DLB2_ST_INVALID_CQ_WEIGHT_LIMIT",
+	"DLB2_ST_SN_SLOTS_UNAVAILABLE",
 	"DLB2_ST_FEATURE_UNAVAILABLE",
 };
 
@@ -98,9 +98,9 @@ struct dlb2_cmd_response {
 	__u32 id;
 };
 
-/*******************/
-/* 'dlb2' commands */
-/*******************/
+/********************************/
+/* 'dlb2' device file commands */
+/********************************/
 
 #define DLB2_DEVICE_VERSION(x) (((x) >> 8) & 0xFF)
 #define DLB2_DEVICE_REVISION(x) ((x) & 0xFF)
@@ -135,7 +135,7 @@ struct dlb2_get_device_version_args {
  * Input parameters:
  * - num_ldb_queues: Number of load-balanced queues.
  * - num_ldb_ports: Number of load-balanced ports that can be allocated from
- *	any class-of-service with available ports.
+ *	from any class-of-service with available ports.
  * - num_cos_ldb_ports[4]: Number of load-balanced ports from
  *	classes-of-service 0-3.
  * - num_dir_ports: Number of directed ports. A directed port has one directed
@@ -155,6 +155,10 @@ struct dlb2_get_device_version_args {
  *	class-of-service N to satisfy the num_ldb_ports_cosN argument. If
  *	unset, attempt to fulfill num_ldb_ports_cosN arguments from other
  *	classes-of-service if class N does not contain enough free ports.
+ * - num_sn_slots[2]: number of sequence number slots from group 0 and 1.
+ * -  pcore_mask: Producer coremask for the domain. Bit map of cores on which
+ *   producer threads for this domain will run.
+ * -  core_mask: EAL coremask
  * - padding1: Reserved for future use.
  *
  * Output parameters:
@@ -187,6 +191,9 @@ struct dlb2_create_sched_domain_args {
 	};
 	__u8 cos_strict;
 	__u8 padding1[3];
+	__u32 num_sn_slots[2];
+	__u64 pcore_bmp[RTE_MAX_LCORE/__BITS_PER_LONG];
+	__u64 core_bmp[RTE_MAX_LCORE/__BITS_PER_LONG];
 };
 
 /*
@@ -208,6 +215,8 @@ struct dlb2_create_sched_domain_args {
  *	contiguous range of history list entries.
  * - num_ldb_credits: Amount of available load-balanced QE storage.
  * - num_dir_credits: Amount of available directed QE storage.
+ * - num_sn_slots[2]: number of available sequence number slots from group
+ *      0 and 1.
  * - response.status: Detailed error code. In certain cases, such as if the
  *	ioctl request arg is invalid, the driver won't set status.
  */
@@ -231,6 +240,7 @@ struct dlb2_get_num_resources_args {
 			__u32 num_credits;
 		};
 	};
+	__u32 num_sn_slots[2];
 };
 
 /*
@@ -365,9 +375,190 @@ struct dlb2_query_cq_poll_mode_args {
 	struct dlb2_cmd_response response;
 };
 
-/********************************/
-/* 'scheduling domain' commands */
-/********************************/
+/*
+ * DLB2_CMD_GET_HW_REG: Read the contents of a HW register
+ *
+ * Input parameters:
+ * - reg_addr: 32 bit address of HW register
+ *
+ * Output parameters:
+ * - reg_val: Contents of a HW register
+ * - response.status: Detailed error code. In certain cases, such as if the
+ *     ioctl request arg is invalid, the driver won't set status.
+ */
+struct dlb2_xstats_args {
+	/* Output parameters */
+	struct dlb2_cmd_response response;
+	__u64 xstats_val;
+	/* Input parameters */
+	__u32 xstats_type;
+	__u32 xstats_id;
+};
+enum dlb2_xtats_type {
+	DEVICE_XSTATS= 0x0,
+	LDB_QUEUE_XSTATS,
+	LDB_PORT_XSTATS,
+	DIR_PQ_XSTATS,
+	MAX_XSTATS
+};
+#define XSTATS_BASE(id) (id << 16)
+
+enum dlb2_ldb_queue_xstats {
+	DLB_CFG_QID_LDB_INFLIGHT_COUNT = XSTATS_BASE(LDB_QUEUE_XSTATS),
+	DLB_CFG_QID_LDB_INFLIGHT_LIMIT,
+	DLB_CFG_QID_ATM_ACTIVE,
+	DLB_CFG_QID_ATM_DEPTH_THRSH,
+	DLB_CFG_QID_NALB_DEPTH_THRSH,
+	DLB_CFG_QID_ATQ_ENQ_CNT,
+	DLB_CFG_QID_LDB_ENQ_CNT,
+};
+
+enum dlb2_ldb_port_xstats {
+	DLB_CFG_CQ_LDB_DEPTH = XSTATS_BASE(LDB_PORT_XSTATS),
+	DLB_CFG_CQ_LDB_TOKEN_COUNT,
+	DLB_CFG_CQ_LDB_TOKEN_DEPTH_SELECT,
+	DLB_CFG_CQ_LDB_INFLIGHT_COUNT,
+};
+
+enum dlb2_dir_pq_xstats {
+	DLB_CFG_CQ_DIR_TOKEN_DEPTH_SELECT = XSTATS_BASE(DIR_PQ_XSTATS),
+	DLB_CFG_CQ_DIR_DEPTH,
+	DLB_CFG_QID_DIR_DEPTH_THRSH,
+	DLB_CFG_QID_DIR_ENQ_CNT,
+};
+
+enum dlb2_user_interface_commands {
+	DLB2_CMD_GET_DEVICE_VERSION,
+	DLB2_CMD_CREATE_SCHED_DOMAIN,
+	DLB2_CMD_GET_NUM_RESOURCES,
+	DLB2_CMD_RESERVED1,
+	DLB2_CMD_RESERVED2,
+	DLB2_CMD_SET_SN_ALLOCATION,
+	DLB2_CMD_GET_SN_ALLOCATION,
+	DLB2_CMD_SET_COS_BW,
+	DLB2_CMD_GET_COS_BW,
+	DLB2_CMD_GET_SN_OCCUPANCY,
+	DLB2_CMD_QUERY_CQ_POLL_MODE,
+	DLB2_CMD_GET_XSTATS,
+
+	/* NUM_DLB2_CMD must be last */
+	NUM_DLB2_CMD,
+};
+
+/*******************************/
+/* 'domain' device file alerts */
+/*******************************/
+
+/*
+ * Scheduling domain device files can be read to receive domain-specific
+ * notifications, for alerts such as hardware errors or device reset.
+ *
+ * Each alert is encoded in a 16B message. The first 8B contains the alert ID,
+ * and the second 8B is optional and contains additional information.
+ * Applications should cast read data to a struct dlb2_domain_alert, and
+ * interpret the struct's alert_id according to dlb2_domain_alert_id. The read
+ * length must be 16B, or the function will return -EINVAL.
+ *
+ * Reads are destructive, and in the case of multiple file descriptors for the
+ * same domain device file, an alert will be read by only one of the file
+ * descriptors.
+ *
+ * The driver stores alerts in a fixed-size alert ring until they are read. If
+ * the alert ring fills completely, subsequent alerts will be dropped. It is
+ * recommended that DLB2 applications dedicate a thread to perform blocking
+ * reads on the device file.
+ */
+enum dlb2_domain_alert_id {
+	/*
+	 * Software issued an illegal enqueue for a port in this domain. An
+	 * illegal enqueue could be:
+	 * - Illegal (excess) completion
+	 * - Illegal fragment
+	 * - Insufficient credits
+	 * aux_alert_data[7:0] contains the port ID, and aux_alert_data[15:8]
+	 * contains a flag indicating whether the port is load-balanced (1) or
+	 * directed (0).
+	 */
+	DLB2_DOMAIN_ALERT_PP_ILLEGAL_ENQ,
+	/*
+	 * Software issued excess CQ token pops for a port in this domain.
+	 * aux_alert_data[7:0] contains the port ID, and aux_alert_data[15:8]
+	 * contains a flag indicating whether the port is load-balanced (1) or
+	 * directed (0).
+	 */
+	DLB2_DOMAIN_ALERT_PP_EXCESS_TOKEN_POPS,
+	/*
+	 * A enqueue contained either an invalid command encoding or a REL,
+	 * REL_T, RLS, FWD, FWD_T, FRAG, or FRAG_T from a directed port.
+	 *
+	 * aux_alert_data[7:0] contains the port ID, and aux_alert_data[15:8]
+	 * contains a flag indicating whether the port is load-balanced (1) or
+	 * directed (0).
+	 */
+	DLB2_DOMAIN_ALERT_ILLEGAL_HCW,
+	/*
+	 * The QID must be valid and less than 128.
+	 *
+	 * aux_alert_data[7:0] contains the port ID, and aux_alert_data[15:8]
+	 * contains a flag indicating whether the port is load-balanced (1) or
+	 * directed (0).
+	 */
+	DLB2_DOMAIN_ALERT_ILLEGAL_QID,
+	/*
+	 * An enqueue went to a disabled QID.
+	 *
+	 * aux_alert_data[7:0] contains the port ID, and aux_alert_data[15:8]
+	 * contains a flag indicating whether the port is load-balanced (1) or
+	 * directed (0).
+	 */
+	DLB2_DOMAIN_ALERT_DISABLED_QID,
+	/*
+	 * The device containing this domain was reset. All applications using
+	 * the device need to exit for the driver to complete the reset
+	 * procedure.
+	 *
+	 * aux_alert_data doesn't contain any information for this alert.
+	 */
+	DLB2_DOMAIN_ALERT_DEVICE_RESET,
+	/*
+	 * User-space has enqueued an alert.
+	 *
+	 * aux_alert_data contains user-provided data.
+	 */
+	DLB2_DOMAIN_ALERT_USER,
+	/*
+	 * The watchdog timer fired for the specified port. This occurs if its
+	 * CQ was not serviced for a large amount of time, likely indicating a
+	 * hung thread.
+	 * aux_alert_data[7:0] contains the port ID, and aux_alert_data[15:8]
+	 * contains a flag indicating whether the port is load-balanced (1) or
+	 * directed (0).
+	 */
+	DLB2_DOMAIN_ALERT_CQ_WATCHDOG_TIMEOUT,
+
+	/* Number of DLB2 domain alerts */
+	NUM_DLB2_DOMAIN_ALERTS
+};
+
+static const char dlb2_domain_alert_strings[][128] = {
+	"DLB2_DOMAIN_ALERT_PP_ILLEGAL_ENQ",
+	"DLB2_DOMAIN_ALERT_PP_EXCESS_TOKEN_POPS",
+	"DLB2_DOMAIN_ALERT_ILLEGAL_HCW",
+	"DLB2_DOMAIN_ALERT_ILLEGAL_QID",
+	"DLB2_DOMAIN_ALERT_DISABLED_QID",
+	"DLB2_DOMAIN_ALERT_DEVICE_RESET",
+	"DLB2_DOMAIN_ALERT_USER",
+	"DLB2_DOMAIN_ALERT_CQ_WATCHDOG_TIMEOUT",
+};
+
+struct dlb2_domain_alert {
+	__u64 alert_id;
+	__u64 aux_alert_data;
+};
+
+/*********************************/
+/* 'domain' device file commands */
+/*********************************/
 
 /*
  * DLB2_DOMAIN_CMD_CREATE_LDB_QUEUE: Configure a load-balanced queue.
@@ -450,7 +641,7 @@ struct dlb2_create_dir_queue_args {
  * - num_hist_list_entries: Number of history list entries. This must be
  *	greater than or equal cq_depth.
  * - cos_id: class-of-service to allocate this port from. Must be between 0 and
- *	3, inclusive. Should be 255 if default.
+ *	3, inclusive.
  * - cos_strict: If set, return an error if there are no available ports in the
  *	requested class-of-service. Else, allocate the port from a different
  *	class-of-service if the requested class has no available ports.
@@ -472,6 +663,8 @@ struct dlb2_create_ldb_port_args {
 	__u16 cq_history_list_size;
 	__u8 cos_id;
 	__u8 cos_strict;
+	__u8 enable_inflight_ctrl;
+	__u16 inflight_threshold;
 };
 
 /*
@@ -485,6 +678,8 @@ struct dlb2_create_ldb_port_args {
  * - qid: Queue ID. If the corresponding directed queue is already created,
  *	specify its ID here. Else this argument must be 0xFFFFFFFF to indicate
  *	that the port is being created before the queue.
+ * - is_producer: If this port is used as a producer i.e., events will be
+ *   primarily enqueued from this port
  *
  * Output parameters:
  * - response.status: Detailed error code. In certain cases, such as if the
@@ -519,6 +714,22 @@ struct dlb2_start_domain_args {
 };
 
 /*
+ * DLB2_DOMAIN_CMD_STOP_DOMAIN: Stop scheduling of a domain. Scheduling can be
+ *	resumed by calling DLB2_DOMAIN_CMD_START_DOMAIN. Sending QEs into the
+ *	device after calling this ioctl will result in undefined behavior.
+ * Input parameters:
+ * - (None)
+ *
+ * Output parameters:
+ * - response.status: Detailed error code. In certain cases, such as if the
+ *	ioctl request arg is invalid, the driver won't set status.
+ */
+struct dlb2_stop_domain_args {
+	/* Output parameters */
+	struct dlb2_cmd_response response;
+};
+
+/*
  * DLB2_DOMAIN_CMD_MAP_QID: Map a load-balanced queue to a load-balanced port.
  * Input parameters:
  * - port_id: Load-balanced port ID.
@@ -632,6 +843,56 @@ struct dlb2_disable_dir_port_args {
 };
 
 /*
+ * DLB2_DOMAIN_CMD_BLOCK_ON_CQ_INTERRUPT: Block on a CQ interrupt until a QE
+ *	arrives for the specified port. If a QE is already present, the ioctl
+ *	will immediately return.
+ *
+ *	Note: Only one thread can block on a CQ's interrupt at a time. Doing
+ *	otherwise can result in hung threads.
+ *
+ * Input parameters:
+ * - port_id: Port ID.
+ * - is_ldb: True if the port is load-balanced, false otherwise.
+ * - arm: Tell the driver to arm the interrupt.
+ * - cq_gen: Current CQ generation bit.
+ * - padding0: Reserved for future use.
+ * - cq_va: VA of the CQ entry where the next QE will be placed.
+ *
+ * Output parameters:
+ * - response.status: Detailed error code. In certain cases, such as if the
+ *	ioctl request arg is invalid, the driver won't set status.
+ */
+struct dlb2_block_on_cq_interrupt_args {
+	/* Output parameters */
+	struct dlb2_cmd_response response;
+	/* Input parameters */
+	__u32 port_id;
+	__u8 is_ldb;
+	__u8 arm;
+	__u8 cq_gen;
+	__u8 padding0;
+	__u64 cq_va;
+};
+
+/*
+ * DLB2_DOMAIN_CMD_ENQUEUE_DOMAIN_ALERT: Enqueue a domain alert that will be
+ *	read by one reader thread.
+ *
+ * Input parameters:
+ * - aux_alert_data: user-defined auxiliary data.
+ *
+ * Output parameters:
+ * - response.status: Detailed error code. In certain cases, such as if the
+ *	ioctl request arg is invalid, the driver won't set status.
+ */
+struct dlb2_enqueue_domain_alert_args {
+	/* Output parameters */
+	struct dlb2_cmd_response response;
+	/* Input parameters */
+	__u64 aux_alert_data;
+};
+
+/*
  * DLB2_DOMAIN_CMD_GET_LDB_QUEUE_DEPTH: Get a load-balanced queue's depth.
  * Input parameters:
  * - queue_id: The load-balanced queue ID.
@@ -693,12 +954,46 @@ struct dlb2_pending_port_unmaps_args {
 };
 
 /*
+ * DLB2_CMD_GET_LDB_PORT_PP_FD: Get file descriptor to mmap a load-balanced
+ *	port's producer port (PP).
+ * DLB2_CMD_GET_LDB_PORT_CQ_FD: Get file descriptor to mmap a load-balanced
+ *	port's consumer queue (CQ).
+ *
+ *	The load-balanced port must have been previously created with the ioctl
+ *	DLB2_CMD_CREATE_LDB_PORT. The fd is used to mmap the PP/CQ region.
+ *
+ * DLB2_CMD_GET_DIR_PORT_PP_FD: Get file descriptor to mmap a directed port's
+ *	producer port (PP).
+ * DLB2_CMD_GET_DIR_PORT_CQ_FD: Get file descriptor to mmap a directed port's
+ *	consumer queue (CQ).
+ *
+ *	The directed port must have been previously created with the ioctl
+ *	DLB2_CMD_CREATE_DIR_PORT. The fd is used to mmap PP/CQ region.
+ *
+ * Input parameters:
+ * - port_id: port ID.
+ * - padding0: Reserved for future use.
+ *
+ * Output parameters:
+ * - response.status: Detailed error code. In certain cases, such as if the
+ *	ioctl request arg is invalid, the driver won't set status.
+ * - response.id: fd.
+ */
+struct dlb2_get_port_fd_args {
+	/* Output parameters */
+	struct dlb2_cmd_response response;
+	/* Input parameters */
+	__u32 port_id;
+	__u32 padding0;
+};
+
+/*
  * DLB2_DOMAIN_CMD_ENABLE_CQ_WEIGHT: Enable QE-weight based scheduling on a
- *      load-balanced port's CQ and configures the CQ's weight limit.
+ *	load-balanced port's CQ and configures the CQ's weight limit.
  *
- *      This must be called after creating the port but before starting the
- *      domain. The QE weight limit must be non-zero and cannot exceed the
- *      CQ's depth.
+ *	This must be called after creating the port but before starting the
+ *	domain. The QE weight limit must be non-zero and cannot exceed the
+ *	CQ's depth.
  *
  * Input parameters:
  * - port_id: Load-balanced port ID.
@@ -706,7 +1001,7 @@ struct dlb2_pending_port_unmaps_args {
  *
  * Output parameters:
  * - response.status: Detailed error code. In certain cases, such as if the
- *      ioctl request arg is invalid, the driver won't set status.
+ *	ioctl request arg is invalid, the driver won't set status.
  * - response.id: number of unmaps in progress.
  */
 struct dlb2_enable_cq_weight_args {
@@ -718,11 +1013,233 @@ struct dlb2_enable_cq_weight_args {
 };
 
 /*
+ * DLB2_DOMAIN_CMD_ENABLE_CQ_EPOLL: Enable epoll support to monitor event
+ *      file descriptors created for directed and load-balanced port's CQs.
+ *	Kernel notifies user-space of events through the eventfds.
+ *
+ *	This must be called after creating the port. It can be called after
+ *	starting the domain.
+ *
+ * Input parameters:
+ * - port_id: Directed or Load-balanced port ID.
+ * - process_id: Process id of the user space application.
+ * - event_fd: Event file descriptor.
+ * - is_ldb: True for load-balanced port and false for directed port.
+ *
+ * Output parameters:
+ * - response.status
+ * - response.id: port_id
+ */
+struct dlb2_enable_cq_epoll_args {
+       /* Output parameters */
+       struct dlb2_cmd_response response;
+       /* Input parameters */
+       __u32 port_id;
+       __u32 process_id;
+       __u32 event_fd;
+       __u8 is_ldb;
+       __u8 padding0[3];
+};
+
+/*
+ * DLB2_DOMAIN_CMD_SET_CQ_INFLIGHT_CTRL: Set Per-CQ inflight control for
+ * 	{ATM,UNO,ORD} QEs.
+ *
+ * Input parameters:
+ * - port_id: Load-balanced port ID.
+ * - enable: True if inflight control is enabled. False otherwise
+ * - threshold: Per CQ inflight threshold.
+ *
+ * Output parameters:
+ * - response.status: Detailed error code. In certain cases, such as if the
+ *	ioctl request arg is invalid, the driver won't set status.
+ */
+struct dlb2_cq_inflight_ctrl_args {
+	/* Output parameters */
+	struct dlb2_cmd_response response;
+	/* Input parameters */
+	__u32 port_id;
+	__u16 enable;
+	__u16 threshold;
+};
+
+enum dlb2_domain_user_interface_commands {
+	DLB2_DOMAIN_CMD_CREATE_LDB_QUEUE,
+	DLB2_DOMAIN_CMD_CREATE_DIR_QUEUE,
+	DLB2_DOMAIN_CMD_CREATE_LDB_PORT,
+	DLB2_DOMAIN_CMD_CREATE_DIR_PORT,
+	DLB2_DOMAIN_CMD_START_DOMAIN,
+	DLB2_DOMAIN_CMD_MAP_QID,
+	DLB2_DOMAIN_CMD_UNMAP_QID,
+	DLB2_DOMAIN_CMD_ENABLE_LDB_PORT,
+	DLB2_DOMAIN_CMD_ENABLE_DIR_PORT,
+	DLB2_DOMAIN_CMD_DISABLE_LDB_PORT,
+	DLB2_DOMAIN_CMD_DISABLE_DIR_PORT,
+	DLB2_DOMAIN_CMD_BLOCK_ON_CQ_INTERRUPT,
+	DLB2_DOMAIN_CMD_ENQUEUE_DOMAIN_ALERT,
+	DLB2_DOMAIN_CMD_GET_LDB_QUEUE_DEPTH,
+	DLB2_DOMAIN_CMD_GET_DIR_QUEUE_DEPTH,
+	DLB2_DOMAIN_CMD_PENDING_PORT_UNMAPS,
+	DLB2_DOMAIN_CMD_GET_LDB_PORT_PP_FD,
+	DLB2_DOMAIN_CMD_GET_LDB_PORT_CQ_FD,
+	DLB2_DOMAIN_CMD_GET_DIR_PORT_PP_FD,
+	DLB2_DOMAIN_CMD_GET_DIR_PORT_CQ_FD,
+	DLB2_DOMAIN_CMD_ENABLE_CQ_WEIGHT,
+	DLB2_DOMAIN_CMD_ENABLE_CQ_EPOLL,
+	DLB2_DOMAIN_CMD_SET_CQ_INFLIGHT_CTRL,
+	DLB2_DOMAIN_CMD_STOP_DOMAIN,
+
+	/* NUM_DLB2_DOMAIN_CMD must be last */
+	NUM_DLB2_DOMAIN_CMD,
+};
+
+/*
  * Mapping sizes for memory mapping the consumer queue (CQ) memory space, and
  * producer port (PP) MMIO space.
  */
 #define DLB2_CQ_SIZE 65536
 #define DLB2_PP_SIZE 4096
 
+/********************/
+/* dlb2 ioctl codes */
+/********************/
+
+#define DLB2_IOC_MAGIC  'h'
 
+#define DLB2_IOC_GET_DEVICE_VERSION				\
+		_IOR(DLB2_IOC_MAGIC,				\
+		     DLB2_CMD_GET_DEVICE_VERSION,		\
+		     struct dlb2_get_device_version_args)
+#define DLB2_IOC_CREATE_SCHED_DOMAIN				\
+		_IOWR(DLB2_IOC_MAGIC,				\
+		      DLB2_CMD_CREATE_SCHED_DOMAIN,		\
+		      struct dlb2_create_sched_domain_args)
+#define DLB2_IOC_GET_NUM_RESOURCES				\
+		_IOR(DLB2_IOC_MAGIC,				\
+		     DLB2_CMD_GET_NUM_RESOURCES,		\
+		     struct dlb2_get_num_resources_args)
+#define DLB2_IOC_SET_SN_ALLOCATION				\
+		_IOWR(DLB2_IOC_MAGIC,				\
+		      DLB2_CMD_SET_SN_ALLOCATION,		\
+		      struct dlb2_set_sn_allocation_args)
+#define DLB2_IOC_GET_SN_ALLOCATION				\
+		_IOWR(DLB2_IOC_MAGIC,				\
+		      DLB2_CMD_GET_SN_ALLOCATION,		\
+		      struct dlb2_get_sn_allocation_args)
+#define DLB2_IOC_SET_COS_BW					\
+		_IOWR(DLB2_IOC_MAGIC,				\
+		      DLB2_CMD_SET_COS_BW,			\
+		      struct dlb2_set_cos_bw_args)
+#define DLB2_IOC_GET_COS_BW					\
+		_IOWR(DLB2_IOC_MAGIC,				\
+		      DLB2_CMD_GET_COS_BW,			\
+		      struct dlb2_get_cos_bw_args)
+#define DLB2_IOC_GET_SN_OCCUPANCY				\
+		_IOWR(DLB2_IOC_MAGIC,				\
+		      DLB2_CMD_GET_SN_OCCUPANCY,		\
+		      struct dlb2_get_sn_occupancy_args)
+#define DLB2_IOC_QUERY_CQ_POLL_MODE				\
+		_IOR(DLB2_IOC_MAGIC,				\
+		     DLB2_CMD_QUERY_CQ_POLL_MODE,		\
+		     struct dlb2_query_cq_poll_mode_args)
+#define DLB2_IOC_CREATE_LDB_QUEUE				\
+		_IOWR(DLB2_IOC_MAGIC,				\
+		      DLB2_DOMAIN_CMD_CREATE_LDB_QUEUE,		\
+		      struct dlb2_create_ldb_queue_args)
+#define DLB2_IOC_CREATE_DIR_QUEUE				\
+		_IOWR(DLB2_IOC_MAGIC,				\
+		      DLB2_DOMAIN_CMD_CREATE_DIR_QUEUE,		\
+		      struct dlb2_create_dir_queue_args)
+#define DLB2_IOC_CREATE_LDB_PORT				\
+		_IOWR(DLB2_IOC_MAGIC,				\
+		      DLB2_DOMAIN_CMD_CREATE_LDB_PORT,		\
+		      struct dlb2_create_ldb_port_args)
+#define DLB2_IOC_CREATE_DIR_PORT				\
+		_IOWR(DLB2_IOC_MAGIC,				\
+		      DLB2_DOMAIN_CMD_CREATE_DIR_PORT,		\
+		      struct dlb2_create_dir_port_args)
+#define DLB2_IOC_START_DOMAIN					\
+		_IOR(DLB2_IOC_MAGIC,				\
+		     DLB2_DOMAIN_CMD_START_DOMAIN,		\
+		     struct dlb2_start_domain_args)
+#define DLB2_IOC_STOP_DOMAIN					\
+		_IOR(DLB2_IOC_MAGIC,				\
+		     DLB2_DOMAIN_CMD_STOP_DOMAIN,		\
+		     struct dlb2_stop_domain_args)
+#define DLB2_IOC_MAP_QID					\
+		_IOWR(DLB2_IOC_MAGIC,				\
+		      DLB2_DOMAIN_CMD_MAP_QID,			\
+		      struct dlb2_map_qid_args)
+#define DLB2_IOC_UNMAP_QID					\
+		_IOWR(DLB2_IOC_MAGIC,				\
+		      DLB2_DOMAIN_CMD_UNMAP_QID,		\
+		      struct dlb2_unmap_qid_args)
+#define DLB2_IOC_ENABLE_LDB_PORT				\
+		_IOWR(DLB2_IOC_MAGIC,				\
+		      DLB2_DOMAIN_CMD_ENABLE_LDB_PORT,		\
+		      struct dlb2_enable_ldb_port_args)
+#define DLB2_IOC_ENABLE_DIR_PORT				\
+		_IOWR(DLB2_IOC_MAGIC,				\
+		      DLB2_DOMAIN_CMD_ENABLE_DIR_PORT,		\
+		      struct dlb2_enable_dir_port_args)
+#define DLB2_IOC_DISABLE_LDB_PORT				\
+		_IOWR(DLB2_IOC_MAGIC,				\
+		      DLB2_DOMAIN_CMD_DISABLE_LDB_PORT,		\
+		      struct dlb2_disable_ldb_port_args)
+#define DLB2_IOC_DISABLE_DIR_PORT				\
+		_IOWR(DLB2_IOC_MAGIC,				\
+		      DLB2_DOMAIN_CMD_DISABLE_DIR_PORT,		\
+		      struct dlb2_disable_dir_port_args)
+#define DLB2_IOC_BLOCK_ON_CQ_INTERRUPT				\
+		_IOWR(DLB2_IOC_MAGIC,				\
+		      DLB2_DOMAIN_CMD_BLOCK_ON_CQ_INTERRUPT,	\
+		      struct dlb2_block_on_cq_interrupt_args)
+#define DLB2_IOC_ENQUEUE_DOMAIN_ALERT				\
+		_IOWR(DLB2_IOC_MAGIC,				\
+		      DLB2_DOMAIN_CMD_ENQUEUE_DOMAIN_ALERT,	\
+		      struct dlb2_enqueue_domain_alert_args)
+#define DLB2_IOC_GET_LDB_QUEUE_DEPTH				\
+		_IOWR(DLB2_IOC_MAGIC,				\
+		      DLB2_DOMAIN_CMD_GET_LDB_QUEUE_DEPTH,	\
+		      struct dlb2_get_ldb_queue_depth_args)
+#define DLB2_IOC_GET_DIR_QUEUE_DEPTH				\
+		_IOWR(DLB2_IOC_MAGIC,				\
+		      DLB2_DOMAIN_CMD_GET_DIR_QUEUE_DEPTH,	\
+		      struct dlb2_get_dir_queue_depth_args)
+#define DLB2_IOC_PENDING_PORT_UNMAPS				\
+		_IOWR(DLB2_IOC_MAGIC,				\
+		      DLB2_DOMAIN_CMD_PENDING_PORT_UNMAPS,	\
+		      struct dlb2_pending_port_unmaps_args)
+#define DLB2_IOC_GET_LDB_PORT_PP_FD				\
+		_IOWR(DLB2_IOC_MAGIC,				\
+		      DLB2_DOMAIN_CMD_GET_LDB_PORT_PP_FD,	\
+		      struct dlb2_get_port_fd_args)
+#define DLB2_IOC_GET_LDB_PORT_CQ_FD				\
+		_IOWR(DLB2_IOC_MAGIC,				\
+		      DLB2_DOMAIN_CMD_GET_LDB_PORT_CQ_FD,	\
+		      struct dlb2_get_port_fd_args)
+#define DLB2_IOC_GET_DIR_PORT_PP_FD				\
+		_IOWR(DLB2_IOC_MAGIC,				\
+		      DLB2_DOMAIN_CMD_GET_DIR_PORT_PP_FD,	\
+		      struct dlb2_get_port_fd_args)
+#define DLB2_IOC_GET_DIR_PORT_CQ_FD				\
+		_IOWR(DLB2_IOC_MAGIC,				\
+		      DLB2_DOMAIN_CMD_GET_DIR_PORT_CQ_FD,	\
+		      struct dlb2_get_port_fd_args)
+#define DLB2_IOC_ENABLE_CQ_EPOLL				\
+               _IOWR(DLB2_IOC_MAGIC,				\
+                     DLB2_DOMAIN_CMD_ENABLE_CQ_EPOLL,		\
+                     struct dlb2_enable_cq_epoll_args)
+#define DLB2_IOC_ENABLE_CQ_WEIGHT				\
+		_IOWR(DLB2_IOC_MAGIC,				\
+		      DLB2_DOMAIN_CMD_ENABLE_CQ_WEIGHT,		\
+		      struct dlb2_enable_cq_weight_args)
+#define DLB2_IOC_SET_CQ_INFLIGHT_CTRL\
+		_IOWR(DLB2_IOC_MAGIC,				\
+		      DLB2_DOMAIN_CMD_SET_CQ_INFLIGHT_CTRL,	\
+		      struct dlb2_cq_inflight_ctrl_args)
+#define DLB2_IOC_GET_XSTATS					\
+		_IOWR(DLB2_IOC_MAGIC,				\
+		      DLB2_CMD_GET_XSTATS,			\
+		      struct dlb2_xstats_args)
 #endif /* __DLB2_USER_H */
diff --git a/drivers/event/dlb2/dlb2_xstats.c b/drivers/event/dlb2/dlb2_xstats.c
index ff15271..aa1cf57 100644
--- a/drivers/event/dlb2/dlb2_xstats.c
+++ b/drivers/event/dlb2/dlb2_xstats.c
@@ -32,11 +32,21 @@ enum dlb2_xstats_type {
 	ldb_pool_size,
 	dir_pool_size,
 	pool_size,
+	total_active_sched_cycles_pct,
+	total_idle_cycles_pct,
+	clk_gated_cycles_pct,
+	no_work_idle_cycles_pct,
+	no_space_idle_cycles_pct,
+	pipeline_friction_idle_cycles_pct,
+	inflight_limit_idle_cycles_pct,
+	fid_limit_idle_cycles_pct,
+	hcw_error_cnt,
 	/* port specific */
 	tx_new,				/**< Send an OP_NEW event */
 	tx_fwd,				/**< Send an OP_FORWARD event */
 	tx_rel,				/**< Send an OP_RELEASE event */
 	tx_implicit_rel,		/**< Issue an implicit event release */
+	tx_frag,			/**< Send a frag event */
 	tx_sched_ordered,		/**< Send a SCHED_TYPE_ORDERED event */
 	tx_sched_unordered,		/**< Send a SCHED_TYPE_PARALLEL event */
 	tx_sched_atomic,		/**< Send a SCHED_TYPE_ATOMIC event */
@@ -65,8 +75,10 @@ enum dlb2_xstats_type {
 	/**< Depth GT 50%, but LE to 75% of the configured hardware threshold */
 	depth_gt75_le100_threshold,
 	/**< Depth GT 75%. but LE to the configured hardware threshold */
-	depth_gt100_threshold
+	depth_gt100_threshold,
 	/**< Depth GT 100% of the configured hw threshold */
+	hw_version
+	/**hw_version*/
 };
 
 typedef uint64_t (*dlb2_xstats_fn)(struct dlb2_eventdev *dlb2,
@@ -152,6 +164,55 @@ dlb2_device_traffic_stat_get(struct dlb2_eventdev *dlb2,
 }
 
 static uint64_t
+dlb2_device_idle_stat_get(struct dlb2_eventdev *dlb2,
+			  int which_stat)
+{
+	struct dlb2_sched_cycles_percent *pct;
+	struct dlb2_sched_idle_counts *cnts;
+	struct dlb2_hw_dev *handle;
+
+	handle = &dlb2->qm_instance;
+	pct = &handle->dev_cycles_pct;
+	cnts = &handle->idle_counts;
+
+	switch (which_stat) {
+	case total_active_sched_cycles_pct:
+		/* Update dlb2_sched_cycles_percent struct
+		 * with idle cnt percentages */
+		if (dlb2_get_sched_idle_counts(dlb2))
+			return -EINVAL;
+		return pct->total_sched_pct;
+
+	case total_idle_cycles_pct:
+		return pct->total_idle_pct;
+
+	case clk_gated_cycles_pct:
+		return pct->clk_gated_pct;
+
+	case no_work_idle_cycles_pct:
+		return pct->nowork_idle_pct;
+
+	case no_space_idle_cycles_pct:
+		return pct->nospace_idle_pct;
+
+	case pipeline_friction_idle_cycles_pct:
+		return pct->pfriction_idle_pct;
+
+	case inflight_limit_idle_cycles_pct:
+		return pct->iflimit_idle_pct;
+
+	case fid_limit_idle_cycles_pct:
+		return pct->fidlimit_idle_pct;
+
+	case hcw_error_cnt:
+		return cnts->hcw_err_cnt;
+
+	default: return -1;
+	}
+	return 0;
+}
+
+static uint64_t
 get_dev_stat(struct dlb2_eventdev *dlb2, uint16_t obj_idx __rte_unused,
 	     enum dlb2_xstats_type type, int extra_arg __rte_unused)
 {
@@ -180,6 +241,18 @@ get_dev_stat(struct dlb2_eventdev *dlb2, uint16_t obj_idx __rte_unused,
 		return dlb2->num_dir_credits;
 	case pool_size:
 		return dlb2->num_credits;
+	case total_active_sched_cycles_pct:
+	case total_idle_cycles_pct:
+	case clk_gated_cycles_pct:
+	case no_work_idle_cycles_pct:
+	case no_space_idle_cycles_pct:
+	case pipeline_friction_idle_cycles_pct:
+	case inflight_limit_idle_cycles_pct:
+	case fid_limit_idle_cycles_pct:
+	case hcw_error_cnt:
+		return dlb2_device_idle_stat_get(dlb2, type);
+	case hw_version:
+		return dlb2->version;
 	default: return -1;
 	}
 }
@@ -238,6 +311,8 @@ get_port_stat(struct dlb2_eventdev *dlb2, uint16_t obj_idx,
 
 	case tx_implicit_rel: return ev_port->stats.tx_implicit_rel;
 
+	case tx_frag: return ev_port->stats.tx_op_cnt[RTE_EVENT_DLB2_OP_FRAG];
+
 	case tx_sched_ordered:
 		return ev_port->stats.tx_sched_cnt[DLB2_SCHED_ORDERED];
 
@@ -255,7 +330,7 @@ get_port_stat(struct dlb2_eventdev *dlb2, uint16_t obj_idx,
 	case outstanding_releases: return ev_port->outstanding_releases;
 
 	case max_outstanding_releases:
-		return DLB2_NUM_HIST_LIST_ENTRIES_PER_LDB_PORT;
+		return dlb2->qm_instance.cfg.resources.num_hist_list_entries;
 
 	case rx_sched_ordered:
 		return ev_port->stats.rx_sched_cnt[DLB2_SCHED_ORDERED];
@@ -378,6 +453,16 @@ dlb2_xstats_init(struct dlb2_eventdev *dlb2)
 		"ldb_pool_size",
 		"dir_pool_size",
 		"pool_size",
+		"total_active_sched_cycles_pct",
+		"total_idle_cycles_pct",
+		"clk_gated_cycles_pct",
+		"no_work_idle_cycles_pct",
+		"no_space_idle_cycles_pct",
+		"pipeline_friction_idle_cycles_pct",
+		"inflight_limit_idle_cycles_pct",
+		"fid_limit_idle_cycles_pct",
+		"hcw_error_cnt",
+		"hw_version",
 	};
 	static const enum dlb2_xstats_type dev_types[] = {
 		rx_ok,
@@ -398,6 +483,16 @@ dlb2_xstats_init(struct dlb2_eventdev *dlb2)
 		ldb_pool_size,
 		dir_pool_size,
 		pool_size,
+		total_active_sched_cycles_pct,
+		total_idle_cycles_pct,
+		clk_gated_cycles_pct,
+		no_work_idle_cycles_pct,
+		no_space_idle_cycles_pct,
+		pipeline_friction_idle_cycles_pct,
+		inflight_limit_idle_cycles_pct,
+		fid_limit_idle_cycles_pct,
+		hcw_error_cnt,
+		hw_version,
 	};
 	/* Note: generated device stats are not allowed to be reset. */
 	static const uint8_t dev_reset_allowed[] = {
@@ -419,6 +514,16 @@ dlb2_xstats_init(struct dlb2_eventdev *dlb2)
 		0, /* ldb_pool_size */
 		0, /* dir_pool_size */
 		0, /* pool_size */
+		0, /* total_active_sched_cycles_pct */
+		0, /* total_idle_cycles_pct */
+		0, /* clk_gated_cycles_pct */
+		0, /* no_work_idle_cycles_pct */
+		0, /* no_space_idle_cycles_pct */
+		0, /* pipeline_friction_idle_cycles_pct */
+		0, /* inflight_limit_idle_cycles_pct */
+		0, /* fid_limit_idle_cycles_pct */
+		0, /* hcw_error_cnt */
+		0, /* hw_version */
 	};
 	static const char * const port_stats[] = {
 		"is_configured",
@@ -441,6 +546,7 @@ dlb2_xstats_init(struct dlb2_eventdev *dlb2)
 		"tx_fwd",
 		"tx_rel",
 		"tx_implicit_rel",
+		"tx_frag",
 		"tx_sched_ordered",
 		"tx_sched_unordered",
 		"tx_sched_atomic",
@@ -475,6 +581,7 @@ dlb2_xstats_init(struct dlb2_eventdev *dlb2)
 		tx_fwd,
 		tx_rel,
 		tx_implicit_rel,
+		tx_frag,
 		tx_sched_ordered,
 		tx_sched_unordered,
 		tx_sched_atomic,
@@ -509,6 +616,7 @@ dlb2_xstats_init(struct dlb2_eventdev *dlb2)
 		1, /* tx_fwd */
 		1, /* tx_rel */
 		1, /* tx_implicit_rel */
+		1, /* tx_frag */
 		1, /* tx_sched_ordered */
 		1, /* tx_sched_unordered */
 		1, /* tx_sched_atomic */
@@ -1093,8 +1201,8 @@ dlb2_eventdev_dump(struct rte_eventdev *dev, FILE *f)
 		fprintf(f, "\tport is %s\n",
 			p->qm_port.is_directed ? "directed" : "load balanced");
 
-		fprintf(f, "\toutstanding releases=%u\n",
-			p->outstanding_releases);
+		fprintf(f, "\tnum frags=%u, outstanding releases=%u\n",
+			p->qm_port.pending_frags, p->outstanding_releases);
 
 		fprintf(f, "\tinflight max=%u, inflight credits=%u\n",
 			p->inflight_max, p->inflight_credits);
@@ -1117,26 +1225,26 @@ dlb2_eventdev_dump(struct rte_eventdev *dev, FILE *f)
 		fprintf(f, "\tcached_ldb_credits=%u\n",
 			p->qm_port.cached_ldb_credits);
 
-		fprintf(f, "\tldb_credits = %u\n",
-			p->qm_port.ldb_credits);
-
 		fprintf(f, "\tcached_dir_credits = %u\n",
 			p->qm_port.cached_dir_credits);
 
-		fprintf(f, "\tdir_credits = %u\n",
-			p->qm_port.dir_credits);
-
 		fprintf(f, "\tcached_credits = %u\n",
 			p->qm_port.cached_credits);
 
-		fprintf(f, "\tdir_credits = %u\n",
-			p->qm_port.credits);
-
 		fprintf(f, "\tgenbit=%d, cq_idx=%d, cq_depth=%d\n",
 			p->qm_port.gen_bit,
 			p->qm_port.cq_idx,
 			p->qm_port.cq_depth);
 
+		if (p->qm_port.dequeue_wait == DLB2_PORT_DEQUEUE_WAIT_POLLING)
+			fprintf(f, "\tdequeue wait mode is polling\n");
+		else if (p->qm_port.dequeue_wait ==
+				DLB2_PORT_DEQUEUE_WAIT_INTERRUPT)
+			fprintf(f, "\tdequeue wait mode is interrupt\n");
+		else if (p->qm_port.dequeue_wait ==
+				DLB2_PORT_DEQUEUE_WAIT_UMWAIT)
+			fprintf(f, "\tdequeue wait mode is umwait\n");
+
 		fprintf(f, "\tinterrupt armed=%d\n",
 			p->qm_port.int_armed);
 
@@ -1193,6 +1301,9 @@ dlb2_eventdev_dump(struct rte_eventdev *dev, FILE *f)
 		fprintf(f, "\t\ttx_implicit_rel %" PRIu64 "\n",
 			p->stats.tx_implicit_rel);
 
+		fprintf(f, "\t\ttx_frag %" PRIu64 "\n",
+			p->stats.tx_op_cnt[RTE_EVENT_DLB2_OP_FRAG]);
+
 		fprintf(f, "\t\ttx_sched_ordered %" PRIu64 "\n",
 			p->stats.tx_sched_cnt[DLB2_SCHED_ORDERED]);
 
diff --git a/drivers/event/dlb2/meson.build b/drivers/event/dlb2/meson.build
index a2e6027..540a3ea 100644
--- a/drivers/event/dlb2/meson.build
+++ b/drivers/event/dlb2/meson.build
@@ -8,60 +8,32 @@ if not is_linux or not dpdk_conf.has('RTE_ARCH_X86_64')
         subdir_done()
 endif
 
-sources = files(
-        'dlb2.c',
-        'dlb2_iface.c',
-        'dlb2_xstats.c',
-        'pf/dlb2_main.c',
-        'pf/dlb2_pf.c',
-        'pf/base/dlb2_resource.c',
-        'rte_pmd_dlb2.c',
-        'dlb2_selftest.c',
+sources = files('dlb2.c',
+		'dlb2_iface.c',
+		'dlb2_xstats.c',
+		'pf/dlb2_main.c',
+		'pf/dlb2_pf.c',
+		'pf/base/dlb2_resource.c',
+		'rte_pmd_dlb2.c',
+		'dlb2_selftest.c'
 )
 
-# compile AVX512 version if:
-# we are building 64-bit binary (checked above) AND binutils
-# can generate proper code
-
-if binutils_ok
-
-    # compile AVX512 version if either:
-    # a. we have AVX512VL supported in minimum instruction set
-    #    baseline
-    # b. it's not minimum instruction set, but supported by
-    #    compiler
-    #
-    # in former case, just add avx512 C file to files list
-    # in latter case, compile c file to static lib, using correct
-    # compiler flags, and then have the .o file from static lib
-    # linked into main lib.
-
-    # check if all required flags already enabled (variant a).
-    dlb2_avx512_on = false
-    if cc.get_define('__AVX512VL__', args: machine_args) != ''
-        dlb2_avx512_on = true
-    endif
+if host_machine.system() == 'linux'
+	sources += files('bifurcated/dlb2_vdev.c')
+endif
 
-    if dlb2_avx512_on == true
+headers = files('dlb2_frag.h', 'rte_pmd_dlb2.h')
 
-        sources += files('dlb2_avx512.c')
-        cflags += '-DCC_AVX512_SUPPORT'
+deps += ['mbuf', 'mempool', 'ring', 'pci', 'bus_pci', 'bus_vdev']
 
-    elif cc.has_multi_arguments('-mavx512vl')
+if meson.version().version_compare('> 0.58.0')
+fs = import('fs')
+dlb_options = fs.read('meson_options.txt').strip().split('\n')
 
-        cflags += '-DCC_AVX512_SUPPORT'
-        avx512_tmplib = static_library('avx512_tmp',
-                               'dlb2_avx512.c',
-                               dependencies: [static_rte_eal, static_rte_eventdev],
-                               c_args: cflags + ['-mavx512vl'])
-        objs += avx512_tmplib.extract_objects('dlb2_avx512.c')
-    else
-        sources += files('dlb2_sse.c')
-    endif
-else
-        sources += files('dlb2_sse.c')
+foreach opt: dlb_options
+	if (opt.strip().startswith('#') or opt.strip() == '')
+		continue
+	endif
+	cflags += '-D' + opt.strip().to_upper().replace(' ','')
+endforeach
 endif
-
-headers = files('rte_pmd_dlb2.h')
-
-deps += ['mbuf', 'mempool', 'ring', 'pci', 'bus_pci']
diff --git a/drivers/event/dlb2/meson_options.txt b/drivers/event/dlb2/meson_options.txt
new file mode 100644
index 0000000..dbc337c
--- /dev/null
+++ b/drivers/event/dlb2/meson_options.txt
@@ -0,0 +1,10 @@
+
+# SPDX-License-Identifier: BSD-3-Clause
+# Copyright(c) 2023-2024 Intel Corporation
+# WARNING: Do not change defaults without knowing the side effects
+
+DLB2_BYPASS_FENCE_ON_PP = 0
+DLB_HW_CREDITS_CHECKS = 0
+DLB_SW_CREDITS_CHECKS = 1
+DLB_TYPE_CHECK = 1
+
diff --git a/drivers/event/dlb2/pf/base/dlb2_hw_types.h b/drivers/event/dlb2/pf/base/dlb2_hw_types.h
index be09363..6483c37 100644
--- a/drivers/event/dlb2/pf/base/dlb2_hw_types.h
+++ b/drivers/event/dlb2/pf/base/dlb2_hw_types.h
@@ -2,12 +2,10 @@
  * Copyright(c) 2016-2020 Intel Corporation
  */
 
-#ifndef __DLB2_HW_TYPES_NEW_H
-#define __DLB2_HW_TYPES_NEW_H
+#ifndef __DLB2_HW_TYPES_H
+#define __DLB2_HW_TYPES_H
 
-#include "../../dlb2_priv.h"
 #include "dlb2_user.h"
-
 #include "dlb2_osdep_list.h"
 #include "dlb2_osdep_types.h"
 #include "dlb2_regs.h"
@@ -18,26 +16,68 @@
 #define DLB2_BIT_SET(x, mask)	((x) |= (mask))
 #define DLB2_BITS_GET(x, mask)	(((x) & (mask)) >> (mask##_LOC))
 
+#define DLB2_SYND2(y)        DLB2_BITS_GET(synd2, DLB2_SYS_ALARM_PF_SYND2_##y)
+#define DLB2_SYND1(y)        DLB2_BITS_GET(synd1, DLB2_SYS_ALARM_PF_SYND1_##y)
+#define DLB2_SYND0(y)        DLB2_BITS_GET(synd0, DLB2_SYS_ALARM_PF_SYND0_##y)
+#define DLB2_SYND(y)         DLB2_BITS_GET(synd, DLB2_SYS_ALARM_HW_SYND_##y)
+
 #define DLB2_MAX_NUM_VDEVS			16
+#define DLB2_MAX_NUM_DOMAINS			32
+#define DLB2_MAX_NUM_LDB_QUEUES			32 /* LDB == load-balanced */
+#define DLB2_MAX_NUM_DIR_QUEUES_V2		64 /* DIR == directed */
+#define DLB2_MAX_NUM_DIR_QUEUES_V2_5		96
+/* When needed for array sizing, the DLB 2.5 macro is used */
+#define DLB2_MAX_NUM_DIR_QUEUES(ver)		(ver == DLB2_HW_V2 ? \
+						 DLB2_MAX_NUM_DIR_QUEUES_V2 : \
+						 DLB2_MAX_NUM_DIR_QUEUES_V2_5)
+#define DLB2_MAX_NUM_LDB_PORTS			64
+#define DLB2_MAX_NUM_DIR_PORTS_V2		DLB2_MAX_NUM_DIR_QUEUES_V2
+#define DLB2_MAX_NUM_DIR_PORTS_V2_5		DLB2_MAX_NUM_DIR_QUEUES_V2_5
+#define DLB2_MAX_NUM_DIR_PORTS(ver)		(ver == DLB2_HW_V2 ? \
+						 DLB2_MAX_NUM_DIR_PORTS_V2 : \
+						 DLB2_MAX_NUM_DIR_PORTS_V2_5)
+#define DLB2_MAX_NUM_LDB_CREDITS		(8 * 1024)
+#define DLB2_MAX_NUM_DIR_CREDITS(ver)		(ver == DLB2_HW_V2 ? 4096 : 0)
+#define DLB2_MAX_NUM_HIST_LIST_ENTRIES		2048
+#define DLB2_MAX_NUM_AQED_ENTRIES		2048
+#define DLB2_MAX_NUM_QIDS_PER_LDB_CQ		8
 #define DLB2_MAX_NUM_SEQUENCE_NUMBER_GROUPS	2
+#define DLB2_MAX_NUM_SEQUENCE_NUMBER_MODES	5
+#define DLB2_QID_PRIORITIES			8
 #define DLB2_NUM_ARB_WEIGHTS			8
-#define DLB2_MAX_NUM_AQED_ENTRIES		2048
 #define DLB2_MAX_WEIGHT				255
 #define DLB2_NUM_COS_DOMAINS			4
-#define DLB2_MAX_NUM_SEQUENCE_NUMBER_GROUPS	2
-#define DLB2_MAX_NUM_SEQUENCE_NUMBER_MODES	5
 #define DLB2_MAX_CQ_COMP_CHECK_LOOPS		409600
-#define DLB2_MAX_QID_EMPTY_CHECK_LOOPS		(4 * DLB2_MAX_NUM_LDB_CREDITS)
-
+#define DLB2_MAX_QID_EMPTY_CHECK_LOOPS(ver)	(4 * DLB2_MAX_NUM_LDB_CREDITS)
+#ifdef FPGA
+#define DLB2_HZ					2000000
+#else
+#define DLB2_HZ					800000000
+#endif
 #define DLB2_FUNC_BAR				0
 #define DLB2_CSR_BAR				2
 
 #define PCI_DEVICE_ID_INTEL_DLB2_PF 0x2710
 #define PCI_DEVICE_ID_INTEL_DLB2_VF 0x2711
-
 #define PCI_DEVICE_ID_INTEL_DLB2_5_PF 0x2714
 #define PCI_DEVICE_ID_INTEL_DLB2_5_VF 0x2715
 
+/* Interrupt related macros */
+#define DLB2_PF_NUM_NON_CQ_INTERRUPT_VECTORS 1
+#define DLB2_PF_NUM_CQ_INTERRUPT_VECTORS     64
+#define DLB2_PF_TOTAL_NUM_INTERRUPT_VECTORS \
+	(DLB2_PF_NUM_NON_CQ_INTERRUPT_VECTORS + \
+	 DLB2_PF_NUM_CQ_INTERRUPT_VECTORS)
+#define DLB2_PF_NUM_COMPRESSED_MODE_VECTORS \
+	(DLB2_PF_NUM_NON_CQ_INTERRUPT_VECTORS + 1)
+#define DLB2_PF_NUM_PACKED_MODE_VECTORS \
+	DLB2_PF_TOTAL_NUM_INTERRUPT_VECTORS
+#define DLB2_PF_COMPRESSED_MODE_CQ_VECTOR_ID \
+	DLB2_PF_NUM_NON_CQ_INTERRUPT_VECTORS
+
+/* DLB non-CQ interrupts (alarm, mailbox, WDT) */
+#define DLB2_INT_NON_CQ 0
+
 #define DLB2_ALARM_HW_SOURCE_SYS 0
 #define DLB2_ALARM_HW_SOURCE_DLB 1
 
@@ -49,6 +89,20 @@
 #define DLB2_ALARM_HW_CHP_AID_ILLEGAL_ENQ	1
 #define DLB2_ALARM_HW_CHP_AID_EXCESS_TOKEN_POPS 2
 
+#define DLB2_VF_NUM_NON_CQ_INTERRUPT_VECTORS 1
+#define DLB2_VF_NUM_CQ_INTERRUPT_VECTORS     31
+#define DLB2_VF_BASE_CQ_VECTOR_ID	     0
+#define DLB2_VF_LAST_CQ_VECTOR_ID	     30
+#define DLB2_VF_MBOX_VECTOR_ID		     31
+#define DLB2_VF_TOTAL_NUM_INTERRUPT_VECTORS \
+	(DLB2_VF_NUM_NON_CQ_INTERRUPT_VECTORS + \
+	 DLB2_VF_NUM_CQ_INTERRUPT_VECTORS)
+
+#define DLB2_VDEV_MAX_NUM_INTERRUPT_VECTORS_V2 \
+	(DLB2_MAX_NUM_LDB_PORTS + DLB2_MAX_NUM_DIR_PORTS_V2 + 1)
+#define DLB2_VDEV_MAX_NUM_INTERRUPT_VECTORS_V2_5 \
+	(DLB2_MAX_NUM_LDB_PORTS + DLB2_MAX_NUM_DIR_PORTS_V2_5 + 1)
+
 /*
  * Hardware-defined base addresses. Those prefixed 'DLB2_DRV' are only used by
  * the PF driver.
@@ -148,7 +202,7 @@ struct dlb2_dir_pq_pair {
 };
 
 enum dlb2_qid_map_state {
-	/* The slot does not contain a valid queue mapping */
+	/* The slot doesn't contain a valid queue mapping */
 	DLB2_QUEUE_UNMAPPED,
 	/* The slot contains a valid queue mapping */
 	DLB2_QUEUE_MAPPED,
@@ -332,6 +386,18 @@ struct dlb2_sw_mbox {
 	void *pf_to_vdev_inject_arg;
 };
 
+struct dlb2_hw_sched_idle_counts {
+	u64 ldb_perf_sched_cnt;
+	u64 ldb_perf_nowork_idle_cnt;
+	u64 ldb_perf_nospace_idle_cnt;
+	u64 ldb_perf_pfriction_idle_cnt;
+	u64 ldb_perf_iflimit_idle_cnt;
+	u64 ldb_perf_fidlimit_idle_cnt;
+	u64 perf_proc_on_cnt;
+	u64 perf_clk_on_cnt;
+	u64 hcw_err_cnt;
+};
+
 struct dlb2_hw {
 	uint8_t ver;
 
@@ -359,4 +425,4 @@ struct dlb2_hw {
 	unsigned int pasid[DLB2_MAX_NUM_VDEVS];
 };
 
-#endif /* __DLB2_HW_TYPES_NEW_H */
+#endif /* __DLB2_HW_TYPES_H */
diff --git a/drivers/event/dlb2/pf/base/dlb2_mbox.h b/drivers/event/dlb2/pf/base/dlb2_mbox.h
new file mode 100644
index 0000000..2ce2d6e
--- /dev/null
+++ b/drivers/event/dlb2/pf/base/dlb2_mbox.h
@@ -0,0 +1,651 @@
+/* SPDX-License-Identifier: BSD-3-Clause
+ * Copyright(c) 2016-2020 Intel Corporation
+ */
+
+#ifndef __DLB2_BASE_DLB2_MBOX_H
+#define __DLB2_BASE_DLB2_MBOX_H
+
+#include "dlb2_osdep_types.h"
+#include "dlb2_regs.h"
+
+#define DLB2_MBOX_INTERFACE_VERSION 1
+
+/*
+ * The PF uses its PF->VF mailbox to send responses to VF requests, as well as
+ * to send requests of its own (e.g. notifying a VF of an impending FLR).
+ * To avoid communication race conditions, e.g. the PF sends a response and then
+ * sends a request before the VF reads the response, the PF->VF mailbox is
+ * divided into two sections:
+ * - Bytes 0-47: PF responses
+ * - Bytes 48-63: PF requests
+ *
+ * Partitioning the PF->VF mailbox allows responses and requests to occupy the
+ * mailbox simultaneously.
+ */
+#define DLB2_PF2VF_RESP_BYTES	  48
+#define DLB2_PF2VF_RESP_BASE	  0
+#define DLB2_PF2VF_RESP_BASE_WORD (DLB2_PF2VF_RESP_BASE / 4)
+
+#define DLB2_PF2VF_REQ_BYTES	  16
+#define DLB2_PF2VF_REQ_BASE	  (DLB2_PF2VF_RESP_BASE + DLB2_PF2VF_RESP_BYTES)
+#define DLB2_PF2VF_REQ_BASE_WORD  (DLB2_PF2VF_REQ_BASE / 4)
+
+/*
+ * Similarly, the VF->PF mailbox is divided into two sections:
+ * - Bytes 0-239: VF requests
+ * -- (Bytes 0-3 are unused due to a hardware errata)
+ * - Bytes 240-255: VF responses
+ */
+#define DLB2_VF2PF_REQ_BYTES	 236
+#define DLB2_VF2PF_REQ_BASE	 4
+#define DLB2_VF2PF_REQ_BASE_WORD (DLB2_VF2PF_REQ_BASE / 4)
+
+#define DLB2_VF2PF_RESP_BYTES	  16
+#define DLB2_VF2PF_RESP_BASE	  (DLB2_VF2PF_REQ_BASE + DLB2_VF2PF_REQ_BYTES)
+#define DLB2_VF2PF_RESP_BASE_WORD (DLB2_VF2PF_RESP_BASE / 4)
+
+/* VF-initiated commands */
+enum dlb2_mbox_cmd_type {
+	DLB2_MBOX_CMD_REGISTER,
+	DLB2_MBOX_CMD_UNREGISTER,
+	DLB2_MBOX_CMD_GET_NUM_RESOURCES,
+	DLB2_MBOX_CMD_CREATE_SCHED_DOMAIN,
+	DLB2_MBOX_CMD_RESET_SCHED_DOMAIN,
+	DLB2_MBOX_CMD_CREATE_LDB_QUEUE,
+	DLB2_MBOX_CMD_CREATE_DIR_QUEUE,
+	DLB2_MBOX_CMD_CREATE_LDB_PORT,
+	DLB2_MBOX_CMD_CREATE_DIR_PORT,
+	DLB2_MBOX_CMD_ENABLE_LDB_PORT,
+	DLB2_MBOX_CMD_DISABLE_LDB_PORT,
+	DLB2_MBOX_CMD_ENABLE_DIR_PORT,
+	DLB2_MBOX_CMD_DISABLE_DIR_PORT,
+	DLB2_MBOX_CMD_LDB_PORT_OWNED_BY_DOMAIN,
+	DLB2_MBOX_CMD_DIR_PORT_OWNED_BY_DOMAIN,
+	DLB2_MBOX_CMD_MAP_QID,
+	DLB2_MBOX_CMD_UNMAP_QID,
+	DLB2_MBOX_CMD_START_DOMAIN,
+	DLB2_MBOX_CMD_ENABLE_LDB_PORT_INTR,
+	DLB2_MBOX_CMD_ENABLE_DIR_PORT_INTR,
+	DLB2_MBOX_CMD_ARM_CQ_INTR,
+	DLB2_MBOX_CMD_GET_NUM_USED_RESOURCES,
+	DLB2_MBOX_CMD_GET_SN_ALLOCATION,
+	DLB2_MBOX_CMD_GET_LDB_QUEUE_DEPTH,
+	DLB2_MBOX_CMD_GET_DIR_QUEUE_DEPTH,
+	DLB2_MBOX_CMD_PENDING_PORT_UNMAPS,
+	DLB2_MBOX_CMD_GET_COS_BW,
+	DLB2_MBOX_CMD_GET_SN_OCCUPANCY,
+	DLB2_MBOX_CMD_QUERY_CQ_POLL_MODE,
+	DLB2_MBOX_CMD_DEV_RESET,
+	DLB2_MBOX_CMD_ENABLE_CQ_WEIGHT,
+
+	/* NUM_QE_CMD_TYPES must be last */
+	NUM_DLB2_MBOX_CMD_TYPES,
+};
+
+static const char dlb2_mbox_cmd_type_strings[][128] = {
+	"DLB2_MBOX_CMD_REGISTER",
+	"DLB2_MBOX_CMD_UNREGISTER",
+	"DLB2_MBOX_CMD_GET_NUM_RESOURCES",
+	"DLB2_MBOX_CMD_CREATE_SCHED_DOMAIN",
+	"DLB2_MBOX_CMD_RESET_SCHED_DOMAIN",
+	"DLB2_MBOX_CMD_CREATE_LDB_QUEUE",
+	"DLB2_MBOX_CMD_CREATE_DIR_QUEUE",
+	"DLB2_MBOX_CMD_CREATE_LDB_PORT",
+	"DLB2_MBOX_CMD_CREATE_DIR_PORT",
+	"DLB2_MBOX_CMD_ENABLE_LDB_PORT",
+	"DLB2_MBOX_CMD_DISABLE_LDB_PORT",
+	"DLB2_MBOX_CMD_ENABLE_DIR_PORT",
+	"DLB2_MBOX_CMD_DISABLE_DIR_PORT",
+	"DLB2_MBOX_CMD_LDB_PORT_OWNED_BY_DOMAIN",
+	"DLB2_MBOX_CMD_DIR_PORT_OWNED_BY_DOMAIN",
+	"DLB2_MBOX_CMD_MAP_QID",
+	"DLB2_MBOX_CMD_UNMAP_QID",
+	"DLB2_MBOX_CMD_START_DOMAIN",
+	"DLB2_MBOX_CMD_ENABLE_LDB_PORT_INTR",
+	"DLB2_MBOX_CMD_ENABLE_DIR_PORT_INTR",
+	"DLB2_MBOX_CMD_ARM_CQ_INTR",
+	"DLB2_MBOX_CMD_GET_NUM_USED_RESOURCES",
+	"DLB2_MBOX_CMD_GET_SN_ALLOCATION",
+	"DLB2_MBOX_CMD_GET_LDB_QUEUE_DEPTH",
+	"DLB2_MBOX_CMD_GET_DIR_QUEUE_DEPTH",
+	"DLB2_MBOX_CMD_PENDING_PORT_UNMAPS",
+	"DLB2_MBOX_CMD_GET_COS_BW",
+	"DLB2_MBOX_CMD_GET_SN_OCCUPANCY",
+	"DLB2_MBOX_CMD_QUERY_CQ_POLL_MODE",
+	"DLB2_MBOX_CMD_DEV_RESET",
+	"DLB2_MBOX_CMD_ENABLE_CQ_WEIGHT",
+};
+
+/* PF-initiated commands */
+enum dlb2_mbox_vf_cmd_type {
+	DLB2_MBOX_VF_CMD_DOMAIN_ALERT,
+	DLB2_MBOX_VF_CMD_NOTIFICATION,
+	DLB2_MBOX_VF_CMD_IN_USE,
+
+	/* NUM_DLB2_MBOX_VF_CMD_TYPES must be last */
+	NUM_DLB2_MBOX_VF_CMD_TYPES,
+};
+
+static const char dlb2_mbox_vf_cmd_type_strings[][128] = {
+	"DLB2_MBOX_VF_CMD_DOMAIN_ALERT",
+	"DLB2_MBOX_VF_CMD_NOTIFICATION",
+	"DLB2_MBOX_VF_CMD_IN_USE",
+};
+
+#define DLB2_MBOX_CMD_TYPE(hdr) \
+	(((struct dlb2_mbox_req_hdr *)hdr)->type)
+
+#define DLB2_MBOX_CMD_STRING(hdr) \
+	dlb2_mbox_cmd_type_strings[DLB2_MBOX_CMD_TYPE(hdr)]
+
+enum dlb2_mbox_status_type {
+	DLB2_MBOX_ST_SUCCESS,
+	DLB2_MBOX_ST_INVALID_CMD_TYPE,
+	DLB2_MBOX_ST_VERSION_MISMATCH,
+	DLB2_MBOX_ST_INVALID_OWNER_VF,
+
+	/* NUM_DLB2_MBOX_STATUS_TYPES must be last */
+	NUM_DLB2_MBOX_STATUS_TYPES,
+};
+
+static const char dlb2_mbox_status_type_strings[][128] = {
+	"DLB2_MBOX_ST_SUCCESS",
+	"DLB2_MBOX_ST_INVALID_CMD_TYPE",
+	"DLB2_MBOX_ST_VERSION_MISMATCH",
+	"DLB2_MBOX_ST_INVALID_OWNER_VF",
+};
+
+/* This structure is always the first field in a request structure */
+struct dlb2_mbox_req_hdr {
+	u32 type;
+};
+
+/* This structure is always the first field in a response structure */
+struct dlb2_mbox_resp_hdr {
+	u32 status;
+};
+
+static inline const char*
+dlb2_mbox_cmd_string(struct dlb2_mbox_req_hdr *hdr)
+{
+	if (hdr->type >= NUM_DLB2_MBOX_CMD_TYPES)
+		return "(invalid request)";
+	else
+		return dlb2_mbox_cmd_type_strings[hdr->type];
+}
+
+static inline const char*
+dlb2_mbox_st_string(struct dlb2_mbox_resp_hdr *hdr)
+{
+	if (hdr->status >= NUM_DLB2_MBOX_STATUS_TYPES)
+		return "(invalid response)";
+	else
+		return dlb2_mbox_status_type_strings[hdr->status];
+}
+
+enum dlb2_mbox_error_code {
+	DLB2_MBOX_SUCCESS,
+	DLB2_MBOX_EINVAL,	/* Invalid argument */
+	DLB2_MBOX_EFAULT,	/* Internal error */
+	DLB2_MBOX_EPERM,	/* The requested resource is locked */
+	DLB2_MBOX_ETIMEDOUT,	/* Operation timed out */
+};
+
+struct dlb2_mbox_register_cmd_req {
+	struct dlb2_mbox_req_hdr hdr;
+	u16 min_interface_version;
+	u16 max_interface_version;
+};
+
+#define DLB2_MBOX_FLAG_IS_AUX_VF  BIT(0)
+#define DLB2_MBOX_FLAG_MBOX_RESET BIT(1)
+
+struct dlb2_mbox_register_cmd_resp {
+	struct dlb2_mbox_resp_hdr hdr;
+	u32 interface_version;
+	u8 pf_id;
+	u8 vf_id;
+	u8 primary_vf_id;
+	u8 padding;
+	u32 flags;
+};
+
+struct dlb2_mbox_unregister_cmd_req {
+	struct dlb2_mbox_req_hdr hdr;
+	u32 padding;
+};
+
+struct dlb2_mbox_unregister_cmd_resp {
+	struct dlb2_mbox_resp_hdr hdr;
+	u32 padding;
+};
+
+struct dlb2_mbox_get_num_resources_cmd_req {
+	struct dlb2_mbox_req_hdr hdr;
+	u32 padding;
+};
+
+struct dlb2_mbox_get_num_resources_cmd_resp {
+	struct dlb2_mbox_resp_hdr hdr;
+	u32 error_code;
+	u16 num_sched_domains;
+	u16 num_ldb_queues;
+	u16 num_ldb_ports;
+	u16 num_cos_ldb_ports[4];
+	u16 num_dir_ports;
+	u32 num_atomic_inflights;
+	u32 num_hist_list_entries;
+	u32 max_contiguous_hist_list_entries;
+	u16 num_ldb_credits;
+	u16 num_dir_credits;
+};
+
+struct dlb2_mbox_create_sched_domain_cmd_req {
+	struct dlb2_mbox_req_hdr hdr;
+	u32 num_ldb_queues;
+	u32 num_ldb_ports;
+	u32 num_cos_ldb_ports[4];
+	u32 num_dir_ports;
+	u32 num_atomic_inflights;
+	u32 num_hist_list_entries;
+	u32 num_ldb_credits;
+	u32 num_dir_credits;
+	u8 cos_strict;
+	u8 padding0[3];
+	u32 padding1;
+};
+
+struct dlb2_mbox_create_sched_domain_cmd_resp {
+	struct dlb2_mbox_resp_hdr hdr;
+	u32 error_code;
+	u32 status;
+	u32 id;
+};
+
+struct dlb2_mbox_reset_sched_domain_cmd_req {
+	struct dlb2_mbox_req_hdr hdr;
+	u32 id;
+};
+
+struct dlb2_mbox_reset_sched_domain_cmd_resp {
+	struct dlb2_mbox_resp_hdr hdr;
+	u32 error_code;
+};
+
+struct dlb2_mbox_create_ldb_queue_cmd_req {
+	struct dlb2_mbox_req_hdr hdr;
+	u32 domain_id;
+	u32 num_sequence_numbers;
+	u32 num_qid_inflights;
+	u32 num_atomic_inflights;
+	u32 lock_id_comp_level;
+	u32 depth_threshold;
+	u32 padding;
+};
+
+struct dlb2_mbox_create_ldb_queue_cmd_resp {
+	struct dlb2_mbox_resp_hdr hdr;
+	u32 error_code;
+	u32 status;
+	u32 id;
+};
+
+struct dlb2_mbox_create_dir_queue_cmd_req {
+	struct dlb2_mbox_req_hdr hdr;
+	u32 domain_id;
+	u32 port_id;
+	u32 depth_threshold;
+};
+
+struct dlb2_mbox_create_dir_queue_cmd_resp {
+	struct dlb2_mbox_resp_hdr hdr;
+	u32 error_code;
+	u32 status;
+	u32 id;
+};
+
+struct dlb2_mbox_create_ldb_port_cmd_req {
+	struct dlb2_mbox_req_hdr hdr;
+	u32 domain_id;
+	u16 cq_depth;
+	u16 cq_history_list_size;
+	u8 cos_id;
+	u8 cos_strict;
+	u16 padding1;
+	u64 cq_base_address;
+};
+
+struct dlb2_mbox_create_ldb_port_cmd_resp {
+	struct dlb2_mbox_resp_hdr hdr;
+	u32 error_code;
+	u32 status;
+	u32 id;
+};
+
+struct dlb2_mbox_create_dir_port_cmd_req {
+	struct dlb2_mbox_req_hdr hdr;
+	u32 domain_id;
+	u64 cq_base_address;
+	u16 cq_depth;
+	u16 padding0;
+	s32 queue_id;
+};
+
+struct dlb2_mbox_create_dir_port_cmd_resp {
+	struct dlb2_mbox_resp_hdr hdr;
+	u32 error_code;
+	u32 status;
+	u32 id;
+};
+
+struct dlb2_mbox_enable_ldb_port_cmd_req {
+	struct dlb2_mbox_req_hdr hdr;
+	u32 domain_id;
+	u32 port_id;
+	u32 padding;
+};
+
+struct dlb2_mbox_enable_ldb_port_cmd_resp {
+	struct dlb2_mbox_resp_hdr hdr;
+	u32 error_code;
+	u32 status;
+	u32 padding;
+};
+
+struct dlb2_mbox_disable_ldb_port_cmd_req {
+	struct dlb2_mbox_req_hdr hdr;
+	u32 domain_id;
+	u32 port_id;
+	u32 padding;
+};
+
+struct dlb2_mbox_disable_ldb_port_cmd_resp {
+	struct dlb2_mbox_resp_hdr hdr;
+	u32 error_code;
+	u32 status;
+	u32 padding;
+};
+
+struct dlb2_mbox_enable_dir_port_cmd_req {
+	struct dlb2_mbox_req_hdr hdr;
+	u32 domain_id;
+	u32 port_id;
+	u32 padding;
+};
+
+struct dlb2_mbox_enable_dir_port_cmd_resp {
+	struct dlb2_mbox_resp_hdr hdr;
+	u32 error_code;
+	u32 status;
+	u32 padding;
+};
+
+struct dlb2_mbox_disable_dir_port_cmd_req {
+	struct dlb2_mbox_req_hdr hdr;
+	u32 domain_id;
+	u32 port_id;
+	u32 padding;
+};
+
+struct dlb2_mbox_disable_dir_port_cmd_resp {
+	struct dlb2_mbox_resp_hdr hdr;
+	u32 error_code;
+	u32 status;
+	u32 padding;
+};
+
+struct dlb2_mbox_ldb_port_owned_by_domain_cmd_req {
+	struct dlb2_mbox_req_hdr hdr;
+	u32 domain_id;
+	u32 port_id;
+	u32 padding;
+};
+
+struct dlb2_mbox_ldb_port_owned_by_domain_cmd_resp {
+	struct dlb2_mbox_resp_hdr hdr;
+	s32 owned;
+};
+
+struct dlb2_mbox_dir_port_owned_by_domain_cmd_req {
+	struct dlb2_mbox_req_hdr hdr;
+	u32 domain_id;
+	u32 port_id;
+	u32 padding;
+};
+
+struct dlb2_mbox_dir_port_owned_by_domain_cmd_resp {
+	struct dlb2_mbox_resp_hdr hdr;
+	s32 owned;
+};
+
+struct dlb2_mbox_map_qid_cmd_req {
+	struct dlb2_mbox_req_hdr hdr;
+	u32 domain_id;
+	u32 port_id;
+	u32 qid;
+	u32 priority;
+	u32 padding0;
+};
+
+struct dlb2_mbox_map_qid_cmd_resp {
+	struct dlb2_mbox_resp_hdr hdr;
+	u32 error_code;
+	u32 status;
+	u32 id;
+};
+
+struct dlb2_mbox_unmap_qid_cmd_req {
+	struct dlb2_mbox_req_hdr hdr;
+	u32 domain_id;
+	u32 port_id;
+	u32 qid;
+};
+
+struct dlb2_mbox_unmap_qid_cmd_resp {
+	struct dlb2_mbox_resp_hdr hdr;
+	u32 error_code;
+	u32 status;
+	u32 padding;
+};
+
+struct dlb2_mbox_start_domain_cmd_req {
+	struct dlb2_mbox_req_hdr hdr;
+	u32 domain_id;
+};
+
+struct dlb2_mbox_start_domain_cmd_resp {
+	struct dlb2_mbox_resp_hdr hdr;
+	u32 error_code;
+	u32 status;
+	u32 padding;
+};
+
+struct dlb2_mbox_enable_ldb_port_intr_cmd_req {
+	struct dlb2_mbox_req_hdr hdr;
+	u16 port_id;
+	u16 thresh;
+	u16 vector;
+	u16 owner_vf;
+	u16 reserved[2];
+};
+
+struct dlb2_mbox_enable_ldb_port_intr_cmd_resp {
+	struct dlb2_mbox_resp_hdr hdr;
+	u32 error_code;
+	u32 status;
+	u32 padding;
+};
+
+struct dlb2_mbox_enable_dir_port_intr_cmd_req {
+	struct dlb2_mbox_req_hdr hdr;
+	u16 port_id;
+	u16 thresh;
+	u16 vector;
+	u16 owner_vf;
+	u16 reserved[2];
+};
+
+struct dlb2_mbox_enable_dir_port_intr_cmd_resp {
+	struct dlb2_mbox_resp_hdr hdr;
+	u32 error_code;
+	u32 status;
+	u32 padding;
+};
+
+struct dlb2_mbox_arm_cq_intr_cmd_req {
+	struct dlb2_mbox_req_hdr hdr;
+	u32 domain_id;
+	u32 port_id;
+	u32 is_ldb;
+};
+
+struct dlb2_mbox_arm_cq_intr_cmd_resp {
+	struct dlb2_mbox_resp_hdr hdr;
+	u32 error_code;
+	u32 status;
+	u32 padding0;
+};
+
+/*
+ * The alert_id and aux_alert_data follows the format of the alerts defined in
+ * dlb2_types.h. The alert id contains an enum dlb2_domain_alert_id value, and
+ * the aux_alert_data value varies depending on the alert.
+ */
+struct dlb2_mbox_vf_alert_cmd_req {
+	struct dlb2_mbox_req_hdr hdr;
+	u32 domain_id;
+	u32 alert_id;
+	u32 aux_alert_data;
+};
+
+enum dlb2_mbox_vf_notification_type {
+	DLB2_MBOX_VF_NOTIFICATION_PRE_RESET,
+
+	/* NUM_DLB2_MBOX_VF_NOTIFICATION_TYPES must be last */
+	NUM_DLB2_MBOX_VF_NOTIFICATION_TYPES,
+};
+
+struct dlb2_mbox_vf_notification_cmd_req {
+	struct dlb2_mbox_req_hdr hdr;
+	u32 notification;
+};
+
+struct dlb2_mbox_vf_in_use_cmd_req {
+	struct dlb2_mbox_req_hdr hdr;
+	u32 padding;
+};
+
+struct dlb2_mbox_vf_in_use_cmd_resp {
+	struct dlb2_mbox_resp_hdr hdr;
+	u32 in_use;
+};
+
+struct dlb2_mbox_get_sn_allocation_cmd_req {
+	struct dlb2_mbox_req_hdr hdr;
+	u32 group_id;
+};
+
+struct dlb2_mbox_get_sn_allocation_cmd_resp {
+	struct dlb2_mbox_resp_hdr hdr;
+	u32 num;
+};
+
+struct dlb2_mbox_get_ldb_queue_depth_cmd_req {
+	struct dlb2_mbox_req_hdr hdr;
+	u32 domain_id;
+	u32 queue_id;
+	u32 padding;
+};
+
+struct dlb2_mbox_get_ldb_queue_depth_cmd_resp {
+	struct dlb2_mbox_resp_hdr hdr;
+	u32 error_code;
+	u32 status;
+	u32 depth;
+};
+
+struct dlb2_mbox_get_dir_queue_depth_cmd_req {
+	struct dlb2_mbox_req_hdr hdr;
+	u32 domain_id;
+	u32 queue_id;
+	u32 padding;
+};
+
+struct dlb2_mbox_get_dir_queue_depth_cmd_resp {
+	struct dlb2_mbox_resp_hdr hdr;
+	u32 error_code;
+	u32 status;
+	u32 depth;
+};
+
+struct dlb2_mbox_pending_port_unmaps_cmd_req {
+	struct dlb2_mbox_req_hdr hdr;
+	u32 domain_id;
+	u32 port_id;
+	u32 padding;
+};
+
+struct dlb2_mbox_pending_port_unmaps_cmd_resp {
+	struct dlb2_mbox_resp_hdr hdr;
+	u32 error_code;
+	u32 status;
+	u32 num;
+};
+
+struct dlb2_mbox_get_cos_bw_cmd_req {
+	struct dlb2_mbox_req_hdr hdr;
+	u32 cos_id;
+};
+
+struct dlb2_mbox_get_cos_bw_cmd_resp {
+	struct dlb2_mbox_resp_hdr hdr;
+	u32 num;
+};
+
+struct dlb2_mbox_get_sn_occupancy_cmd_req {
+	struct dlb2_mbox_req_hdr hdr;
+	u32 group_id;
+};
+
+struct dlb2_mbox_get_sn_occupancy_cmd_resp {
+	struct dlb2_mbox_resp_hdr hdr;
+	u32 num;
+};
+
+struct dlb2_mbox_query_cq_poll_mode_cmd_req {
+	struct dlb2_mbox_req_hdr hdr;
+	u32 padding;
+};
+
+struct dlb2_mbox_query_cq_poll_mode_cmd_resp {
+	struct dlb2_mbox_resp_hdr hdr;
+	u32 error_code;
+	u32 status;
+	u32 mode;
+};
+
+struct dlb2_mbox_dev_reset_cmd_req {
+	struct dlb2_mbox_req_hdr hdr;
+	u32 padding;
+};
+
+struct dlb2_mbox_dev_reset_cmd_resp {
+	struct dlb2_mbox_resp_hdr hdr;
+	u32 error_code;
+};
+
+struct dlb2_mbox_enable_cq_weight_cmd_req {
+	struct dlb2_mbox_req_hdr hdr;
+	u32 domain_id;
+	u32 port_id;
+	u32 limit;
+};
+
+struct dlb2_mbox_enable_cq_weight_cmd_resp {
+	struct dlb2_mbox_resp_hdr hdr;
+	u32 error_code;
+	u32 status;
+	u32 padding;
+};
+
+#endif /* __DLB2_BASE_DLB2_MBOX_H */
diff --git a/drivers/event/dlb2/pf/base/dlb2_osdep.h b/drivers/event/dlb2/pf/base/dlb2_osdep.h
index cffe22f..0e25b80 100644
--- a/drivers/event/dlb2/pf/base/dlb2_osdep.h
+++ b/drivers/event/dlb2/pf/base/dlb2_osdep.h
@@ -16,9 +16,7 @@
 #include <rte_log.h>
 #include <rte_spinlock.h>
 #include "../dlb2_main.h"
-
 #include "dlb2_resource.h"
-
 #include "../../dlb2_log.h"
 #include "../../dlb2_user.h"
 
@@ -91,7 +89,7 @@ static inline void *os_map_producer_port(struct dlb2_hw *hw,
 	uint64_t pp_dma_base;
 
 	pp_dma_base = (uintptr_t)hw->func_kva + DLB2_PP_BASE(is_ldb);
-	addr = (pp_dma_base + (rte_mem_page_size() * port_id));
+	addr = (pp_dma_base + (PAGE_SIZE * port_id));
 
 	return (void *)(uintptr_t)addr;
 }
@@ -131,6 +129,48 @@ static inline void os_fence_hcw(struct dlb2_hw *hw, u64 *pp_addr)
 }
 
 /**
+ * os_enqueue_four_hcws() - enqueue four HCWs to DLB
+ * @hw: dlb2_hw handle for a particular device.
+ * @hcw: pointer to the 64B-aligned contiguous HCW memory
+ * @addr: producer port address
+ */
+static inline void os_enqueue_four_hcws(struct dlb2_hw *hw,
+					struct dlb2_hcw *hcw,
+					void *addr)
+{
+	struct dlb2_dev *dlb2_dev;
+	dlb2_dev = container_of(hw, struct dlb2_dev, hw);
+
+	dlb2_dev->enqueue_four(addr, hcw);
+}
+
+/**
+ * os_notify_user_space() - notify user space
+ * @hw: dlb2_hw handle for a particular device.
+ * @domain_id: ID of domain to notify.
+ * @alert_id: alert ID.
+ * @aux_alert_data: additional alert data.
+ *
+ * This function notifies user space of an alert (such as a hardware alarm).
+ *
+ * Return:
+ * Returns 0 upon success, <0 otherwise.
+ */
+static inline int os_notify_user_space(struct dlb2_hw *hw,
+				       u32 domain_id,
+				       u64 alert_id,
+				       u64 aux_alert_data)
+{
+	RTE_SET_USED(hw);
+	RTE_SET_USED(domain_id);
+	RTE_SET_USED(alert_id);
+	RTE_SET_USED(aux_alert_data);
+
+	/* Not called for PF PMD */
+	return -1;
+}
+
+/**
  * DLB2_HW_ERR() - log an error message
  * @dlb2: dlb2_hw handle for a particular device.
  * @...: variable string args.
@@ -199,17 +239,19 @@ static inline void os_schedule_work(struct dlb2_hw *hw)
 
 	dlb2_dev = container_of(hw, struct dlb2_dev, hw);
 
+	dlb2_dev->worker_launched = true;
+
 	ret = rte_ctrl_thread_create(&complete_queue_map_unmap_thread,
 				     "dlb_queue_unmap_waiter",
 				     NULL,
 				     dlb2_complete_queue_map_unmap,
 				     dlb2_dev);
-	if (ret)
+	if (ret) {
 		DLB2_ERR(dlb2_dev,
 			 "Could not create queue complete map/unmap thread, err=%d\n",
 			 ret);
-	else
-		dlb2_dev->worker_launched = true;
+		dlb2_dev->worker_launched = false;
+	}
 }
 
 /**
diff --git a/drivers/event/dlb2/pf/base/dlb2_regs.h b/drivers/event/dlb2/pf/base/dlb2_regs.h
index 7167f3d..b699932 100644
--- a/drivers/event/dlb2/pf/base/dlb2_regs.h
+++ b/drivers/event/dlb2/pf/base/dlb2_regs.h
@@ -7,6 +7,11 @@
 
 #include "dlb2_osdep_types.h"
 
+enum dlb2_hw_ver {
+	DLB2_HW_VER_2,
+	DLB2_HW_VER_2_5,
+};
+
 #define DLB2_PF_VF2PF_MAILBOX_BYTES 256
 #define DLB2_PF_VF2PF_MAILBOX(vf_id, x) \
 	(0x1000 + 0x4 * (x) + (vf_id) * 0x10000)
@@ -752,212 +757,6 @@
 #define DLB2_SYS_DIR_CQ_ADDR_L_RSVD0_LOC	0
 #define DLB2_SYS_DIR_CQ_ADDR_L_ADDR_L_LOC	6
 
-#define DLB2_SYS_PM_SMON_COMP_MASK1 0x10003024
-#define DLB2_SYS_PM_SMON_COMP_MASK1_RST 0xffffffff
-
-#define DLB2_SYS_PM_SMON_COMP_MASK1_COMP_MASK1	0xFFFFFFFF
-#define DLB2_SYS_PM_SMON_COMP_MASK1_COMP_MASK1_LOC	0
-
-#define DLB2_SYS_PM_SMON_COMP_MASK0 0x10003020
-#define DLB2_SYS_PM_SMON_COMP_MASK0_RST 0xffffffff
-
-#define DLB2_SYS_PM_SMON_COMP_MASK0_COMP_MASK0	0xFFFFFFFF
-#define DLB2_SYS_PM_SMON_COMP_MASK0_COMP_MASK0_LOC	0
-
-#define DLB2_SYS_PM_SMON_MAX_TMR 0x1000301c
-#define DLB2_SYS_PM_SMON_MAX_TMR_RST 0x0
-
-#define DLB2_SYS_PM_SMON_MAX_TMR_MAXVALUE	0xFFFFFFFF
-#define DLB2_SYS_PM_SMON_MAX_TMR_MAXVALUE_LOC	0
-
-#define DLB2_SYS_PM_SMON_TMR 0x10003018
-#define DLB2_SYS_PM_SMON_TMR_RST 0x0
-
-#define DLB2_SYS_PM_SMON_TMR_TIMER_VAL	0xFFFFFFFF
-#define DLB2_SYS_PM_SMON_TMR_TIMER_VAL_LOC	0
-
-#define DLB2_SYS_PM_SMON_ACTIVITYCNTR1 0x10003014
-#define DLB2_SYS_PM_SMON_ACTIVITYCNTR1_RST 0x0
-
-#define DLB2_SYS_PM_SMON_ACTIVITYCNTR1_COUNTER1	0xFFFFFFFF
-#define DLB2_SYS_PM_SMON_ACTIVITYCNTR1_COUNTER1_LOC	0
-
-#define DLB2_SYS_PM_SMON_ACTIVITYCNTR0 0x10003010
-#define DLB2_SYS_PM_SMON_ACTIVITYCNTR0_RST 0x0
-
-#define DLB2_SYS_PM_SMON_ACTIVITYCNTR0_COUNTER0	0xFFFFFFFF
-#define DLB2_SYS_PM_SMON_ACTIVITYCNTR0_COUNTER0_LOC	0
-
-#define DLB2_SYS_PM_SMON_COMPARE1 0x1000300c
-#define DLB2_SYS_PM_SMON_COMPARE1_RST 0x0
-
-#define DLB2_SYS_PM_SMON_COMPARE1_COMPARE1	0xFFFFFFFF
-#define DLB2_SYS_PM_SMON_COMPARE1_COMPARE1_LOC	0
-
-#define DLB2_SYS_PM_SMON_COMPARE0 0x10003008
-#define DLB2_SYS_PM_SMON_COMPARE0_RST 0x0
-
-#define DLB2_SYS_PM_SMON_COMPARE0_COMPARE0	0xFFFFFFFF
-#define DLB2_SYS_PM_SMON_COMPARE0_COMPARE0_LOC	0
-
-#define DLB2_SYS_PM_SMON_CFG1 0x10003004
-#define DLB2_SYS_PM_SMON_CFG1_RST 0x0
-
-#define DLB2_SYS_PM_SMON_CFG1_MODE0	0x000000FF
-#define DLB2_SYS_PM_SMON_CFG1_MODE1	0x0000FF00
-#define DLB2_SYS_PM_SMON_CFG1_RSVD	0xFFFF0000
-#define DLB2_SYS_PM_SMON_CFG1_MODE0_LOC	0
-#define DLB2_SYS_PM_SMON_CFG1_MODE1_LOC	8
-#define DLB2_SYS_PM_SMON_CFG1_RSVD_LOC	16
-
-#define DLB2_SYS_PM_SMON_CFG0 0x10003000
-#define DLB2_SYS_PM_SMON_CFG0_RST 0x40000000
-
-#define DLB2_SYS_PM_SMON_CFG0_SMON_ENABLE		0x00000001
-#define DLB2_SYS_PM_SMON_CFG0_RSVD2			0x0000000E
-#define DLB2_SYS_PM_SMON_CFG0_SMON0_FUNCTION		0x00000070
-#define DLB2_SYS_PM_SMON_CFG0_SMON0_FUNCTION_COMPARE	0x00000080
-#define DLB2_SYS_PM_SMON_CFG0_SMON1_FUNCTION		0x00000700
-#define DLB2_SYS_PM_SMON_CFG0_SMON1_FUNCTION_COMPARE	0x00000800
-#define DLB2_SYS_PM_SMON_CFG0_SMON_MODE		0x0000F000
-#define DLB2_SYS_PM_SMON_CFG0_STOPCOUNTEROVFL	0x00010000
-#define DLB2_SYS_PM_SMON_CFG0_INTCOUNTEROVFL		0x00020000
-#define DLB2_SYS_PM_SMON_CFG0_STATCOUNTER0OVFL	0x00040000
-#define DLB2_SYS_PM_SMON_CFG0_STATCOUNTER1OVFL	0x00080000
-#define DLB2_SYS_PM_SMON_CFG0_STOPTIMEROVFL		0x00100000
-#define DLB2_SYS_PM_SMON_CFG0_INTTIMEROVFL		0x00200000
-#define DLB2_SYS_PM_SMON_CFG0_STATTIMEROVFL		0x00400000
-#define DLB2_SYS_PM_SMON_CFG0_RSVD1			0x00800000
-#define DLB2_SYS_PM_SMON_CFG0_TIMER_PRESCALE		0x1F000000
-#define DLB2_SYS_PM_SMON_CFG0_RSVD0			0x20000000
-#define DLB2_SYS_PM_SMON_CFG0_VERSION		0xC0000000
-#define DLB2_SYS_PM_SMON_CFG0_SMON_ENABLE_LOC		0
-#define DLB2_SYS_PM_SMON_CFG0_RSVD2_LOC			1
-#define DLB2_SYS_PM_SMON_CFG0_SMON0_FUNCTION_LOC		4
-#define DLB2_SYS_PM_SMON_CFG0_SMON0_FUNCTION_COMPARE_LOC	7
-#define DLB2_SYS_PM_SMON_CFG0_SMON1_FUNCTION_LOC		8
-#define DLB2_SYS_PM_SMON_CFG0_SMON1_FUNCTION_COMPARE_LOC	11
-#define DLB2_SYS_PM_SMON_CFG0_SMON_MODE_LOC			12
-#define DLB2_SYS_PM_SMON_CFG0_STOPCOUNTEROVFL_LOC		16
-#define DLB2_SYS_PM_SMON_CFG0_INTCOUNTEROVFL_LOC		17
-#define DLB2_SYS_PM_SMON_CFG0_STATCOUNTER0OVFL_LOC		18
-#define DLB2_SYS_PM_SMON_CFG0_STATCOUNTER1OVFL_LOC		19
-#define DLB2_SYS_PM_SMON_CFG0_STOPTIMEROVFL_LOC		20
-#define DLB2_SYS_PM_SMON_CFG0_INTTIMEROVFL_LOC		21
-#define DLB2_SYS_PM_SMON_CFG0_STATTIMEROVFL_LOC		22
-#define DLB2_SYS_PM_SMON_CFG0_RSVD1_LOC			23
-#define DLB2_SYS_PM_SMON_CFG0_TIMER_PRESCALE_LOC		24
-#define DLB2_SYS_PM_SMON_CFG0_RSVD0_LOC			29
-#define DLB2_SYS_PM_SMON_CFG0_VERSION_LOC			30
-
-#define DLB2_SYS_SMON_COMP_MASK1(x) \
-	(0x18002024 + (x) * 0x40)
-#define DLB2_SYS_SMON_COMP_MASK1_RST 0xffffffff
-
-#define DLB2_SYS_SMON_COMP_MASK1_COMP_MASK1	0xFFFFFFFF
-#define DLB2_SYS_SMON_COMP_MASK1_COMP_MASK1_LOC	0
-
-#define DLB2_SYS_SMON_COMP_MASK0(x) \
-	(0x18002020 + (x) * 0x40)
-#define DLB2_SYS_SMON_COMP_MASK0_RST 0xffffffff
-
-#define DLB2_SYS_SMON_COMP_MASK0_COMP_MASK0	0xFFFFFFFF
-#define DLB2_SYS_SMON_COMP_MASK0_COMP_MASK0_LOC	0
-
-#define DLB2_SYS_SMON_MAX_TMR(x) \
-	(0x1800201c + (x) * 0x40)
-#define DLB2_SYS_SMON_MAX_TMR_RST 0x0
-
-#define DLB2_SYS_SMON_MAX_TMR_MAXVALUE	0xFFFFFFFF
-#define DLB2_SYS_SMON_MAX_TMR_MAXVALUE_LOC	0
-
-#define DLB2_SYS_SMON_TMR(x) \
-	(0x18002018 + (x) * 0x40)
-#define DLB2_SYS_SMON_TMR_RST 0x0
-
-#define DLB2_SYS_SMON_TMR_TIMER_VAL	0xFFFFFFFF
-#define DLB2_SYS_SMON_TMR_TIMER_VAL_LOC	0
-
-#define DLB2_SYS_SMON_ACTIVITYCNTR1(x) \
-	(0x18002014 + (x) * 0x40)
-#define DLB2_SYS_SMON_ACTIVITYCNTR1_RST 0x0
-
-#define DLB2_SYS_SMON_ACTIVITYCNTR1_COUNTER1	0xFFFFFFFF
-#define DLB2_SYS_SMON_ACTIVITYCNTR1_COUNTER1_LOC	0
-
-#define DLB2_SYS_SMON_ACTIVITYCNTR0(x) \
-	(0x18002010 + (x) * 0x40)
-#define DLB2_SYS_SMON_ACTIVITYCNTR0_RST 0x0
-
-#define DLB2_SYS_SMON_ACTIVITYCNTR0_COUNTER0	0xFFFFFFFF
-#define DLB2_SYS_SMON_ACTIVITYCNTR0_COUNTER0_LOC	0
-
-#define DLB2_SYS_SMON_COMPARE1(x) \
-	(0x1800200c + (x) * 0x40)
-#define DLB2_SYS_SMON_COMPARE1_RST 0x0
-
-#define DLB2_SYS_SMON_COMPARE1_COMPARE1	0xFFFFFFFF
-#define DLB2_SYS_SMON_COMPARE1_COMPARE1_LOC	0
-
-#define DLB2_SYS_SMON_COMPARE0(x) \
-	(0x18002008 + (x) * 0x40)
-#define DLB2_SYS_SMON_COMPARE0_RST 0x0
-
-#define DLB2_SYS_SMON_COMPARE0_COMPARE0	0xFFFFFFFF
-#define DLB2_SYS_SMON_COMPARE0_COMPARE0_LOC	0
-
-#define DLB2_SYS_SMON_CFG1(x) \
-	(0x18002004 + (x) * 0x40)
-#define DLB2_SYS_SMON_CFG1_RST 0x0
-
-#define DLB2_SYS_SMON_CFG1_MODE0	0x000000FF
-#define DLB2_SYS_SMON_CFG1_MODE1	0x0000FF00
-#define DLB2_SYS_SMON_CFG1_RSVD	0xFFFF0000
-#define DLB2_SYS_SMON_CFG1_MODE0_LOC	0
-#define DLB2_SYS_SMON_CFG1_MODE1_LOC	8
-#define DLB2_SYS_SMON_CFG1_RSVD_LOC	16
-
-#define DLB2_SYS_SMON_CFG0(x) \
-	(0x18002000 + (x) * 0x40)
-#define DLB2_SYS_SMON_CFG0_RST 0x40000000
-
-#define DLB2_SYS_SMON_CFG0_SMON_ENABLE		0x00000001
-#define DLB2_SYS_SMON_CFG0_RSVD2			0x0000000E
-#define DLB2_SYS_SMON_CFG0_SMON0_FUNCTION		0x00000070
-#define DLB2_SYS_SMON_CFG0_SMON0_FUNCTION_COMPARE	0x00000080
-#define DLB2_SYS_SMON_CFG0_SMON1_FUNCTION		0x00000700
-#define DLB2_SYS_SMON_CFG0_SMON1_FUNCTION_COMPARE	0x00000800
-#define DLB2_SYS_SMON_CFG0_SMON_MODE			0x0000F000
-#define DLB2_SYS_SMON_CFG0_STOPCOUNTEROVFL		0x00010000
-#define DLB2_SYS_SMON_CFG0_INTCOUNTEROVFL		0x00020000
-#define DLB2_SYS_SMON_CFG0_STATCOUNTER0OVFL		0x00040000
-#define DLB2_SYS_SMON_CFG0_STATCOUNTER1OVFL		0x00080000
-#define DLB2_SYS_SMON_CFG0_STOPTIMEROVFL		0x00100000
-#define DLB2_SYS_SMON_CFG0_INTTIMEROVFL		0x00200000
-#define DLB2_SYS_SMON_CFG0_STATTIMEROVFL		0x00400000
-#define DLB2_SYS_SMON_CFG0_RSVD1			0x00800000
-#define DLB2_SYS_SMON_CFG0_TIMER_PRESCALE		0x1F000000
-#define DLB2_SYS_SMON_CFG0_RSVD0			0x20000000
-#define DLB2_SYS_SMON_CFG0_VERSION			0xC0000000
-#define DLB2_SYS_SMON_CFG0_SMON_ENABLE_LOC			0
-#define DLB2_SYS_SMON_CFG0_RSVD2_LOC				1
-#define DLB2_SYS_SMON_CFG0_SMON0_FUNCTION_LOC		4
-#define DLB2_SYS_SMON_CFG0_SMON0_FUNCTION_COMPARE_LOC	7
-#define DLB2_SYS_SMON_CFG0_SMON1_FUNCTION_LOC		8
-#define DLB2_SYS_SMON_CFG0_SMON1_FUNCTION_COMPARE_LOC	11
-#define DLB2_SYS_SMON_CFG0_SMON_MODE_LOC			12
-#define DLB2_SYS_SMON_CFG0_STOPCOUNTEROVFL_LOC		16
-#define DLB2_SYS_SMON_CFG0_INTCOUNTEROVFL_LOC		17
-#define DLB2_SYS_SMON_CFG0_STATCOUNTER0OVFL_LOC		18
-#define DLB2_SYS_SMON_CFG0_STATCOUNTER1OVFL_LOC		19
-#define DLB2_SYS_SMON_CFG0_STOPTIMEROVFL_LOC			20
-#define DLB2_SYS_SMON_CFG0_INTTIMEROVFL_LOC			21
-#define DLB2_SYS_SMON_CFG0_STATTIMEROVFL_LOC			22
-#define DLB2_SYS_SMON_CFG0_RSVD1_LOC				23
-#define DLB2_SYS_SMON_CFG0_TIMER_PRESCALE_LOC		24
-#define DLB2_SYS_SMON_CFG0_RSVD0_LOC				29
-#define DLB2_SYS_SMON_CFG0_VERSION_LOC			30
-
 #define DLB2_SYS_INGRESS_ALARM_ENBL 0x10000300
 #define DLB2_SYS_INGRESS_ALARM_ENBL_RST 0x0
 
@@ -1328,6 +1127,128 @@
 #define DLB2_SYS_ALARM_HW_SYND_MORE_LOC	30
 #define DLB2_SYS_ALARM_HW_SYND_VALID_LOC	31
 
+#define DLB2_SYS_CNT_4 0x10001010
+#define DLB2_SYS_CNT_4_RST 0x0
+
+#define DLB2_SYS_CNT_4_CNT   0xFFFFFFFF
+#define DLB2_SYS_CNT_4_CNT_LOC       0
+
+#define DLB2_SYS_CNT_5 0x10001014
+#define DLB2_SYS_CNT_5_RST 0x0
+
+#define DLB2_SYS_CNT_5_CNT   0xFFFFFFFF
+#define DLB2_SYS_CNT_5_CNT_LOC       0
+
+#define DLB2_SYS_WRITE_BUFFER_CTL 0x18001040
+#define DLB2_SYS_WRITE_BUFFER_CTL_RST 0x0
+
+#define DLB2_SYS_WRITE_BUFFER_CTL_RSVD3(ver) \
+	(ver == DLB2_HW_V2 ? \
+	 DLB2_SYS_WRITE_BUFFER_CTL_RSVD3_V2 : \
+	 DLB2_SYS_WRITE_BUFFER_CTL_RSVD3_V2_5)
+#define DLB2_SYS_WRITE_BUFFER_CTL_WRITE_SINGLE_BEATS(ver) \
+	(ver == DLB2_HW_V2 ? \
+	 DLB2_SYS_WRITE_BUFFER_CTL_WRITE_SINGLE_BEATS_V2 : \
+	 DLB2_SYS_WRITE_BUFFER_CTL_WRITE_SINGLE_BEATS_V2_5)
+#define DLB2_SYS_WRITE_BUFFER_CTL_HOLD_SCH_SM(ver) \
+	(ver == DLB2_HW_V2 ? \
+	 DLB2_SYS_WRITE_BUFFER_CTL_HOLD_SCH_SM_V2 : \
+	 DLB2_SYS_WRITE_BUFFER_CTL_HOLD_SCH_SM_V2_5)
+#define DLB2_SYS_WRITE_BUFFER_CTL_RSVD2(ver) \
+	(ver == DLB2_HW_V2 ? \
+	 DLB2_SYS_WRITE_BUFFER_CTL_RSVD2_V2 : \
+	 DLB2_SYS_WRITE_BUFFER_CTL_RSVD2_V2_5)
+#define DLB2_SYS_WRITE_BUFFER_CTL_ARB_SCH_MASK(ver) \
+	(ver == DLB2_HW_V2 ? \
+	 DLB2_SYS_WRITE_BUFFER_CTL_ARB_SCH_MASK_V2 : \
+	 DLB2_SYS_WRITE_BUFFER_CTL_ARB_SCH_MASK_V2_5)
+#define DLB2_SYS_WRITE_BUFFER_CTL_ARB_MSI_MASK(ver) \
+	(ver == DLB2_HW_V2 ? \
+	 DLB2_SYS_WRITE_BUFFER_CTL_ARB_MSI_MASK_V2 : \
+	 DLB2_SYS_WRITE_BUFFER_CTL_ARB_MSI_MASK_V2_5)
+#define DLB2_SYS_WRITE_BUFFER_CTL_SINGLE_STEP_SCH(ver) \
+	(ver == DLB2_HW_V2 ? \
+	 DLB2_SYS_WRITE_BUFFER_CTL_SINGLE_STEP_SCH_V2 : \
+	 DLB2_SYS_WRITE_BUFFER_CTL_SINGLE_STEP_SCH_V2_5)
+#define DLB2_SYS_WRITE_BUFFER_CTL_SINGLE_STEP_MSI(ver) \
+	(ver == DLB2_HW_V2 ? \
+	 DLB2_SYS_WRITE_BUFFER_CTL_SINGLE_STEP_MSI_V2 : \
+	 DLB2_SYS_WRITE_BUFFER_CTL_SINGLE_STEP_MSI_V2_5)
+#define DLB2_SYS_WRITE_BUFFER_CTL_ENABLE_DEBUG(ver) \
+	(ver == DLB2_HW_V2 ? \
+	 DLB2_SYS_WRITE_BUFFER_CTL_ENABLE_DEBUG_V2 : \
+	 DLB2_SYS_WRITE_BUFFER_CTL_ENABLE_DEBUG_V2_5)
+#define DLB2_SYS_WRITE_BUFFER_CTL_RSVD1(ver) \
+	(ver == DLB2_HW_V2 ? \
+	 DLB2_SYS_WRITE_BUFFER_CTL_RSVD1_V2 : \
+	 DLB2_SYS_WRITE_BUFFER_CTL_RSVD1_V2_5)
+#define DLB2_SYS_WRITE_BUFFER_CTL_SCH_RATE_LIMIT(ver) \
+	(ver == DLB2_HW_V2 ? \
+	 DLB2_SYS_WRITE_BUFFER_CTL_SCH_RATE_LIMIT_V2 : \
+	 DLB2_SYS_WRITE_BUFFER_CTL_SCH_RATE_LIMIT_V2_5)
+#define DLB2_SYS_WRITE_BUFFER_CTL_EARLY_DIR_INT(ver) \
+	(ver == DLB2_HW_V2 ? \
+	 DLB2_SYS_WRITE_BUFFER_CTL_EARLY_DIR_INT_V2 : \
+	 DLB2_SYS_WRITE_BUFFER_CTL_EARLY_DIR_INT_V2_5)
+#define DLB2_SYS_WRITE_BUFFER_CTL_RSVD0(ver) \
+	(ver == DLB2_HW_V2 ? \
+	 DLB2_SYS_WRITE_BUFFER_CTL_RSVD0_V2 : \
+	 DLB2_SYS_WRITE_BUFFER_CTL_RSVD0_V2_5)
+
+#define DLB2_SYS_WRITE_BUFFER_CTL_RSVD3_V2		0x00000001
+#define DLB2_SYS_WRITE_BUFFER_CTL_WRITE_SINGLE_BEATS_V2	0x00000002
+#define DLB2_SYS_WRITE_BUFFER_CTL_HOLD_SCH_SM_V2	0x00000004
+#define DLB2_SYS_WRITE_BUFFER_CTL_RSVD2_V2		0x00000008
+#define DLB2_SYS_WRITE_BUFFER_CTL_ARB_SCH_MASK_V2	0x00000010
+#define DLB2_SYS_WRITE_BUFFER_CTL_ARB_MSI_MASK_V2	0x00000020
+#define DLB2_SYS_WRITE_BUFFER_CTL_SINGLE_STEP_SCH_V2	0x00000040
+#define DLB2_SYS_WRITE_BUFFER_CTL_SINGLE_STEP_MSI_V2	0x00000080
+#define DLB2_SYS_WRITE_BUFFER_CTL_ENABLE_DEBUG_V2	0x00000100
+#define DLB2_SYS_WRITE_BUFFER_CTL_RSVD1_V2		0x0000FE00
+#define DLB2_SYS_WRITE_BUFFER_CTL_SCH_RATE_LIMIT_V2	0x00070000
+#define DLB2_SYS_WRITE_BUFFER_CTL_EARLY_DIR_INT_V2	0x00080000
+#define DLB2_SYS_WRITE_BUFFER_CTL_RSVD0_V2		0xFFF00000
+#define DLB2_SYS_WRITE_BUFFER_CTL_RSVD3_V2_LOC		0
+#define DLB2_SYS_WRITE_BUFFER_CTL_WRITE_SINGLE_BEATS_V2_LOC	1
+#define DLB2_SYS_WRITE_BUFFER_CTL_HOLD_SCH_SM_V2_LOC	2
+#define DLB2_SYS_WRITE_BUFFER_CTL_RSVD2_V2_LOC		3
+#define DLB2_SYS_WRITE_BUFFER_CTL_ARB_SCH_MASK_V2_LOC	4
+#define DLB2_SYS_WRITE_BUFFER_CTL_ARB_MSI_MASK_V2_LOC	5
+#define DLB2_SYS_WRITE_BUFFER_CTL_SINGLE_STEP_SCH_V2_LOC	6
+#define DLB2_SYS_WRITE_BUFFER_CTL_SINGLE_STEP_MSI_V2_LOC	7
+#define DLB2_SYS_WRITE_BUFFER_CTL_ENABLE_DEBUG_V2_LOC	8
+#define DLB2_SYS_WRITE_BUFFER_CTL_RSVD1_V2_LOC		9
+#define DLB2_SYS_WRITE_BUFFER_CTL_SCH_RATE_LIMIT_V2_LOC	16
+#define DLB2_SYS_WRITE_BUFFER_CTL_EARLY_DIR_INT_V2_LOC	19
+#define DLB2_SYS_WRITE_BUFFER_CTL_RSVD0_V2_LOC		20
+
+#define DLB2_SYS_WRITE_BUFFER_CTL_IGNORE_DIR_WB_V_V2_5	0x00000001
+#define DLB2_SYS_WRITE_BUFFER_CTL_WRITE_SINGLE_BEATS_V2_5	0x00000002
+#define DLB2_SYS_WRITE_BUFFER_CTL_HOLD_SCH_SM_V2_5	0x00000004
+#define DLB2_SYS_WRITE_BUFFER_CTL_IGNORE_LDB_WB_V_V2_5	0x00000008
+#define DLB2_SYS_WRITE_BUFFER_CTL_ARB_SCH_MASK_V2_5	0x00000010
+#define DLB2_SYS_WRITE_BUFFER_CTL_ARB_MSI_MASK_V2_5	0x00000020
+#define DLB2_SYS_WRITE_BUFFER_CTL_SINGLE_STEP_SCH_V2_5	0x00000040
+#define DLB2_SYS_WRITE_BUFFER_CTL_SINGLE_STEP_MSI_V2_5	0x00000080
+#define DLB2_SYS_WRITE_BUFFER_CTL_ENABLE_DEBUG_V2_5	0x00000100
+#define DLB2_SYS_WRITE_BUFFER_CTL_RSVD1_V2_5		0x0000FE00
+#define DLB2_SYS_WRITE_BUFFER_CTL_SCH_RATE_LIMIT_V2_5	0x00070000
+#define DLB2_SYS_WRITE_BUFFER_CTL_EARLY_DIR_INT_V2_5	0x00080000
+#define DLB2_SYS_WRITE_BUFFER_CTL_RSVD0_V2_5		0xFFF00000
+#define DLB2_SYS_WRITE_BUFFER_CTL_IGNORE_DIR_WB_V_V2_5_LOC	0
+#define DLB2_SYS_WRITE_BUFFER_CTL_WRITE_SINGLE_BEATS_V2_5_LOC	1
+#define DLB2_SYS_WRITE_BUFFER_CTL_HOLD_SCH_SM_V2_5_LOC	2
+#define DLB2_SYS_WRITE_BUFFER_CTL_IGNORE_LDB_WB_V_V2_5_LOC	3
+#define DLB2_SYS_WRITE_BUFFER_CTL_ARB_SCH_MASK_V2_5_LOC	4
+#define DLB2_SYS_WRITE_BUFFER_CTL_ARB_MSI_MASK_V2_5_LOC	5
+#define DLB2_SYS_WRITE_BUFFER_CTL_SINGLE_STEP_SCH_V2_5_LOC	6
+#define DLB2_SYS_WRITE_BUFFER_CTL_SINGLE_STEP_MSI_V2_5_LOC	7
+#define DLB2_SYS_WRITE_BUFFER_CTL_ENABLE_DEBUG_V2_5_LOC	8
+#define DLB2_SYS_WRITE_BUFFER_CTL_RSVD1_V2_5_LOC		9
+#define DLB2_SYS_WRITE_BUFFER_CTL_SCH_RATE_LIMIT_V2_5_LOC	16
+#define DLB2_SYS_WRITE_BUFFER_CTL_EARLY_DIR_INT_V2_5_LOC	19
+#define DLB2_SYS_WRITE_BUFFER_CTL_RSVD0_V2_5_LOC		20
+
 #define DLB2_AQED_QID_FID_LIM(x) \
 	(0x20000000 + (x) * 0x1000)
 #define DLB2_AQED_QID_FID_LIM_RST 0x7ff
@@ -1358,94 +1279,6 @@
 #define DLB2_AQED_CFG_ARB_WEIGHTS_TQPRI_ATM_0_PRI2_LOC	16
 #define DLB2_AQED_CFG_ARB_WEIGHTS_TQPRI_ATM_0_PRI3_LOC	24
 
-#define DLB2_AQED_SMON_ACTIVITYCNTR0 0x2c00004c
-#define DLB2_AQED_SMON_ACTIVITYCNTR0_RST 0x0
-
-#define DLB2_AQED_SMON_ACTIVITYCNTR0_COUNTER0	0xFFFFFFFF
-#define DLB2_AQED_SMON_ACTIVITYCNTR0_COUNTER0_LOC	0
-
-#define DLB2_AQED_SMON_ACTIVITYCNTR1 0x2c000050
-#define DLB2_AQED_SMON_ACTIVITYCNTR1_RST 0x0
-
-#define DLB2_AQED_SMON_ACTIVITYCNTR1_COUNTER1	0xFFFFFFFF
-#define DLB2_AQED_SMON_ACTIVITYCNTR1_COUNTER1_LOC	0
-
-#define DLB2_AQED_SMON_COMPARE0 0x2c000054
-#define DLB2_AQED_SMON_COMPARE0_RST 0x0
-
-#define DLB2_AQED_SMON_COMPARE0_COMPARE0	0xFFFFFFFF
-#define DLB2_AQED_SMON_COMPARE0_COMPARE0_LOC	0
-
-#define DLB2_AQED_SMON_COMPARE1 0x2c000058
-#define DLB2_AQED_SMON_COMPARE1_RST 0x0
-
-#define DLB2_AQED_SMON_COMPARE1_COMPARE1	0xFFFFFFFF
-#define DLB2_AQED_SMON_COMPARE1_COMPARE1_LOC	0
-
-#define DLB2_AQED_SMON_CFG0 0x2c00005c
-#define DLB2_AQED_SMON_CFG0_RST 0x40000000
-
-#define DLB2_AQED_SMON_CFG0_SMON_ENABLE		0x00000001
-#define DLB2_AQED_SMON_CFG0_SMON_0TRIGGER_ENABLE	0x00000002
-#define DLB2_AQED_SMON_CFG0_RSVZ0			0x0000000C
-#define DLB2_AQED_SMON_CFG0_SMON0_FUNCTION		0x00000070
-#define DLB2_AQED_SMON_CFG0_SMON0_FUNCTION_COMPARE	0x00000080
-#define DLB2_AQED_SMON_CFG0_SMON1_FUNCTION		0x00000700
-#define DLB2_AQED_SMON_CFG0_SMON1_FUNCTION_COMPARE	0x00000800
-#define DLB2_AQED_SMON_CFG0_SMON_MODE		0x0000F000
-#define DLB2_AQED_SMON_CFG0_STOPCOUNTEROVFL		0x00010000
-#define DLB2_AQED_SMON_CFG0_INTCOUNTEROVFL		0x00020000
-#define DLB2_AQED_SMON_CFG0_STATCOUNTER0OVFL		0x00040000
-#define DLB2_AQED_SMON_CFG0_STATCOUNTER1OVFL		0x00080000
-#define DLB2_AQED_SMON_CFG0_STOPTIMEROVFL		0x00100000
-#define DLB2_AQED_SMON_CFG0_INTTIMEROVFL		0x00200000
-#define DLB2_AQED_SMON_CFG0_STATTIMEROVFL		0x00400000
-#define DLB2_AQED_SMON_CFG0_RSVZ1			0x00800000
-#define DLB2_AQED_SMON_CFG0_TIMER_PRESCALE		0x1F000000
-#define DLB2_AQED_SMON_CFG0_RSVZ2			0x20000000
-#define DLB2_AQED_SMON_CFG0_VERSION			0xC0000000
-#define DLB2_AQED_SMON_CFG0_SMON_ENABLE_LOC			0
-#define DLB2_AQED_SMON_CFG0_SMON_0TRIGGER_ENABLE_LOC		1
-#define DLB2_AQED_SMON_CFG0_RSVZ0_LOC			2
-#define DLB2_AQED_SMON_CFG0_SMON0_FUNCTION_LOC		4
-#define DLB2_AQED_SMON_CFG0_SMON0_FUNCTION_COMPARE_LOC	7
-#define DLB2_AQED_SMON_CFG0_SMON1_FUNCTION_LOC		8
-#define DLB2_AQED_SMON_CFG0_SMON1_FUNCTION_COMPARE_LOC	11
-#define DLB2_AQED_SMON_CFG0_SMON_MODE_LOC			12
-#define DLB2_AQED_SMON_CFG0_STOPCOUNTEROVFL_LOC		16
-#define DLB2_AQED_SMON_CFG0_INTCOUNTEROVFL_LOC		17
-#define DLB2_AQED_SMON_CFG0_STATCOUNTER0OVFL_LOC		18
-#define DLB2_AQED_SMON_CFG0_STATCOUNTER1OVFL_LOC		19
-#define DLB2_AQED_SMON_CFG0_STOPTIMEROVFL_LOC		20
-#define DLB2_AQED_SMON_CFG0_INTTIMEROVFL_LOC			21
-#define DLB2_AQED_SMON_CFG0_STATTIMEROVFL_LOC		22
-#define DLB2_AQED_SMON_CFG0_RSVZ1_LOC			23
-#define DLB2_AQED_SMON_CFG0_TIMER_PRESCALE_LOC		24
-#define DLB2_AQED_SMON_CFG0_RSVZ2_LOC			29
-#define DLB2_AQED_SMON_CFG0_VERSION_LOC			30
-
-#define DLB2_AQED_SMON_CFG1 0x2c000060
-#define DLB2_AQED_SMON_CFG1_RST 0x0
-
-#define DLB2_AQED_SMON_CFG1_MODE0	0x000000FF
-#define DLB2_AQED_SMON_CFG1_MODE1	0x0000FF00
-#define DLB2_AQED_SMON_CFG1_RSVZ0	0xFFFF0000
-#define DLB2_AQED_SMON_CFG1_MODE0_LOC	0
-#define DLB2_AQED_SMON_CFG1_MODE1_LOC	8
-#define DLB2_AQED_SMON_CFG1_RSVZ0_LOC	16
-
-#define DLB2_AQED_SMON_MAX_TMR 0x2c000064
-#define DLB2_AQED_SMON_MAX_TMR_RST 0x0
-
-#define DLB2_AQED_SMON_MAX_TMR_MAXVALUE	0xFFFFFFFF
-#define DLB2_AQED_SMON_MAX_TMR_MAXVALUE_LOC	0
-
-#define DLB2_AQED_SMON_TMR 0x2c000068
-#define DLB2_AQED_SMON_TMR_RST 0x0
-
-#define DLB2_AQED_SMON_TMR_TIMER	0xFFFFFFFF
-#define DLB2_AQED_SMON_TMR_TIMER_LOC	0
-
 #define DLB2_ATM_QID2CQIDIX_00(x) \
 	(0x30080000 + (x) * 0x1000)
 #define DLB2_ATM_QID2CQIDIX_00_RST 0x0
@@ -1486,94 +1319,6 @@
 #define DLB2_ATM_CFG_ARB_WEIGHTS_SCHED_BIN_BIN2_LOC	16
 #define DLB2_ATM_CFG_ARB_WEIGHTS_SCHED_BIN_BIN3_LOC	24
 
-#define DLB2_ATM_SMON_ACTIVITYCNTR0 0x3c000050
-#define DLB2_ATM_SMON_ACTIVITYCNTR0_RST 0x0
-
-#define DLB2_ATM_SMON_ACTIVITYCNTR0_COUNTER0	0xFFFFFFFF
-#define DLB2_ATM_SMON_ACTIVITYCNTR0_COUNTER0_LOC	0
-
-#define DLB2_ATM_SMON_ACTIVITYCNTR1 0x3c000054
-#define DLB2_ATM_SMON_ACTIVITYCNTR1_RST 0x0
-
-#define DLB2_ATM_SMON_ACTIVITYCNTR1_COUNTER1	0xFFFFFFFF
-#define DLB2_ATM_SMON_ACTIVITYCNTR1_COUNTER1_LOC	0
-
-#define DLB2_ATM_SMON_COMPARE0 0x3c000058
-#define DLB2_ATM_SMON_COMPARE0_RST 0x0
-
-#define DLB2_ATM_SMON_COMPARE0_COMPARE0	0xFFFFFFFF
-#define DLB2_ATM_SMON_COMPARE0_COMPARE0_LOC	0
-
-#define DLB2_ATM_SMON_COMPARE1 0x3c00005c
-#define DLB2_ATM_SMON_COMPARE1_RST 0x0
-
-#define DLB2_ATM_SMON_COMPARE1_COMPARE1	0xFFFFFFFF
-#define DLB2_ATM_SMON_COMPARE1_COMPARE1_LOC	0
-
-#define DLB2_ATM_SMON_CFG0 0x3c000060
-#define DLB2_ATM_SMON_CFG0_RST 0x40000000
-
-#define DLB2_ATM_SMON_CFG0_SMON_ENABLE		0x00000001
-#define DLB2_ATM_SMON_CFG0_SMON_0TRIGGER_ENABLE	0x00000002
-#define DLB2_ATM_SMON_CFG0_RSVZ0			0x0000000C
-#define DLB2_ATM_SMON_CFG0_SMON0_FUNCTION		0x00000070
-#define DLB2_ATM_SMON_CFG0_SMON0_FUNCTION_COMPARE	0x00000080
-#define DLB2_ATM_SMON_CFG0_SMON1_FUNCTION		0x00000700
-#define DLB2_ATM_SMON_CFG0_SMON1_FUNCTION_COMPARE	0x00000800
-#define DLB2_ATM_SMON_CFG0_SMON_MODE			0x0000F000
-#define DLB2_ATM_SMON_CFG0_STOPCOUNTEROVFL		0x00010000
-#define DLB2_ATM_SMON_CFG0_INTCOUNTEROVFL		0x00020000
-#define DLB2_ATM_SMON_CFG0_STATCOUNTER0OVFL		0x00040000
-#define DLB2_ATM_SMON_CFG0_STATCOUNTER1OVFL		0x00080000
-#define DLB2_ATM_SMON_CFG0_STOPTIMEROVFL		0x00100000
-#define DLB2_ATM_SMON_CFG0_INTTIMEROVFL		0x00200000
-#define DLB2_ATM_SMON_CFG0_STATTIMEROVFL		0x00400000
-#define DLB2_ATM_SMON_CFG0_RSVZ1			0x00800000
-#define DLB2_ATM_SMON_CFG0_TIMER_PRESCALE		0x1F000000
-#define DLB2_ATM_SMON_CFG0_RSVZ2			0x20000000
-#define DLB2_ATM_SMON_CFG0_VERSION			0xC0000000
-#define DLB2_ATM_SMON_CFG0_SMON_ENABLE_LOC			0
-#define DLB2_ATM_SMON_CFG0_SMON_0TRIGGER_ENABLE_LOC		1
-#define DLB2_ATM_SMON_CFG0_RSVZ0_LOC				2
-#define DLB2_ATM_SMON_CFG0_SMON0_FUNCTION_LOC		4
-#define DLB2_ATM_SMON_CFG0_SMON0_FUNCTION_COMPARE_LOC	7
-#define DLB2_ATM_SMON_CFG0_SMON1_FUNCTION_LOC		8
-#define DLB2_ATM_SMON_CFG0_SMON1_FUNCTION_COMPARE_LOC	11
-#define DLB2_ATM_SMON_CFG0_SMON_MODE_LOC			12
-#define DLB2_ATM_SMON_CFG0_STOPCOUNTEROVFL_LOC		16
-#define DLB2_ATM_SMON_CFG0_INTCOUNTEROVFL_LOC		17
-#define DLB2_ATM_SMON_CFG0_STATCOUNTER0OVFL_LOC		18
-#define DLB2_ATM_SMON_CFG0_STATCOUNTER1OVFL_LOC		19
-#define DLB2_ATM_SMON_CFG0_STOPTIMEROVFL_LOC			20
-#define DLB2_ATM_SMON_CFG0_INTTIMEROVFL_LOC			21
-#define DLB2_ATM_SMON_CFG0_STATTIMEROVFL_LOC			22
-#define DLB2_ATM_SMON_CFG0_RSVZ1_LOC				23
-#define DLB2_ATM_SMON_CFG0_TIMER_PRESCALE_LOC		24
-#define DLB2_ATM_SMON_CFG0_RSVZ2_LOC				29
-#define DLB2_ATM_SMON_CFG0_VERSION_LOC			30
-
-#define DLB2_ATM_SMON_CFG1 0x3c000064
-#define DLB2_ATM_SMON_CFG1_RST 0x0
-
-#define DLB2_ATM_SMON_CFG1_MODE0	0x000000FF
-#define DLB2_ATM_SMON_CFG1_MODE1	0x0000FF00
-#define DLB2_ATM_SMON_CFG1_RSVZ0	0xFFFF0000
-#define DLB2_ATM_SMON_CFG1_MODE0_LOC	0
-#define DLB2_ATM_SMON_CFG1_MODE1_LOC	8
-#define DLB2_ATM_SMON_CFG1_RSVZ0_LOC	16
-
-#define DLB2_ATM_SMON_MAX_TMR 0x3c000068
-#define DLB2_ATM_SMON_MAX_TMR_RST 0x0
-
-#define DLB2_ATM_SMON_MAX_TMR_MAXVALUE	0xFFFFFFFF
-#define DLB2_ATM_SMON_MAX_TMR_MAXVALUE_LOC	0
-
-#define DLB2_ATM_SMON_TMR 0x3c00006c
-#define DLB2_ATM_SMON_TMR_RST 0x0
-
-#define DLB2_ATM_SMON_TMR_TIMER	0xFFFFFFFF
-#define DLB2_ATM_SMON_TMR_TIMER_LOC	0
-
 #define DLB2_CHP_CFG_DIR_VAS_CRD(x) \
 	(0x40000000 + (x) * 0x1000)
 #define DLB2_CHP_CFG_DIR_VAS_CRD_RST 0x0
@@ -2009,6 +1754,18 @@
 #define DLB2_CHP_CFG_CHP_CSR_CTRL_PAD_FIRST_WRITE_DIR_LOC		22
 #define DLB2_CHP_CFG_CHP_CSR_CTRL_RSVZ0_LOC				23
 
+#define DLB2_CHP_CFG_CNTR_CHP_ERR_DROP_L 0x4400000c
+#define DLB2_CHP_CFG_CNTR_CHP_ERR_DROP_L_RST 0x0
+
+#define DLB2_CHP_CFG_CNTR_CHP_ERR_DROP_L_COUNT       0xFFFFFFFF
+#define DLB2_CHP_CFG_CNTR_CHP_ERR_DROP_L_COUNT_LOC   0
+
+#define DLB2_CHP_CFG_CNTR_CHP_ERR_DROP_H 0x44000010
+#define DLB2_CHP_CFG_CNTR_CHP_ERR_DROP_H_RST 0x0
+
+#define DLB2_CHP_CFG_CNTR_CHP_ERR_DROP_H_COUNT       0xFFFFFFFF
+#define DLB2_CHP_CFG_CNTR_CHP_ERR_DROP_H_COUNT_LOC   0
+
 #define DLB2_V2CHP_DIR_CQ_INTR_ARMED0 0x4400005c
 #define DLB2_V2_5CHP_DIR_CQ_INTR_ARMED0 0x4400004c
 #define DLB2_CHP_DIR_CQ_INTR_ARMED0(ver) \
@@ -2227,94 +1984,6 @@
 #define DLB2_CHP_CFG_LDB_WD_THRESHOLD_WD_THRESHOLD_LOC	0
 #define DLB2_CHP_CFG_LDB_WD_THRESHOLD_RSVZ0_LOC		8
 
-#define DLB2_CHP_SMON_COMPARE0 0x4c000000
-#define DLB2_CHP_SMON_COMPARE0_RST 0x0
-
-#define DLB2_CHP_SMON_COMPARE0_COMPARE0	0xFFFFFFFF
-#define DLB2_CHP_SMON_COMPARE0_COMPARE0_LOC	0
-
-#define DLB2_CHP_SMON_COMPARE1 0x4c000004
-#define DLB2_CHP_SMON_COMPARE1_RST 0x0
-
-#define DLB2_CHP_SMON_COMPARE1_COMPARE1	0xFFFFFFFF
-#define DLB2_CHP_SMON_COMPARE1_COMPARE1_LOC	0
-
-#define DLB2_CHP_SMON_CFG0 0x4c000008
-#define DLB2_CHP_SMON_CFG0_RST 0x40000000
-
-#define DLB2_CHP_SMON_CFG0_SMON_ENABLE		0x00000001
-#define DLB2_CHP_SMON_CFG0_SMON_0TRIGGER_ENABLE	0x00000002
-#define DLB2_CHP_SMON_CFG0_RSVZ0			0x0000000C
-#define DLB2_CHP_SMON_CFG0_SMON0_FUNCTION		0x00000070
-#define DLB2_CHP_SMON_CFG0_SMON0_FUNCTION_COMPARE	0x00000080
-#define DLB2_CHP_SMON_CFG0_SMON1_FUNCTION		0x00000700
-#define DLB2_CHP_SMON_CFG0_SMON1_FUNCTION_COMPARE	0x00000800
-#define DLB2_CHP_SMON_CFG0_SMON_MODE			0x0000F000
-#define DLB2_CHP_SMON_CFG0_STOPCOUNTEROVFL		0x00010000
-#define DLB2_CHP_SMON_CFG0_INTCOUNTEROVFL		0x00020000
-#define DLB2_CHP_SMON_CFG0_STATCOUNTER0OVFL		0x00040000
-#define DLB2_CHP_SMON_CFG0_STATCOUNTER1OVFL		0x00080000
-#define DLB2_CHP_SMON_CFG0_STOPTIMEROVFL		0x00100000
-#define DLB2_CHP_SMON_CFG0_INTTIMEROVFL		0x00200000
-#define DLB2_CHP_SMON_CFG0_STATTIMEROVFL		0x00400000
-#define DLB2_CHP_SMON_CFG0_RSVZ1			0x00800000
-#define DLB2_CHP_SMON_CFG0_TIMER_PRESCALE		0x1F000000
-#define DLB2_CHP_SMON_CFG0_RSVZ2			0x20000000
-#define DLB2_CHP_SMON_CFG0_VERSION			0xC0000000
-#define DLB2_CHP_SMON_CFG0_SMON_ENABLE_LOC			0
-#define DLB2_CHP_SMON_CFG0_SMON_0TRIGGER_ENABLE_LOC		1
-#define DLB2_CHP_SMON_CFG0_RSVZ0_LOC				2
-#define DLB2_CHP_SMON_CFG0_SMON0_FUNCTION_LOC		4
-#define DLB2_CHP_SMON_CFG0_SMON0_FUNCTION_COMPARE_LOC	7
-#define DLB2_CHP_SMON_CFG0_SMON1_FUNCTION_LOC		8
-#define DLB2_CHP_SMON_CFG0_SMON1_FUNCTION_COMPARE_LOC	11
-#define DLB2_CHP_SMON_CFG0_SMON_MODE_LOC			12
-#define DLB2_CHP_SMON_CFG0_STOPCOUNTEROVFL_LOC		16
-#define DLB2_CHP_SMON_CFG0_INTCOUNTEROVFL_LOC		17
-#define DLB2_CHP_SMON_CFG0_STATCOUNTER0OVFL_LOC		18
-#define DLB2_CHP_SMON_CFG0_STATCOUNTER1OVFL_LOC		19
-#define DLB2_CHP_SMON_CFG0_STOPTIMEROVFL_LOC			20
-#define DLB2_CHP_SMON_CFG0_INTTIMEROVFL_LOC			21
-#define DLB2_CHP_SMON_CFG0_STATTIMEROVFL_LOC			22
-#define DLB2_CHP_SMON_CFG0_RSVZ1_LOC				23
-#define DLB2_CHP_SMON_CFG0_TIMER_PRESCALE_LOC		24
-#define DLB2_CHP_SMON_CFG0_RSVZ2_LOC				29
-#define DLB2_CHP_SMON_CFG0_VERSION_LOC			30
-
-#define DLB2_CHP_SMON_CFG1 0x4c00000c
-#define DLB2_CHP_SMON_CFG1_RST 0x0
-
-#define DLB2_CHP_SMON_CFG1_MODE0	0x000000FF
-#define DLB2_CHP_SMON_CFG1_MODE1	0x0000FF00
-#define DLB2_CHP_SMON_CFG1_RSVZ0	0xFFFF0000
-#define DLB2_CHP_SMON_CFG1_MODE0_LOC	0
-#define DLB2_CHP_SMON_CFG1_MODE1_LOC	8
-#define DLB2_CHP_SMON_CFG1_RSVZ0_LOC	16
-
-#define DLB2_CHP_SMON_ACTIVITYCNTR0 0x4c000010
-#define DLB2_CHP_SMON_ACTIVITYCNTR0_RST 0x0
-
-#define DLB2_CHP_SMON_ACTIVITYCNTR0_COUNTER0	0xFFFFFFFF
-#define DLB2_CHP_SMON_ACTIVITYCNTR0_COUNTER0_LOC	0
-
-#define DLB2_CHP_SMON_ACTIVITYCNTR1 0x4c000014
-#define DLB2_CHP_SMON_ACTIVITYCNTR1_RST 0x0
-
-#define DLB2_CHP_SMON_ACTIVITYCNTR1_COUNTER1	0xFFFFFFFF
-#define DLB2_CHP_SMON_ACTIVITYCNTR1_COUNTER1_LOC	0
-
-#define DLB2_CHP_SMON_MAX_TMR 0x4c000018
-#define DLB2_CHP_SMON_MAX_TMR_RST 0x0
-
-#define DLB2_CHP_SMON_MAX_TMR_MAXVALUE	0xFFFFFFFF
-#define DLB2_CHP_SMON_MAX_TMR_MAXVALUE_LOC	0
-
-#define DLB2_CHP_SMON_TMR 0x4c00001c
-#define DLB2_CHP_SMON_TMR_RST 0x0
-
-#define DLB2_CHP_SMON_TMR_TIMER	0xFFFFFFFF
-#define DLB2_CHP_SMON_TMR_TIMER_LOC	0
-
 #define DLB2_CHP_CTRL_DIAG_02 0x4c000028
 #define DLB2_CHP_CTRL_DIAG_02_RST 0x1555
 
@@ -2456,270 +2125,6 @@
 #define DLB2_DP_DIR_CSR_CTRL_INT_INF5_SYND_DIS_LOC	15
 #define DLB2_DP_DIR_CSR_CTRL_RSVZ0_LOC		16
 
-#define DLB2_DP_SMON_ACTIVITYCNTR0 0x5c000058
-#define DLB2_DP_SMON_ACTIVITYCNTR0_RST 0x0
-
-#define DLB2_DP_SMON_ACTIVITYCNTR0_COUNTER0	0xFFFFFFFF
-#define DLB2_DP_SMON_ACTIVITYCNTR0_COUNTER0_LOC	0
-
-#define DLB2_DP_SMON_ACTIVITYCNTR1 0x5c00005c
-#define DLB2_DP_SMON_ACTIVITYCNTR1_RST 0x0
-
-#define DLB2_DP_SMON_ACTIVITYCNTR1_COUNTER1	0xFFFFFFFF
-#define DLB2_DP_SMON_ACTIVITYCNTR1_COUNTER1_LOC	0
-
-#define DLB2_DP_SMON_COMPARE0 0x5c000060
-#define DLB2_DP_SMON_COMPARE0_RST 0x0
-
-#define DLB2_DP_SMON_COMPARE0_COMPARE0	0xFFFFFFFF
-#define DLB2_DP_SMON_COMPARE0_COMPARE0_LOC	0
-
-#define DLB2_DP_SMON_COMPARE1 0x5c000064
-#define DLB2_DP_SMON_COMPARE1_RST 0x0
-
-#define DLB2_DP_SMON_COMPARE1_COMPARE1	0xFFFFFFFF
-#define DLB2_DP_SMON_COMPARE1_COMPARE1_LOC	0
-
-#define DLB2_DP_SMON_CFG0 0x5c000068
-#define DLB2_DP_SMON_CFG0_RST 0x40000000
-
-#define DLB2_DP_SMON_CFG0_SMON_ENABLE		0x00000001
-#define DLB2_DP_SMON_CFG0_SMON_0TRIGGER_ENABLE	0x00000002
-#define DLB2_DP_SMON_CFG0_RSVZ0			0x0000000C
-#define DLB2_DP_SMON_CFG0_SMON0_FUNCTION		0x00000070
-#define DLB2_DP_SMON_CFG0_SMON0_FUNCTION_COMPARE	0x00000080
-#define DLB2_DP_SMON_CFG0_SMON1_FUNCTION		0x00000700
-#define DLB2_DP_SMON_CFG0_SMON1_FUNCTION_COMPARE	0x00000800
-#define DLB2_DP_SMON_CFG0_SMON_MODE			0x0000F000
-#define DLB2_DP_SMON_CFG0_STOPCOUNTEROVFL		0x00010000
-#define DLB2_DP_SMON_CFG0_INTCOUNTEROVFL		0x00020000
-#define DLB2_DP_SMON_CFG0_STATCOUNTER0OVFL		0x00040000
-#define DLB2_DP_SMON_CFG0_STATCOUNTER1OVFL		0x00080000
-#define DLB2_DP_SMON_CFG0_STOPTIMEROVFL		0x00100000
-#define DLB2_DP_SMON_CFG0_INTTIMEROVFL		0x00200000
-#define DLB2_DP_SMON_CFG0_STATTIMEROVFL		0x00400000
-#define DLB2_DP_SMON_CFG0_RSVZ1			0x00800000
-#define DLB2_DP_SMON_CFG0_TIMER_PRESCALE		0x1F000000
-#define DLB2_DP_SMON_CFG0_RSVZ2			0x20000000
-#define DLB2_DP_SMON_CFG0_VERSION			0xC0000000
-#define DLB2_DP_SMON_CFG0_SMON_ENABLE_LOC		0
-#define DLB2_DP_SMON_CFG0_SMON_0TRIGGER_ENABLE_LOC	1
-#define DLB2_DP_SMON_CFG0_RSVZ0_LOC			2
-#define DLB2_DP_SMON_CFG0_SMON0_FUNCTION_LOC		4
-#define DLB2_DP_SMON_CFG0_SMON0_FUNCTION_COMPARE_LOC	7
-#define DLB2_DP_SMON_CFG0_SMON1_FUNCTION_LOC		8
-#define DLB2_DP_SMON_CFG0_SMON1_FUNCTION_COMPARE_LOC	11
-#define DLB2_DP_SMON_CFG0_SMON_MODE_LOC		12
-#define DLB2_DP_SMON_CFG0_STOPCOUNTEROVFL_LOC	16
-#define DLB2_DP_SMON_CFG0_INTCOUNTEROVFL_LOC		17
-#define DLB2_DP_SMON_CFG0_STATCOUNTER0OVFL_LOC	18
-#define DLB2_DP_SMON_CFG0_STATCOUNTER1OVFL_LOC	19
-#define DLB2_DP_SMON_CFG0_STOPTIMEROVFL_LOC		20
-#define DLB2_DP_SMON_CFG0_INTTIMEROVFL_LOC		21
-#define DLB2_DP_SMON_CFG0_STATTIMEROVFL_LOC		22
-#define DLB2_DP_SMON_CFG0_RSVZ1_LOC			23
-#define DLB2_DP_SMON_CFG0_TIMER_PRESCALE_LOC		24
-#define DLB2_DP_SMON_CFG0_RSVZ2_LOC			29
-#define DLB2_DP_SMON_CFG0_VERSION_LOC		30
-
-#define DLB2_DP_SMON_CFG1 0x5c00006c
-#define DLB2_DP_SMON_CFG1_RST 0x0
-
-#define DLB2_DP_SMON_CFG1_MODE0	0x000000FF
-#define DLB2_DP_SMON_CFG1_MODE1	0x0000FF00
-#define DLB2_DP_SMON_CFG1_RSVZ0	0xFFFF0000
-#define DLB2_DP_SMON_CFG1_MODE0_LOC	0
-#define DLB2_DP_SMON_CFG1_MODE1_LOC	8
-#define DLB2_DP_SMON_CFG1_RSVZ0_LOC	16
-
-#define DLB2_DP_SMON_MAX_TMR 0x5c000070
-#define DLB2_DP_SMON_MAX_TMR_RST 0x0
-
-#define DLB2_DP_SMON_MAX_TMR_MAXVALUE	0xFFFFFFFF
-#define DLB2_DP_SMON_MAX_TMR_MAXVALUE_LOC	0
-
-#define DLB2_DP_SMON_TMR 0x5c000074
-#define DLB2_DP_SMON_TMR_RST 0x0
-
-#define DLB2_DP_SMON_TMR_TIMER	0xFFFFFFFF
-#define DLB2_DP_SMON_TMR_TIMER_LOC	0
-
-#define DLB2_DQED_SMON_ACTIVITYCNTR0 0x6c000024
-#define DLB2_DQED_SMON_ACTIVITYCNTR0_RST 0x0
-
-#define DLB2_DQED_SMON_ACTIVITYCNTR0_COUNTER0	0xFFFFFFFF
-#define DLB2_DQED_SMON_ACTIVITYCNTR0_COUNTER0_LOC	0
-
-#define DLB2_DQED_SMON_ACTIVITYCNTR1 0x6c000028
-#define DLB2_DQED_SMON_ACTIVITYCNTR1_RST 0x0
-
-#define DLB2_DQED_SMON_ACTIVITYCNTR1_COUNTER1	0xFFFFFFFF
-#define DLB2_DQED_SMON_ACTIVITYCNTR1_COUNTER1_LOC	0
-
-#define DLB2_DQED_SMON_COMPARE0 0x6c00002c
-#define DLB2_DQED_SMON_COMPARE0_RST 0x0
-
-#define DLB2_DQED_SMON_COMPARE0_COMPARE0	0xFFFFFFFF
-#define DLB2_DQED_SMON_COMPARE0_COMPARE0_LOC	0
-
-#define DLB2_DQED_SMON_COMPARE1 0x6c000030
-#define DLB2_DQED_SMON_COMPARE1_RST 0x0
-
-#define DLB2_DQED_SMON_COMPARE1_COMPARE1	0xFFFFFFFF
-#define DLB2_DQED_SMON_COMPARE1_COMPARE1_LOC	0
-
-#define DLB2_DQED_SMON_CFG0 0x6c000034
-#define DLB2_DQED_SMON_CFG0_RST 0x40000000
-
-#define DLB2_DQED_SMON_CFG0_SMON_ENABLE		0x00000001
-#define DLB2_DQED_SMON_CFG0_SMON_0TRIGGER_ENABLE	0x00000002
-#define DLB2_DQED_SMON_CFG0_RSVZ0			0x0000000C
-#define DLB2_DQED_SMON_CFG0_SMON0_FUNCTION		0x00000070
-#define DLB2_DQED_SMON_CFG0_SMON0_FUNCTION_COMPARE	0x00000080
-#define DLB2_DQED_SMON_CFG0_SMON1_FUNCTION		0x00000700
-#define DLB2_DQED_SMON_CFG0_SMON1_FUNCTION_COMPARE	0x00000800
-#define DLB2_DQED_SMON_CFG0_SMON_MODE		0x0000F000
-#define DLB2_DQED_SMON_CFG0_STOPCOUNTEROVFL		0x00010000
-#define DLB2_DQED_SMON_CFG0_INTCOUNTEROVFL		0x00020000
-#define DLB2_DQED_SMON_CFG0_STATCOUNTER0OVFL		0x00040000
-#define DLB2_DQED_SMON_CFG0_STATCOUNTER1OVFL		0x00080000
-#define DLB2_DQED_SMON_CFG0_STOPTIMEROVFL		0x00100000
-#define DLB2_DQED_SMON_CFG0_INTTIMEROVFL		0x00200000
-#define DLB2_DQED_SMON_CFG0_STATTIMEROVFL		0x00400000
-#define DLB2_DQED_SMON_CFG0_RSVZ1			0x00800000
-#define DLB2_DQED_SMON_CFG0_TIMER_PRESCALE		0x1F000000
-#define DLB2_DQED_SMON_CFG0_RSVZ2			0x20000000
-#define DLB2_DQED_SMON_CFG0_VERSION			0xC0000000
-#define DLB2_DQED_SMON_CFG0_SMON_ENABLE_LOC			0
-#define DLB2_DQED_SMON_CFG0_SMON_0TRIGGER_ENABLE_LOC		1
-#define DLB2_DQED_SMON_CFG0_RSVZ0_LOC			2
-#define DLB2_DQED_SMON_CFG0_SMON0_FUNCTION_LOC		4
-#define DLB2_DQED_SMON_CFG0_SMON0_FUNCTION_COMPARE_LOC	7
-#define DLB2_DQED_SMON_CFG0_SMON1_FUNCTION_LOC		8
-#define DLB2_DQED_SMON_CFG0_SMON1_FUNCTION_COMPARE_LOC	11
-#define DLB2_DQED_SMON_CFG0_SMON_MODE_LOC			12
-#define DLB2_DQED_SMON_CFG0_STOPCOUNTEROVFL_LOC		16
-#define DLB2_DQED_SMON_CFG0_INTCOUNTEROVFL_LOC		17
-#define DLB2_DQED_SMON_CFG0_STATCOUNTER0OVFL_LOC		18
-#define DLB2_DQED_SMON_CFG0_STATCOUNTER1OVFL_LOC		19
-#define DLB2_DQED_SMON_CFG0_STOPTIMEROVFL_LOC		20
-#define DLB2_DQED_SMON_CFG0_INTTIMEROVFL_LOC			21
-#define DLB2_DQED_SMON_CFG0_STATTIMEROVFL_LOC		22
-#define DLB2_DQED_SMON_CFG0_RSVZ1_LOC			23
-#define DLB2_DQED_SMON_CFG0_TIMER_PRESCALE_LOC		24
-#define DLB2_DQED_SMON_CFG0_RSVZ2_LOC			29
-#define DLB2_DQED_SMON_CFG0_VERSION_LOC			30
-
-#define DLB2_DQED_SMON_CFG1 0x6c000038
-#define DLB2_DQED_SMON_CFG1_RST 0x0
-
-#define DLB2_DQED_SMON_CFG1_MODE0	0x000000FF
-#define DLB2_DQED_SMON_CFG1_MODE1	0x0000FF00
-#define DLB2_DQED_SMON_CFG1_RSVZ0	0xFFFF0000
-#define DLB2_DQED_SMON_CFG1_MODE0_LOC	0
-#define DLB2_DQED_SMON_CFG1_MODE1_LOC	8
-#define DLB2_DQED_SMON_CFG1_RSVZ0_LOC	16
-
-#define DLB2_DQED_SMON_MAX_TMR 0x6c00003c
-#define DLB2_DQED_SMON_MAX_TMR_RST 0x0
-
-#define DLB2_DQED_SMON_MAX_TMR_MAXVALUE	0xFFFFFFFF
-#define DLB2_DQED_SMON_MAX_TMR_MAXVALUE_LOC	0
-
-#define DLB2_DQED_SMON_TMR 0x6c000040
-#define DLB2_DQED_SMON_TMR_RST 0x0
-
-#define DLB2_DQED_SMON_TMR_TIMER	0xFFFFFFFF
-#define DLB2_DQED_SMON_TMR_TIMER_LOC	0
-
-#define DLB2_QED_SMON_ACTIVITYCNTR0 0x7c000024
-#define DLB2_QED_SMON_ACTIVITYCNTR0_RST 0x0
-
-#define DLB2_QED_SMON_ACTIVITYCNTR0_COUNTER0	0xFFFFFFFF
-#define DLB2_QED_SMON_ACTIVITYCNTR0_COUNTER0_LOC	0
-
-#define DLB2_QED_SMON_ACTIVITYCNTR1 0x7c000028
-#define DLB2_QED_SMON_ACTIVITYCNTR1_RST 0x0
-
-#define DLB2_QED_SMON_ACTIVITYCNTR1_COUNTER1	0xFFFFFFFF
-#define DLB2_QED_SMON_ACTIVITYCNTR1_COUNTER1_LOC	0
-
-#define DLB2_QED_SMON_COMPARE0 0x7c00002c
-#define DLB2_QED_SMON_COMPARE0_RST 0x0
-
-#define DLB2_QED_SMON_COMPARE0_COMPARE0	0xFFFFFFFF
-#define DLB2_QED_SMON_COMPARE0_COMPARE0_LOC	0
-
-#define DLB2_QED_SMON_COMPARE1 0x7c000030
-#define DLB2_QED_SMON_COMPARE1_RST 0x0
-
-#define DLB2_QED_SMON_COMPARE1_COMPARE1	0xFFFFFFFF
-#define DLB2_QED_SMON_COMPARE1_COMPARE1_LOC	0
-
-#define DLB2_QED_SMON_CFG0 0x7c000034
-#define DLB2_QED_SMON_CFG0_RST 0x40000000
-
-#define DLB2_QED_SMON_CFG0_SMON_ENABLE		0x00000001
-#define DLB2_QED_SMON_CFG0_SMON_0TRIGGER_ENABLE	0x00000002
-#define DLB2_QED_SMON_CFG0_RSVZ0			0x0000000C
-#define DLB2_QED_SMON_CFG0_SMON0_FUNCTION		0x00000070
-#define DLB2_QED_SMON_CFG0_SMON0_FUNCTION_COMPARE	0x00000080
-#define DLB2_QED_SMON_CFG0_SMON1_FUNCTION		0x00000700
-#define DLB2_QED_SMON_CFG0_SMON1_FUNCTION_COMPARE	0x00000800
-#define DLB2_QED_SMON_CFG0_SMON_MODE			0x0000F000
-#define DLB2_QED_SMON_CFG0_STOPCOUNTEROVFL		0x00010000
-#define DLB2_QED_SMON_CFG0_INTCOUNTEROVFL		0x00020000
-#define DLB2_QED_SMON_CFG0_STATCOUNTER0OVFL		0x00040000
-#define DLB2_QED_SMON_CFG0_STATCOUNTER1OVFL		0x00080000
-#define DLB2_QED_SMON_CFG0_STOPTIMEROVFL		0x00100000
-#define DLB2_QED_SMON_CFG0_INTTIMEROVFL		0x00200000
-#define DLB2_QED_SMON_CFG0_STATTIMEROVFL		0x00400000
-#define DLB2_QED_SMON_CFG0_RSVZ1			0x00800000
-#define DLB2_QED_SMON_CFG0_TIMER_PRESCALE		0x1F000000
-#define DLB2_QED_SMON_CFG0_RSVZ2			0x20000000
-#define DLB2_QED_SMON_CFG0_VERSION			0xC0000000
-#define DLB2_QED_SMON_CFG0_SMON_ENABLE_LOC			0
-#define DLB2_QED_SMON_CFG0_SMON_0TRIGGER_ENABLE_LOC		1
-#define DLB2_QED_SMON_CFG0_RSVZ0_LOC				2
-#define DLB2_QED_SMON_CFG0_SMON0_FUNCTION_LOC		4
-#define DLB2_QED_SMON_CFG0_SMON0_FUNCTION_COMPARE_LOC	7
-#define DLB2_QED_SMON_CFG0_SMON1_FUNCTION_LOC		8
-#define DLB2_QED_SMON_CFG0_SMON1_FUNCTION_COMPARE_LOC	11
-#define DLB2_QED_SMON_CFG0_SMON_MODE_LOC			12
-#define DLB2_QED_SMON_CFG0_STOPCOUNTEROVFL_LOC		16
-#define DLB2_QED_SMON_CFG0_INTCOUNTEROVFL_LOC		17
-#define DLB2_QED_SMON_CFG0_STATCOUNTER0OVFL_LOC		18
-#define DLB2_QED_SMON_CFG0_STATCOUNTER1OVFL_LOC		19
-#define DLB2_QED_SMON_CFG0_STOPTIMEROVFL_LOC			20
-#define DLB2_QED_SMON_CFG0_INTTIMEROVFL_LOC			21
-#define DLB2_QED_SMON_CFG0_STATTIMEROVFL_LOC			22
-#define DLB2_QED_SMON_CFG0_RSVZ1_LOC				23
-#define DLB2_QED_SMON_CFG0_TIMER_PRESCALE_LOC		24
-#define DLB2_QED_SMON_CFG0_RSVZ2_LOC				29
-#define DLB2_QED_SMON_CFG0_VERSION_LOC			30
-
-#define DLB2_QED_SMON_CFG1 0x7c000038
-#define DLB2_QED_SMON_CFG1_RST 0x0
-
-#define DLB2_QED_SMON_CFG1_MODE0	0x000000FF
-#define DLB2_QED_SMON_CFG1_MODE1	0x0000FF00
-#define DLB2_QED_SMON_CFG1_RSVZ0	0xFFFF0000
-#define DLB2_QED_SMON_CFG1_MODE0_LOC	0
-#define DLB2_QED_SMON_CFG1_MODE1_LOC	8
-#define DLB2_QED_SMON_CFG1_RSVZ0_LOC	16
-
-#define DLB2_QED_SMON_MAX_TMR 0x7c00003c
-#define DLB2_QED_SMON_MAX_TMR_RST 0x0
-
-#define DLB2_QED_SMON_MAX_TMR_MAXVALUE	0xFFFFFFFF
-#define DLB2_QED_SMON_MAX_TMR_MAXVALUE_LOC	0
-
-#define DLB2_QED_SMON_TMR 0x7c000040
-#define DLB2_QED_SMON_TMR_RST 0x0
-
-#define DLB2_QED_SMON_TMR_TIMER	0xFFFFFFFF
-#define DLB2_QED_SMON_TMR_TIMER_LOC	0
-
 #define DLB2_V2NALB_CFG_ARB_WEIGHTS_TQPRI_ATQ_0 0x84000000
 #define DLB2_V2_5NALB_CFG_ARB_WEIGHTS_TQPRI_ATQ_0 0x74000000
 #define DLB2_NALB_CFG_ARB_WEIGHTS_TQPRI_ATQ_0(ver) \
@@ -2804,94 +2209,6 @@
 #define DLB2_NALB_CFG_ARB_WEIGHTS_TQPRI_REPLAY_1_RSVZ0	0xFFFFFFFF
 #define DLB2_NALB_CFG_ARB_WEIGHTS_TQPRI_REPLAY_1_RSVZ0_LOC	0
 
-#define DLB2_NALB_SMON_ACTIVITYCNTR0 0x8c000064
-#define DLB2_NALB_SMON_ACTIVITYCNTR0_RST 0x0
-
-#define DLB2_NALB_SMON_ACTIVITYCNTR0_COUNTER0	0xFFFFFFFF
-#define DLB2_NALB_SMON_ACTIVITYCNTR0_COUNTER0_LOC	0
-
-#define DLB2_NALB_SMON_ACTIVITYCNTR1 0x8c000068
-#define DLB2_NALB_SMON_ACTIVITYCNTR1_RST 0x0
-
-#define DLB2_NALB_SMON_ACTIVITYCNTR1_COUNTER1	0xFFFFFFFF
-#define DLB2_NALB_SMON_ACTIVITYCNTR1_COUNTER1_LOC	0
-
-#define DLB2_NALB_SMON_COMPARE0 0x8c00006c
-#define DLB2_NALB_SMON_COMPARE0_RST 0x0
-
-#define DLB2_NALB_SMON_COMPARE0_COMPARE0	0xFFFFFFFF
-#define DLB2_NALB_SMON_COMPARE0_COMPARE0_LOC	0
-
-#define DLB2_NALB_SMON_COMPARE1 0x8c000070
-#define DLB2_NALB_SMON_COMPARE1_RST 0x0
-
-#define DLB2_NALB_SMON_COMPARE1_COMPARE1	0xFFFFFFFF
-#define DLB2_NALB_SMON_COMPARE1_COMPARE1_LOC	0
-
-#define DLB2_NALB_SMON_CFG0 0x8c000074
-#define DLB2_NALB_SMON_CFG0_RST 0x40000000
-
-#define DLB2_NALB_SMON_CFG0_SMON_ENABLE		0x00000001
-#define DLB2_NALB_SMON_CFG0_SMON_0TRIGGER_ENABLE	0x00000002
-#define DLB2_NALB_SMON_CFG0_RSVZ0			0x0000000C
-#define DLB2_NALB_SMON_CFG0_SMON0_FUNCTION		0x00000070
-#define DLB2_NALB_SMON_CFG0_SMON0_FUNCTION_COMPARE	0x00000080
-#define DLB2_NALB_SMON_CFG0_SMON1_FUNCTION		0x00000700
-#define DLB2_NALB_SMON_CFG0_SMON1_FUNCTION_COMPARE	0x00000800
-#define DLB2_NALB_SMON_CFG0_SMON_MODE		0x0000F000
-#define DLB2_NALB_SMON_CFG0_STOPCOUNTEROVFL		0x00010000
-#define DLB2_NALB_SMON_CFG0_INTCOUNTEROVFL		0x00020000
-#define DLB2_NALB_SMON_CFG0_STATCOUNTER0OVFL		0x00040000
-#define DLB2_NALB_SMON_CFG0_STATCOUNTER1OVFL		0x00080000
-#define DLB2_NALB_SMON_CFG0_STOPTIMEROVFL		0x00100000
-#define DLB2_NALB_SMON_CFG0_INTTIMEROVFL		0x00200000
-#define DLB2_NALB_SMON_CFG0_STATTIMEROVFL		0x00400000
-#define DLB2_NALB_SMON_CFG0_RSVZ1			0x00800000
-#define DLB2_NALB_SMON_CFG0_TIMER_PRESCALE		0x1F000000
-#define DLB2_NALB_SMON_CFG0_RSVZ2			0x20000000
-#define DLB2_NALB_SMON_CFG0_VERSION			0xC0000000
-#define DLB2_NALB_SMON_CFG0_SMON_ENABLE_LOC			0
-#define DLB2_NALB_SMON_CFG0_SMON_0TRIGGER_ENABLE_LOC		1
-#define DLB2_NALB_SMON_CFG0_RSVZ0_LOC			2
-#define DLB2_NALB_SMON_CFG0_SMON0_FUNCTION_LOC		4
-#define DLB2_NALB_SMON_CFG0_SMON0_FUNCTION_COMPARE_LOC	7
-#define DLB2_NALB_SMON_CFG0_SMON1_FUNCTION_LOC		8
-#define DLB2_NALB_SMON_CFG0_SMON1_FUNCTION_COMPARE_LOC	11
-#define DLB2_NALB_SMON_CFG0_SMON_MODE_LOC			12
-#define DLB2_NALB_SMON_CFG0_STOPCOUNTEROVFL_LOC		16
-#define DLB2_NALB_SMON_CFG0_INTCOUNTEROVFL_LOC		17
-#define DLB2_NALB_SMON_CFG0_STATCOUNTER0OVFL_LOC		18
-#define DLB2_NALB_SMON_CFG0_STATCOUNTER1OVFL_LOC		19
-#define DLB2_NALB_SMON_CFG0_STOPTIMEROVFL_LOC		20
-#define DLB2_NALB_SMON_CFG0_INTTIMEROVFL_LOC			21
-#define DLB2_NALB_SMON_CFG0_STATTIMEROVFL_LOC		22
-#define DLB2_NALB_SMON_CFG0_RSVZ1_LOC			23
-#define DLB2_NALB_SMON_CFG0_TIMER_PRESCALE_LOC		24
-#define DLB2_NALB_SMON_CFG0_RSVZ2_LOC			29
-#define DLB2_NALB_SMON_CFG0_VERSION_LOC			30
-
-#define DLB2_NALB_SMON_CFG1 0x8c000078
-#define DLB2_NALB_SMON_CFG1_RST 0x0
-
-#define DLB2_NALB_SMON_CFG1_MODE0	0x000000FF
-#define DLB2_NALB_SMON_CFG1_MODE1	0x0000FF00
-#define DLB2_NALB_SMON_CFG1_RSVZ0	0xFFFF0000
-#define DLB2_NALB_SMON_CFG1_MODE0_LOC	0
-#define DLB2_NALB_SMON_CFG1_MODE1_LOC	8
-#define DLB2_NALB_SMON_CFG1_RSVZ0_LOC	16
-
-#define DLB2_NALB_SMON_MAX_TMR 0x8c00007c
-#define DLB2_NALB_SMON_MAX_TMR_RST 0x0
-
-#define DLB2_NALB_SMON_MAX_TMR_MAXVALUE	0xFFFFFFFF
-#define DLB2_NALB_SMON_MAX_TMR_MAXVALUE_LOC	0
-
-#define DLB2_NALB_SMON_TMR 0x8c000080
-#define DLB2_NALB_SMON_TMR_RST 0x0
-
-#define DLB2_NALB_SMON_TMR_TIMER	0xFFFFFFFF
-#define DLB2_NALB_SMON_TMR_TIMER_LOC	0
-
 #define DLB2_V2RO_GRP_0_SLT_SHFT(x) \
 	(0x96000000 + (x) * 0x4)
 #define DLB2_V2_5RO_GRP_0_SLT_SHFT(x) \
@@ -2954,94 +2271,6 @@
 #define DLB2_RO_CFG_CTRL_GENERAL_0_RR_EN_LOC			1
 #define DLB2_RO_CFG_CTRL_GENERAL_0_RSZV0_LOC			2
 
-#define DLB2_RO_SMON_ACTIVITYCNTR0 0x9c000030
-#define DLB2_RO_SMON_ACTIVITYCNTR0_RST 0x0
-
-#define DLB2_RO_SMON_ACTIVITYCNTR0_COUNTER0	0xFFFFFFFF
-#define DLB2_RO_SMON_ACTIVITYCNTR0_COUNTER0_LOC	0
-
-#define DLB2_RO_SMON_ACTIVITYCNTR1 0x9c000034
-#define DLB2_RO_SMON_ACTIVITYCNTR1_RST 0x0
-
-#define DLB2_RO_SMON_ACTIVITYCNTR1_COUNTER1	0xFFFFFFFF
-#define DLB2_RO_SMON_ACTIVITYCNTR1_COUNTER1_LOC	0
-
-#define DLB2_RO_SMON_COMPARE0 0x9c000038
-#define DLB2_RO_SMON_COMPARE0_RST 0x0
-
-#define DLB2_RO_SMON_COMPARE0_COMPARE0	0xFFFFFFFF
-#define DLB2_RO_SMON_COMPARE0_COMPARE0_LOC	0
-
-#define DLB2_RO_SMON_COMPARE1 0x9c00003c
-#define DLB2_RO_SMON_COMPARE1_RST 0x0
-
-#define DLB2_RO_SMON_COMPARE1_COMPARE1	0xFFFFFFFF
-#define DLB2_RO_SMON_COMPARE1_COMPARE1_LOC	0
-
-#define DLB2_RO_SMON_CFG0 0x9c000040
-#define DLB2_RO_SMON_CFG0_RST 0x40000000
-
-#define DLB2_RO_SMON_CFG0_SMON_ENABLE		0x00000001
-#define DLB2_RO_SMON_CFG0_SMON_0TRIGGER_ENABLE	0x00000002
-#define DLB2_RO_SMON_CFG0_RSVZ0			0x0000000C
-#define DLB2_RO_SMON_CFG0_SMON0_FUNCTION		0x00000070
-#define DLB2_RO_SMON_CFG0_SMON0_FUNCTION_COMPARE	0x00000080
-#define DLB2_RO_SMON_CFG0_SMON1_FUNCTION		0x00000700
-#define DLB2_RO_SMON_CFG0_SMON1_FUNCTION_COMPARE	0x00000800
-#define DLB2_RO_SMON_CFG0_SMON_MODE			0x0000F000
-#define DLB2_RO_SMON_CFG0_STOPCOUNTEROVFL		0x00010000
-#define DLB2_RO_SMON_CFG0_INTCOUNTEROVFL		0x00020000
-#define DLB2_RO_SMON_CFG0_STATCOUNTER0OVFL		0x00040000
-#define DLB2_RO_SMON_CFG0_STATCOUNTER1OVFL		0x00080000
-#define DLB2_RO_SMON_CFG0_STOPTIMEROVFL		0x00100000
-#define DLB2_RO_SMON_CFG0_INTTIMEROVFL		0x00200000
-#define DLB2_RO_SMON_CFG0_STATTIMEROVFL		0x00400000
-#define DLB2_RO_SMON_CFG0_RSVZ1			0x00800000
-#define DLB2_RO_SMON_CFG0_TIMER_PRESCALE		0x1F000000
-#define DLB2_RO_SMON_CFG0_RSVZ2			0x20000000
-#define DLB2_RO_SMON_CFG0_VERSION			0xC0000000
-#define DLB2_RO_SMON_CFG0_SMON_ENABLE_LOC		0
-#define DLB2_RO_SMON_CFG0_SMON_0TRIGGER_ENABLE_LOC	1
-#define DLB2_RO_SMON_CFG0_RSVZ0_LOC			2
-#define DLB2_RO_SMON_CFG0_SMON0_FUNCTION_LOC		4
-#define DLB2_RO_SMON_CFG0_SMON0_FUNCTION_COMPARE_LOC	7
-#define DLB2_RO_SMON_CFG0_SMON1_FUNCTION_LOC		8
-#define DLB2_RO_SMON_CFG0_SMON1_FUNCTION_COMPARE_LOC	11
-#define DLB2_RO_SMON_CFG0_SMON_MODE_LOC		12
-#define DLB2_RO_SMON_CFG0_STOPCOUNTEROVFL_LOC	16
-#define DLB2_RO_SMON_CFG0_INTCOUNTEROVFL_LOC		17
-#define DLB2_RO_SMON_CFG0_STATCOUNTER0OVFL_LOC	18
-#define DLB2_RO_SMON_CFG0_STATCOUNTER1OVFL_LOC	19
-#define DLB2_RO_SMON_CFG0_STOPTIMEROVFL_LOC		20
-#define DLB2_RO_SMON_CFG0_INTTIMEROVFL_LOC		21
-#define DLB2_RO_SMON_CFG0_STATTIMEROVFL_LOC		22
-#define DLB2_RO_SMON_CFG0_RSVZ1_LOC			23
-#define DLB2_RO_SMON_CFG0_TIMER_PRESCALE_LOC		24
-#define DLB2_RO_SMON_CFG0_RSVZ2_LOC			29
-#define DLB2_RO_SMON_CFG0_VERSION_LOC		30
-
-#define DLB2_RO_SMON_CFG1 0x9c000044
-#define DLB2_RO_SMON_CFG1_RST 0x0
-
-#define DLB2_RO_SMON_CFG1_MODE0	0x000000FF
-#define DLB2_RO_SMON_CFG1_MODE1	0x0000FF00
-#define DLB2_RO_SMON_CFG1_RSVZ0	0xFFFF0000
-#define DLB2_RO_SMON_CFG1_MODE0_LOC	0
-#define DLB2_RO_SMON_CFG1_MODE1_LOC	8
-#define DLB2_RO_SMON_CFG1_RSVZ0_LOC	16
-
-#define DLB2_RO_SMON_MAX_TMR 0x9c000048
-#define DLB2_RO_SMON_MAX_TMR_RST 0x0
-
-#define DLB2_RO_SMON_MAX_TMR_MAXVALUE	0xFFFFFFFF
-#define DLB2_RO_SMON_MAX_TMR_MAXVALUE_LOC	0
-
-#define DLB2_RO_SMON_TMR 0x9c00004c
-#define DLB2_RO_SMON_TMR_RST 0x0
-
-#define DLB2_RO_SMON_TMR_TIMER	0xFFFFFFFF
-#define DLB2_RO_SMON_TMR_TIMER_LOC	0
-
 #define DLB2_V2LSP_CQ2PRIOV(x) \
 	(0xa0000000 + (x) * 0x1000)
 #define DLB2_V2_5LSP_CQ2PRIOV(x) \
@@ -3238,6 +2467,15 @@
 #define DLB2_LSP_CQ_LDB_INFL_LIM_LIMIT_LOC	0
 #define DLB2_LSP_CQ_LDB_INFL_LIM_RSVD0_LOC	12
 
+#define DLB2_LSP_CQ_LDB_INFL_THRESH(x) \
+	(0x90580000 + (x) * 0x1000)
+#define DLB2_LSP_CQ_LDB_INFL_THRESH_RST 0x0
+
+#define DLB2_LSP_CQ_LDB_INFL_THRESH_THRESH	0x00000FFF
+#define DLB2_LSP_CQ_LDB_INFL_THRESH_RSVD0	0xFFFFF000
+#define DLB2_LSP_CQ_LDB_INFL_THRESH_THRESH_LOC	0
+#define DLB2_LSP_CQ_LDB_INFL_THRESH_RSVD0_LOC	12
+
 #define DLB2_V2LSP_CQ_LDB_TKN_CNT(x) \
 	(0xa0580000 + (x) * 0x1000)
 #define DLB2_V2_5LSP_CQ_LDB_TKN_CNT(x) \
@@ -3778,6 +3016,100 @@
 #define DLB2_LSP_CFG_SHDW_RANGE_COS_RSVZ0_LOC		9
 #define DLB2_LSP_CFG_SHDW_RANGE_COS_NO_EXTRA_CREDIT_LOC	31
 
+#define DLB2_LSP_LDB_SCHED_PERF_0_L 0xa400010c
+#define DLB2_LSP_LDB_SCHED_PERF_0_L_RST 0x0
+
+#define DLB2_LSP_LDB_SCHED_PERF_0_L_COUNT    0xFFFFFFFF
+#define DLB2_LSP_LDB_SCHED_PERF_0_L_COUNT_LOC        0
+
+#define DLB2_LSP_LDB_SCHED_PERF_0_H 0xa4000110
+#define DLB2_LSP_LDB_SCHED_PERF_0_H_RST 0x0
+
+#define DLB2_LSP_LDB_SCHED_PERF_0_H_COUNT    0xFFFFFFFF
+#define DLB2_LSP_LDB_SCHED_PERF_0_H_COUNT_LOC        0
+
+#define DLB2_LSP_LDB_SCHED_PERF_1_L 0xa4000114
+#define DLB2_LSP_LDB_SCHED_PERF_1_L_RST 0x0
+
+#define DLB2_LSP_LDB_SCHED_PERF_1_L_COUNT    0xFFFFFFFF
+#define DLB2_LSP_LDB_SCHED_PERF_1_L_COUNT_LOC        0
+
+#define DLB2_LSP_LDB_SCHED_PERF_1_H 0xa4000118
+#define DLB2_LSP_LDB_SCHED_PERF_1_H_RST 0x0
+
+#define DLB2_LSP_LDB_SCHED_PERF_1_H_COUNT    0xFFFFFFFF
+#define DLB2_LSP_LDB_SCHED_PERF_1_H_COUNT_LOC        0
+
+#define DLB2_LSP_LDB_SCHED_PERF_2_L 0xa400011c
+#define DLB2_LSP_LDB_SCHED_PERF_2_L_RST 0x0
+
+#define DLB2_LSP_LDB_SCHED_PERF_2_L_COUNT    0xFFFFFFFF
+#define DLB2_LSP_LDB_SCHED_PERF_2_L_COUNT_LOC        0
+
+#define DLB2_LSP_LDB_SCHED_PERF_2_H 0xa4000120
+#define DLB2_LSP_LDB_SCHED_PERF_2_H_RST 0x0
+
+#define DLB2_LSP_LDB_SCHED_PERF_2_H_COUNT    0xFFFFFFFF
+#define DLB2_LSP_LDB_SCHED_PERF_2_H_COUNT_LOC        0
+
+#define DLB2_LSP_LDB_SCHED_PERF_3_L 0xa4000124
+#define DLB2_LSP_LDB_SCHED_PERF_3_L_RST 0x0
+
+#define DLB2_LSP_LDB_SCHED_PERF_3_L_COUNT    0xFFFFFFFF
+#define DLB2_LSP_LDB_SCHED_PERF_3_L_COUNT_LOC        0
+
+#define DLB2_LSP_LDB_SCHED_PERF_3_H 0xa4000128
+#define DLB2_LSP_LDB_SCHED_PERF_3_H_RST 0x0
+
+#define DLB2_LSP_LDB_SCHED_PERF_3_H_COUNT    0xFFFFFFFF
+#define DLB2_LSP_LDB_SCHED_PERF_3_H_COUNT_LOC        0
+
+#define DLB2_LSP_LDB_SCHED_PERF_4_L 0xa400012c
+#define DLB2_LSP_LDB_SCHED_PERF_4_L_RST 0x0
+
+#define DLB2_LSP_LDB_SCHED_PERF_4_L_COUNT    0xFFFFFFFF
+#define DLB2_LSP_LDB_SCHED_PERF_4_L_COUNT_LOC        0
+
+#define DLB2_LSP_LDB_SCHED_PERF_4_H 0xa4000130
+#define DLB2_LSP_LDB_SCHED_PERF_4_H_RST 0x0
+
+#define DLB2_LSP_LDB_SCHED_PERF_4_H_COUNT    0xFFFFFFFF
+#define DLB2_LSP_LDB_SCHED_PERF_4_H_COUNT_LOC        0
+
+#define DLB2_LSP_LDB_SCHED_PERF_5_L 0xa4000134
+#define DLB2_LSP_LDB_SCHED_PERF_5_L_RST 0x0
+
+#define DLB2_LSP_LDB_SCHED_PERF_5_L_COUNT    0xFFFFFFFF
+#define DLB2_LSP_LDB_SCHED_PERF_5_L_COUNT_LOC        0
+
+#define DLB2_LSP_LDB_SCHED_PERF_5_H 0xa4000138
+#define DLB2_LSP_LDB_SCHED_PERF_5_H_RST 0x0
+
+#define DLB2_LSP_LDB_SCHED_PERF_5_H_COUNT    0xFFFFFFFF
+#define DLB2_LSP_LDB_SCHED_PERF_5_H_COUNT_LOC        0
+
+#define DLB2_LSP_LDB_SCHED_PERF_6_L 0xa400013c
+#define DLB2_LSP_LDB_SCHED_PERF_6_L_RST 0x0
+
+#define DLB2_LSP_LDB_SCHED_PERF_6_L_COUNT    0xFFFFFFFF
+#define DLB2_LSP_LDB_SCHED_PERF_6_L_COUNT_LOC        0
+
+#define DLB2_LSP_LDB_SCHED_PERF_6_H 0xa4000140
+#define DLB2_LSP_LDB_SCHED_PERF_6_H_RST 0x0
+
+#define DLB2_LSP_LDB_SCHED_PERF_6_H_COUNT    0xFFFFFFFF
+#define DLB2_LSP_LDB_SCHED_PERF_6_H_COUNT_LOC        0
+
+#define DLB2_LSP_LDB_SCHED_PERF_CTRL 0xa4000144
+#define DLB2_LSP_LDB_SCHED_PERF_CTRL_RST 0x0
+
+#define DLB2_LSP_LDB_SCHED_PERF_CTRL_ENAB    0x00000001
+#define DLB2_LSP_LDB_SCHED_PERF_CTRL_CLR     0x00000002
+#define DLB2_LSP_LDB_SCHED_PERF_CTRL_RSVZ0   0xFFFFFFFC
+#define DLB2_LSP_LDB_SCHED_PERF_CTRL_ENAB_LOC        0
+#define DLB2_LSP_LDB_SCHED_PERF_CTRL_CLR_LOC 1
+#define DLB2_LSP_LDB_SCHED_PERF_CTRL_RSVZ0_LOC       2
+
 #define DLB2_V2LSP_CFG_CTRL_GENERAL_0 0xac000000
 #define DLB2_V2_5LSP_CFG_CTRL_GENERAL_0 0x9c000000
 #define DLB2_LSP_CFG_CTRL_GENERAL_0(ver) \
@@ -3812,9 +3144,6 @@
 #define DLB2_LSP_CFG_CTRL_GENERAL_0_ATM_SINGLE_CMP_V2	0x01000000
 #define DLB2_LSP_CFG_CTRL_GENERAL_0_LDB_CE_TOG_ARB_V2	0x02000000
 #define DLB2_LSP_CFG_CTRL_GENERAL_0_RSVZ1_V2		0x04000000
-#define DLB2_LSP_CFG_CTRL_GENERAL_0_SMON0_VALID_SEL_V2	0x18000000
-#define DLB2_LSP_CFG_CTRL_GENERAL_0_SMON0_VALUE_SEL_V2	0x20000000
-#define DLB2_LSP_CFG_CTRL_GENERAL_0_SMON0_COMPARE_SEL_V2	0xC0000000
 #define DLB2_LSP_CFG_CTRL_GENERAL_0_DISAB_ATQ_EMPTY_ARB_V2_LOC	0
 #define DLB2_LSP_CFG_CTRL_GENERAL_0_INC_TOK_UNIT_IDLE_V2_LOC		1
 #define DLB2_LSP_CFG_CTRL_GENERAL_0_DISAB_RLIST_PRI_V2_LOC		2
@@ -3841,9 +3170,6 @@
 #define DLB2_LSP_CFG_CTRL_GENERAL_0_ATM_SINGLE_CMP_V2_LOC		24
 #define DLB2_LSP_CFG_CTRL_GENERAL_0_LDB_CE_TOG_ARB_V2_LOC		25
 #define DLB2_LSP_CFG_CTRL_GENERAL_0_RSVZ1_V2_LOC			26
-#define DLB2_LSP_CFG_CTRL_GENERAL_0_SMON0_VALID_SEL_V2_LOC		27
-#define DLB2_LSP_CFG_CTRL_GENERAL_0_SMON0_VALUE_SEL_V2_LOC		29
-#define DLB2_LSP_CFG_CTRL_GENERAL_0_SMON0_COMPARE_SEL_V2_LOC		30
 
 #define DLB2_LSP_CFG_CTRL_GENERAL_0_DISAB_ATQ_EMPTY_ARB_V2_5	0x00000001
 #define DLB2_LSP_CFG_CTRL_GENERAL_0_INC_TOK_UNIT_IDLE_V2_5	0x00000002
@@ -3872,9 +3198,6 @@
 #define DLB2_LSP_CFG_CTRL_GENERAL_0_ATM_SINGLE_CMP_V2_5	0x01000000
 #define DLB2_LSP_CFG_CTRL_GENERAL_0_LDB_CE_TOG_ARB_V2_5	0x02000000
 #define DLB2_LSP_CFG_CTRL_GENERAL_0_RSVZ1_V2_5		0x04000000
-#define DLB2_LSP_CFG_CTRL_GENERAL_0_SMON0_VALID_SEL_V2_5	0x18000000
-#define DLB2_LSP_CFG_CTRL_GENERAL_0_SMON0_VALUE_SEL_V2_5	0x20000000
-#define DLB2_LSP_CFG_CTRL_GENERAL_0_SMON0_COMPARE_SEL_V2_5	0xC0000000
 #define DLB2_LSP_CFG_CTRL_GENERAL_0_DISAB_ATQ_EMPTY_ARB_V2_5_LOC	0
 #define DLB2_LSP_CFG_CTRL_GENERAL_0_INC_TOK_UNIT_IDLE_V2_5_LOC	1
 #define DLB2_LSP_CFG_CTRL_GENERAL_0_DISAB_RLIST_PRI_V2_5_LOC		2
@@ -3902,97 +3225,6 @@
 #define DLB2_LSP_CFG_CTRL_GENERAL_0_ATM_SINGLE_CMP_V2_5_LOC		24
 #define DLB2_LSP_CFG_CTRL_GENERAL_0_LDB_CE_TOG_ARB_V2_5_LOC		25
 #define DLB2_LSP_CFG_CTRL_GENERAL_0_RSVZ1_V2_5_LOC			26
-#define DLB2_LSP_CFG_CTRL_GENERAL_0_SMON0_VALID_SEL_V2_5_LOC		27
-#define DLB2_LSP_CFG_CTRL_GENERAL_0_SMON0_VALUE_SEL_V2_5_LOC		29
-#define DLB2_LSP_CFG_CTRL_GENERAL_0_SMON0_COMPARE_SEL_V2_5_LOC	30
-
-#define DLB2_LSP_SMON_COMPARE0 0xac000048
-#define DLB2_LSP_SMON_COMPARE0_RST 0x0
-
-#define DLB2_LSP_SMON_COMPARE0_COMPARE0	0xFFFFFFFF
-#define DLB2_LSP_SMON_COMPARE0_COMPARE0_LOC	0
-
-#define DLB2_LSP_SMON_COMPARE1 0xac00004c
-#define DLB2_LSP_SMON_COMPARE1_RST 0x0
-
-#define DLB2_LSP_SMON_COMPARE1_COMPARE1	0xFFFFFFFF
-#define DLB2_LSP_SMON_COMPARE1_COMPARE1_LOC	0
-
-#define DLB2_LSP_SMON_CFG0 0xac000050
-#define DLB2_LSP_SMON_CFG0_RST 0x40000000
-
-#define DLB2_LSP_SMON_CFG0_SMON_ENABLE		0x00000001
-#define DLB2_LSP_SMON_CFG0_SMON_0TRIGGER_ENABLE	0x00000002
-#define DLB2_LSP_SMON_CFG0_RSVZ0			0x0000000C
-#define DLB2_LSP_SMON_CFG0_SMON0_FUNCTION		0x00000070
-#define DLB2_LSP_SMON_CFG0_SMON0_FUNCTION_COMPARE	0x00000080
-#define DLB2_LSP_SMON_CFG0_SMON1_FUNCTION		0x00000700
-#define DLB2_LSP_SMON_CFG0_SMON1_FUNCTION_COMPARE	0x00000800
-#define DLB2_LSP_SMON_CFG0_SMON_MODE			0x0000F000
-#define DLB2_LSP_SMON_CFG0_STOPCOUNTEROVFL		0x00010000
-#define DLB2_LSP_SMON_CFG0_INTCOUNTEROVFL		0x00020000
-#define DLB2_LSP_SMON_CFG0_STATCOUNTER0OVFL		0x00040000
-#define DLB2_LSP_SMON_CFG0_STATCOUNTER1OVFL		0x00080000
-#define DLB2_LSP_SMON_CFG0_STOPTIMEROVFL		0x00100000
-#define DLB2_LSP_SMON_CFG0_INTTIMEROVFL		0x00200000
-#define DLB2_LSP_SMON_CFG0_STATTIMEROVFL		0x00400000
-#define DLB2_LSP_SMON_CFG0_RSVZ1			0x00800000
-#define DLB2_LSP_SMON_CFG0_TIMER_PRESCALE		0x1F000000
-#define DLB2_LSP_SMON_CFG0_RSVZ2			0x20000000
-#define DLB2_LSP_SMON_CFG0_VERSION			0xC0000000
-#define DLB2_LSP_SMON_CFG0_SMON_ENABLE_LOC			0
-#define DLB2_LSP_SMON_CFG0_SMON_0TRIGGER_ENABLE_LOC		1
-#define DLB2_LSP_SMON_CFG0_RSVZ0_LOC				2
-#define DLB2_LSP_SMON_CFG0_SMON0_FUNCTION_LOC		4
-#define DLB2_LSP_SMON_CFG0_SMON0_FUNCTION_COMPARE_LOC	7
-#define DLB2_LSP_SMON_CFG0_SMON1_FUNCTION_LOC		8
-#define DLB2_LSP_SMON_CFG0_SMON1_FUNCTION_COMPARE_LOC	11
-#define DLB2_LSP_SMON_CFG0_SMON_MODE_LOC			12
-#define DLB2_LSP_SMON_CFG0_STOPCOUNTEROVFL_LOC		16
-#define DLB2_LSP_SMON_CFG0_INTCOUNTEROVFL_LOC		17
-#define DLB2_LSP_SMON_CFG0_STATCOUNTER0OVFL_LOC		18
-#define DLB2_LSP_SMON_CFG0_STATCOUNTER1OVFL_LOC		19
-#define DLB2_LSP_SMON_CFG0_STOPTIMEROVFL_LOC			20
-#define DLB2_LSP_SMON_CFG0_INTTIMEROVFL_LOC			21
-#define DLB2_LSP_SMON_CFG0_STATTIMEROVFL_LOC			22
-#define DLB2_LSP_SMON_CFG0_RSVZ1_LOC				23
-#define DLB2_LSP_SMON_CFG0_TIMER_PRESCALE_LOC		24
-#define DLB2_LSP_SMON_CFG0_RSVZ2_LOC				29
-#define DLB2_LSP_SMON_CFG0_VERSION_LOC			30
-
-#define DLB2_LSP_SMON_CFG1 0xac000054
-#define DLB2_LSP_SMON_CFG1_RST 0x0
-
-#define DLB2_LSP_SMON_CFG1_MODE0	0x000000FF
-#define DLB2_LSP_SMON_CFG1_MODE1	0x0000FF00
-#define DLB2_LSP_SMON_CFG1_RSVZ0	0xFFFF0000
-#define DLB2_LSP_SMON_CFG1_MODE0_LOC	0
-#define DLB2_LSP_SMON_CFG1_MODE1_LOC	8
-#define DLB2_LSP_SMON_CFG1_RSVZ0_LOC	16
-
-#define DLB2_LSP_SMON_ACTIVITYCNTR0 0xac000058
-#define DLB2_LSP_SMON_ACTIVITYCNTR0_RST 0x0
-
-#define DLB2_LSP_SMON_ACTIVITYCNTR0_COUNTER0	0xFFFFFFFF
-#define DLB2_LSP_SMON_ACTIVITYCNTR0_COUNTER0_LOC	0
-
-#define DLB2_LSP_SMON_ACTIVITYCNTR1 0xac00005c
-#define DLB2_LSP_SMON_ACTIVITYCNTR1_RST 0x0
-
-#define DLB2_LSP_SMON_ACTIVITYCNTR1_COUNTER1	0xFFFFFFFF
-#define DLB2_LSP_SMON_ACTIVITYCNTR1_COUNTER1_LOC	0
-
-#define DLB2_LSP_SMON_MAX_TMR 0xac000060
-#define DLB2_LSP_SMON_MAX_TMR_RST 0x0
-
-#define DLB2_LSP_SMON_MAX_TMR_MAXVALUE	0xFFFFFFFF
-#define DLB2_LSP_SMON_MAX_TMR_MAXVALUE_LOC	0
-
-#define DLB2_LSP_SMON_TMR 0xac000064
-#define DLB2_LSP_SMON_TMR_RST 0x0
-
-#define DLB2_LSP_SMON_TMR_TIMER	0xFFFFFFFF
-#define DLB2_LSP_SMON_TMR_TIMER_LOC	0
 
 #define DLB2_V2CM_DIAG_RESET_STS 0xb4000000
 #define DLB2_V2_5CM_DIAG_RESET_STS 0xa4000000
@@ -4156,6 +3388,30 @@
 #define DLB2_CM_CFG_PM_PMCSR_DISABLE_DISABLE_LOC	0
 #define DLB2_CM_CFG_PM_PMCSR_DISABLE_RSVZ0_LOC	1
 
+#define DLB2_CM_CLK_ON_CNT_L 0xb400001c
+#define DLB2_CM_CLK_ON_CNT_L_RST 0x0
+
+#define DLB2_CM_CLK_ON_CNT_L_COUNT   0xFFFFFFFF
+#define DLB2_CM_CLK_ON_CNT_L_COUNT_LOC       0
+
+#define DLB2_CM_CLK_ON_CNT_H 0xb4000020
+#define DLB2_CM_CLK_ON_CNT_H_RST 0x0
+
+#define DLB2_CM_CLK_ON_CNT_H_COUNT   0xFFFFFFFF
+#define DLB2_CM_CLK_ON_CNT_H_COUNT_LOC       0
+
+#define DLB2_CM_PROC_ON_CNT_L 0xb4000024
+#define DLB2_CM_PROC_ON_CNT_L_RST 0x0
+
+#define DLB2_CM_PROC_ON_CNT_L_COUNT  0xFFFFFFFF
+#define DLB2_CM_PROC_ON_CNT_L_COUNT_LOC      0
+
+#define DLB2_CM_PROC_ON_CNT_H 0xb4000028
+#define DLB2_CM_PROC_ON_CNT_H_RST 0x0
+
+#define DLB2_CM_PROC_ON_CNT_H_COUNT  0xFFFFFFFF
+#define DLB2_CM_PROC_ON_CNT_H_COUNT_LOC      0
+
 #define DLB2_VF_VF2PF_MAILBOX_BYTES 256
 #define DLB2_VF_VF2PF_MAILBOX(x) \
 	(0x1000 + (x) * 0x4)
diff --git a/drivers/event/dlb2/pf/base/dlb2_resource.c b/drivers/event/dlb2/pf/base/dlb2_resource.c
index dd8390a..e8a8680 100644
--- a/drivers/event/dlb2/pf/base/dlb2_resource.c
+++ b/drivers/event/dlb2/pf/base/dlb2_resource.c
@@ -5,12 +5,12 @@
 #include "dlb2_user.h"
 
 #include "dlb2_hw_types.h"
+#include "dlb2_mbox.h"
 #include "dlb2_osdep.h"
 #include "dlb2_osdep_bitmap.h"
 #include "dlb2_osdep_types.h"
 #include "dlb2_regs.h"
 #include "dlb2_resource.h"
-
 #include "../../dlb2_priv.h"
 #include "../../dlb2_inline_fns.h"
 
@@ -32,6 +32,12 @@
 #define DLB2_FUNC_LIST_FOR_SAFE(head, ptr, ptr_tmp, it, it_tmp) \
 	DLB2_LIST_FOR_EACH_SAFE((head), ptr, ptr_tmp, func_list, it, it_tmp)
 
+static int dlb2_domain_drain_ldb_cqs(struct dlb2_hw *hw,
+				      struct dlb2_hw_domain *domain,
+				      bool toggle_port);
+static int dlb2_domain_drain_dir_cqs(struct dlb2_hw *hw,
+				     struct dlb2_hw_domain *domain,
+				     bool toggle_port);
 /*
  * The PF driver cannot assume that a register write will affect subsequent HCW
  * writes. To ensure a write completes, the driver must read back a CSR. This
@@ -43,6 +49,18 @@ static inline void dlb2_flush_csr(struct dlb2_hw *hw)
 	DLB2_CSR_RD(hw, DLB2_SYS_TOTAL_VAS(hw->ver));
 }
 
+static void dlb2_init_fn_rsrc_lists(struct dlb2_function_resources *rsrc)
+{
+	int i;
+	dlb2_list_init_head(&rsrc->avail_domains);
+	dlb2_list_init_head(&rsrc->used_domains);
+	dlb2_list_init_head(&rsrc->avail_ldb_queues);
+	dlb2_list_init_head(&rsrc->avail_dir_pq_pairs);
+
+	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++)
+		dlb2_list_init_head(&rsrc->avail_ldb_ports[i]);
+}
+
 static void dlb2_init_domain_rsrc_lists(struct dlb2_hw_domain *domain)
 {
 	int i;
@@ -59,18 +77,6 @@ static void dlb2_init_domain_rsrc_lists(struct dlb2_hw_domain *domain)
 		dlb2_list_init_head(&domain->avail_ldb_ports[i]);
 }
 
-static void dlb2_init_fn_rsrc_lists(struct dlb2_function_resources *rsrc)
-{
-	int i;
-	dlb2_list_init_head(&rsrc->avail_domains);
-	dlb2_list_init_head(&rsrc->used_domains);
-	dlb2_list_init_head(&rsrc->avail_ldb_queues);
-	dlb2_list_init_head(&rsrc->avail_dir_pq_pairs);
-
-	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++)
-		dlb2_list_init_head(&rsrc->avail_ldb_ports[i]);
-}
-
 /**
  * dlb2_resource_free() - free device state memory
  * @hw: dlb2_hw handle for a particular device.
@@ -173,6 +179,7 @@ int dlb2_resource_init(struct dlb2_hw *hw, enum dlb2_hw_ver ver, const void *pro
 		else
 			port = &hw->rsrcs.ldb_ports[hw->ldb_pp_allocations[i]];
 
+
 		dlb2_list_add(&hw->pf.avail_ldb_ports[cos_id],
 			      &port->func_list);
 	}
@@ -252,3260 +259,7086 @@ int dlb2_resource_init(struct dlb2_hw *hw, enum dlb2_hw_ver ver, const void *pro
 	return ret;
 }
 
-/**
- * dlb2_clr_pmcsr_disable() - power on bulk of DLB 2.0 logic
- * @hw: dlb2_hw handle for a particular device.
- * @ver: device version.
- *
- * Clearing the PMCSR must be done at initialization to make the device fully
- * operational.
- */
-void dlb2_clr_pmcsr_disable(struct dlb2_hw *hw, enum dlb2_hw_ver ver)
+static struct dlb2_hw_domain *dlb2_get_domain_from_id(struct dlb2_hw *hw,
+						      u32 id,
+						      bool vdev_req,
+						      unsigned int vdev_id)
 {
-	u32 pmcsr_dis;
+	struct dlb2_list_entry *iteration __attribute__((unused));
+	struct dlb2_function_resources *rsrcs;
+	struct dlb2_hw_domain *domain;
 
-	pmcsr_dis = DLB2_CSR_RD(hw, DLB2_CM_CFG_PM_PMCSR_DISABLE(ver));
+	if (id >= DLB2_MAX_NUM_DOMAINS)
+		return NULL;
 
-	DLB2_BITS_CLR(pmcsr_dis, DLB2_CM_CFG_PM_PMCSR_DISABLE_DISABLE);
+	if (!vdev_req)
+		return &hw->domains[id];
 
-	DLB2_CSR_WR(hw, DLB2_CM_CFG_PM_PMCSR_DISABLE(ver), pmcsr_dis);
+	rsrcs = &hw->vdev[vdev_id];
+
+	DLB2_FUNC_LIST_FOR(rsrcs->used_domains, domain, iteration) {
+		if (domain->id.virt_id == id)
+			return domain;
+	}
+
+	return NULL;
 }
 
-/**
- * dlb2_hw_get_num_resources() - query the PCI function's available resources
- * @hw: dlb2_hw handle for a particular device.
- * @arg: pointer to resource counts.
- * @vdev_req: indicates whether this request came from a vdev.
- * @vdev_id: If vdev_req is true, this contains the vdev's ID.
- *
- * This function returns the number of available resources for the PF or for a
- * VF.
- *
- * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
- * device.
- *
- * Return:
- * Returns 0 upon success, -EINVAL if vdev_req is true and vdev_id is
- * invalid.
- */
-int dlb2_hw_get_num_resources(struct dlb2_hw *hw,
-			      struct dlb2_get_num_resources_args *arg,
-			      bool vdev_req,
-			      unsigned int vdev_id)
+static struct dlb2_ldb_port *dlb2_get_ldb_port_from_id(struct dlb2_hw *hw,
+						       u32 id,
+						       bool vdev_req,
+						       unsigned int vdev_id)
 {
+	struct dlb2_list_entry *iter1 __attribute__((unused));
+	struct dlb2_list_entry *iter2 __attribute__((unused));
 	struct dlb2_function_resources *rsrcs;
-	struct dlb2_bitmap *map;
+	struct dlb2_hw_domain *domain;
+	struct dlb2_ldb_port *port;
 	int i;
 
-	if (vdev_req && vdev_id >= DLB2_MAX_NUM_VDEVS)
-		return -EINVAL;
-
-	if (vdev_req)
-		rsrcs = &hw->vdev[vdev_id];
-	else
-		rsrcs = &hw->pf;
-
-	arg->num_sched_domains = rsrcs->num_avail_domains;
+	if (id >= DLB2_MAX_NUM_LDB_PORTS)
+		return NULL;
 
-	arg->num_ldb_queues = rsrcs->num_avail_ldb_queues;
+	rsrcs = (vdev_req) ? &hw->vdev[vdev_id] : &hw->pf;
 
-	arg->num_ldb_ports = 0;
-	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++)
-		arg->num_ldb_ports += rsrcs->num_avail_ldb_ports[i];
+	if (!vdev_req)
+		return &hw->rsrcs.ldb_ports[id];
 
-	arg->num_cos_ldb_ports[0] = rsrcs->num_avail_ldb_ports[0];
-	arg->num_cos_ldb_ports[1] = rsrcs->num_avail_ldb_ports[1];
-	arg->num_cos_ldb_ports[2] = rsrcs->num_avail_ldb_ports[2];
-	arg->num_cos_ldb_ports[3] = rsrcs->num_avail_ldb_ports[3];
+	DLB2_FUNC_LIST_FOR(rsrcs->used_domains, domain, iter1) {
+		for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
+			DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i],
+					  port,
+					  iter2) {
+				if (port->id.virt_id == id)
+					return port;
+			}
+		}
+	}
 
-	arg->num_dir_ports = rsrcs->num_avail_dir_pq_pairs;
+	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
+		DLB2_FUNC_LIST_FOR(rsrcs->avail_ldb_ports[i], port, iter1) {
+			if (port->id.virt_id == id)
+				return port;
+		}
+	}
 
-	arg->num_atomic_inflights = rsrcs->num_avail_aqed_entries;
+	return NULL;
+}
 
-	map = rsrcs->avail_hist_list_entries;
+static struct dlb2_ldb_port *
+dlb2_get_domain_used_ldb_port(u32 id,
+			      bool vdev_req,
+			      struct dlb2_hw_domain *domain)
+{
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_ldb_port *port;
+	int i;
 
-	arg->num_hist_list_entries = dlb2_bitmap_count(map);
+	if (id >= DLB2_MAX_NUM_LDB_PORTS)
+		return NULL;
 
-	arg->max_contiguous_hist_list_entries =
-		dlb2_bitmap_longest_set_range(map);
+	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
+		DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i], port, iter) {
+			if ((!vdev_req && port->id.phys_id == id) ||
+			    (vdev_req && port->id.virt_id == id))
+				return port;
+		}
 
-	if (hw->ver == DLB2_HW_V2) {
-		arg->num_ldb_credits = rsrcs->num_avail_qed_entries;
-		arg->num_dir_credits = rsrcs->num_avail_dqed_entries;
-	} else {
-		arg->num_credits = rsrcs->num_avail_entries;
+		DLB2_DOM_LIST_FOR(domain->avail_ldb_ports[i], port, iter) {
+			if ((!vdev_req && port->id.phys_id == id) ||
+			    (vdev_req && port->id.virt_id == id))
+				return port;
+		}
 	}
-	return 0;
-}
-
-static void dlb2_configure_domain_credits_v2_5(struct dlb2_hw *hw,
-					       struct dlb2_hw_domain *domain)
-{
-	u32 reg = 0;
 
-	DLB2_BITS_SET(reg, domain->num_credits, DLB2_CHP_CFG_LDB_VAS_CRD_COUNT);
-	DLB2_CSR_WR(hw, DLB2_CHP_CFG_VAS_CRD(domain->id.phys_id), reg);
+	return NULL;
 }
 
-static void dlb2_configure_domain_credits_v2(struct dlb2_hw *hw,
-					     struct dlb2_hw_domain *domain)
+static struct dlb2_ldb_port *
+dlb2_get_domain_ldb_port(u32 id,
+			 bool vdev_req,
+			 struct dlb2_hw_domain *domain)
 {
-	u32 reg = 0;
-
-	DLB2_BITS_SET(reg, domain->num_ldb_credits,
-		      DLB2_CHP_CFG_LDB_VAS_CRD_COUNT);
-	DLB2_CSR_WR(hw, DLB2_CHP_CFG_LDB_VAS_CRD(domain->id.phys_id), reg);
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_ldb_port *port;
+	int i;
 
-	reg = 0;
-	DLB2_BITS_SET(reg, domain->num_dir_credits,
-		      DLB2_CHP_CFG_DIR_VAS_CRD_COUNT);
-	DLB2_CSR_WR(hw, DLB2_CHP_CFG_DIR_VAS_CRD(domain->id.phys_id), reg);
-}
+	if (id >= DLB2_MAX_NUM_LDB_PORTS)
+		return NULL;
 
-static void dlb2_configure_domain_credits(struct dlb2_hw *hw,
-					  struct dlb2_hw_domain *domain)
-{
-	if (hw->ver == DLB2_HW_V2)
-		dlb2_configure_domain_credits_v2(hw, domain);
-	else
-		dlb2_configure_domain_credits_v2_5(hw, domain);
-}
+	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
+		DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i], port, iter) {
+			if ((!vdev_req && port->id.phys_id == id) ||
+			    (vdev_req && port->id.virt_id == id))
+				return port;
+		}
 
-static int dlb2_attach_credits(struct dlb2_function_resources *rsrcs,
-			       struct dlb2_hw_domain *domain,
-			       u32 num_credits,
-			       struct dlb2_cmd_response *resp)
-{
-	if (rsrcs->num_avail_entries < num_credits) {
-		resp->status = DLB2_ST_CREDITS_UNAVAILABLE;
-		return -EINVAL;
+		DLB2_DOM_LIST_FOR(domain->avail_ldb_ports[i], port, iter) {
+			if ((!vdev_req && port->id.phys_id == id) ||
+			    (vdev_req && port->id.virt_id == id))
+				return port;
+		}
 	}
 
-	rsrcs->num_avail_entries -= num_credits;
-	domain->num_credits += num_credits;
-	return 0;
+	return NULL;
 }
 
-static struct dlb2_ldb_port *
-dlb2_get_next_ldb_port(struct dlb2_hw *hw,
-		       struct dlb2_function_resources *rsrcs,
-		       u32 domain_id,
-		       u32 cos_id)
+static struct dlb2_dir_pq_pair *dlb2_get_dir_pq_from_id(struct dlb2_hw *hw,
+							u32 id,
+							bool vdev_req,
+							unsigned int vdev_id)
 {
-	struct dlb2_list_entry *iter;
-	struct dlb2_ldb_port *port;
-	RTE_SET_USED(iter);
-
-	/*
-	 * To reduce the odds of consecutive load-balanced ports mapping to the
-	 * same queue(s), the driver attempts to allocate ports whose neighbors
-	 * are owned by a different domain.
-	 */
-	DLB2_FUNC_LIST_FOR(rsrcs->avail_ldb_ports[cos_id], port, iter) {
-		u32 next, prev;
-		u32 phys_id;
+	struct dlb2_list_entry *iter1 __attribute__((unused));
+	struct dlb2_list_entry *iter2 __attribute__((unused));
+	struct dlb2_function_resources *rsrcs;
+	struct dlb2_dir_pq_pair *port;
+	struct dlb2_hw_domain *domain;
 
-		phys_id = port->id.phys_id;
-		next = phys_id + 1;
-		prev = phys_id - 1;
+	if (id >= DLB2_MAX_NUM_DIR_PORTS(hw->ver))
+		return NULL;
 
-		if (phys_id == DLB2_MAX_NUM_LDB_PORTS - 1)
-			next = 0;
-		if (phys_id == 0)
-			prev = DLB2_MAX_NUM_LDB_PORTS - 1;
+	rsrcs = (vdev_req) ? &hw->vdev[vdev_id] : &hw->pf;
 
-		if (!hw->rsrcs.ldb_ports[next].owned ||
-		    hw->rsrcs.ldb_ports[next].domain_id.phys_id == domain_id)
-			continue;
+	if (!vdev_req)
+		return &hw->rsrcs.dir_pq_pairs[id];
 
-		if (!hw->rsrcs.ldb_ports[prev].owned ||
-		    hw->rsrcs.ldb_ports[prev].domain_id.phys_id == domain_id)
-			continue;
+	DLB2_FUNC_LIST_FOR(rsrcs->used_domains, domain, iter1) {
+		DLB2_DOM_LIST_FOR(domain->used_dir_pq_pairs, port, iter2) {
+			if (port->id.virt_id == id)
+				return port;
+		}
+	}
 
-		return port;
+	DLB2_FUNC_LIST_FOR(rsrcs->avail_dir_pq_pairs, port, iter1) {
+		if (port->id.virt_id == id)
+			return port;
 	}
 
-	/*
-	 * Failing that, the driver looks for a port with one neighbor owned by
-	 * a different domain and the other unallocated.
-	 */
-	DLB2_FUNC_LIST_FOR(rsrcs->avail_ldb_ports[cos_id], port, iter) {
-		u32 next, prev;
-		u32 phys_id;
+	return NULL;
+}
 
-		phys_id = port->id.phys_id;
-		next = phys_id + 1;
-		prev = phys_id - 1;
-
-		if (phys_id == DLB2_MAX_NUM_LDB_PORTS - 1)
-			next = 0;
-		if (phys_id == 0)
-			prev = DLB2_MAX_NUM_LDB_PORTS - 1;
+static struct dlb2_dir_pq_pair *
+dlb2_get_domain_used_dir_pq(struct dlb2_hw *hw,
+			    u32 id,
+			    bool vdev_req,
+			    struct dlb2_hw_domain *domain)
+{
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_dir_pq_pair *port;
 
-		if (!hw->rsrcs.ldb_ports[prev].owned &&
-		    hw->rsrcs.ldb_ports[next].owned &&
-		    hw->rsrcs.ldb_ports[next].domain_id.phys_id != domain_id)
-			return port;
+	if (id >= DLB2_MAX_NUM_DIR_PORTS(hw->ver))
+		return NULL;
 
-		if (!hw->rsrcs.ldb_ports[next].owned &&
-		    hw->rsrcs.ldb_ports[prev].owned &&
-		    hw->rsrcs.ldb_ports[prev].domain_id.phys_id != domain_id)
+	DLB2_DOM_LIST_FOR(domain->used_dir_pq_pairs, port, iter) {
+		if ((!vdev_req && port->id.phys_id == id) ||
+		    (vdev_req && port->id.virt_id == id))
 			return port;
 	}
 
-	/*
-	 * Failing that, the driver looks for a port with both neighbors
-	 * unallocated.
-	 */
-	DLB2_FUNC_LIST_FOR(rsrcs->avail_ldb_ports[cos_id], port, iter) {
-		u32 next, prev;
-		u32 phys_id;
+	return NULL;
+}
 
-		phys_id = port->id.phys_id;
-		next = phys_id + 1;
-		prev = phys_id - 1;
+static struct dlb2_dir_pq_pair *
+dlb2_get_domain_dir_pq(struct dlb2_hw *hw,
+		       u32 id,
+		       bool vdev_req,
+		       struct dlb2_hw_domain *domain)
+{
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_dir_pq_pair *port;
 
-		if (phys_id == DLB2_MAX_NUM_LDB_PORTS - 1)
-			next = 0;
-		if (phys_id == 0)
-			prev = DLB2_MAX_NUM_LDB_PORTS - 1;
+	if (id >= DLB2_MAX_NUM_DIR_PORTS(hw->ver))
+		return NULL;
 
-		if (!hw->rsrcs.ldb_ports[prev].owned &&
-		    !hw->rsrcs.ldb_ports[next].owned)
+	DLB2_DOM_LIST_FOR(domain->used_dir_pq_pairs, port, iter) {
+		if ((!vdev_req && port->id.phys_id == id) ||
+		    (vdev_req && port->id.virt_id == id))
 			return port;
 	}
 
-	/* If all else fails, the driver returns the next available port. */
-	return DLB2_FUNC_LIST_HEAD(rsrcs->avail_ldb_ports[cos_id],
-				   typeof(*port));
+	DLB2_DOM_LIST_FOR(domain->avail_dir_pq_pairs, port, iter) {
+		if ((!vdev_req && port->id.phys_id == id) ||
+		    (vdev_req && port->id.virt_id == id))
+			return port;
+	}
+
+	return NULL;
 }
 
-static int __dlb2_attach_ldb_ports(struct dlb2_hw *hw,
-				   struct dlb2_function_resources *rsrcs,
-				   struct dlb2_hw_domain *domain,
-				   u32 num_ports,
-				   u32 cos_id,
-				   struct dlb2_cmd_response *resp)
+static struct dlb2_ldb_queue *
+dlb2_get_ldb_queue_from_id(struct dlb2_hw *hw,
+			   u32 id,
+			   bool vdev_req,
+			   unsigned int vdev_id)
 {
-	unsigned int i;
+	struct dlb2_list_entry *iter1 __attribute__((unused));
+	struct dlb2_list_entry *iter2 __attribute__((unused));
+	struct dlb2_function_resources *rsrcs;
+	struct dlb2_hw_domain *domain;
+	struct dlb2_ldb_queue *queue;
 
-	if (rsrcs->num_avail_ldb_ports[cos_id] < num_ports) {
-		resp->status = DLB2_ST_LDB_PORTS_UNAVAILABLE;
-		return -EINVAL;
-	}
+	if (id >= DLB2_MAX_NUM_LDB_QUEUES)
+		return NULL;
 
-	for (i = 0; i < num_ports; i++) {
-		struct dlb2_ldb_port *port;
+	rsrcs = (vdev_req) ? &hw->vdev[vdev_id] : &hw->pf;
 
-		port = dlb2_get_next_ldb_port(hw, rsrcs,
-					      domain->id.phys_id, cos_id);
-		if (port == NULL) {
-			DLB2_HW_ERR(hw,
-				    "[%s()] Internal error: domain validation failed\n",
-				    __func__);
-			return -EFAULT;
+	if (!vdev_req)
+		return &hw->rsrcs.ldb_queues[id];
+
+	DLB2_FUNC_LIST_FOR(rsrcs->used_domains, domain, iter1) {
+		DLB2_DOM_LIST_FOR(domain->used_ldb_queues, queue, iter2) {
+			if (queue->id.virt_id == id)
+				return queue;
 		}
+	}
 
-		dlb2_list_del(&rsrcs->avail_ldb_ports[cos_id],
-			      &port->func_list);
+	DLB2_FUNC_LIST_FOR(rsrcs->avail_ldb_queues, queue, iter1) {
+		if (queue->id.virt_id == id)
+			return queue;
+	}
 
-		port->domain_id = domain->id;
-		port->owned = true;
+	return NULL;
+}
 
-		dlb2_list_add(&domain->avail_ldb_ports[cos_id],
-			      &port->domain_list);
-	}
+static struct dlb2_ldb_queue *
+dlb2_get_domain_ldb_queue(u32 id,
+			  bool vdev_req,
+			  struct dlb2_hw_domain *domain)
+{
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_ldb_queue *queue;
 
-	rsrcs->num_avail_ldb_ports[cos_id] -= num_ports;
+	if (id >= DLB2_MAX_NUM_LDB_QUEUES)
+		return NULL;
 
-	return 0;
+	DLB2_DOM_LIST_FOR(domain->used_ldb_queues, queue, iter) {
+		if ((!vdev_req && queue->id.phys_id == id) ||
+		    (vdev_req && queue->id.virt_id == id))
+			return queue;
+	}
+
+	return NULL;
 }
 
+#define DLB2_XFER_LL_RSRC(dst, src, num, type_t, name) ({		     \
+	struct dlb2_list_entry *it1 __attribute__((unused));		     \
+	struct dlb2_list_entry *it2 __attribute__((unused));		     \
+	struct dlb2_function_resources *_src = src;			     \
+	struct dlb2_function_resources *_dst = dst;			     \
+	type_t *ptr, *tmp __attribute__((unused));			     \
+	unsigned int i = 0;						     \
+									     \
+	DLB2_FUNC_LIST_FOR_SAFE(_src->avail_##name##s, ptr, tmp, it1, it2) { \
+		if (i++ == (num))					     \
+			break;						     \
+									     \
+		dlb2_list_del(&_src->avail_##name##s, &ptr->func_list);      \
+		dlb2_list_add(&_dst->avail_##name##s,  &ptr->func_list);     \
+		_src->num_avail_##name##s--;				     \
+		_dst->num_avail_##name##s++;				     \
+	}								     \
+})
+
+#define DLB2_XFER_LL_IDX_RSRC(dst, src, num, idx, type_t, name) ({	       \
+	struct dlb2_list_entry *it1 __attribute__((unused));		       \
+	struct dlb2_list_entry *it2 __attribute__((unused));		       \
+	struct dlb2_function_resources *_src = src;			       \
+	struct dlb2_function_resources *_dst = dst;			       \
+	type_t *ptr, *tmp __attribute__((unused));			       \
+	unsigned int i = 0;						       \
+									       \
+	DLB2_FUNC_LIST_FOR_SAFE(_src->avail_##name##s[idx],		       \
+				 ptr, tmp, it1, it2) {			       \
+		if (i++ == (num))					       \
+			break;						       \
+									       \
+		dlb2_list_del(&_src->avail_##name##s[idx], &ptr->func_list);   \
+		dlb2_list_add(&_dst->avail_##name##s[idx],  &ptr->func_list);  \
+		_src->num_avail_##name##s[idx]--;			       \
+		_dst->num_avail_##name##s[idx]++;			       \
+	}								       \
+})
+
+#define DLB2_VF_ID_CLEAR(head, type_t) ({ \
+	struct dlb2_list_entry *iter __attribute__((unused));  \
+	type_t *var;					       \
+							       \
+	DLB2_FUNC_LIST_FOR(head, var, iter)		       \
+		var->id.vdev_owned = false;		       \
+})
 
-static int dlb2_attach_ldb_ports(struct dlb2_hw *hw,
-				 struct dlb2_function_resources *rsrcs,
-				 struct dlb2_hw_domain *domain,
-				 struct dlb2_create_sched_domain_args *args,
-				 struct dlb2_cmd_response *resp)
+/**
+ * dlb2_update_vdev_sched_domains() - update the domains assigned to a vdev
+ * @hw: dlb2_hw handle for a particular device.
+ * @id: virtual device ID
+ * @num: number of scheduling domains to assign to this vdev
+ *
+ * This function assigns num scheduling domains to the specified vdev. If the
+ * vdev already has domains assigned, this existing assignment is adjusted
+ * accordingly.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ *
+ * Return:
+ * Returns 0 upon success, <0 otherwise.
+ *
+ * Errors:
+ * EINVAL - id is invalid, or the requested number of resources are
+ *	    unavailable.
+ * EPERM  - The vdev's resource assignment is locked and cannot be changed.
+ */
+int dlb2_update_vdev_sched_domains(struct dlb2_hw *hw, u32 id, u32 num)
 {
-	unsigned int i, j;
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_function_resources *src, *dst;
+	struct dlb2_hw_domain *domain;
+	unsigned int orig;
 	int ret;
 
-	if (args->cos_strict) {
-		for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
-			u32 num = args->num_cos_ldb_ports[i];
+	if (id >= DLB2_MAX_NUM_VDEVS)
+		return -EINVAL;
 
-			/* Allocate ports from specific classes-of-service */
-			ret = __dlb2_attach_ldb_ports(hw,
-						      rsrcs,
-						      domain,
-						      num,
-						      i,
-						      resp);
-			if (ret)
-				return ret;
-		}
-	} else {
-		unsigned int k;
-		u32 cos_id;
+	src = &hw->pf;
+	dst = &hw->vdev[id];
 
-		/*
-		 * Attempt to allocate from specific class-of-service, but
-		 * fallback to the other classes if that fails.
-		 */
-		for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
-			for (j = 0; j < args->num_cos_ldb_ports[i]; j++) {
-				for (k = 0; k < DLB2_NUM_COS_DOMAINS; k++) {
-					cos_id = (i + k) % DLB2_NUM_COS_DOMAINS;
+	/* If the VF is locked, its resource assignment can't be changed */
+	if (dlb2_vdev_is_locked(hw, id))
+		return -EPERM;
 
-					ret = __dlb2_attach_ldb_ports(hw,
-								      rsrcs,
-								      domain,
-								      1,
-								      cos_id,
-								      resp);
-					if (ret == 0)
-						break;
-				}
+	orig = dst->num_avail_domains;
 
-				if (ret)
-					return ret;
-			}
-		}
-	}
+	/*
+	 * Detach the destination VF's current resources before checking if
+	 * enough are available, and set their IDs accordingly.
+	 */
+	DLB2_VF_ID_CLEAR(dst->avail_domains, struct dlb2_hw_domain);
 
-	/* Allocate num_ldb_ports from any class-of-service */
-	for (i = 0; i < args->num_ldb_ports; i++) {
-		for (j = 0; j < DLB2_NUM_COS_DOMAINS; j++) {
-			/* Allocate from best performing cos */
-			u32 cos_idx = j + DLB2_MAX_NUM_LDB_PORTS;
-			u32 cos_id = hw->ldb_pp_allocations[cos_idx];
-			ret = __dlb2_attach_ldb_ports(hw,
-						      rsrcs,
-						      domain,
-						      1,
-						      cos_id,
-						      resp);
-			if (ret == 0)
-				break;
-		}
+	DLB2_XFER_LL_RSRC(src, dst, orig, struct dlb2_hw_domain, domain);
 
-		if (ret)
-			return ret;
+	/* Are there enough available resources to satisfy the request? */
+	if (num > src->num_avail_domains) {
+		num = orig;
+		ret = -EINVAL;
+	} else {
+		ret = 0;
 	}
 
-	return 0;
+	DLB2_XFER_LL_RSRC(dst, src, num, struct dlb2_hw_domain, domain);
+
+	/* Set the domains' VF backpointer */
+	DLB2_FUNC_LIST_FOR(dst->avail_domains, domain, iter)
+		domain->parent_func = dst;
+
+	return ret;
 }
 
-static int dlb2_attach_dir_ports(struct dlb2_hw *hw,
-				 struct dlb2_function_resources *rsrcs,
-				 struct dlb2_hw_domain *domain,
-				 u32 num_ports,
-				 struct dlb2_cmd_response *resp)
+/**
+ * dlb2_update_vdev_ldb_queues() - update the LDB queues assigned to a vdev
+ * @hw: dlb2_hw handle for a particular device.
+ * @id: virtual device ID
+ * @num: number of LDB queues to assign to this vdev
+ *
+ * This function assigns num LDB queues to the specified vdev. If the vdev
+ * already has LDB queues assigned, this existing assignment is adjusted
+ * accordingly.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ *
+ * Return:
+ * Returns 0 upon success, <0 otherwise.
+ *
+ * Errors:
+ * EINVAL - id is invalid, or the requested number of resources are
+ *	    unavailable.
+ * EPERM  - The vdev's resource assignment is locked and cannot be changed.
+ */
+int dlb2_update_vdev_ldb_queues(struct dlb2_hw *hw, u32 id, u32 num)
 {
-	int num_res = hw->num_prod_cores;
-	unsigned int i;
+	struct dlb2_function_resources *src, *dst;
+	unsigned int orig;
+	int ret;
 
-	if (rsrcs->num_avail_dir_pq_pairs < num_ports) {
-		resp->status = DLB2_ST_DIR_PORTS_UNAVAILABLE;
+	if (id >= DLB2_MAX_NUM_VDEVS)
 		return -EINVAL;
-	}
 
-	for (i = 0; i < num_ports; i++) {
-		struct dlb2_dir_pq_pair *port;
+	src = &hw->pf;
+	dst = &hw->vdev[id];
 
-		port = DLB2_FUNC_LIST_HEAD(rsrcs->avail_dir_pq_pairs,
-					   typeof(*port));
-		if (port == NULL) {
-			DLB2_HW_ERR(hw,
-				    "[%s()] Internal error: domain validation failed\n",
-				    __func__);
-			return -EFAULT;
-		}
+	/* If the VF is locked, its resource assignment can't be changed */
+	if (dlb2_vdev_is_locked(hw, id))
+		return -EPERM;
 
-		if (num_res) {
-			dlb2_list_add(&domain->rsvd_dir_pq_pairs,
-				      &port->domain_list);
-			num_res--;
-		} else {
-			dlb2_list_add(&domain->avail_dir_pq_pairs,
-			&port->domain_list);
-		}
+	orig = dst->num_avail_ldb_queues;
 
-		dlb2_list_del(&rsrcs->avail_dir_pq_pairs, &port->func_list);
+	/*
+	 * Detach the destination VF's current resources before checking if
+	 * enough are available, and set their IDs accordingly.
+	 */
+	DLB2_VF_ID_CLEAR(dst->avail_ldb_queues, struct dlb2_ldb_queue);
 
-		port->domain_id = domain->id;
-		port->owned = true;
+	DLB2_XFER_LL_RSRC(src, dst, orig, struct dlb2_ldb_queue, ldb_queue);
+
+	/* Are there enough available resources to satisfy the request? */
+	if (num > src->num_avail_ldb_queues) {
+		num = orig;
+		ret = -EINVAL;
+	} else {
+		ret = 0;
 	}
 
-	rsrcs->num_avail_dir_pq_pairs -= num_ports;
+	DLB2_XFER_LL_RSRC(dst, src, num, struct dlb2_ldb_queue, ldb_queue);
 
-	return 0;
+	return ret;
 }
 
-static int dlb2_attach_ldb_credits(struct dlb2_function_resources *rsrcs,
-				   struct dlb2_hw_domain *domain,
-				   u32 num_credits,
-				   struct dlb2_cmd_response *resp)
+/**
+ * dlb2_update_vdev_ldb_cos_ports() - update the LDB ports assigned to a vdev
+ * @hw: dlb2_hw handle for a particular device.
+ * @id: virtual device ID
+ * @cos: class-of-service ID
+ * @num: number of LDB ports to assign to this vdev
+ *
+ * This function assigns num LDB ports from class-of-service cos to the
+ * specified vdev. If the vdev already has LDB ports from this class-of-service
+ * assigned, this existing assignment is adjusted accordingly.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ *
+ * Return:
+ * Returns 0 upon success, <0 otherwise.
+ *
+ * Errors:
+ * EINVAL - id is invalid, or the requested number of resources are
+ *	    unavailable.
+ * EPERM  - The vdev's resource assignment is locked and cannot be changed.
+ */
+int dlb2_update_vdev_ldb_cos_ports(struct dlb2_hw *hw,
+				   u32 id,
+				   u32 cos,
+				   u32 num)
 {
-	if (rsrcs->num_avail_qed_entries < num_credits) {
-		resp->status = DLB2_ST_LDB_CREDITS_UNAVAILABLE;
+	struct dlb2_function_resources *src, *dst;
+	unsigned int orig;
+	int ret;
+
+	if (id >= DLB2_MAX_NUM_VDEVS)
 		return -EINVAL;
+
+	src = &hw->pf;
+	dst = &hw->vdev[id];
+
+	/* If the VF is locked, its resource assignment can't be changed */
+	if (dlb2_vdev_is_locked(hw, id))
+		return -EPERM;
+
+	orig = dst->num_avail_ldb_ports[cos];
+
+	/*
+	 * Detach the destination VF's current resources before checking if
+	 * enough are available, and set their IDs accordingly.
+	 */
+	DLB2_VF_ID_CLEAR(dst->avail_ldb_ports[cos], struct dlb2_ldb_port);
+
+	DLB2_XFER_LL_IDX_RSRC(src, dst, orig, cos,
+			      struct dlb2_ldb_port, ldb_port);
+
+	/* Are there enough available resources to satisfy the request? */
+	if (num > src->num_avail_ldb_ports[cos]) {
+		num = orig;
+		ret = -EINVAL;
+	} else {
+		ret = 0;
 	}
 
-	rsrcs->num_avail_qed_entries -= num_credits;
-	domain->num_ldb_credits += num_credits;
-	return 0;
+	DLB2_XFER_LL_IDX_RSRC(dst, src, num, cos,
+			      struct dlb2_ldb_port, ldb_port);
+
+	return ret;
 }
 
-static int dlb2_attach_dir_credits(struct dlb2_function_resources *rsrcs,
-				   struct dlb2_hw_domain *domain,
-				   u32 num_credits,
-				   struct dlb2_cmd_response *resp)
+static int dlb2_add_vdev_ldb_ports(struct dlb2_hw *hw, u32 id, u32 num)
 {
-	if (rsrcs->num_avail_dqed_entries < num_credits) {
-		resp->status = DLB2_ST_DIR_CREDITS_UNAVAILABLE;
-		return -EINVAL;
-	}
+	struct dlb2_function_resources *src, *dst;
+	u32 avail, orig[DLB2_NUM_COS_DOMAINS];
+	int ret, i;
 
-	rsrcs->num_avail_dqed_entries -= num_credits;
-	domain->num_dir_credits += num_credits;
-	return 0;
-}
+	if (num == 0)
+		return 0;
 
+	src = &hw->pf;
+	dst = &hw->vdev[id];
 
-static int dlb2_attach_atomic_inflights(struct dlb2_function_resources *rsrcs,
-					struct dlb2_hw_domain *domain,
-					u32 num_atomic_inflights,
-					struct dlb2_cmd_response *resp)
-{
-	if (rsrcs->num_avail_aqed_entries < num_atomic_inflights) {
-		resp->status = DLB2_ST_ATOMIC_INFLIGHTS_UNAVAILABLE;
+	avail = 0;
+	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++)
+		avail += src->num_avail_ldb_ports[i];
+
+	if (avail < num)
 		return -EINVAL;
+
+	/* Add ports to each CoS until num have been added */
+	for (i = 0; i < DLB2_NUM_COS_DOMAINS && num > 0; i++) {
+		u32 curr = dst->num_avail_ldb_ports[i];
+		u32 num_to_add;
+
+		avail = src->num_avail_ldb_ports[i];
+
+		/* Don't attempt to add more than are available */
+		num_to_add = num < avail ? num : avail;
+
+		ret = dlb2_update_vdev_ldb_cos_ports(hw, id, i,
+						     curr + num_to_add);
+		if (ret)
+			goto cleanup;
+
+		orig[i] = curr;
+		num -= num_to_add;
 	}
 
-	rsrcs->num_avail_aqed_entries -= num_atomic_inflights;
-	domain->num_avail_aqed_entries += num_atomic_inflights;
 	return 0;
+
+cleanup:
+	DLB2_HW_ERR(hw,
+		    "[%s()] Internal error: failed to add ldb ports\n",
+		    __func__);
+
+	/* Internal error, attempt to recover original configuration */
+	for (i--; i >= 0; i--)
+		dlb2_update_vdev_ldb_cos_ports(hw, id, i, orig[i]);
+
+	return ret;
 }
 
-static int
-dlb2_attach_domain_hist_list_entries(struct dlb2_function_resources *rsrcs,
-				     struct dlb2_hw_domain *domain,
-				     u32 num_hist_list_entries,
-				     struct dlb2_cmd_response *resp)
+static int dlb2_del_vdev_ldb_ports(struct dlb2_hw *hw, u32 id, u32 num)
 {
-	struct dlb2_bitmap *bitmap;
-	int base;
+	struct dlb2_function_resources *dst;
+	u32 orig[DLB2_NUM_COS_DOMAINS];
+	int ret, i;
 
-	if (num_hist_list_entries) {
-		bitmap = rsrcs->avail_hist_list_entries;
+	if (num == 0)
+		return 0;
 
-		base = dlb2_bitmap_find_set_bit_range(bitmap,
-						      num_hist_list_entries);
-		if (base < 0)
-			goto error;
+	dst = &hw->vdev[id];
 
-		domain->total_hist_list_entries = num_hist_list_entries;
-		domain->avail_hist_list_entries = num_hist_list_entries;
-		domain->hist_list_entry_base = base;
-		domain->hist_list_entry_offset = 0;
+	/* Remove ports from each CoS until num have been removed */
+	for (i = 0; i < DLB2_NUM_COS_DOMAINS && num > 0; i++) {
+		u32 curr = dst->num_avail_ldb_ports[i];
+		u32 num_to_del;
 
-		dlb2_bitmap_clear_range(bitmap, base, num_hist_list_entries);
+		/* Don't attempt to remove more than dst owns */
+		num_to_del = num < curr ? num : curr;
+
+		ret = dlb2_update_vdev_ldb_cos_ports(hw, id, i,
+						     curr - num_to_del);
+		if (ret)
+			goto cleanup;
+
+		orig[i] = curr;
+		num -= curr;
 	}
+
 	return 0;
 
-error:
-	resp->status = DLB2_ST_HIST_LIST_ENTRIES_UNAVAILABLE;
-	return -EINVAL;
+cleanup:
+	DLB2_HW_ERR(hw,
+		    "[%s()] Internal error: failed to remove ldb ports\n",
+		    __func__);
+
+	/* Internal error, attempt to recover original configuration */
+	for (i--; i >= 0; i--)
+		dlb2_update_vdev_ldb_cos_ports(hw, id, i, orig[i]);
+
+	return ret;
 }
 
-static int dlb2_attach_ldb_queues(struct dlb2_hw *hw,
-				  struct dlb2_function_resources *rsrcs,
-				  struct dlb2_hw_domain *domain,
-				  u32 num_queues,
-				  struct dlb2_cmd_response *resp)
+/**
+ * dlb2_update_vdev_ldb_ports() - update the LDB ports assigned to a vdev
+ * @hw: dlb2_hw handle for a particular device.
+ * @id: virtual device ID
+ * @num: number of LDB ports to assign to this vdev
+ *
+ * This function assigns num LDB ports to the specified vdev. If the vdev
+ * already has LDB ports assigned, this existing assignment is adjusted
+ * accordingly.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ *
+ * Return:
+ * Returns 0 upon success, <0 otherwise.
+ *
+ * Errors:
+ * EINVAL - id is invalid, or the requested number of resources are
+ *	    unavailable.
+ * EPERM  - The vdev's resource assignment is locked and cannot be changed.
+ */
+int dlb2_update_vdev_ldb_ports(struct dlb2_hw *hw, u32 id, u32 num)
 {
-	unsigned int i;
+	struct dlb2_function_resources *dst;
+	unsigned int orig;
+	int i;
 
-	if (rsrcs->num_avail_ldb_queues < num_queues) {
-		resp->status = DLB2_ST_LDB_QUEUES_UNAVAILABLE;
+	if (id >= DLB2_MAX_NUM_VDEVS)
 		return -EINVAL;
-	}
-
-	for (i = 0; i < num_queues; i++) {
-		struct dlb2_ldb_queue *queue;
-
-		queue = DLB2_FUNC_LIST_HEAD(rsrcs->avail_ldb_queues,
-					    typeof(*queue));
-		if (queue == NULL) {
-			DLB2_HW_ERR(hw,
-				    "[%s()] Internal error: domain validation failed\n",
-				    __func__);
-			return -EFAULT;
-		}
 
-		dlb2_list_del(&rsrcs->avail_ldb_queues, &queue->func_list);
+	dst = &hw->vdev[id];
 
-		queue->domain_id = domain->id;
-		queue->owned = true;
+	/* If the VF is locked, its resource assignment can't be changed */
+	if (dlb2_vdev_is_locked(hw, id))
+		return -EPERM;
 
-		dlb2_list_add(&domain->avail_ldb_queues, &queue->domain_list);
-	}
+	orig = 0;
 
-	rsrcs->num_avail_ldb_queues -= num_queues;
+	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++)
+		orig += dst->num_avail_ldb_ports[i];
 
-	return 0;
+	if (orig == num)
+		return 0;
+	else if (orig < num)
+		return dlb2_add_vdev_ldb_ports(hw, id, num - orig);
+	else
+		return dlb2_del_vdev_ldb_ports(hw, id, orig - num);
 }
 
-static int
-dlb2_pp_profile(struct dlb2_hw *hw, int port, int cpu, bool is_ldb)
+/**
+ * dlb2_update_vdev_dir_ports() - update the DIR ports assigned to a vdev
+ * @hw: dlb2_hw handle for a particular device.
+ * @id: virtual device ID
+ * @num: number of DIR ports to assign to this vdev
+ *
+ * This function assigns num DIR ports to the specified vdev. If the vdev
+ * already has DIR ports assigned, this existing assignment is adjusted
+ * accordingly.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ *
+ * Return:
+ * Returns 0 upon success, <0 otherwise.
+ *
+ * Errors:
+ * EINVAL - id is invalid, or the requested number of resources are
+ *	    unavailable.
+ * EPERM  - The vdev's resource assignment is locked and cannot be changed.
+ */
+int dlb2_update_vdev_dir_ports(struct dlb2_hw *hw, u32 id, u32 num)
 {
-	u64 cycle_start = 0ULL, cycle_end = 0ULL;
-	struct dlb2_hcw hcw_mem[DLB2_HCW_MEM_SIZE], *hcw;
-	void __iomem *pp_addr;
-	cpu_set_t cpuset;
-	int i;
+	struct dlb2_function_resources *src, *dst;
+	unsigned int orig;
+	int ret;
 
-	CPU_ZERO(&cpuset);
-	CPU_SET(cpu, &cpuset);
-	sched_setaffinity(0, sizeof(cpuset), &cpuset);
+	if (id >= DLB2_MAX_NUM_VDEVS)
+		return -EINVAL;
 
-	pp_addr = os_map_producer_port(hw, port, is_ldb);
+	src = &hw->pf;
+	dst = &hw->vdev[id];
 
-	/* Point hcw to a 64B-aligned location */
-	hcw = (struct dlb2_hcw *)((uintptr_t)&hcw_mem[DLB2_HCW_64B_OFF] &
-	      ~DLB2_HCW_ALIGN_MASK);
+	/* If the VF is locked, its resource assignment can't be changed */
+	if (dlb2_vdev_is_locked(hw, id))
+		return -EPERM;
+
+	orig = dst->num_avail_dir_pq_pairs;
 
 	/*
-	 * Program the first HCW for a completion and token return and
-	 * the other HCWs as NOOPS
+	 * Detach the destination VF's current resources before checking if
+	 * enough are available, and set their IDs accordingly.
 	 */
+	DLB2_VF_ID_CLEAR(dst->avail_dir_pq_pairs, struct dlb2_dir_pq_pair);
 
-	memset(hcw, 0, (DLB2_HCW_MEM_SIZE - DLB2_HCW_64B_OFF) * sizeof(*hcw));
-	hcw->qe_comp = 1;
-	hcw->cq_token = 1;
-	hcw->lock_id = 1;
+	DLB2_XFER_LL_RSRC(src, dst, orig,
+			  struct dlb2_dir_pq_pair, dir_pq_pair);
 
-	cycle_start = rte_get_tsc_cycles();
-	for (i = 0; i < DLB2_NUM_PROBE_ENQS; i++)
-		dlb2_movdir64b(pp_addr, hcw);
+	/* Are there enough available resources to satisfy the request? */
+	if (num > src->num_avail_dir_pq_pairs) {
+		num = orig;
+		ret = -EINVAL;
+	} else {
+		ret = 0;
+	}
 
-	cycle_end = rte_get_tsc_cycles();
+	DLB2_XFER_LL_RSRC(dst, src, num,
+			  struct dlb2_dir_pq_pair, dir_pq_pair);
 
-	os_unmap_producer_port(hw, pp_addr);
-	return (int)(cycle_end - cycle_start);
+	return ret;
 }
 
-static void *
-dlb2_pp_profile_func(void *data)
+static int dlb2_transfer_bitmap_resources(struct dlb2_bitmap *src,
+					  struct dlb2_bitmap *dst,
+					  u32 num)
 {
-	struct dlb2_pp_thread_data *thread_data = data;
-	int cycles;
+	int orig, ret, base;
 
-	cycles = dlb2_pp_profile(thread_data->hw, thread_data->pp,
-	thread_data->cpu, thread_data->is_ldb);
+	/*
+	 * Reassign the dest's bitmap entries to the source's before checking
+	 * if a contiguous chunk of size 'num' is available. The reassignment
+	 * may be necessary to create a sufficiently large contiguous chunk.
+	 */
+	orig = dlb2_bitmap_count(dst);
 
-	thread_data->cycles = cycles;
+	dlb2_bitmap_or(src, src, dst);
 
-	return NULL;
+	dlb2_bitmap_zero(dst);
+
+	/* Are there enough available resources to satisfy the request? */
+	base = dlb2_bitmap_find_set_bit_range(src, num);
+
+	if (base == -ENOENT) {
+		num = orig;
+		base = dlb2_bitmap_find_set_bit_range(src, num);
+		ret = -EINVAL;
+	} else {
+		ret = 0;
+	}
+
+	dlb2_bitmap_set_range(dst, base, num);
+
+	dlb2_bitmap_clear_range(src, base, num);
+
+	return ret;
 }
 
-static int dlb2_pp_cycle_comp(const void *a, const void *b)
+/**
+ * dlb2_update_vdev_ldb_credits() - update the vdev's assigned LDB credits
+ * @hw: dlb2_hw handle for a particular device.
+ * @id: virtual device ID
+ * @num: number of LDB credit credits to assign to this vdev
+ *
+ * This function assigns num LDB credit to the specified vdev. If the vdev
+ * already has LDB credits assigned, this existing assignment is adjusted
+ * accordingly. vdevs are assigned a contiguous chunk of credits, so this
+ * function may fail if a sufficiently large contiguous chunk is not available.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ *
+ * Return:
+ * Returns 0 upon success, <0 otherwise.
+ *
+ * Errors:
+ * EINVAL - id is invalid, or the requested number of resources are
+ *	    unavailable.
+ * EPERM  - The vdev's resource assignment is locked and cannot be changed.
+ */
+int dlb2_update_vdev_ldb_credits(struct dlb2_hw *hw, u32 id, u32 num)
 {
-	const struct dlb2_pp_thread_data *x = a;
-	const struct dlb2_pp_thread_data *y = b;
+	struct dlb2_function_resources *src, *dst;
+	u32 orig;
 
-	return x->cycles - y->cycles;
-}
+	if (id >= DLB2_MAX_NUM_VDEVS)
+		return -EINVAL;
 
+	if (hw->ver == DLB2_HW_V2_5)
+		return -EINVAL;
 
-/* Probe producer ports from different CPU cores */
-static void
-dlb2_get_pp_allocation(struct dlb2_hw *hw, int cpu, int port_type)
-{
-	struct dlb2_pp_thread_data dlb2_thread_data[DLB2_MAX_NUM_DIR_PORTS_V2_5];
-	struct dlb2_dev *dlb2_dev = container_of(hw, struct dlb2_dev, hw);
-	struct dlb2_pp_thread_data cos_cycles[DLB2_NUM_COS_DOMAINS];
-	int ver = DLB2_HW_DEVICE_FROM_PCI_ID(dlb2_dev->pdev);
-	int num_ports_per_sort, num_ports, num_sort, i, err;
-	bool is_ldb = (port_type == DLB2_LDB_PORT);
-	int *port_allocations;
-	pthread_t pthread;
+	src = &hw->pf;
+	dst = &hw->vdev[id];
 
-	if (is_ldb) {
-		port_allocations = hw->ldb_pp_allocations;
-		num_ports = DLB2_MAX_NUM_LDB_PORTS;
-		num_sort = DLB2_NUM_COS_DOMAINS;
-	} else {
-		port_allocations = hw->dir_pp_allocations;
-		num_ports = DLB2_MAX_NUM_DIR_PORTS(ver);
-		num_sort = 1;
-	}
+	/* If the VF is locked, its resource assignment can't be changed */
+	if (dlb2_vdev_is_locked(hw, id))
+		return -EPERM;
 
-	num_ports_per_sort = num_ports / num_sort;
+	/*
+	 * Detach the destination VF's current resources before checking
+	 * if enough are available.
+	 */
+	orig = dst->num_avail_qed_entries;
+	src->num_avail_qed_entries += orig;
+	dst->num_avail_qed_entries = 0;
 
-	dlb2_dev->enqueue_four = dlb2_movdir64b;
+	if (src->num_avail_qed_entries < num) {
+		src->num_avail_qed_entries -= orig;
+		dst->num_avail_qed_entries = orig;
+		return -EINVAL;
+	}
 
-	DLB2_LOG_INFO(" for %s: cpu core used in pp profiling: %d\n",
-		      is_ldb ? "LDB" : "DIR", cpu);
+	src->num_avail_qed_entries -= num;
+	dst->num_avail_qed_entries += num;
 
-	memset(cos_cycles, 0, num_sort * sizeof(struct dlb2_pp_thread_data));
-	for (i = 0; i < num_ports; i++) {
-		int cos = (i >> DLB2_NUM_COS_DOMAINS) % DLB2_NUM_COS_DOMAINS;
-		dlb2_thread_data[i].is_ldb = is_ldb;
-		dlb2_thread_data[i].pp = i;
-		dlb2_thread_data[i].cycles = 0;
-		dlb2_thread_data[i].hw = hw;
-		dlb2_thread_data[i].cpu = cpu;
+	return 0;
+}
 
-		err = pthread_create(&pthread, NULL, &dlb2_pp_profile_func,
-				     &dlb2_thread_data[i]);
-		if (err) {
-			DLB2_LOG_ERR(": thread creation failed! err=%d", err);
-			return;
-		}
+/**
+ * dlb2_update_vdev_dir_credits() - update the vdev's assigned DIR credits
+ * @hw: dlb2_hw handle for a particular device.
+ * @id: virtual device ID
+ * @num: number of DIR credits to assign to this vdev
+ *
+ * This function assigns num DIR credit to the specified vdev. If the vdev
+ * already has DIR credits assigned, this existing assignment is adjusted
+ * accordingly. vdevs are assigned a contiguous chunk of credits, so this
+ * function may fail if a sufficiently large contiguous chunk is not available.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ *
+ * Return:
+ * Returns 0 upon success, <0 otherwise.
+ *
+ * Errors:
+ * EINVAL - id is invalid, or the requested number of resources are
+ *	    unavailable.
+ * EPERM  - The vdev's resource assignment is locked and cannot be changed.
+ */
+int dlb2_update_vdev_dir_credits(struct dlb2_hw *hw, u32 id, u32 num)
+{
+	struct dlb2_function_resources *src, *dst;
+	u32 orig;
 
-		err = pthread_join(pthread, NULL);
-		if (err) {
-			DLB2_LOG_ERR(": thread join failed! err=%d", err);
-			return;
-		}
+	if (id >= DLB2_MAX_NUM_VDEVS)
+		return -EINVAL;
 
-		if (is_ldb)
-			cos_cycles[cos].cycles += dlb2_thread_data[i].cycles;
+	if (hw->ver == DLB2_HW_V2_5)
+		return -EINVAL;
 
-		if ((i + 1) % num_ports_per_sort == 0) {
-			int index = 0;
+	src = &hw->pf;
+	dst = &hw->vdev[id];
 
-			if (is_ldb) {
-				cos_cycles[cos].pp = cos;
-				index = cos * num_ports_per_sort;
-			}
-			/*
-			 * For LDB ports first sort with in a cos. Later sort
-			 * the best cos based on total cycles for the cos.
-			 * For DIR ports, there is a single sort across all
-			 * ports.
-			 */
-			qsort(&dlb2_thread_data[index], num_ports_per_sort,
-			      sizeof(struct dlb2_pp_thread_data),
-			      dlb2_pp_cycle_comp);
-		}
-	}
+	/* If the VF is locked, its resource assignment can't be changed */
+	if (dlb2_vdev_is_locked(hw, id))
+		return -EPERM;
 
 	/*
-	 * Sort by best cos aggregated over all ports per cos
-	 * Note: After DLB2_MAX_NUM_LDB_PORTS sorted cos is stored and so'pp'
-	 * is cos_id and not port id.
+	 * Detach the destination VF's current resources before checking
+	 * if enough are available.
 	 */
-	if (is_ldb) {
-		qsort(cos_cycles, num_sort, sizeof(struct dlb2_pp_thread_data),
-		      dlb2_pp_cycle_comp);
-		for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++)
-			port_allocations[i + DLB2_MAX_NUM_LDB_PORTS] = cos_cycles[i].pp;
-	}
+	orig = dst->num_avail_dqed_entries;
+	src->num_avail_dqed_entries += orig;
+	dst->num_avail_dqed_entries = 0;
 
-	for (i = 0; i < num_ports; i++) {
-		port_allocations[i] = dlb2_thread_data[i].pp;
-		DLB2_LOG_INFO(": pp %d cycles %d", port_allocations[i],
-			      dlb2_thread_data[i].cycles);
+	if (src->num_avail_dqed_entries < num) {
+		src->num_avail_dqed_entries -= orig;
+		dst->num_avail_dqed_entries = orig;
+		return -EINVAL;
 	}
 
+	src->num_avail_dqed_entries -= num;
+	dst->num_avail_dqed_entries += num;
+
+	return 0;
 }
 
-int
-dlb2_resource_probe(struct dlb2_hw *hw, const void *probe_args)
+/**
+ * dlb2_update_vdev_credits() - update the vdev's assigned credits
+ * @hw: dlb2_hw handle for a particular device.
+ * @id: virtual device ID
+ * @num: number of credits to assign to this vdev
+ *
+ * This function assigns num credits to the specified vdev. If the vdev
+ * already has credits assigned, this existing assignment is adjusted
+ * accordingly. vdevs are assigned a contiguous chunk of credits, so this
+ * function may fail if a sufficiently large contiguous chunk is not available.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ *
+ * Return:
+ * Returns 0 upon success, <0 otherwise.
+ *
+ * Errors:
+ * EINVAL - id is invalid, or the requested number of resources are
+ *	    unavailable.
+ * EPERM  - The vdev's resource assignment is locked and cannot be changed.
+ */
+int dlb2_update_vdev_credits(struct dlb2_hw *hw, u32 id, u32 num)
 {
-	const struct dlb2_devargs *args = (const struct dlb2_devargs *)probe_args;
-	const char *mask = args ? args->producer_coremask : NULL;
-	int cpu = 0, cnt = 0, cores[RTE_MAX_LCORE], i;
+	struct dlb2_function_resources *src, *dst;
+	u32 orig;
 
-	if (args) {
-		mask = (const char *)args->producer_coremask;
-	}
+	if (id >= DLB2_MAX_NUM_VDEVS)
+		return -EINVAL;
 
-	if (mask && rte_eal_parse_coremask(mask, cores)) {
-		DLB2_LOG_ERR(": Invalid producer coremask=%s", mask);
-		return -1;
-	}
+	if (hw->ver == DLB2_HW_V2)
+		return -EINVAL;
 
-	hw->num_prod_cores = 0;
-	for (i = 0; i < RTE_MAX_LCORE; i++) {
-		bool is_pcore = (mask && cores[i] != -1);
+	src = &hw->pf;
+	dst = &hw->vdev[id];
 
-		if (rte_lcore_is_enabled(i)) {
-			if (is_pcore) {
-				/*
-				 * Populate the producer cores from parsed
-				 * coremask
-				 */
-				hw->prod_core_list[cores[i]] = i;
-				hw->num_prod_cores++;
+	/* If the VF is locked, its resource assignment can't be changed */
+	if (dlb2_vdev_is_locked(hw, id))
+		return -EPERM;
 
-			} else if ((++cnt == DLB2_EAL_PROBE_CORE ||
-			   rte_lcore_count() < DLB2_EAL_PROBE_CORE)) {
-				/*
-				 * If no producer coremask is provided, use the
-				 * second EAL core to probe
-				 */
-				cpu = i;
-				break;
-			}
-		} else if (is_pcore) {
-			DLB2_LOG_ERR("Producer coremask(%s) must be a subset of EAL coremask",
-				     mask);
-			return -1;
-		}
+	/*
+	 * Detach the destination VF's current resources before checking
+	 * if enough are available.
+	 */
+	orig = dst->num_avail_entries;
+	src->num_avail_entries += orig;
+	dst->num_avail_entries = 0;
 
+	if (src->num_avail_entries < num) {
+		src->num_avail_entries -= orig;
+		dst->num_avail_entries = orig;
+		return -EINVAL;
 	}
-	/* Use the first core in producer coremask to probe */
-	if (hw->num_prod_cores)
-		cpu = hw->prod_core_list[0];
 
-	dlb2_get_pp_allocation(hw, cpu, DLB2_LDB_PORT);
-	dlb2_get_pp_allocation(hw, cpu, DLB2_DIR_PORT);
+	src->num_avail_entries -= num;
+	dst->num_avail_entries += num;
 
 	return 0;
 }
 
-static int
-dlb2_domain_attach_resources(struct dlb2_hw *hw,
-			     struct dlb2_function_resources *rsrcs,
-			     struct dlb2_hw_domain *domain,
-			     struct dlb2_create_sched_domain_args *args,
-			     struct dlb2_cmd_response *resp)
+/**
+ * dlb2_update_vdev_hist_list_entries() - update the vdev's assigned HL entries
+ * @hw: dlb2_hw handle for a particular device.
+ * @id: virtual device ID
+ * @num: number of history list entries to assign to this vdev
+ *
+ * This function assigns num history list entries to the specified vdev. If the
+ * vdev already has history list entries assigned, this existing assignment is
+ * adjusted accordingly. vdevs are assigned a contiguous chunk of entries, so
+ * this function may fail if a sufficiently large contiguous chunk is not
+ * available.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ *
+ * Return:
+ * Returns 0 upon success, <0 otherwise.
+ *
+ * Errors:
+ * EINVAL - id is invalid, or the requested number of resources are
+ *	    unavailable.
+ * EPERM  - The vdev's resource assignment is locked and cannot be changed.
+ */
+int dlb2_update_vdev_hist_list_entries(struct dlb2_hw *hw, u32 id, u32 num)
 {
-	int ret;
+	struct dlb2_function_resources *src, *dst;
 
-	ret = dlb2_attach_ldb_queues(hw,
-				     rsrcs,
-				     domain,
-				     args->num_ldb_queues,
-				     resp);
-	if (ret)
-		return ret;
+	if (id >= DLB2_MAX_NUM_VDEVS)
+		return -EINVAL;
 
-	ret = dlb2_attach_ldb_ports(hw,
-				    rsrcs,
-				    domain,
-				    args,
-				    resp);
-	if (ret)
-		return ret;
+	src = &hw->pf;
+	dst = &hw->vdev[id];
 
-	ret = dlb2_attach_dir_ports(hw,
-				    rsrcs,
-				    domain,
-				    args->num_dir_ports,
-				    resp);
-	if (ret)
-		return ret;
+	/* If the VF is locked, its resource assignment can't be changed */
+	if (dlb2_vdev_is_locked(hw, id))
+		return -EPERM;
 
-	if (hw->ver == DLB2_HW_V2) {
-		ret = dlb2_attach_ldb_credits(rsrcs,
-					      domain,
-					      args->num_ldb_credits,
-					      resp);
-		if (ret)
-			return ret;
+	return dlb2_transfer_bitmap_resources(src->avail_hist_list_entries,
+					      dst->avail_hist_list_entries,
+					      num);
+}
 
-		ret = dlb2_attach_dir_credits(rsrcs,
-					      domain,
-					      args->num_dir_credits,
-					      resp);
-		if (ret)
-			return ret;
-	} else {  /* DLB 2.5 */
-		ret = dlb2_attach_credits(rsrcs,
-					  domain,
-					  args->num_credits,
-					  resp);
-		if (ret)
-			return ret;
-	}
+/**
+ * dlb2_update_vdev_atomic_inflights() - update the vdev's atomic inflights
+ * @hw: dlb2_hw handle for a particular device.
+ * @id: virtual device ID
+ * @num: number of atomic inflights to assign to this vdev
+ *
+ * This function assigns num atomic inflights to the specified vdev. If the vdev
+ * already has atomic inflights assigned, this existing assignment is adjusted
+ * accordingly. vdevs are assigned a contiguous chunk of entries, so this
+ * function may fail if a sufficiently large contiguous chunk is not available.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ *
+ * Return:
+ * Returns 0 upon success, <0 otherwise.
+ *
+ * Errors:
+ * EINVAL - id is invalid, or the requested number of resources are
+ *	    unavailable.
+ * EPERM  - The vdev's resource assignment is locked and cannot be changed.
+ */
+int dlb2_update_vdev_atomic_inflights(struct dlb2_hw *hw, u32 id, u32 num)
+{
+	struct dlb2_function_resources *src, *dst;
+	u32 orig;
 
-	ret = dlb2_attach_domain_hist_list_entries(rsrcs,
-						   domain,
-						   args->num_hist_list_entries,
-						   resp);
-	if (ret)
-		return ret;
+	if (id >= DLB2_MAX_NUM_VDEVS)
+		return -EINVAL;
 
-	ret = dlb2_attach_atomic_inflights(rsrcs,
-					   domain,
-					   args->num_atomic_inflights,
-					   resp);
-	if (ret)
-		return ret;
+	src = &hw->pf;
+	dst = &hw->vdev[id];
 
-	dlb2_configure_domain_credits(hw, domain);
+	/* If the VF is locked, its resource assignment can't be changed */
+	if (dlb2_vdev_is_locked(hw, id))
+		return -EPERM;
 
-	domain->configured = true;
+	/*
+	 * Detach the destination VF's current resources before checking
+	 * if enough are available.
+	 */
+	orig = dst->num_avail_aqed_entries;
+	src->num_avail_aqed_entries += orig;
+	dst->num_avail_aqed_entries = 0;
 
-	domain->started = false;
+	if (src->num_avail_aqed_entries < num) {
+		src->num_avail_aqed_entries -= orig;
+		dst->num_avail_aqed_entries = orig;
+		return -EINVAL;
+	}
 
-	rsrcs->num_avail_domains--;
+	src->num_avail_aqed_entries -= num;
+	dst->num_avail_aqed_entries += num;
 
 	return 0;
 }
 
-static int
-dlb2_verify_create_sched_dom_args(struct dlb2_function_resources *rsrcs,
-				  struct dlb2_create_sched_domain_args *args,
-				  struct dlb2_cmd_response *resp,
-				  struct dlb2_hw *hw,
-				  struct dlb2_hw_domain **out_domain)
+static int dlb2_attach_ldb_queues(struct dlb2_hw *hw,
+				  struct dlb2_function_resources *rsrcs,
+				  struct dlb2_hw_domain *domain,
+				  u32 num_queues,
+				  struct dlb2_cmd_response *resp)
 {
-	u32 num_avail_ldb_ports, req_ldb_ports;
-	struct dlb2_bitmap *avail_hl_entries;
-	unsigned int max_contig_hl_range;
-	struct dlb2_hw_domain *domain;
-	int i;
-
-	avail_hl_entries = rsrcs->avail_hist_list_entries;
-
-	max_contig_hl_range = dlb2_bitmap_longest_set_range(avail_hl_entries);
-
-	num_avail_ldb_ports = 0;
-	req_ldb_ports = 0;
-	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
-		num_avail_ldb_ports += rsrcs->num_avail_ldb_ports[i];
-
-		req_ldb_ports += args->num_cos_ldb_ports[i];
-	}
-
-	req_ldb_ports += args->num_ldb_ports;
+	unsigned int i;
 
-	if (rsrcs->num_avail_domains < 1) {
-		resp->status = DLB2_ST_DOMAIN_UNAVAILABLE;
+	if (rsrcs->num_avail_ldb_queues < num_queues) {
+		resp->status = DLB2_ST_LDB_QUEUES_UNAVAILABLE;
 		return -EINVAL;
 	}
 
-	domain = DLB2_FUNC_LIST_HEAD(rsrcs->avail_domains, typeof(*domain));
-	if (domain == NULL) {
-		resp->status = DLB2_ST_DOMAIN_UNAVAILABLE;
-		return -EFAULT;
-	}
-
-	if (rsrcs->num_avail_ldb_queues < args->num_ldb_queues) {
-		resp->status = DLB2_ST_LDB_QUEUES_UNAVAILABLE;
-		return -EINVAL;
-	}
-
-	if (req_ldb_ports > num_avail_ldb_ports) {
-		resp->status = DLB2_ST_LDB_PORTS_UNAVAILABLE;
-		return -EINVAL;
-	}
+	for (i = 0; i < num_queues; i++) {
+		struct dlb2_ldb_queue *queue;
 
-	for (i = 0; args->cos_strict && i < DLB2_NUM_COS_DOMAINS; i++) {
-		if (args->num_cos_ldb_ports[i] >
-		    rsrcs->num_avail_ldb_ports[i]) {
-			resp->status = DLB2_ST_LDB_PORTS_UNAVAILABLE;
-			return -EINVAL;
+		queue = DLB2_FUNC_LIST_HEAD(rsrcs->avail_ldb_queues,
+					    typeof(*queue));
+		if (!queue) {
+			DLB2_HW_ERR(hw,
+				    "[%s()] Internal error: domain validation failed\n",
+				    __func__);
+			return -EFAULT;
 		}
-	}
-
-	if (args->num_ldb_queues > 0 && req_ldb_ports == 0) {
-		resp->status = DLB2_ST_LDB_PORT_REQUIRED_FOR_LDB_QUEUES;
-		return -EINVAL;
-	}
 
-	if (rsrcs->num_avail_dir_pq_pairs < args->num_dir_ports) {
-		resp->status = DLB2_ST_DIR_PORTS_UNAVAILABLE;
-		return -EINVAL;
-	}
-	if (hw->ver == DLB2_HW_V2_5) {
-		if (rsrcs->num_avail_entries < args->num_credits) {
-			resp->status = DLB2_ST_CREDITS_UNAVAILABLE;
-			return -EINVAL;
-		}
-	} else {
-		if (rsrcs->num_avail_qed_entries < args->num_ldb_credits) {
-			resp->status = DLB2_ST_LDB_CREDITS_UNAVAILABLE;
-			return -EINVAL;
-		}
-		if (rsrcs->num_avail_dqed_entries < args->num_dir_credits) {
-			resp->status = DLB2_ST_DIR_CREDITS_UNAVAILABLE;
-			return -EINVAL;
-		}
-	}
+		dlb2_list_del(&rsrcs->avail_ldb_queues, &queue->func_list);
 
-	if (rsrcs->num_avail_aqed_entries < args->num_atomic_inflights) {
-		resp->status = DLB2_ST_ATOMIC_INFLIGHTS_UNAVAILABLE;
-		return -EINVAL;
-	}
+		queue->domain_id = domain->id;
+		queue->owned = true;
 
-	if (max_contig_hl_range < args->num_hist_list_entries) {
-		resp->status = DLB2_ST_HIST_LIST_ENTRIES_UNAVAILABLE;
-		return -EINVAL;
+		dlb2_list_add(&domain->avail_ldb_queues, &queue->domain_list);
 	}
 
-	*out_domain = domain;
+	rsrcs->num_avail_ldb_queues -= num_queues;
 
 	return 0;
 }
 
-static void
-dlb2_log_create_sched_domain_args(struct dlb2_hw *hw,
-				  struct dlb2_create_sched_domain_args *args,
-				  bool vdev_req,
-				  unsigned int vdev_id)
-{
-	DLB2_HW_DBG(hw, "DLB2 create sched domain arguments:\n");
-	if (vdev_req)
-		DLB2_HW_DBG(hw, "(Request from vdev %d)\n", vdev_id);
-	DLB2_HW_DBG(hw, "\tNumber of LDB queues:          %d\n",
-		    args->num_ldb_queues);
-	DLB2_HW_DBG(hw, "\tNumber of LDB ports (any CoS): %d\n",
-		    args->num_ldb_ports);
-	DLB2_HW_DBG(hw, "\tNumber of LDB ports (CoS 0):   %d\n",
-		    args->num_cos_ldb_ports[0]);
-	DLB2_HW_DBG(hw, "\tNumber of LDB ports (CoS 1):   %d\n",
-		    args->num_cos_ldb_ports[1]);
-	DLB2_HW_DBG(hw, "\tNumber of LDB ports (CoS 2):   %d\n",
-		    args->num_cos_ldb_ports[2]);
-	DLB2_HW_DBG(hw, "\tNumber of LDB ports (CoS 3):   %d\n",
-		    args->num_cos_ldb_ports[3]);
-	DLB2_HW_DBG(hw, "\tStrict CoS allocation:         %d\n",
-		    args->cos_strict);
-	DLB2_HW_DBG(hw, "\tNumber of DIR ports:           %d\n",
-		    args->num_dir_ports);
-	DLB2_HW_DBG(hw, "\tNumber of ATM inflights:       %d\n",
-		    args->num_atomic_inflights);
-	DLB2_HW_DBG(hw, "\tNumber of hist list entries:   %d\n",
-		    args->num_hist_list_entries);
-	if (hw->ver == DLB2_HW_V2) {
-		DLB2_HW_DBG(hw, "\tNumber of LDB credits:         %d\n",
-			    args->num_ldb_credits);
-		DLB2_HW_DBG(hw, "\tNumber of DIR credits:         %d\n",
-			    args->num_dir_credits);
-	} else {
-		DLB2_HW_DBG(hw, "\tNumber of credits:         %d\n",
-			    args->num_credits);
-	}
-}
-
-/**
- * dlb2_hw_create_sched_domain() - create a scheduling domain
- * @hw: dlb2_hw handle for a particular device.
- * @args: scheduling domain creation arguments.
- * @resp: response structure.
- * @vdev_req: indicates whether this request came from a vdev.
- * @vdev_id: If vdev_req is true, this contains the vdev's ID.
- *
- * This function creates a scheduling domain containing the resources specified
- * in args. The individual resources (queues, ports, credits) can be configured
- * after creating a scheduling domain.
- *
- * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
- * device.
- *
- * Return:
- * Returns 0 upon success, < 0 otherwise. If an error occurs, resp->status is
- * assigned a detailed error code from enum dlb2_error. If successful, resp->id
- * contains the domain ID.
- *
- * resp->id contains a virtual ID if vdev_req is true.
- *
- * Errors:
- * EINVAL - A requested resource is unavailable, or the requested domain name
- *	    is already in use.
- * EFAULT - Internal error (resp->status not set).
- */
-int dlb2_hw_create_sched_domain(struct dlb2_hw *hw,
-				struct dlb2_create_sched_domain_args *args,
-				struct dlb2_cmd_response *resp,
-				bool vdev_req,
-				unsigned int vdev_id)
+static struct dlb2_ldb_port *
+dlb2_get_next_ldb_port(struct dlb2_hw *hw,
+		       struct dlb2_function_resources *rsrcs,
+		       u32 domain_id,
+		       u32 cos_id)
 {
-	struct dlb2_function_resources *rsrcs;
-	struct dlb2_hw_domain *domain;
-	int ret;
-
-	rsrcs = (vdev_req) ? &hw->vdev[vdev_id] : &hw->pf;
-
-	dlb2_log_create_sched_domain_args(hw, args, vdev_req, vdev_id);
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_ldb_port *port;
 
 	/*
-	 * Verify that hardware resources are available before attempting to
-	 * satisfy the request. This simplifies the error unwinding code.
+	 * To reduce the odds of consecutive load-balanced ports mapping to the
+	 * same queue(s), the driver attempts to allocate ports whose neighbors
+	 * are owned by a different domain.
 	 */
-	ret = dlb2_verify_create_sched_dom_args(rsrcs, args, resp, hw, &domain);
-	if (ret)
-		return ret;
-
-	dlb2_init_domain_rsrc_lists(domain);
-
-	ret = dlb2_domain_attach_resources(hw, rsrcs, domain, args, resp);
-	if (ret) {
-		DLB2_HW_ERR(hw,
-			    "[%s()] Internal error: failed to verify args.\n",
-			    __func__);
+	DLB2_FUNC_LIST_FOR(rsrcs->avail_ldb_ports[cos_id], port, iter) {
+		u32 next, prev;
+		u32 phys_id;
 
-		return ret;
-	}
+		phys_id = port->id.phys_id;
+		next = phys_id + 1;
+		prev = phys_id - 1;
 
-	dlb2_list_del(&rsrcs->avail_domains, &domain->func_list);
+		if (phys_id == DLB2_MAX_NUM_LDB_PORTS - 1)
+			next = 0;
+		if (phys_id == 0)
+			prev = DLB2_MAX_NUM_LDB_PORTS - 1;
 
-	dlb2_list_add(&rsrcs->used_domains, &domain->func_list);
+		if (!hw->rsrcs.ldb_ports[next].owned ||
+		    hw->rsrcs.ldb_ports[next].domain_id.phys_id == domain_id)
+			continue;
 
-	resp->id = (vdev_req) ? domain->id.virt_id : domain->id.phys_id;
-	resp->status = 0;
+		if (!hw->rsrcs.ldb_ports[prev].owned ||
+		    hw->rsrcs.ldb_ports[prev].domain_id.phys_id == domain_id)
+			continue;
 
-	return 0;
-}
+		return port;
+	}
 
-static void dlb2_dir_port_cq_disable(struct dlb2_hw *hw,
-				     struct dlb2_dir_pq_pair *port)
-{
-	u32 reg = 0;
+	/*
+	 * Failing that, the driver looks for a port with one neighbor owned by
+	 * a different domain and the other unallocated.
+	 */
+	DLB2_FUNC_LIST_FOR(rsrcs->avail_ldb_ports[cos_id], port, iter) {
+		u32 next, prev;
+		u32 phys_id;
 
-	DLB2_BIT_SET(reg, DLB2_LSP_CQ_DIR_DSBL_DISABLED);
-	DLB2_CSR_WR(hw, DLB2_LSP_CQ_DIR_DSBL(hw->ver, port->id.phys_id), reg);
+		phys_id = port->id.phys_id;
+		next = phys_id + 1;
+		prev = phys_id - 1;
 
-	dlb2_flush_csr(hw);
-}
+		if (phys_id == DLB2_MAX_NUM_LDB_PORTS - 1)
+			next = 0;
+		if (phys_id == 0)
+			prev = DLB2_MAX_NUM_LDB_PORTS - 1;
 
-static u32 dlb2_dir_cq_token_count(struct dlb2_hw *hw,
-				   struct dlb2_dir_pq_pair *port)
-{
-	u32 cnt;
+		if (!hw->rsrcs.ldb_ports[prev].owned &&
+		    hw->rsrcs.ldb_ports[next].owned &&
+		    hw->rsrcs.ldb_ports[next].domain_id.phys_id != domain_id)
+			return port;
 
-	cnt = DLB2_CSR_RD(hw,
-			  DLB2_LSP_CQ_DIR_TKN_CNT(hw->ver, port->id.phys_id));
+		if (!hw->rsrcs.ldb_ports[next].owned &&
+		    hw->rsrcs.ldb_ports[prev].owned &&
+		    hw->rsrcs.ldb_ports[prev].domain_id.phys_id != domain_id)
+			return port;
+	}
 
 	/*
-	 * Account for the initial token count, which is used in order to
-	 * provide a CQ with depth less than 8.
+	 * Failing that, the driver looks for a port with both neighbors
+	 * unallocated.
 	 */
+	DLB2_FUNC_LIST_FOR(rsrcs->avail_ldb_ports[cos_id], port, iter) {
+		u32 next, prev;
+		u32 phys_id;
 
-	return DLB2_BITS_GET(cnt, DLB2_LSP_CQ_DIR_TKN_CNT_COUNT) -
-	       port->init_tkn_cnt;
-}
+		phys_id = port->id.phys_id;
+		next = phys_id + 1;
+		prev = phys_id - 1;
 
-static int dlb2_drain_dir_cq(struct dlb2_hw *hw,
-			      struct dlb2_dir_pq_pair *port)
-{
-	unsigned int port_id = port->id.phys_id;
-	u32 cnt;
+		if (phys_id == DLB2_MAX_NUM_LDB_PORTS - 1)
+			next = 0;
+		if (phys_id == 0)
+			prev = DLB2_MAX_NUM_LDB_PORTS - 1;
 
-	/* Return any outstanding tokens */
-	cnt = dlb2_dir_cq_token_count(hw, port);
+		if (!hw->rsrcs.ldb_ports[prev].owned &&
+		    !hw->rsrcs.ldb_ports[next].owned)
+			return port;
+	}
 
-	if (cnt != 0) {
-		struct dlb2_hcw hcw_mem[8], *hcw;
-		void __iomem *pp_addr;
+	/* If all else fails, the driver returns the next available port. */
+	return DLB2_FUNC_LIST_HEAD(rsrcs->avail_ldb_ports[cos_id],
+				   typeof(*port));
+}
 
-		pp_addr = os_map_producer_port(hw, port_id, false);
+static int __dlb2_attach_ldb_ports(struct dlb2_hw *hw,
+				   struct dlb2_function_resources *rsrcs,
+				   struct dlb2_hw_domain *domain,
+				   u32 num_ports,
+				   u32 cos_id,
+				   struct dlb2_cmd_response *resp)
+{
+	unsigned int i;
 
-		/* Point hcw to a 64B-aligned location */
-		hcw = (struct dlb2_hcw *)((uintptr_t)&hcw_mem[4] & ~0x3F);
+	if (rsrcs->num_avail_ldb_ports[cos_id] < num_ports) {
+		resp->status = DLB2_ST_LDB_PORTS_UNAVAILABLE;
+		return -EINVAL;
+	}
 
-		/*
-		 * Program the first HCW for a batch token return and
-		 * the rest as NOOPS
-		 */
-		memset(hcw, 0, 4 * sizeof(*hcw));
-		hcw->cq_token = 1;
-		hcw->lock_id = cnt - 1;
+	for (i = 0; i < num_ports; i++) {
+		struct dlb2_ldb_port *port;
+
+		port = dlb2_get_next_ldb_port(hw, rsrcs,
+					      domain->id.phys_id, cos_id);
+		if (!port) {
+			DLB2_HW_ERR(hw,
+				    "[%s()] Internal error: domain validation failed\n",
+				    __func__);
+			return -EFAULT;
+		}
 
-		dlb2_movdir64b(pp_addr, hcw);
+		dlb2_list_del(&rsrcs->avail_ldb_ports[cos_id],
+			      &port->func_list);
 
-		os_fence_hcw(hw, pp_addr);
+		port->domain_id = domain->id;
+		port->owned = true;
 
-		os_unmap_producer_port(hw, pp_addr);
+		dlb2_list_add(&domain->avail_ldb_ports[cos_id],
+			      &port->domain_list);
 	}
 
-	return cnt;
+	rsrcs->num_avail_ldb_ports[cos_id] -= num_ports;
+
+	return 0;
 }
 
-static void dlb2_dir_port_cq_enable(struct dlb2_hw *hw,
-				    struct dlb2_dir_pq_pair *port)
+static int dlb2_attach_ldb_ports(struct dlb2_hw *hw,
+				 struct dlb2_function_resources *rsrcs,
+				 struct dlb2_hw_domain *domain,
+				 struct dlb2_create_sched_domain_args *args,
+				 struct dlb2_cmd_response *resp)
 {
-	u32 reg = 0;
-
-	DLB2_CSR_WR(hw, DLB2_LSP_CQ_DIR_DSBL(hw->ver, port->id.phys_id), reg);
+	unsigned int i, j;
+	int ret;
 
-	dlb2_flush_csr(hw);
-}
+	if (args->cos_strict) {
+		for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
+			u32 num = args->num_cos_ldb_ports[i];
 
-static int dlb2_domain_drain_dir_cqs(struct dlb2_hw *hw,
-				     struct dlb2_hw_domain *domain,
-				     bool toggle_port)
-{
-	struct dlb2_list_entry *iter;
-	struct dlb2_dir_pq_pair *port;
-	int drain_cnt = 0;
-	RTE_SET_USED(iter);
+			/* Allocate ports from specific classes-of-service */
+			ret = __dlb2_attach_ldb_ports(hw,
+						      rsrcs,
+						      domain,
+						      num,
+						      i,
+						      resp);
+			if (ret)
+				return ret;
+		}
+	} else {
+		unsigned int k;
+		u32 cos_id;
 
-	DLB2_DOM_LIST_FOR(domain->used_dir_pq_pairs, port, iter) {
 		/*
-		 * Can't drain a port if it's not configured, and there's
-		 * nothing to drain if its queue is unconfigured.
+		 * Attempt to allocate from specific class-of-service, but
+		 * fallback to the other classes if that fails.
 		 */
-		if (!port->port_configured || !port->queue_configured)
-			continue;
-
-		if (toggle_port)
-			dlb2_dir_port_cq_disable(hw, port);
+		for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
+			for (j = 0; j < args->num_cos_ldb_ports[i]; j++) {
+				for (k = 0; k < DLB2_NUM_COS_DOMAINS; k++) {
+					cos_id = (i + k) % DLB2_NUM_COS_DOMAINS;
 
-		drain_cnt = dlb2_drain_dir_cq(hw, port);
+					ret = __dlb2_attach_ldb_ports(hw,
+								      rsrcs,
+								      domain,
+								      1,
+								      cos_id,
+								      resp);
+					if (ret == 0)
+						break;
+				}
 
-		if (toggle_port)
-			dlb2_dir_port_cq_enable(hw, port);
+				if (ret)
+					return ret;
+			}
+		}
 	}
 
-	return drain_cnt;
-}
-
-static u32 dlb2_dir_queue_depth(struct dlb2_hw *hw,
-				struct dlb2_dir_pq_pair *queue)
-{
-	u32 cnt;
+	/* Allocate num_ldb_ports from any class-of-service */
+	for (i = 0; i < args->num_ldb_ports; i++) {
+		for (j = 0; j < DLB2_NUM_COS_DOMAINS; j++) {
+			/* Allocate from best performing cos */
+			u32 cos_idx = j + DLB2_MAX_NUM_LDB_PORTS;
+			u32 cos_id = hw->ldb_pp_allocations[cos_idx];
 
-	cnt = DLB2_CSR_RD(hw, DLB2_LSP_QID_DIR_ENQUEUE_CNT(hw->ver,
-						      queue->id.phys_id));
+			ret = __dlb2_attach_ldb_ports(hw,
+						      rsrcs,
+						      domain,
+						      1,
+						      cos_id,
+						      resp);
+			if (ret == 0)
+				break;
+		}
 
-	return DLB2_BITS_GET(cnt, DLB2_LSP_QID_DIR_ENQUEUE_CNT_COUNT);
-}
+		if (ret)
+			return ret;
+	}
 
-static bool dlb2_dir_queue_is_empty(struct dlb2_hw *hw,
-				    struct dlb2_dir_pq_pair *queue)
-{
-	return dlb2_dir_queue_depth(hw, queue) == 0;
+	return 0;
 }
 
-static bool dlb2_domain_dir_queues_empty(struct dlb2_hw *hw,
-					 struct dlb2_hw_domain *domain)
+static int dlb2_attach_dir_ports(struct dlb2_hw *hw,
+				 struct dlb2_function_resources *rsrcs,
+				 struct dlb2_hw_domain *domain,
+				 u32 num_ports,
+				 struct dlb2_cmd_response *resp)
 {
-	struct dlb2_list_entry *iter;
-	struct dlb2_dir_pq_pair *queue;
-	RTE_SET_USED(iter);
+	int num_res = hw->num_prod_cores;
+	unsigned int i;
 
-	DLB2_DOM_LIST_FOR(domain->used_dir_pq_pairs, queue, iter) {
-		if (!dlb2_dir_queue_is_empty(hw, queue))
-			return false;
+	if (rsrcs->num_avail_dir_pq_pairs < num_ports) {
+		resp->status = DLB2_ST_DIR_PORTS_UNAVAILABLE;
+		return -EINVAL;
 	}
 
-	return true;
-}
-static int dlb2_domain_drain_dir_queues(struct dlb2_hw *hw,
-					struct dlb2_hw_domain *domain)
-{
-	int i;
-
-	/* If the domain hasn't been started, there's no traffic to drain */
-	if (!domain->started)
-		return 0;
-
-	for (i = 0; i < DLB2_MAX_QID_EMPTY_CHECK_LOOPS; i++) {
-		int drain_cnt;
-
-		drain_cnt = dlb2_domain_drain_dir_cqs(hw, domain, false);
-
-		if (dlb2_domain_dir_queues_empty(hw, domain))
-			break;
+	for (i = 0; i < num_ports; i++) {
+		struct dlb2_dir_pq_pair *port;
 
-		/*
-		 * Allow time for DLB to schedule QEs before draining
-		 * the CQs again.
-		 */
-		if (!drain_cnt)
-			rte_delay_us(1);
+		port = DLB2_FUNC_LIST_HEAD(rsrcs->avail_dir_pq_pairs,
+					   typeof(*port));
+		if (!port) {
+			DLB2_HW_ERR(hw,
+				    "[%s()] Internal error: domain validation failed\n",
+				    __func__);
+			return -EFAULT;
+		}
+		if (num_res) {
+			dlb2_list_add(&domain->rsvd_dir_pq_pairs,
+				      &port->domain_list);
+			num_res--;
+		} else {
+			dlb2_list_add(&domain->avail_dir_pq_pairs,
+				      &port->domain_list);
+		}
+		dlb2_list_del(&rsrcs->avail_dir_pq_pairs, &port->func_list);
 
-	}
+		port->domain_id = domain->id;
+		port->owned = true;
 
-	if (i == DLB2_MAX_QID_EMPTY_CHECK_LOOPS) {
-		DLB2_HW_ERR(hw,
-			    "[%s()] Internal error: failed to empty queues\n",
-			    __func__);
-		return -EFAULT;
 	}
 
-	/*
-	 * Drain the CQs one more time. For the queues to go empty, they would
-	 * have scheduled one or more QEs.
-	 */
-	dlb2_domain_drain_dir_cqs(hw, domain, true);
+	rsrcs->num_avail_dir_pq_pairs -= num_ports;
 
 	return 0;
 }
 
-static void dlb2_ldb_port_cq_enable(struct dlb2_hw *hw,
-				    struct dlb2_ldb_port *port)
+static int dlb2_attach_ldb_credits(struct dlb2_function_resources *rsrcs,
+				   struct dlb2_hw_domain *domain,
+				   u32 num_credits,
+				   struct dlb2_cmd_response *resp)
 {
-	u32 reg = 0;
-
-	/*
-	 * Don't re-enable the port if a removal is pending. The caller should
-	 * mark this port as enabled (if it isn't already), and when the
-	 * removal completes the port will be enabled.
-	 */
-	if (port->num_pending_removals)
-		return;
-
-	DLB2_CSR_WR(hw, DLB2_LSP_CQ_LDB_DSBL(hw->ver, port->id.phys_id), reg);
+	if (rsrcs->num_avail_qed_entries < num_credits) {
+		resp->status = DLB2_ST_LDB_CREDITS_UNAVAILABLE;
+		return -EINVAL;
+	}
 
-	dlb2_flush_csr(hw);
+	rsrcs->num_avail_qed_entries -= num_credits;
+	domain->num_ldb_credits += num_credits;
+	return 0;
 }
 
-static void dlb2_ldb_port_cq_disable(struct dlb2_hw *hw,
-				     struct dlb2_ldb_port *port)
+static int dlb2_attach_dir_credits(struct dlb2_function_resources *rsrcs,
+				   struct dlb2_hw_domain *domain,
+				   u32 num_credits,
+				   struct dlb2_cmd_response *resp)
 {
-	u32 reg = 0;
+	if (rsrcs->num_avail_dqed_entries < num_credits) {
+		resp->status = DLB2_ST_DIR_CREDITS_UNAVAILABLE;
+		return -EINVAL;
+	}
 
-	DLB2_BIT_SET(reg, DLB2_LSP_CQ_LDB_DSBL_DISABLED);
-	DLB2_CSR_WR(hw, DLB2_LSP_CQ_LDB_DSBL(hw->ver, port->id.phys_id), reg);
-
-	dlb2_flush_csr(hw);
+	rsrcs->num_avail_dqed_entries -= num_credits;
+	domain->num_dir_credits += num_credits;
+	return 0;
 }
 
-static u32 dlb2_ldb_cq_inflight_count(struct dlb2_hw *hw,
-				      struct dlb2_ldb_port *port)
+static int dlb2_attach_credits(struct dlb2_function_resources *rsrcs,
+			       struct dlb2_hw_domain *domain,
+			       u32 num_credits,
+			       struct dlb2_cmd_response *resp)
 {
-	u32 cnt;
-
-	cnt = DLB2_CSR_RD(hw,
-			  DLB2_LSP_CQ_LDB_INFL_CNT(hw->ver, port->id.phys_id));
+	if (rsrcs->num_avail_entries < num_credits) {
+		resp->status = DLB2_ST_LDB_CREDITS_UNAVAILABLE;
+		return -EINVAL;
+	}
 
-	return DLB2_BITS_GET(cnt, DLB2_LSP_CQ_LDB_INFL_CNT_COUNT);
+	rsrcs->num_avail_entries -= num_credits;
+	domain->num_credits += num_credits;
+	return 0;
 }
 
-static u32 dlb2_ldb_cq_token_count(struct dlb2_hw *hw,
-				   struct dlb2_ldb_port *port)
+static int dlb2_attach_atomic_inflights(struct dlb2_function_resources *rsrcs,
+					struct dlb2_hw_domain *domain,
+					u32 num_atomic_inflights,
+					struct dlb2_cmd_response *resp)
 {
-	u32 cnt;
-
-	cnt = DLB2_CSR_RD(hw,
-			  DLB2_LSP_CQ_LDB_TKN_CNT(hw->ver, port->id.phys_id));
-
-	/*
-	 * Account for the initial token count, which is used in order to
-	 * provide a CQ with depth less than 8.
-	 */
+	if (rsrcs->num_avail_aqed_entries < num_atomic_inflights) {
+		resp->status = DLB2_ST_ATOMIC_INFLIGHTS_UNAVAILABLE;
+		return -EINVAL;
+	}
 
-	return DLB2_BITS_GET(cnt, DLB2_LSP_CQ_LDB_TKN_CNT_TOKEN_COUNT) -
-		port->init_tkn_cnt;
+	rsrcs->num_avail_aqed_entries -= num_atomic_inflights;
+	domain->num_avail_aqed_entries += num_atomic_inflights;
+	return 0;
 }
 
-static int dlb2_drain_ldb_cq(struct dlb2_hw *hw, struct dlb2_ldb_port *port)
+static int
+dlb2_attach_domain_hist_list_entries(struct dlb2_function_resources *rsrcs,
+				     struct dlb2_hw_domain *domain,
+				     u32 num_hist_list_entries,
+				     struct dlb2_cmd_response *resp)
 {
-	u32 infl_cnt, tkn_cnt;
-	unsigned int i;
-
-	infl_cnt = dlb2_ldb_cq_inflight_count(hw, port);
-	tkn_cnt = dlb2_ldb_cq_token_count(hw, port);
-
-	if (infl_cnt || tkn_cnt) {
-		struct dlb2_hcw hcw_mem[8], *hcw;
-		void __iomem *pp_addr;
-
-		pp_addr = os_map_producer_port(hw, port->id.phys_id, true);
-
-		/* Point hcw to a 64B-aligned location */
-		hcw = (struct dlb2_hcw *)((uintptr_t)&hcw_mem[4] & ~0x3F);
-
-		/*
-		 * Program the first HCW for a completion and token return and
-		 * the other HCWs as NOOPS
-		 */
-
-		memset(hcw, 0, 4 * sizeof(*hcw));
-		hcw->qe_comp = (infl_cnt > 0);
-		hcw->cq_token = (tkn_cnt > 0);
-		hcw->lock_id = tkn_cnt - 1;
-
-		/* Return tokens in the first HCW */
-		dlb2_movdir64b(pp_addr, hcw);
+	struct dlb2_bitmap *bitmap;
+	int base;
 
-		hcw->cq_token = 0;
+	if (num_hist_list_entries) {
+		bitmap = rsrcs->avail_hist_list_entries;
 
-		/* Issue remaining completions (if any) */
-		for (i = 1; i < infl_cnt; i++)
-			dlb2_movdir64b(pp_addr, hcw);
+		base = dlb2_bitmap_find_set_bit_range(bitmap,
+						      num_hist_list_entries);
+		if (base < 0)
+			goto error;
 
-		os_fence_hcw(hw, pp_addr);
+		domain->total_hist_list_entries = num_hist_list_entries;
+		domain->avail_hist_list_entries = num_hist_list_entries;
+		domain->hist_list_entry_base = base;
+		domain->hist_list_entry_offset = 0;
 
-		os_unmap_producer_port(hw, pp_addr);
+		dlb2_bitmap_clear_range(bitmap, base, num_hist_list_entries);
 	}
+	return 0;
 
-	return tkn_cnt;
+error:
+	resp->status = DLB2_ST_HIST_LIST_ENTRIES_UNAVAILABLE;
+	return -EINVAL;
 }
 
-static int dlb2_domain_drain_ldb_cqs(struct dlb2_hw *hw,
-				      struct dlb2_hw_domain *domain,
-				      bool toggle_port)
+static int
+dlb2_verify_create_sched_dom_args(struct dlb2_function_resources *rsrcs,
+				  struct dlb2_create_sched_domain_args *args,
+				  struct dlb2_cmd_response *resp,
+				  struct dlb2_hw *hw,
+				  struct dlb2_hw_domain **out_domain)
 {
-	struct dlb2_list_entry *iter;
-	struct dlb2_ldb_port *port;
-	int drain_cnt = 0;
+	u32 num_avail_ldb_ports, req_ldb_ports;
+	struct dlb2_bitmap *avail_hl_entries;
+	unsigned int max_contig_hl_range;
+	struct dlb2_hw_domain *domain;
 	int i;
-	RTE_SET_USED(iter);
 
-	/* If the domain hasn't been started, there's no traffic to drain */
-	if (!domain->started)
-		return 0;
+	avail_hl_entries = rsrcs->avail_hist_list_entries;
+
+	max_contig_hl_range = dlb2_bitmap_longest_set_range(avail_hl_entries);
 
+	num_avail_ldb_ports = 0;
+	req_ldb_ports = 0;
 	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
-		DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i], port, iter) {
-			if (toggle_port)
-				dlb2_ldb_port_cq_disable(hw, port);
+		num_avail_ldb_ports += rsrcs->num_avail_ldb_ports[i];
 
-			drain_cnt = dlb2_drain_ldb_cq(hw, port);
+		req_ldb_ports += args->num_cos_ldb_ports[i];
+	}
 
-			if (toggle_port)
-				dlb2_ldb_port_cq_enable(hw, port);
-		}
+	req_ldb_ports += args->num_ldb_ports;
+
+	if (rsrcs->num_avail_domains < 1) {
+		resp->status = DLB2_ST_DOMAIN_UNAVAILABLE;
+		return -EINVAL;
 	}
 
-	return drain_cnt;
-}
+	domain = DLB2_FUNC_LIST_HEAD(rsrcs->avail_domains, typeof(*domain));
+	if (!domain) {
+		resp->status = DLB2_ST_DOMAIN_UNAVAILABLE;
+		return -EFAULT;
+	}
 
-static u32 dlb2_ldb_queue_depth(struct dlb2_hw *hw,
-				struct dlb2_ldb_queue *queue)
-{
-	u32 aqed, ldb, atm;
+	if (rsrcs->num_avail_ldb_queues < args->num_ldb_queues) {
+		resp->status = DLB2_ST_LDB_QUEUES_UNAVAILABLE;
+		return -EINVAL;
+	}
 
-	aqed = DLB2_CSR_RD(hw, DLB2_LSP_QID_AQED_ACTIVE_CNT(hw->ver,
-						       queue->id.phys_id));
-	ldb = DLB2_CSR_RD(hw, DLB2_LSP_QID_LDB_ENQUEUE_CNT(hw->ver,
-						      queue->id.phys_id));
-	atm = DLB2_CSR_RD(hw,
-			  DLB2_LSP_QID_ATM_ACTIVE(hw->ver, queue->id.phys_id));
+	if (req_ldb_ports > num_avail_ldb_ports) {
+		resp->status = DLB2_ST_LDB_PORTS_UNAVAILABLE;
+		return -EINVAL;
+	}
 
-	return DLB2_BITS_GET(aqed, DLB2_LSP_QID_AQED_ACTIVE_CNT_COUNT)
-	       + DLB2_BITS_GET(ldb, DLB2_LSP_QID_LDB_ENQUEUE_CNT_COUNT)
-	       + DLB2_BITS_GET(atm, DLB2_LSP_QID_ATM_ACTIVE_COUNT);
-}
+	for (i = 0; args->cos_strict && i < DLB2_NUM_COS_DOMAINS; i++) {
+		if (args->num_cos_ldb_ports[i] >
+		    rsrcs->num_avail_ldb_ports[i]) {
+			resp->status = DLB2_ST_LDB_PORTS_UNAVAILABLE;
+			return -EINVAL;
+		}
+	}
 
-static bool dlb2_ldb_queue_is_empty(struct dlb2_hw *hw,
-				    struct dlb2_ldb_queue *queue)
-{
-	return dlb2_ldb_queue_depth(hw, queue) == 0;
-}
+	if (args->num_ldb_queues > 0 && req_ldb_ports == 0) {
+		resp->status = DLB2_ST_LDB_PORT_REQUIRED_FOR_LDB_QUEUES;
+		return -EINVAL;
+	}
 
-static bool dlb2_domain_mapped_queues_empty(struct dlb2_hw *hw,
-					    struct dlb2_hw_domain *domain)
-{
-	struct dlb2_list_entry *iter;
-	struct dlb2_ldb_queue *queue;
-	RTE_SET_USED(iter);
+	if (rsrcs->num_avail_dir_pq_pairs < args->num_dir_ports) {
+		resp->status = DLB2_ST_DIR_PORTS_UNAVAILABLE;
+		return -EINVAL;
+	}
+	if (hw->ver == DLB2_HW_V2_5) {
+		if (rsrcs->num_avail_entries < args->num_credits) {
+			resp->status = DLB2_ST_LDB_CREDITS_UNAVAILABLE;
+			return -EINVAL;
+		}
+	} else {
+		if (rsrcs->num_avail_qed_entries < args->num_ldb_credits) {
+			resp->status = DLB2_ST_LDB_CREDITS_UNAVAILABLE;
+			return -EINVAL;
+		}
+		if (rsrcs->num_avail_dqed_entries < args->num_dir_credits) {
+			resp->status = DLB2_ST_DIR_CREDITS_UNAVAILABLE;
+			return -EINVAL;
+		}
+	}
 
-	DLB2_DOM_LIST_FOR(domain->used_ldb_queues, queue, iter) {
-		if (queue->num_mappings == 0)
-			continue;
+	if (rsrcs->num_avail_aqed_entries < args->num_atomic_inflights) {
+		resp->status = DLB2_ST_ATOMIC_INFLIGHTS_UNAVAILABLE;
+		return -EINVAL;
+	}
 
-		if (!dlb2_ldb_queue_is_empty(hw, queue))
-			return false;
+	if (max_contig_hl_range < args->num_hist_list_entries) {
+		resp->status = DLB2_ST_HIST_LIST_ENTRIES_UNAVAILABLE;
+		return -EINVAL;
 	}
 
-	return true;
+	*out_domain = domain;
+
+	return 0;
 }
 
-static int dlb2_domain_drain_mapped_queues(struct dlb2_hw *hw,
-					   struct dlb2_hw_domain *domain)
+static int
+dlb2_verify_create_ldb_queue_args(struct dlb2_hw *hw,
+				  u32 domain_id,
+				  struct dlb2_create_ldb_queue_args *args,
+				  struct dlb2_cmd_response *resp,
+				  bool vdev_req,
+				  unsigned int vdev_id,
+				  struct dlb2_hw_domain **out_domain,
+				  struct dlb2_ldb_queue **out_queue)
 {
+	struct dlb2_hw_domain *domain;
+	struct dlb2_ldb_queue *queue;
 	int i;
 
-	/* If the domain hasn't been started, there's no traffic to drain */
-	if (!domain->started)
-		return 0;
+	domain = dlb2_get_domain_from_id(hw, domain_id, vdev_req, vdev_id);
 
-	if (domain->num_pending_removals > 0) {
-		DLB2_HW_ERR(hw,
-			    "[%s()] Internal error: failed to unmap domain queues\n",
-			    __func__);
-		return -EFAULT;
+	if (!domain) {
+		resp->status = DLB2_ST_INVALID_DOMAIN_ID;
+		return -EINVAL;
 	}
 
-	for (i = 0; i < DLB2_MAX_QID_EMPTY_CHECK_LOOPS; i++) {
-		int drain_cnt;
-
-		drain_cnt = dlb2_domain_drain_ldb_cqs(hw, domain, false);
+	if (!domain->configured) {
+		resp->status = DLB2_ST_DOMAIN_NOT_CONFIGURED;
+		return -EINVAL;
+	}
 
-		if (dlb2_domain_mapped_queues_empty(hw, domain))
-			break;
-
-		/*
-		 * Allow time for DLB to schedule QEs before draining
-		 * the CQs again.
-		 */
-		if (!drain_cnt)
-			rte_delay_us(1);
+	if (domain->started) {
+		resp->status = DLB2_ST_DOMAIN_STARTED;
+		return -EINVAL;
 	}
 
-	if (i == DLB2_MAX_QID_EMPTY_CHECK_LOOPS) {
-		DLB2_HW_ERR(hw,
-			    "[%s()] Internal error: failed to empty queues\n",
-			    __func__);
-		return -EFAULT;
+	queue = DLB2_DOM_LIST_HEAD(domain->avail_ldb_queues, typeof(*queue));
+	if (!queue) {
+		resp->status = DLB2_ST_LDB_QUEUES_UNAVAILABLE;
+		return -EINVAL;
 	}
 
-	/*
-	 * Drain the CQs one more time. For the queues to go empty, they would
-	 * have scheduled one or more QEs.
-	 */
-	dlb2_domain_drain_ldb_cqs(hw, domain, true);
-
-	return 0;
-}
-
-static void dlb2_domain_enable_ldb_cqs(struct dlb2_hw *hw,
-				       struct dlb2_hw_domain *domain)
-{
-	struct dlb2_list_entry *iter;
-	struct dlb2_ldb_port *port;
-	int i;
-	RTE_SET_USED(iter);
+	if (args->num_sequence_numbers) {
+		for (i = 0; i < DLB2_MAX_NUM_SEQUENCE_NUMBER_GROUPS; i++) {
+			struct dlb2_sn_group *group = &hw->rsrcs.sn_groups[i];
 
-	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
-		DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i], port, iter) {
-			port->enabled = true;
+			if (group->sequence_numbers_per_queue ==
+			    args->num_sequence_numbers &&
+			    !dlb2_sn_group_full(group))
+				break;
+		}
 
-			dlb2_ldb_port_cq_enable(hw, port);
+		if (i == DLB2_MAX_NUM_SEQUENCE_NUMBER_GROUPS) {
+			resp->status = DLB2_ST_SEQUENCE_NUMBERS_UNAVAILABLE;
+			return -EINVAL;
 		}
 	}
-}
-
-static struct dlb2_ldb_queue *
-dlb2_get_ldb_queue_from_id(struct dlb2_hw *hw,
-			   u32 id,
-			   bool vdev_req,
-			   unsigned int vdev_id)
-{
-	struct dlb2_list_entry *iter1;
-	struct dlb2_list_entry *iter2;
-	struct dlb2_function_resources *rsrcs;
-	struct dlb2_hw_domain *domain;
-	struct dlb2_ldb_queue *queue;
-	RTE_SET_USED(iter1);
-	RTE_SET_USED(iter2);
-
-	if (id >= DLB2_MAX_NUM_LDB_QUEUES)
-		return NULL;
 
-	rsrcs = (vdev_req) ? &hw->vdev[vdev_id] : &hw->pf;
+	if (args->num_qid_inflights < 1 || args->num_qid_inflights > 2048) {
+		resp->status = DLB2_ST_INVALID_QID_INFLIGHT_ALLOCATION;
+		return -EINVAL;
+	}
 
-	if (!vdev_req)
-		return &hw->rsrcs.ldb_queues[id];
+	/* Inflights must be <= number of sequence numbers if ordered */
+	if (args->num_sequence_numbers != 0 &&
+	    args->num_qid_inflights > args->num_sequence_numbers) {
+		resp->status = DLB2_ST_INVALID_QID_INFLIGHT_ALLOCATION;
+		return -EINVAL;
+	}
 
-	DLB2_FUNC_LIST_FOR(rsrcs->used_domains, domain, iter1) {
-		DLB2_DOM_LIST_FOR(domain->used_ldb_queues, queue, iter2) {
-			if (queue->id.virt_id == id)
-				return queue;
-		}
+	if (domain->num_avail_aqed_entries < args->num_atomic_inflights) {
+		resp->status = DLB2_ST_ATOMIC_INFLIGHTS_UNAVAILABLE;
+		return -EINVAL;
 	}
 
-	DLB2_FUNC_LIST_FOR(rsrcs->avail_ldb_queues, queue, iter1) {
-		if (queue->id.virt_id == id)
-			return queue;
+	if (args->num_atomic_inflights &&
+	    args->lock_id_comp_level != 0 &&
+	    args->lock_id_comp_level != 64 &&
+	    args->lock_id_comp_level != 128 &&
+	    args->lock_id_comp_level != 256 &&
+	    args->lock_id_comp_level != 512 &&
+	    args->lock_id_comp_level != 1024 &&
+	    args->lock_id_comp_level != 2048 &&
+	    args->lock_id_comp_level != 4096 &&
+	    args->lock_id_comp_level != 65536) {
+		resp->status = DLB2_ST_INVALID_LOCK_ID_COMP_LEVEL;
+		return -EINVAL;
 	}
 
-	return NULL;
+	*out_domain = domain;
+	*out_queue = queue;
+
+	return 0;
 }
 
-static struct dlb2_hw_domain *dlb2_get_domain_from_id(struct dlb2_hw *hw,
-						      u32 id,
-						      bool vdev_req,
-						      unsigned int vdev_id)
+static int
+dlb2_verify_create_dir_queue_args(struct dlb2_hw *hw,
+				  u32 domain_id,
+				  struct dlb2_create_dir_queue_args *args,
+				  struct dlb2_cmd_response *resp,
+				  bool vdev_req,
+				  unsigned int vdev_id,
+				  struct dlb2_hw_domain **out_domain,
+				  struct dlb2_dir_pq_pair **out_queue)
 {
-	struct dlb2_list_entry *iteration;
-	struct dlb2_function_resources *rsrcs;
 	struct dlb2_hw_domain *domain;
-	RTE_SET_USED(iteration);
+	struct dlb2_dir_pq_pair *pq;
 
-	if (id >= DLB2_MAX_NUM_DOMAINS)
-		return NULL;
+	domain = dlb2_get_domain_from_id(hw, domain_id, vdev_req, vdev_id);
 
-	if (!vdev_req)
-		return &hw->domains[id];
+	if (!domain) {
+		resp->status = DLB2_ST_INVALID_DOMAIN_ID;
+		return -EINVAL;
+	}
 
-	rsrcs = &hw->vdev[vdev_id];
+	if (!domain->configured) {
+		resp->status = DLB2_ST_DOMAIN_NOT_CONFIGURED;
+		return -EINVAL;
+	}
 
-	DLB2_FUNC_LIST_FOR(rsrcs->used_domains, domain, iteration) {
-		if (domain->id.virt_id == id)
-			return domain;
+	if (domain->started) {
+		resp->status = DLB2_ST_DOMAIN_STARTED;
+		return -EINVAL;
 	}
 
-	return NULL;
+	/*
+	 * If the user claims the port is already configured, validate the port
+	 * ID, its domain, and whether the port is configured.
+	 */
+	if (args->port_id != -1) {
+		pq = dlb2_get_domain_used_dir_pq(hw,
+						 args->port_id,
+						 vdev_req,
+						 domain);
+
+		if (!pq || pq->domain_id.phys_id != domain->id.phys_id ||
+		    !pq->port_configured) {
+			resp->status = DLB2_ST_INVALID_PORT_ID;
+			return -EINVAL;
+		}
+	} else {
+		/*
+		 * If the queue's port is not configured, validate that a free
+		 * port-queue pair is available.
+		 */
+		pq = DLB2_DOM_LIST_HEAD(domain->avail_dir_pq_pairs,
+					typeof(*pq));
+		if (!pq) {
+			resp->status = DLB2_ST_DIR_QUEUES_UNAVAILABLE;
+			return -EINVAL;
+		}
+	}
+
+	*out_domain = domain;
+	*out_queue = pq;
+
+	return 0;
 }
 
-static int dlb2_port_slot_state_transition(struct dlb2_hw *hw,
-					   struct dlb2_ldb_port *port,
-					   struct dlb2_ldb_queue *queue,
-					   int slot,
-					   enum dlb2_qid_map_state new_state)
+static void dlb2_configure_ldb_queue(struct dlb2_hw *hw,
+				     struct dlb2_hw_domain *domain,
+				     struct dlb2_ldb_queue *queue,
+				     struct dlb2_create_ldb_queue_args *args,
+				     bool vdev_req,
+				     unsigned int vdev_id)
 {
-	enum dlb2_qid_map_state curr_state = port->qid_map[slot].state;
-	struct dlb2_hw_domain *domain;
-	int domain_id;
+	struct dlb2_sn_group *sn_group;
+	unsigned int offs;
+	u32 reg = 0;
+	u32 alimit;
 
-	domain_id = port->domain_id.phys_id;
+	/* QID write permissions are turned on when the domain is started */
+	offs = domain->id.phys_id * DLB2_MAX_NUM_LDB_QUEUES + queue->id.phys_id;
 
-	domain = dlb2_get_domain_from_id(hw, domain_id, false, 0);
-	if (domain == NULL) {
-		DLB2_HW_ERR(hw,
-			    "[%s()] Internal error: unable to find domain %d\n",
-			    __func__, domain_id);
-		return -EINVAL;
-	}
+	DLB2_CSR_WR(hw, DLB2_SYS_LDB_VASQID_V(offs), reg);
 
-	switch (curr_state) {
-	case DLB2_QUEUE_UNMAPPED:
-		switch (new_state) {
-		case DLB2_QUEUE_MAPPED:
-			queue->num_mappings++;
-			port->num_mappings++;
-			break;
-		case DLB2_QUEUE_MAP_IN_PROG:
-			queue->num_pending_additions++;
-			domain->num_pending_additions++;
-			break;
-		default:
-			goto error;
-		}
+	/*
+	 * Unordered QIDs get 4K inflights, ordered get as many as the number
+	 * of sequence numbers.
+	 */
+	DLB2_BITS_SET(reg, args->num_qid_inflights,
+		      DLB2_LSP_QID_LDB_INFL_LIM_LIMIT);
+	DLB2_CSR_WR(hw, DLB2_LSP_QID_LDB_INFL_LIM(hw->ver,
+						  queue->id.phys_id), reg);
+
+	alimit = queue->aqed_limit;
+
+	if (alimit > DLB2_MAX_NUM_AQED_ENTRIES)
+		alimit = DLB2_MAX_NUM_AQED_ENTRIES;
+
+	reg = 0;
+	DLB2_BITS_SET(reg, alimit, DLB2_LSP_QID_AQED_ACTIVE_LIM_LIMIT);
+	DLB2_CSR_WR(hw,
+		    DLB2_LSP_QID_AQED_ACTIVE_LIM(hw->ver,
+						 queue->id.phys_id), reg);
+
+	reg = 0;
+	switch (args->lock_id_comp_level) {
+	case 64:
+		DLB2_BITS_SET(reg, 1, DLB2_AQED_QID_HID_WIDTH_COMPRESS_CODE);
 		break;
-	case DLB2_QUEUE_MAPPED:
-		switch (new_state) {
-		case DLB2_QUEUE_UNMAPPED:
-			queue->num_mappings--;
-			port->num_mappings--;
-			break;
-		case DLB2_QUEUE_UNMAP_IN_PROG:
-			port->num_pending_removals++;
-			domain->num_pending_removals++;
-			break;
-		case DLB2_QUEUE_MAPPED:
-			/* Priority change, nothing to update */
-			break;
-		default:
-			goto error;
-		}
+	case 128:
+		DLB2_BITS_SET(reg, 2, DLB2_AQED_QID_HID_WIDTH_COMPRESS_CODE);
 		break;
-	case DLB2_QUEUE_MAP_IN_PROG:
-		switch (new_state) {
-		case DLB2_QUEUE_UNMAPPED:
-			queue->num_pending_additions--;
-			domain->num_pending_additions--;
-			break;
-		case DLB2_QUEUE_MAPPED:
-			queue->num_mappings++;
-			port->num_mappings++;
-			queue->num_pending_additions--;
-			domain->num_pending_additions--;
-			break;
-		default:
-			goto error;
-		}
+	case 256:
+		DLB2_BITS_SET(reg, 3, DLB2_AQED_QID_HID_WIDTH_COMPRESS_CODE);
 		break;
-	case DLB2_QUEUE_UNMAP_IN_PROG:
-		switch (new_state) {
-		case DLB2_QUEUE_UNMAPPED:
-			port->num_pending_removals--;
-			domain->num_pending_removals--;
-			queue->num_mappings--;
-			port->num_mappings--;
-			break;
-		case DLB2_QUEUE_MAPPED:
-			port->num_pending_removals--;
-			domain->num_pending_removals--;
-			break;
-		case DLB2_QUEUE_UNMAP_IN_PROG_PENDING_MAP:
-			/* Nothing to update */
-			break;
-		default:
-			goto error;
-		}
+	case 512:
+		DLB2_BITS_SET(reg, 4, DLB2_AQED_QID_HID_WIDTH_COMPRESS_CODE);
 		break;
-	case DLB2_QUEUE_UNMAP_IN_PROG_PENDING_MAP:
-		switch (new_state) {
-		case DLB2_QUEUE_UNMAP_IN_PROG:
-			/* Nothing to update */
-			break;
-		case DLB2_QUEUE_UNMAPPED:
-			/*
-			 * An UNMAP_IN_PROG_PENDING_MAP slot briefly
-			 * becomes UNMAPPED before it transitions to
-			 * MAP_IN_PROG.
-			 */
-			queue->num_mappings--;
-			port->num_mappings--;
-			port->num_pending_removals--;
-			domain->num_pending_removals--;
-			break;
-		default:
-			goto error;
-		}
+	case 1024:
+		DLB2_BITS_SET(reg, 5, DLB2_AQED_QID_HID_WIDTH_COMPRESS_CODE);
+		break;
+	case 2048:
+		DLB2_BITS_SET(reg, 6, DLB2_AQED_QID_HID_WIDTH_COMPRESS_CODE);
+		break;
+	case 4096:
+		DLB2_BITS_SET(reg, 7, DLB2_AQED_QID_HID_WIDTH_COMPRESS_CODE);
 		break;
 	default:
-		goto error;
+		/* No compression by default */
+		break;
 	}
 
-	port->qid_map[slot].state = new_state;
+	DLB2_CSR_WR(hw, DLB2_AQED_QID_HID_WIDTH(queue->id.phys_id), reg);
 
-	DLB2_HW_DBG(hw,
-		    "[%s()] queue %d -> port %d state transition (%d -> %d)\n",
-		    __func__, queue->id.phys_id, port->id.phys_id,
-		    curr_state, new_state);
-	return 0;
+	reg = 0;
+	/* Don't timestamp QEs that pass through this queue */
+	DLB2_CSR_WR(hw, DLB2_SYS_LDB_QID_ITS(queue->id.phys_id), reg);
 
-error:
-	DLB2_HW_ERR(hw,
-		    "[%s()] Internal error: invalid queue %d -> port %d state transition (%d -> %d)\n",
-		    __func__, queue->id.phys_id, port->id.phys_id,
-		    curr_state, new_state);
-	return -EFAULT;
-}
+	DLB2_BITS_SET(reg, args->depth_threshold,
+		      DLB2_LSP_QID_ATM_DEPTH_THRSH_THRESH);
+	DLB2_CSR_WR(hw,
+		    DLB2_LSP_QID_ATM_DEPTH_THRSH(hw->ver,
+						 queue->id.phys_id), reg);
 
-static bool dlb2_port_find_slot(struct dlb2_ldb_port *port,
-				enum dlb2_qid_map_state state,
-				int *slot)
-{
-	int i;
+	reg = 0;
+	DLB2_BITS_SET(reg, args->depth_threshold,
+		      DLB2_LSP_QID_NALDB_DEPTH_THRSH_THRESH);
+	DLB2_CSR_WR(hw,
+		    DLB2_LSP_QID_NALDB_DEPTH_THRSH(hw->ver, queue->id.phys_id),
+		    reg);
 
-	for (i = 0; i < DLB2_MAX_NUM_QIDS_PER_LDB_CQ; i++) {
-		if (port->qid_map[i].state == state)
-			break;
-	}
+	/*
+	 * This register limits the number of inflight flows a queue can have
+	 * at one time.  It has an upper bound of 2048, but can be
+	 * over-subscribed. 512 is chosen so that a single queue doesn't use
+	 * the entire atomic storage, but can use a substantial portion if
+	 * needed.
+	 */
+	reg = 0;
+	DLB2_BITS_SET(reg, 512, DLB2_AQED_QID_FID_LIM_QID_FID_LIMIT);
+	DLB2_CSR_WR(hw, DLB2_AQED_QID_FID_LIM(queue->id.phys_id), reg);
 
-	*slot = i;
+	/* Configure SNs */
+	reg = 0;
+	sn_group = &hw->rsrcs.sn_groups[queue->sn_group];
+	DLB2_BITS_SET(reg, sn_group->mode, DLB2_CHP_ORD_QID_SN_MAP_MODE);
+	DLB2_BITS_SET(reg, queue->sn_slot, DLB2_CHP_ORD_QID_SN_MAP_SLOT);
+	DLB2_BITS_SET(reg, sn_group->id, DLB2_CHP_ORD_QID_SN_MAP_GRP);
 
-	return (i < DLB2_MAX_NUM_QIDS_PER_LDB_CQ);
-}
+	DLB2_CSR_WR(hw,
+		    DLB2_CHP_ORD_QID_SN_MAP(hw->ver, queue->id.phys_id), reg);
 
-static bool dlb2_port_find_slot_queue(struct dlb2_ldb_port *port,
-				      enum dlb2_qid_map_state state,
-				      struct dlb2_ldb_queue *queue,
-				      int *slot)
-{
-	int i;
+  /* If queue is atomic, enable FID and if Ordered, enable SN */
+	reg = 0;
+	DLB2_BITS_SET(reg, (args->num_sequence_numbers != 0),
+		 DLB2_SYS_LDB_QID_CFG_V_SN_CFG_V);
+	DLB2_BITS_SET(reg, (args->num_atomic_inflights != 0),
+		 DLB2_SYS_LDB_QID_CFG_V_FID_CFG_V);
 
-	for (i = 0; i < DLB2_MAX_NUM_QIDS_PER_LDB_CQ; i++) {
-		if (port->qid_map[i].state == state &&
-		    port->qid_map[i].qid == queue->id.phys_id)
-			break;
-	}
+	DLB2_CSR_WR(hw, DLB2_SYS_LDB_QID_CFG_V(queue->id.phys_id), reg);
 
-	*slot = i;
+	if (vdev_req) {
+		offs = vdev_id * DLB2_MAX_NUM_LDB_QUEUES + queue->id.virt_id;
 
-	return (i < DLB2_MAX_NUM_QIDS_PER_LDB_CQ);
+		reg = 0;
+		DLB2_BIT_SET(reg, DLB2_SYS_VF_LDB_VQID_V_VQID_V);
+		DLB2_CSR_WR(hw, DLB2_SYS_VF_LDB_VQID_V(offs), reg);
+
+		reg = 0;
+		DLB2_BITS_SET(reg, queue->id.phys_id,
+			      DLB2_SYS_VF_LDB_VQID2QID_QID);
+		DLB2_CSR_WR(hw, DLB2_SYS_VF_LDB_VQID2QID(offs), reg);
+
+		reg = 0;
+		DLB2_BITS_SET(reg, queue->id.virt_id,
+			      DLB2_SYS_LDB_QID2VQID_VQID);
+		DLB2_CSR_WR(hw, DLB2_SYS_LDB_QID2VQID(queue->id.phys_id), reg);
+	}
+
+	reg = 0;
+	DLB2_BIT_SET(reg, DLB2_SYS_LDB_QID_V_QID_V);
+	DLB2_CSR_WR(hw, DLB2_SYS_LDB_QID_V(queue->id.phys_id), reg);
 }
 
-/*
- * dlb2_ldb_queue_{enable, disable}_mapped_cqs() don't operate exactly as
- * their function names imply, and should only be called by the dynamic CQ
- * mapping code.
- */
-static void dlb2_ldb_queue_disable_mapped_cqs(struct dlb2_hw *hw,
-					      struct dlb2_hw_domain *domain,
-					      struct dlb2_ldb_queue *queue)
+static void dlb2_configure_dir_queue(struct dlb2_hw *hw,
+				     struct dlb2_hw_domain *domain,
+				     struct dlb2_dir_pq_pair *queue,
+				     struct dlb2_create_dir_queue_args *args,
+				     bool vdev_req,
+				     unsigned int vdev_id)
 {
-	struct dlb2_list_entry *iter;
-	struct dlb2_ldb_port *port;
-	int slot, i;
-	RTE_SET_USED(iter);
+	unsigned int offs;
+	u32 reg = 0;
 
-	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
-		DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i], port, iter) {
-			enum dlb2_qid_map_state state = DLB2_QUEUE_MAPPED;
+	/* QID write permissions are turned on when the domain is started */
+	offs = domain->id.phys_id * DLB2_MAX_NUM_DIR_QUEUES(hw->ver) +
+		queue->id.phys_id;
 
-			if (!dlb2_port_find_slot_queue(port, state,
-						       queue, &slot))
-				continue;
+	DLB2_CSR_WR(hw, DLB2_SYS_DIR_VASQID_V(offs), reg);
 
-			if (port->enabled)
-				dlb2_ldb_port_cq_disable(hw, port);
-		}
-	}
-}
+	/* Don't timestamp QEs that pass through this queue */
+	DLB2_CSR_WR(hw, DLB2_SYS_DIR_QID_ITS(queue->id.phys_id), reg);
 
-static void dlb2_ldb_queue_enable_mapped_cqs(struct dlb2_hw *hw,
-					     struct dlb2_hw_domain *domain,
-					     struct dlb2_ldb_queue *queue)
-{
-	struct dlb2_list_entry *iter;
-	struct dlb2_ldb_port *port;
-	int slot, i;
-	RTE_SET_USED(iter);
+	reg = 0;
+	DLB2_BITS_SET(reg, args->depth_threshold,
+		      DLB2_LSP_QID_DIR_DEPTH_THRSH_THRESH);
+	DLB2_CSR_WR(hw,
+		    DLB2_LSP_QID_DIR_DEPTH_THRSH(hw->ver, queue->id.phys_id),
+		    reg);
 
-	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
-		DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i], port, iter) {
-			enum dlb2_qid_map_state state = DLB2_QUEUE_MAPPED;
+	if (vdev_req) {
+		offs = vdev_id * DLB2_MAX_NUM_DIR_QUEUES(hw->ver) +
+			queue->id.virt_id;
 
-			if (!dlb2_port_find_slot_queue(port, state,
-						       queue, &slot))
-				continue;
+		reg = 0;
+		DLB2_BIT_SET(reg, DLB2_SYS_VF_DIR_VQID_V_VQID_V);
+		DLB2_CSR_WR(hw, DLB2_SYS_VF_DIR_VQID_V(offs), reg);
 
-			if (port->enabled)
-				dlb2_ldb_port_cq_enable(hw, port);
-		}
+		reg = 0;
+		DLB2_BITS_SET(reg, queue->id.phys_id,
+			      DLB2_SYS_VF_DIR_VQID2QID_QID);
+		DLB2_CSR_WR(hw, DLB2_SYS_VF_DIR_VQID2QID(offs), reg);
 	}
-}
 
-static void dlb2_ldb_port_clear_queue_if_status(struct dlb2_hw *hw,
-						struct dlb2_ldb_port *port,
-						int slot)
-{
-	u32 ctrl = 0;
+	reg = 0;
+	DLB2_BIT_SET(reg, DLB2_SYS_DIR_QID_V_QID_V);
+	DLB2_CSR_WR(hw, DLB2_SYS_DIR_QID_V(queue->id.phys_id), reg);
 
-	DLB2_BITS_SET(ctrl, port->id.phys_id, DLB2_LSP_LDB_SCHED_CTRL_CQ);
-	DLB2_BITS_SET(ctrl, slot, DLB2_LSP_LDB_SCHED_CTRL_QIDIX);
-	DLB2_BIT_SET(ctrl, DLB2_LSP_LDB_SCHED_CTRL_INFLIGHT_OK_V);
+	queue->queue_configured = true;
+}
 
-	DLB2_CSR_WR(hw, DLB2_LSP_LDB_SCHED_CTRL(hw->ver), ctrl);
+static bool
+dlb2_cq_depth_is_valid(u32 depth)
+{
+	if (depth != 1 && depth != 2 &&
+	    depth != 4 && depth != 8 &&
+	    depth != 16 && depth != 32 &&
+	    depth != 64 && depth != 128 &&
+	    depth != 256 && depth != 512 &&
+	    depth != 1024)
+		return false;
 
-	dlb2_flush_csr(hw);
+	return true;
 }
 
-static void dlb2_ldb_port_set_queue_if_status(struct dlb2_hw *hw,
+static int
+dlb2_verify_create_ldb_port_args(struct dlb2_hw *hw,
+				 u32 domain_id,
+				 uintptr_t cq_dma_base,
+				 struct dlb2_create_ldb_port_args *args,
+				 struct dlb2_cmd_response *resp,
+				 bool vdev_req,
+				 unsigned int vdev_id,
+				 struct dlb2_hw_domain **out_domain,
+				 struct dlb2_ldb_port **out_port,
+				 int *out_cos_id)
+{
+	struct dlb2_hw_domain *domain;
+	struct dlb2_ldb_port *port;
+	int i, id;
+
+	domain = dlb2_get_domain_from_id(hw, domain_id, vdev_req, vdev_id);
+
+	if (!domain) {
+		resp->status = DLB2_ST_INVALID_DOMAIN_ID;
+		return -EINVAL;
+	}
+
+	if (!domain->configured) {
+		resp->status = DLB2_ST_DOMAIN_NOT_CONFIGURED;
+		return -EINVAL;
+	}
+
+	if (domain->started) {
+		resp->status = DLB2_ST_DOMAIN_STARTED;
+		return -EINVAL;
+	}
+
+	if (args->cos_id >= DLB2_NUM_COS_DOMAINS &&
+	    (args->cos_id != DLB2_COS_DEFAULT || args->cos_strict)) {
+		resp->status = DLB2_ST_INVALID_COS_ID;
+		return -EINVAL;
+	}
+
+	if (args->cos_strict) {
+		id = args->cos_id;
+		port = DLB2_DOM_LIST_HEAD(domain->avail_ldb_ports[id],
+					  typeof(*port));
+	} else {
+		for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
+			if (args->cos_id == DLB2_COS_DEFAULT) {
+				/* Allocate from best performing cos */
+				u32 cos_idx = i + DLB2_MAX_NUM_LDB_PORTS;
+				id = hw->ldb_pp_allocations[cos_idx];
+			} else {
+				id = (args->cos_id + i) % DLB2_NUM_COS_DOMAINS;
+			}
+
+			port = DLB2_DOM_LIST_HEAD(domain->avail_ldb_ports[id],
+						  typeof(*port));
+			if (port)
+				break;
+		}
+	}
+
+	if (!port) {
+		resp->status = DLB2_ST_LDB_PORTS_UNAVAILABLE;
+		return -EINVAL;
+	}
+
+	DLB2_LOG_INFO(": LDB: cos=%d port:%d\n", id, port->id.phys_id);
+	/* Check cache-line alignment */
+	if ((cq_dma_base & 0x3F) != 0) {
+		resp->status = DLB2_ST_INVALID_CQ_VIRT_ADDR;
+		return -EINVAL;
+	}
+
+	if (!dlb2_cq_depth_is_valid(args->cq_depth)) {
+		resp->status = DLB2_ST_INVALID_CQ_DEPTH;
+		return -EINVAL;
+	}
+
+	/* The history list size must be >= 1 */
+	if (!args->cq_history_list_size) {
+		resp->status = DLB2_ST_INVALID_HIST_LIST_DEPTH;
+		return -EINVAL;
+	}
+
+	if (args->cq_history_list_size > domain->avail_hist_list_entries) {
+		resp->status = DLB2_ST_HIST_LIST_ENTRIES_UNAVAILABLE;
+		return -EINVAL;
+	}
+
+	*out_domain = domain;
+	*out_port = port;
+	*out_cos_id = id;
+
+	return 0;
+}
+
+static int
+dlb2_verify_create_dir_port_args(struct dlb2_hw *hw,
+				 u32 domain_id,
+				 uintptr_t cq_dma_base,
+				 struct dlb2_create_dir_port_args *args,
+				 struct dlb2_cmd_response *resp,
+				 bool vdev_req,
+				 unsigned int vdev_id,
+				 struct dlb2_hw_domain **out_domain,
+				 struct dlb2_dir_pq_pair **out_port)
+{
+	struct dlb2_hw_domain *domain;
+	struct dlb2_dir_pq_pair *pq;
+
+	domain = dlb2_get_domain_from_id(hw, domain_id, vdev_req, vdev_id);
+
+	if (!domain) {
+		resp->status = DLB2_ST_INVALID_DOMAIN_ID;
+		return -EINVAL;
+	}
+
+	if (!domain->configured) {
+		resp->status = DLB2_ST_DOMAIN_NOT_CONFIGURED;
+		return -EINVAL;
+	}
+
+	if (domain->started) {
+		resp->status = DLB2_ST_DOMAIN_STARTED;
+		return -EINVAL;
+	}
+
+	if (args->queue_id != -1) {
+		/*
+		 * If the user claims the queue is already configured, validate
+		 * the queue ID, its domain, and whether the queue is
+		 * configured.
+		 */
+		pq = dlb2_get_domain_used_dir_pq(hw,
+						 args->queue_id,
+						 vdev_req,
+						 domain);
+
+		if (!pq || pq->domain_id.phys_id != domain->id.phys_id ||
+		    !pq->queue_configured) {
+			resp->status = DLB2_ST_INVALID_DIR_QUEUE_ID;
+			return -EINVAL;
+		}
+	} else {
+		/*
+		 * If the port's queue is not configured, validate that a free
+		 * port-queue pair is available.
+		 *
+		 * First try the 'res' list if the port is producer OR if
+		 * 'avail' list is empty else fall back to 'avail' list
+		 */
+		if (!dlb2_list_empty(&domain->rsvd_dir_pq_pairs) &&
+		    (args->is_producer ||
+		     dlb2_list_empty(&domain->avail_dir_pq_pairs)))
+			pq = DLB2_DOM_LIST_HEAD(domain->rsvd_dir_pq_pairs,
+						typeof(*pq));
+		else
+			pq = DLB2_DOM_LIST_HEAD(domain->avail_dir_pq_pairs,
+						typeof(*pq));
+		if (!pq) {
+			resp->status = DLB2_ST_DIR_PORTS_UNAVAILABLE;
+			return -EINVAL;
+		}
+		DLB2_LOG_INFO(": DIR: port:%d is_producer=%d\n", pq->id.phys_id, args->is_producer);
+	}
+
+	/* Check cache-line alignment */
+	if ((cq_dma_base & 0x3F) != 0) {
+		resp->status = DLB2_ST_INVALID_CQ_VIRT_ADDR;
+		return -EINVAL;
+	}
+
+	if (!dlb2_cq_depth_is_valid(args->cq_depth)) {
+		resp->status = DLB2_ST_INVALID_CQ_DEPTH;
+		return -EINVAL;
+	}
+
+	*out_domain = domain;
+	*out_port = pq;
+
+	return 0;
+}
+
+static int dlb2_verify_start_stop_domain_args(struct dlb2_hw *hw,
+					      u32 domain_id,
+					      bool start_domain,
+					      struct dlb2_cmd_response *resp,
+					      bool vdev_req,
+					      unsigned int vdev_id,
+					      struct dlb2_hw_domain **out_domain)
+{
+	struct dlb2_hw_domain *domain;
+
+	domain = dlb2_get_domain_from_id(hw, domain_id, vdev_req, vdev_id);
+
+	if (!domain) {
+		resp->status = DLB2_ST_INVALID_DOMAIN_ID;
+		return -EINVAL;
+	}
+
+	if (!domain->configured) {
+		resp->status = DLB2_ST_DOMAIN_NOT_CONFIGURED;
+		return -EINVAL;
+	}
+
+	if (!(domain->started ^ start_domain)) {
+		resp->status = start_domain ? DLB2_ST_DOMAIN_STARTED : DLB2_ST_DOMAIN_NOT_STARTED;
+		return -EINVAL;
+	}
+
+	*out_domain = domain;
+
+	return 0;
+}
+
+static int dlb2_verify_map_qid_args(struct dlb2_hw *hw,
+				    u32 domain_id,
+				    struct dlb2_map_qid_args *args,
+				    struct dlb2_cmd_response *resp,
+				    bool vdev_req,
+				    unsigned int vdev_id,
+				    struct dlb2_hw_domain **out_domain,
+				    struct dlb2_ldb_port **out_port,
+				    struct dlb2_ldb_queue **out_queue)
+{
+	struct dlb2_hw_domain *domain;
+	struct dlb2_ldb_queue *queue;
+	struct dlb2_ldb_port *port;
+	int id;
+
+	domain = dlb2_get_domain_from_id(hw, domain_id, vdev_req, vdev_id);
+
+	if (!domain) {
+		resp->status = DLB2_ST_INVALID_DOMAIN_ID;
+		return -EINVAL;
+	}
+
+	if (!domain->configured) {
+		resp->status = DLB2_ST_DOMAIN_NOT_CONFIGURED;
+		return -EINVAL;
+	}
+
+	id = args->port_id;
+
+	port = dlb2_get_domain_used_ldb_port(id, vdev_req, domain);
+
+	if (!port || !port->configured) {
+		resp->status = DLB2_ST_INVALID_PORT_ID;
+		return -EINVAL;
+	}
+
+	if (args->priority >= DLB2_QID_PRIORITIES) {
+		resp->status = DLB2_ST_INVALID_PRIORITY;
+		return -EINVAL;
+	}
+
+	queue = dlb2_get_domain_ldb_queue(args->qid, vdev_req, domain);
+
+	if (!queue || !queue->configured) {
+		resp->status = DLB2_ST_INVALID_QID;
+		return -EINVAL;
+	}
+
+	if (queue->domain_id.phys_id != domain->id.phys_id) {
+		resp->status = DLB2_ST_INVALID_QID;
+		return -EINVAL;
+	}
+
+	if (port->domain_id.phys_id != domain->id.phys_id) {
+		resp->status = DLB2_ST_INVALID_PORT_ID;
+		return -EINVAL;
+	}
+
+	*out_domain = domain;
+	*out_queue = queue;
+	*out_port = port;
+
+	return 0;
+}
+
+static bool dlb2_port_find_slot(struct dlb2_ldb_port *port,
+				enum dlb2_qid_map_state state,
+				int *slot)
+{
+	int i;
+
+	for (i = 0; i < DLB2_MAX_NUM_QIDS_PER_LDB_CQ; i++) {
+		if (port->qid_map[i].state == state)
+			break;
+	}
+
+	*slot = i;
+
+	return (i < DLB2_MAX_NUM_QIDS_PER_LDB_CQ);
+}
+
+static bool dlb2_port_find_slot_queue(struct dlb2_ldb_port *port,
+				      enum dlb2_qid_map_state state,
+				      struct dlb2_ldb_queue *queue,
+				      int *slot)
+{
+	int i;
+
+	for (i = 0; i < DLB2_MAX_NUM_QIDS_PER_LDB_CQ; i++) {
+		if (port->qid_map[i].state == state &&
+		    port->qid_map[i].qid == queue->id.phys_id)
+			break;
+	}
+
+	*slot = i;
+
+	return (i < DLB2_MAX_NUM_QIDS_PER_LDB_CQ);
+}
+
+static bool
+dlb2_port_find_slot_with_pending_map_queue(struct dlb2_ldb_port *port,
+					   struct dlb2_ldb_queue *queue,
+					   int *slot)
+{
+	int i;
+
+	for (i = 0; i < DLB2_MAX_NUM_QIDS_PER_LDB_CQ; i++) {
+		struct dlb2_ldb_port_qid_map *map = &port->qid_map[i];
+
+		if (map->state == DLB2_QUEUE_UNMAP_IN_PROG_PENDING_MAP &&
+		    map->pending_qid == queue->id.phys_id)
+			break;
+	}
+
+	*slot = i;
+
+	return (i < DLB2_MAX_NUM_QIDS_PER_LDB_CQ);
+}
+
+static int dlb2_port_slot_state_transition(struct dlb2_hw *hw,
+					   struct dlb2_ldb_port *port,
+					   struct dlb2_ldb_queue *queue,
+					   int slot,
+					   enum dlb2_qid_map_state new_state)
+{
+	enum dlb2_qid_map_state curr_state = port->qid_map[slot].state;
+	struct dlb2_hw_domain *domain;
+	int domain_id;
+
+	domain_id = port->domain_id.phys_id;
+
+	domain = dlb2_get_domain_from_id(hw, domain_id, false, 0);
+	if (!domain) {
+		DLB2_HW_ERR(hw,
+			    "[%s()] Internal error: unable to find domain %d\n",
+			    __func__, domain_id);
+		return -EINVAL;
+	}
+
+	switch (curr_state) {
+	case DLB2_QUEUE_UNMAPPED:
+		switch (new_state) {
+		case DLB2_QUEUE_MAPPED:
+			queue->num_mappings++;
+			port->num_mappings++;
+			break;
+		case DLB2_QUEUE_MAP_IN_PROG:
+			queue->num_pending_additions++;
+			domain->num_pending_additions++;
+			break;
+		default:
+			goto error;
+		}
+		break;
+	case DLB2_QUEUE_MAPPED:
+		switch (new_state) {
+		case DLB2_QUEUE_UNMAPPED:
+			queue->num_mappings--;
+			port->num_mappings--;
+			break;
+		case DLB2_QUEUE_UNMAP_IN_PROG:
+			port->num_pending_removals++;
+			domain->num_pending_removals++;
+			break;
+		case DLB2_QUEUE_MAPPED:
+			/* Priority change, nothing to update */
+			break;
+		default:
+			goto error;
+		}
+		break;
+	case DLB2_QUEUE_MAP_IN_PROG:
+		switch (new_state) {
+		case DLB2_QUEUE_UNMAPPED:
+			queue->num_pending_additions--;
+			domain->num_pending_additions--;
+			break;
+		case DLB2_QUEUE_MAPPED:
+			queue->num_mappings++;
+			port->num_mappings++;
+			queue->num_pending_additions--;
+			domain->num_pending_additions--;
+			break;
+		default:
+			goto error;
+		}
+		break;
+	case DLB2_QUEUE_UNMAP_IN_PROG:
+		switch (new_state) {
+		case DLB2_QUEUE_UNMAPPED:
+			port->num_pending_removals--;
+			domain->num_pending_removals--;
+			queue->num_mappings--;
+			port->num_mappings--;
+			break;
+		case DLB2_QUEUE_MAPPED:
+			port->num_pending_removals--;
+			domain->num_pending_removals--;
+			break;
+		case DLB2_QUEUE_UNMAP_IN_PROG_PENDING_MAP:
+			/* Nothing to update */
+			break;
+		default:
+			goto error;
+		}
+		break;
+	case DLB2_QUEUE_UNMAP_IN_PROG_PENDING_MAP:
+		switch (new_state) {
+		case DLB2_QUEUE_UNMAP_IN_PROG:
+			/* Nothing to update */
+			break;
+		case DLB2_QUEUE_UNMAPPED:
+			/*
+			 * An UNMAP_IN_PROG_PENDING_MAP slot briefly
+			 * becomes UNMAPPED before it transitions to
+			 * MAP_IN_PROG.
+			 */
+			queue->num_mappings--;
+			port->num_mappings--;
+			port->num_pending_removals--;
+			domain->num_pending_removals--;
+			break;
+		default:
+			goto error;
+		}
+		break;
+	default:
+		goto error;
+	}
+
+	port->qid_map[slot].state = new_state;
+
+	DLB2_HW_DBG(hw,
+		    "[%s()] queue %d -> port %d state transition (%d -> %d)\n",
+		    __func__, queue->id.phys_id, port->id.phys_id,
+		    curr_state, new_state);
+	return 0;
+
+error:
+	DLB2_HW_ERR(hw,
+		    "[%s()] Internal error: invalid queue %d -> port %d state transition (%d -> %d)\n",
+		    __func__, queue->id.phys_id, port->id.phys_id,
+		    curr_state, new_state);
+	return -EFAULT;
+}
+
+static int dlb2_verify_map_qid_slot_available(struct dlb2_ldb_port *port,
+					      struct dlb2_ldb_queue *queue,
+					      struct dlb2_cmd_response *resp)
+{
+	enum dlb2_qid_map_state state;
+	int i;
+
+	/* Unused slot available? */
+	if (port->num_mappings < DLB2_MAX_NUM_QIDS_PER_LDB_CQ)
+		return 0;
+
+	/*
+	 * If the queue is already mapped (from the application's perspective),
+	 * this is simply a priority update.
+	 */
+	state = DLB2_QUEUE_MAPPED;
+	if (dlb2_port_find_slot_queue(port, state, queue, &i))
+		return 0;
+
+	state = DLB2_QUEUE_MAP_IN_PROG;
+	if (dlb2_port_find_slot_queue(port, state, queue, &i))
+		return 0;
+
+	if (dlb2_port_find_slot_with_pending_map_queue(port, queue, &i))
+		return 0;
+
+	/*
+	 * If the slot contains an unmap in progress, it's considered
+	 * available.
+	 */
+	state = DLB2_QUEUE_UNMAP_IN_PROG;
+	if (dlb2_port_find_slot(port, state, &i))
+		return 0;
+
+	state = DLB2_QUEUE_UNMAPPED;
+	if (dlb2_port_find_slot(port, state, &i))
+		return 0;
+
+	resp->status = DLB2_ST_NO_QID_SLOTS_AVAILABLE;
+	return -EINVAL;
+}
+
+static int dlb2_verify_unmap_qid_args(struct dlb2_hw *hw,
+				      u32 domain_id,
+				      struct dlb2_unmap_qid_args *args,
+				      struct dlb2_cmd_response *resp,
+				      bool vdev_req,
+				      unsigned int vdev_id,
+				      struct dlb2_hw_domain **out_domain,
+				      struct dlb2_ldb_port **out_port,
+				      struct dlb2_ldb_queue **out_queue)
+{
+	enum dlb2_qid_map_state state;
+	struct dlb2_hw_domain *domain;
+	struct dlb2_ldb_queue *queue;
+	struct dlb2_ldb_port *port;
+	int slot;
+	int id;
+
+	domain = dlb2_get_domain_from_id(hw, domain_id, vdev_req, vdev_id);
+
+	if (!domain) {
+		resp->status = DLB2_ST_INVALID_DOMAIN_ID;
+		return -EINVAL;
+	}
+
+	if (!domain->configured) {
+		resp->status = DLB2_ST_DOMAIN_NOT_CONFIGURED;
+		return -EINVAL;
+	}
+
+	id = args->port_id;
+
+	port = dlb2_get_domain_used_ldb_port(id, vdev_req, domain);
+
+	if (!port || !port->configured) {
+		resp->status = DLB2_ST_INVALID_PORT_ID;
+		return -EINVAL;
+	}
+
+	if (port->domain_id.phys_id != domain->id.phys_id) {
+		resp->status = DLB2_ST_INVALID_PORT_ID;
+		return -EINVAL;
+	}
+
+	queue = dlb2_get_domain_ldb_queue(args->qid, vdev_req, domain);
+
+	if (!queue || !queue->configured) {
+		DLB2_HW_ERR(hw, "[%s()] Can't unmap unconfigured queue %d\n",
+			    __func__, args->qid);
+		resp->status = DLB2_ST_INVALID_QID;
+		return -EINVAL;
+	}
+
+	/*
+	 * Verify that the port has the queue mapped. From the application's
+	 * perspective a queue is mapped if it is actually mapped, the map is
+	 * in progress, or the map is blocked pending an unmap.
+	 */
+	state = DLB2_QUEUE_MAPPED;
+	if (dlb2_port_find_slot_queue(port, state, queue, &slot))
+		goto done;
+
+	state = DLB2_QUEUE_MAP_IN_PROG;
+	if (dlb2_port_find_slot_queue(port, state, queue, &slot))
+		goto done;
+
+	if (dlb2_port_find_slot_with_pending_map_queue(port, queue, &slot))
+		goto done;
+
+	resp->status = DLB2_ST_INVALID_QID;
+	return -EINVAL;
+
+done:
+	*out_domain = domain;
+	*out_port = port;
+	*out_queue = queue;
+
+	return 0;
+}
+
+static int
+dlb2_verify_enable_ldb_port_args(struct dlb2_hw *hw,
+				 u32 domain_id,
+				 struct dlb2_enable_ldb_port_args *args,
+				 struct dlb2_cmd_response *resp,
+				 bool vdev_req,
+				 unsigned int vdev_id,
+				 struct dlb2_hw_domain **out_domain,
+				 struct dlb2_ldb_port **out_port)
+{
+	struct dlb2_hw_domain *domain;
+	struct dlb2_ldb_port *port;
+	int id;
+
+	domain = dlb2_get_domain_from_id(hw, domain_id, vdev_req, vdev_id);
+
+	if (!domain) {
+		resp->status = DLB2_ST_INVALID_DOMAIN_ID;
+		return -EINVAL;
+	}
+
+	if (!domain->configured) {
+		resp->status = DLB2_ST_DOMAIN_NOT_CONFIGURED;
+		return -EINVAL;
+	}
+
+	id = args->port_id;
+
+	port = dlb2_get_domain_used_ldb_port(id, vdev_req, domain);
+
+	if (!port || !port->configured) {
+		resp->status = DLB2_ST_INVALID_PORT_ID;
+		return -EINVAL;
+	}
+
+	*out_domain = domain;
+	*out_port = port;
+
+	return 0;
+}
+
+static int
+dlb2_verify_enable_dir_port_args(struct dlb2_hw *hw,
+				 u32 domain_id,
+				 struct dlb2_enable_dir_port_args *args,
+				 struct dlb2_cmd_response *resp,
+				 bool vdev_req,
+				 unsigned int vdev_id,
+				 struct dlb2_hw_domain **out_domain,
+				 struct dlb2_dir_pq_pair **out_port)
+{
+	struct dlb2_hw_domain *domain;
+	struct dlb2_dir_pq_pair *port;
+	int id;
+
+	domain = dlb2_get_domain_from_id(hw, domain_id, vdev_req, vdev_id);
+
+	if (!domain) {
+		resp->status = DLB2_ST_INVALID_DOMAIN_ID;
+		return -EINVAL;
+	}
+
+	if (!domain->configured) {
+		resp->status = DLB2_ST_DOMAIN_NOT_CONFIGURED;
+		return -EINVAL;
+	}
+
+	id = args->port_id;
+
+	port = dlb2_get_domain_used_dir_pq(hw, id, vdev_req, domain);
+
+	if (!port || !port->port_configured) {
+		resp->status = DLB2_ST_INVALID_PORT_ID;
+		return -EINVAL;
+	}
+
+	*out_domain = domain;
+	*out_port = port;
+
+	return 0;
+}
+
+static int
+dlb2_verify_disable_ldb_port_args(struct dlb2_hw *hw,
+				  u32 domain_id,
+				  struct dlb2_disable_ldb_port_args *args,
+				  struct dlb2_cmd_response *resp,
+				  bool vdev_req,
+				  unsigned int vdev_id,
+				  struct dlb2_hw_domain **out_domain,
+				  struct dlb2_ldb_port **out_port)
+{
+	struct dlb2_hw_domain *domain;
+	struct dlb2_ldb_port *port;
+	int id;
+
+	domain = dlb2_get_domain_from_id(hw, domain_id, vdev_req, vdev_id);
+
+	if (!domain) {
+		resp->status = DLB2_ST_INVALID_DOMAIN_ID;
+		return -EINVAL;
+	}
+
+	if (!domain->configured) {
+		resp->status = DLB2_ST_DOMAIN_NOT_CONFIGURED;
+		return -EINVAL;
+	}
+
+	id = args->port_id;
+
+	port = dlb2_get_domain_used_ldb_port(id, vdev_req, domain);
+
+	if (!port || !port->configured) {
+		resp->status = DLB2_ST_INVALID_PORT_ID;
+		return -EINVAL;
+	}
+
+	*out_domain = domain;
+	*out_port = port;
+
+	return 0;
+}
+
+static int
+dlb2_verify_disable_dir_port_args(struct dlb2_hw *hw,
+				  u32 domain_id,
+				  struct dlb2_disable_dir_port_args *args,
+				  struct dlb2_cmd_response *resp,
+				  bool vdev_req,
+				  unsigned int vdev_id,
+				  struct dlb2_hw_domain **out_domain,
+				  struct dlb2_dir_pq_pair **out_port)
+{
+	struct dlb2_hw_domain *domain;
+	struct dlb2_dir_pq_pair *port;
+	int id;
+
+	domain = dlb2_get_domain_from_id(hw, domain_id, vdev_req, vdev_id);
+
+	if (!domain) {
+		resp->status = DLB2_ST_INVALID_DOMAIN_ID;
+		return -EINVAL;
+	}
+
+	if (!domain->configured) {
+		resp->status = DLB2_ST_DOMAIN_NOT_CONFIGURED;
+		return -EINVAL;
+	}
+
+	id = args->port_id;
+
+	port = dlb2_get_domain_used_dir_pq(hw, id, vdev_req, domain);
+
+	if (!port || !port->port_configured) {
+		resp->status = DLB2_ST_INVALID_PORT_ID;
+		return -EINVAL;
+	}
+
+	*out_domain = domain;
+	*out_port = port;
+
+	return 0;
+}
+
+static void dlb2_configure_domain_credits_v2(struct dlb2_hw *hw,
+					     struct dlb2_hw_domain *domain)
+{
+	u32 reg = 0;
+
+	DLB2_BITS_SET(reg, domain->num_ldb_credits,
+		      DLB2_CHP_CFG_LDB_VAS_CRD_COUNT);
+	DLB2_CSR_WR(hw, DLB2_CHP_CFG_LDB_VAS_CRD(domain->id.phys_id), reg);
+
+	reg = 0;
+	DLB2_BITS_SET(reg, domain->num_dir_credits,
+		      DLB2_CHP_CFG_DIR_VAS_CRD_COUNT);
+	DLB2_CSR_WR(hw, DLB2_CHP_CFG_DIR_VAS_CRD(domain->id.phys_id), reg);
+}
+
+static void dlb2_configure_domain_credits_v2_5(struct dlb2_hw *hw,
+					       struct dlb2_hw_domain *domain)
+{
+	u32 reg = 0;
+
+	DLB2_BITS_SET(reg, domain->num_credits, DLB2_CHP_CFG_LDB_VAS_CRD_COUNT);
+	DLB2_CSR_WR(hw, DLB2_CHP_CFG_VAS_CRD(domain->id.phys_id), reg);
+}
+
+static void dlb2_configure_domain_credits(struct dlb2_hw *hw,
+					  struct dlb2_hw_domain *domain)
+{
+	if (hw->ver == DLB2_HW_V2)
+		dlb2_configure_domain_credits_v2(hw, domain);
+	else
+		dlb2_configure_domain_credits_v2_5(hw, domain);
+}
+
+static int dlb2_pp_profile(struct dlb2_hw *hw, int port, int cpu, bool is_ldb)
+{
+	u64 cycle_start = 0ULL, cycle_end = 0ULL;
+	struct dlb2_hcw hcw_mem[DLB2_HCW_MEM_SIZE], *hcw;
+	void __iomem *pp_addr;
+	cpu_set_t cpuset;
+	int i;
+
+	CPU_ZERO(&cpuset);
+	CPU_SET(cpu, &cpuset);
+	sched_setaffinity(0, sizeof(cpuset), &cpuset);
+
+	pp_addr = os_map_producer_port(hw, port, is_ldb);
+
+	/* Point hcw to a 64B-aligned location */
+	hcw = (struct dlb2_hcw *)((uintptr_t)&hcw_mem[DLB2_HCW_64B_OFF] &
+				  ~DLB2_HCW_ALIGN_MASK);
+
+	/*
+	 * Program the first HCW for a completion and token return and
+	 * the other HCWs as NOOPS
+	 */
+
+	memset(hcw, 0, (DLB2_HCW_MEM_SIZE - DLB2_HCW_64B_OFF) * sizeof(*hcw));
+	hcw->qe_comp = 1;
+	hcw->cq_token = 1;
+	hcw->lock_id = 1;
+
+	cycle_start = rte_get_tsc_cycles();
+	for (i = 0; i < DLB2_NUM_PROBE_ENQS; i++)
+		os_enqueue_four_hcws(hw, hcw, pp_addr);
+
+	cycle_end = rte_get_tsc_cycles();
+
+	os_unmap_producer_port(hw, pp_addr);
+	return (int)(cycle_end - cycle_start);
+}
+
+static void *dlb2_pp_profile_func(void *data)
+{
+	struct dlb2_pp_thread_data *thread_data = data;
+	int cycles;
+
+	cycles = dlb2_pp_profile(thread_data->hw, thread_data->pp,
+				 thread_data->cpu, thread_data->is_ldb);
+
+	thread_data->cycles = cycles;
+
+	return NULL;
+}
+
+static int dlb2_pp_cycle_comp(const void *a, const void *b)
+{
+	const struct dlb2_pp_thread_data *x = a;
+	const struct dlb2_pp_thread_data *y = b;
+
+	return x->cycles - y->cycles;
+}
+
+/* Probe producer ports from different CPU cores */
+static void
+dlb2_get_pp_allocation(struct dlb2_hw *hw, int cpu, int port_type)
+{
+	struct dlb2_pp_thread_data dlb2_thread_data[DLB2_MAX_NUM_DIR_PORTS_V2_5];
+	struct dlb2_dev *dlb2_dev = container_of(hw, struct dlb2_dev, hw);
+	struct dlb2_pp_thread_data cos_cycles[DLB2_NUM_COS_DOMAINS];
+	int ver = DLB2_HW_DEVICE_FROM_PCI_ID(dlb2_dev->pdev);
+	int num_ports_per_sort, num_ports, num_sort, i, err;
+	bool is_ldb = (port_type == DLB2_LDB_PORT);
+	int *port_allocations;
+	pthread_t pthread;
+
+	if (is_ldb) {
+		port_allocations = hw->ldb_pp_allocations;
+		num_ports = DLB2_MAX_NUM_LDB_PORTS;
+		num_sort = DLB2_NUM_COS_DOMAINS;
+	} else {
+		port_allocations = hw->dir_pp_allocations;
+		num_ports = DLB2_MAX_NUM_DIR_PORTS(ver);
+		num_sort = 1;
+	}
+	num_ports_per_sort = num_ports / num_sort;
+	if (rte_cpu_get_flag_enabled(RTE_CPUFLAG_MOVDIR64B))
+		dlb2_dev->enqueue_four = dlb2_movdir64b;
+	else
+		dlb2_dev->enqueue_four = dlb2_movntdq;
+
+	DLB2_LOG_INFO(" for %s: cpu core used in pp profiling: %d\n",
+				is_ldb ? "LDB" : "DIR", cpu);
+
+	memset(cos_cycles, 0, num_sort * sizeof(struct dlb2_pp_thread_data));
+	for (i = 0; i < num_ports; i++) {
+		int cos = (i >> DLB2_NUM_COS_DOMAINS) % DLB2_NUM_COS_DOMAINS;
+
+		dlb2_thread_data[i].is_ldb = is_ldb;
+		dlb2_thread_data[i].pp = i;
+		dlb2_thread_data[i].cycles = 0;
+		dlb2_thread_data[i].hw = hw;
+		dlb2_thread_data[i].cpu = cpu;
+
+		err = pthread_create(&pthread, NULL, &dlb2_pp_profile_func,
+				     &dlb2_thread_data[i]);
+		if (err) {
+			DLB2_LOG_ERR(": thread creation failed! err=%d", err);
+			return;
+		}
+
+		err = pthread_join(pthread, NULL);
+		if (err) {
+			DLB2_LOG_ERR(": thread join failed! err=%d", err);
+			return;
+		}
+		if (is_ldb)
+			cos_cycles[cos].cycles += dlb2_thread_data[i].cycles;
+
+		if ((i + 1) % num_ports_per_sort == 0) {
+			int index = 0;
+
+			if (is_ldb) {
+				cos_cycles[cos].pp = cos;
+				index = cos * num_ports_per_sort;
+			}
+			/*
+			 * For LDB ports first sort with in a cos. Later sort
+			 * the best cos based on total cycles for the cos.
+			 * For DIR ports, there is a single sort across all
+			 * ports.
+			 */
+			qsort(&dlb2_thread_data[index], num_ports_per_sort,
+			      sizeof(struct dlb2_pp_thread_data),
+			      dlb2_pp_cycle_comp);
+		}
+	}
+
+	/*
+	 * Sort by best cos aggregated over all ports per cos
+	 * Note: After DLB2_MAX_NUM_LDB_PORTS sorted cos is stored and so'pp'
+	 * is cos_id and not port id.
+	 */
+	if (is_ldb) {
+		qsort(cos_cycles, num_sort, sizeof(struct dlb2_pp_thread_data),
+		      dlb2_pp_cycle_comp);
+		for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++)
+			port_allocations[i + DLB2_MAX_NUM_LDB_PORTS] = cos_cycles[i].pp;
+	}
+
+	for (i = 0; i < num_ports; i++) {
+		port_allocations[i] = dlb2_thread_data[i].pp;
+		DLB2_LOG_INFO(": pp %d cycles %d", port_allocations[i],
+			      dlb2_thread_data[i].cycles);
+	}
+}
+
+int
+dlb2_resource_probe(struct dlb2_hw *hw, const void *probe_args)
+{
+	const struct dlb2_devargs *args = (const struct dlb2_devargs *)probe_args;
+	const char *mask = args && strlen(args->producer_coremask) ? args->producer_coremask : NULL;
+	int cpu = 0, cores[RTE_MAX_LCORE];
+	unsigned int i;
+
+	if (mask && rte_eal_parse_coremask(mask, cores)) {
+		DLB2_LOG_ERR(": Invalid producer coremask=%s", mask);
+		return -1;
+	}
+
+	hw->num_prod_cores = 0;
+	for (i = 0; i < RTE_MAX_LCORE; i++) {
+		if (mask) {
+			if (cores[i] != -1) {
+
+				/*
+				 * Populate the producer cores from parsed
+				 * coremask
+				 */
+				hw->prod_core_list[cores[i]] = i;
+				hw->num_prod_cores++;
+			}
+		} else if (rte_lcore_is_enabled(i) &&
+			   (i != rte_get_main_lcore() || rte_lcore_count() == 1)) {
+				/*
+				 * If no producer coremask is provided, use the
+				 * second EAL core to probe
+				 */
+				cpu = i;
+				break;
+		}
+	}
+	/* Use the first core in producer coremask to probe */
+	if (hw->num_prod_cores)
+		cpu = hw->prod_core_list[0];
+
+	dlb2_get_pp_allocation(hw, cpu, DLB2_LDB_PORT);
+	dlb2_get_pp_allocation(hw, cpu, DLB2_DIR_PORT);
+
+	return 0;
+}
+
+static int
+dlb2_domain_attach_resources(struct dlb2_hw *hw,
+			     struct dlb2_function_resources *rsrcs,
+			     struct dlb2_hw_domain *domain,
+			     struct dlb2_create_sched_domain_args *args,
+			     struct dlb2_cmd_response *resp)
+{
+	int ret;
+
+	ret = dlb2_attach_ldb_queues(hw,
+				     rsrcs,
+				     domain,
+				     args->num_ldb_queues,
+				     resp);
+	if (ret)
+		return ret;
+
+	ret = dlb2_attach_ldb_ports(hw,
+				    rsrcs,
+				    domain,
+				    args,
+				    resp);
+	if (ret)
+		return ret;
+
+	ret = dlb2_attach_dir_ports(hw,
+				    rsrcs,
+				    domain,
+				    args->num_dir_ports,
+				    resp);
+	if (ret)
+		return ret;
+
+	if (hw->ver == DLB2_HW_V2) {
+		ret = dlb2_attach_ldb_credits(rsrcs,
+					      domain,
+					      args->num_ldb_credits,
+					      resp);
+		if (ret)
+			return ret;
+
+		ret = dlb2_attach_dir_credits(rsrcs,
+					      domain,
+					      args->num_dir_credits,
+					      resp);
+		if (ret)
+			return ret;
+	} else {  /* DLB 2.5 */
+		ret = dlb2_attach_credits(rsrcs,
+					  domain,
+					  args->num_credits,
+					  resp);
+		if (ret)
+			return ret;
+	}
+
+	ret = dlb2_attach_domain_hist_list_entries(rsrcs,
+						   domain,
+						   args->num_hist_list_entries,
+						   resp);
+	if (ret)
+		return ret;
+
+	ret = dlb2_attach_atomic_inflights(rsrcs,
+					   domain,
+					   args->num_atomic_inflights,
+					   resp);
+	if (ret)
+		return ret;
+
+	dlb2_configure_domain_credits(hw, domain);
+
+	domain->configured = true;
+
+	domain->started = false;
+
+	rsrcs->num_avail_domains--;
+
+	return 0;
+}
+
+static int
+dlb2_ldb_queue_attach_to_sn_group(struct dlb2_hw *hw,
+				  struct dlb2_ldb_queue *queue,
+				  struct dlb2_create_ldb_queue_args *args)
+{
+	int slot = -1;
+	int i;
+
+	queue->sn_cfg_valid = false;
+
+	if (args->num_sequence_numbers == 0)
+		return 0;
+
+	for (i = 0; i < DLB2_MAX_NUM_SEQUENCE_NUMBER_GROUPS; i++) {
+		struct dlb2_sn_group *group = &hw->rsrcs.sn_groups[i];
+
+		if (group->sequence_numbers_per_queue ==
+		    args->num_sequence_numbers &&
+		    !dlb2_sn_group_full(group)) {
+			slot = dlb2_sn_group_alloc_slot(group);
+			if (slot >= 0)
+				break;
+		}
+	}
+
+	if (slot == -1) {
+		DLB2_HW_ERR(hw,
+			    "[%s():%d] Internal error: no sequence number slots available\n",
+			    __func__, __LINE__);
+		return -EFAULT;
+	}
+
+	queue->sn_cfg_valid = true;
+	queue->sn_group = i;
+	queue->sn_slot = slot;
+	return 0;
+}
+
+static int
+dlb2_ldb_queue_attach_resources(struct dlb2_hw *hw,
+				struct dlb2_hw_domain *domain,
+				struct dlb2_ldb_queue *queue,
+				struct dlb2_create_ldb_queue_args *args)
+{
+	int ret;
+	ret = dlb2_ldb_queue_attach_to_sn_group(hw, queue, args);
+	if (ret)
+		return ret;
+
+	/* Attach QID inflights */
+	queue->num_qid_inflights = args->num_qid_inflights;
+
+	/* Attach atomic inflights */
+	queue->aqed_limit = args->num_atomic_inflights;
+
+	domain->num_avail_aqed_entries -= args->num_atomic_inflights;
+	domain->num_used_aqed_entries += args->num_atomic_inflights;
+
+	return 0;
+}
+
+static void dlb2_ldb_port_cq_enable(struct dlb2_hw *hw,
+				    struct dlb2_ldb_port *port)
+{
+	u32 reg = 0;
+
+	/*
+	 * Don't re-enable the port if a removal is pending. The caller should
+	 * mark this port as enabled (if it isn't already), and when the
+	 * removal completes the port will be enabled.
+	 */
+	if (port->num_pending_removals)
+		return;
+
+	DLB2_CSR_WR(hw, DLB2_LSP_CQ_LDB_DSBL(hw->ver, port->id.phys_id), reg);
+
+	dlb2_flush_csr(hw);
+}
+
+static void dlb2_ldb_port_cq_disable(struct dlb2_hw *hw,
+				     struct dlb2_ldb_port *port)
+{
+	u32 reg = 0;
+
+	DLB2_BIT_SET(reg, DLB2_LSP_CQ_LDB_DSBL_DISABLED);
+	DLB2_CSR_WR(hw, DLB2_LSP_CQ_LDB_DSBL(hw->ver, port->id.phys_id), reg);
+
+	dlb2_flush_csr(hw);
+}
+
+static void dlb2_dir_port_cq_enable(struct dlb2_hw *hw,
+				    struct dlb2_dir_pq_pair *port)
+{
+	u32 reg = 0;
+
+	DLB2_CSR_WR(hw, DLB2_LSP_CQ_DIR_DSBL(hw->ver, port->id.phys_id), reg);
+
+	dlb2_flush_csr(hw);
+}
+
+static void dlb2_dir_port_cq_disable(struct dlb2_hw *hw,
+				     struct dlb2_dir_pq_pair *port)
+{
+	u32 reg = 0;
+
+	DLB2_BIT_SET(reg, DLB2_LSP_CQ_DIR_DSBL_DISABLED);
+	DLB2_CSR_WR(hw, DLB2_LSP_CQ_DIR_DSBL(hw->ver, port->id.phys_id), reg);
+
+	dlb2_flush_csr(hw);
+}
+
+static void dlb2_ldb_port_configure_pp(struct dlb2_hw *hw,
+				       struct dlb2_hw_domain *domain,
+				       struct dlb2_ldb_port *port,
+				       bool vdev_req,
+				       unsigned int vdev_id)
+{
+	u32 reg = 0;
+
+	DLB2_BITS_SET(reg, domain->id.phys_id, DLB2_SYS_LDB_PP2VAS_VAS);
+	DLB2_CSR_WR(hw, DLB2_SYS_LDB_PP2VAS(port->id.phys_id), reg);
+
+	if (vdev_req) {
+		unsigned int offs;
+		u32 virt_id;
+
+		/*
+		 * DLB uses producer port address bits 17:12 to determine the
+		 * producer port ID. In Scalable IOV mode, PP accesses come
+		 * through the PF MMIO window for the physical producer port,
+		 * so for translation purposes the virtual and physical port
+		 * IDs are equal.
+		 */
+		if (hw->virt_mode == DLB2_VIRT_SRIOV)
+			virt_id = port->id.virt_id;
+		else
+			virt_id = port->id.phys_id;
+
+		reg = 0;
+		DLB2_BITS_SET(reg, port->id.phys_id, DLB2_SYS_VF_LDB_VPP2PP_PP);
+		offs = vdev_id * DLB2_MAX_NUM_LDB_PORTS + virt_id;
+		DLB2_CSR_WR(hw, DLB2_SYS_VF_LDB_VPP2PP(offs), reg);
+
+		reg = 0;
+		DLB2_BITS_SET(reg, vdev_id, DLB2_SYS_LDB_PP2VDEV_VDEV);
+		DLB2_CSR_WR(hw, DLB2_SYS_LDB_PP2VDEV(port->id.phys_id), reg);
+
+		reg = 0;
+		DLB2_BIT_SET(reg, DLB2_SYS_VF_LDB_VPP_V_VPP_V);
+		DLB2_CSR_WR(hw, DLB2_SYS_VF_LDB_VPP_V(offs), reg);
+	}
+
+	reg = 0;
+	DLB2_BIT_SET(reg, DLB2_SYS_LDB_PP_V_PP_V);
+	DLB2_CSR_WR(hw, DLB2_SYS_LDB_PP_V(port->id.phys_id), reg);
+}
+
+static int dlb2_ldb_port_configure_cq(struct dlb2_hw *hw,
+				      struct dlb2_hw_domain *domain,
+				      struct dlb2_ldb_port *port,
+				      uintptr_t cq_dma_base,
+				      struct dlb2_create_ldb_port_args *args,
+				      bool vdev_req,
+				      unsigned int vdev_id)
+{
+	u32 hl_base = 0;
+	u32 reg = 0;
+	u32 ds = 0;
+
+	/* The CQ address is 64B-aligned, and the DLB only wants bits [63:6] */
+	DLB2_BITS_SET(reg, cq_dma_base >> 6, DLB2_SYS_LDB_CQ_ADDR_L_ADDR_L);
+	DLB2_CSR_WR(hw, DLB2_SYS_LDB_CQ_ADDR_L(port->id.phys_id), reg);
+
+	reg = cq_dma_base >> 32;
+	DLB2_CSR_WR(hw, DLB2_SYS_LDB_CQ_ADDR_U(port->id.phys_id), reg);
+
+	/*
+	 * 'ro' == relaxed ordering. This setting allows DLB2 to write
+	 * cache lines out-of-order (but QEs within a cache line are always
+	 * updated in-order).
+	 */
+	reg = 0;
+	DLB2_BITS_SET(reg, vdev_id, DLB2_SYS_LDB_CQ2VF_PF_RO_VF);
+	DLB2_BITS_SET(reg,
+		 !vdev_req && (hw->virt_mode != DLB2_VIRT_SIOV),
+		 DLB2_SYS_LDB_CQ2VF_PF_RO_IS_PF);
+	DLB2_BIT_SET(reg, DLB2_SYS_LDB_CQ2VF_PF_RO_RO);
+
+	DLB2_CSR_WR(hw, DLB2_SYS_LDB_CQ2VF_PF_RO(port->id.phys_id), reg);
+
+	port->cq_depth = args->cq_depth;
+
+	if (args->cq_depth <= 8) {
+		ds = 1;
+	} else if (args->cq_depth == 16) {
+		ds = 2;
+	} else if (args->cq_depth == 32) {
+		ds = 3;
+	} else if (args->cq_depth == 64) {
+		ds = 4;
+	} else if (args->cq_depth == 128) {
+		ds = 5;
+	} else if (args->cq_depth == 256) {
+		ds = 6;
+	} else if (args->cq_depth == 512) {
+		ds = 7;
+	} else if (args->cq_depth == 1024) {
+		ds = 8;
+	} else {
+		DLB2_HW_ERR(hw,
+			    "[%s():%d] Internal error: invalid CQ depth\n",
+			    __func__, __LINE__);
+		return -EFAULT;
+	}
+
+	reg = 0;
+	DLB2_BITS_SET(reg, ds,
+		      DLB2_CHP_LDB_CQ_TKN_DEPTH_SEL_TOKEN_DEPTH_SELECT);
+	DLB2_CSR_WR(hw,
+		    DLB2_CHP_LDB_CQ_TKN_DEPTH_SEL(hw->ver, port->id.phys_id),
+		    reg);
+
+	/*
+	 * To support CQs with depth less than 8, program the token count
+	 * register with a non-zero initial value. Operations such as domain
+	 * reset must take this initial value into account when quiescing the
+	 * CQ.
+	 */
+	port->init_tkn_cnt = 0;
+
+	if (args->cq_depth < 8) {
+		reg = 0;
+		port->init_tkn_cnt = 8 - args->cq_depth;
+
+		DLB2_BITS_SET(reg,
+			      port->init_tkn_cnt,
+			      DLB2_LSP_CQ_LDB_TKN_CNT_TOKEN_COUNT);
+		DLB2_CSR_WR(hw,
+			    DLB2_LSP_CQ_LDB_TKN_CNT(hw->ver, port->id.phys_id),
+			    reg);
+	} else {
+		DLB2_CSR_WR(hw,
+			    DLB2_LSP_CQ_LDB_TKN_CNT(hw->ver, port->id.phys_id),
+			    DLB2_LSP_CQ_LDB_TKN_CNT_RST);
+	}
+
+	reg = 0;
+	DLB2_BITS_SET(reg, ds,
+		      DLB2_LSP_CQ_LDB_TKN_DEPTH_SEL_TOKEN_DEPTH_SELECT_V2);
+	DLB2_CSR_WR(hw,
+		    DLB2_LSP_CQ_LDB_TKN_DEPTH_SEL(hw->ver, port->id.phys_id),
+		    reg);
+
+	/* Reset the CQ write pointer */
+	DLB2_CSR_WR(hw,
+		    DLB2_CHP_LDB_CQ_WPTR(hw->ver, port->id.phys_id),
+		    DLB2_CHP_LDB_CQ_WPTR_RST);
+
+	reg = 0;
+	DLB2_BITS_SET(reg,
+		      port->hist_list_entry_limit - 1,
+		      DLB2_CHP_HIST_LIST_LIM_LIMIT);
+	DLB2_CSR_WR(hw, DLB2_CHP_HIST_LIST_LIM(hw->ver, port->id.phys_id), reg);
+
+	DLB2_BITS_SET(hl_base, port->hist_list_entry_base,
+		      DLB2_CHP_HIST_LIST_BASE_BASE);
+	DLB2_CSR_WR(hw,
+		    DLB2_CHP_HIST_LIST_BASE(hw->ver, port->id.phys_id),
+		    hl_base);
+
+	/*
+	 * The inflight limit sets a cap on the number of QEs for which this CQ
+	 * can owe completions at one time.
+	 */
+	reg = 0;
+	DLB2_BITS_SET(reg, args->cq_history_list_size,
+		      DLB2_LSP_CQ_LDB_INFL_LIM_LIMIT);
+	DLB2_CSR_WR(hw, DLB2_LSP_CQ_LDB_INFL_LIM(hw->ver, port->id.phys_id),
+		    reg);
+
+	reg = 0;
+	DLB2_BITS_SET(reg, DLB2_BITS_GET(hl_base, DLB2_CHP_HIST_LIST_BASE_BASE),
+		      DLB2_CHP_HIST_LIST_PUSH_PTR_PUSH_PTR);
+	DLB2_CSR_WR(hw, DLB2_CHP_HIST_LIST_PUSH_PTR(hw->ver, port->id.phys_id),
+		    reg);
+
+	reg = 0;
+	DLB2_BITS_SET(reg, DLB2_BITS_GET(hl_base, DLB2_CHP_HIST_LIST_BASE_BASE),
+		      DLB2_CHP_HIST_LIST_POP_PTR_POP_PTR);
+	DLB2_CSR_WR(hw, DLB2_CHP_HIST_LIST_POP_PTR(hw->ver, port->id.phys_id),
+		    reg);
+
+	/*
+	 * Address translation (AT) settings: 0: untranslated, 2: translated
+	 * (see ATS spec regarding Address Type field for more details)
+	 */
+
+	if (hw->ver == DLB2_HW_V2) {
+		reg = 0;
+		DLB2_CSR_WR(hw, DLB2_SYS_LDB_CQ_AT(port->id.phys_id), reg);
+	}
+
+	if (vdev_req && hw->virt_mode == DLB2_VIRT_SIOV) {
+		reg = 0;
+		DLB2_BITS_SET(reg, hw->pasid[vdev_id],
+			      DLB2_SYS_LDB_CQ_PASID_PASID);
+		DLB2_BIT_SET(reg, DLB2_SYS_LDB_CQ_PASID_FMT2);
+	}
+
+	DLB2_CSR_WR(hw, DLB2_SYS_LDB_CQ_PASID(hw->ver, port->id.phys_id), reg);
+
+	reg = 0;
+	DLB2_BITS_SET(reg, domain->id.phys_id, DLB2_CHP_LDB_CQ2VAS_CQ2VAS);
+	DLB2_CSR_WR(hw, DLB2_CHP_LDB_CQ2VAS(hw->ver, port->id.phys_id), reg);
+
+	/* Disable the port's QID mappings */
+	reg = 0;
+	DLB2_CSR_WR(hw, DLB2_LSP_CQ2PRIOV(hw->ver, port->id.phys_id), reg);
+
+        if (hw->ver == DLB2_HW_V2_5) {
+                reg = 0;
+		DLB2_BITS_SET(reg, args->enable_inflight_ctrl, DLB2_LSP_CFG_CTRL_GENERAL_0_ENAB_IF_THRESH_V2_5);
+		DLB2_CSR_WR(hw, DLB2_V2_5LSP_CFG_CTRL_GENERAL_0, reg);
+
+                if (args->enable_inflight_ctrl) {
+                        reg = 0;
+			DLB2_BITS_SET(reg, args->inflight_threshold, DLB2_LSP_CQ_LDB_INFL_THRESH_THRESH);
+			DLB2_CSR_WR(hw, DLB2_LSP_CQ_LDB_INFL_THRESH(port->id.phys_id), reg);
+                }
+        }
+
+        return 0;
+}
+
+static int dlb2_configure_ldb_port(struct dlb2_hw *hw,
+				   struct dlb2_hw_domain *domain,
+				   struct dlb2_ldb_port *port,
+				   uintptr_t cq_dma_base,
+				   struct dlb2_create_ldb_port_args *args,
+				   bool vdev_req,
+				   unsigned int vdev_id)
+{
+	int ret, i;
+
+	port->hist_list_entry_base = domain->hist_list_entry_base +
+				     domain->hist_list_entry_offset;
+	port->hist_list_entry_limit = port->hist_list_entry_base +
+				      args->cq_history_list_size;
+
+	domain->hist_list_entry_offset += args->cq_history_list_size;
+	domain->avail_hist_list_entries -= args->cq_history_list_size;
+
+	ret = dlb2_ldb_port_configure_cq(hw,
+					 domain,
+					 port,
+					 cq_dma_base,
+					 args,
+					 vdev_req,
+					 vdev_id);
+	if (ret)
+		return ret;
+
+	dlb2_ldb_port_configure_pp(hw,
+				   domain,
+				   port,
+				   vdev_req,
+				   vdev_id);
+
+	dlb2_ldb_port_cq_enable(hw, port);
+
+	for (i = 0; i < DLB2_MAX_NUM_QIDS_PER_LDB_CQ; i++)
+		port->qid_map[i].state = DLB2_QUEUE_UNMAPPED;
+	port->num_mappings = 0;
+
+	port->enabled = true;
+
+	port->configured = true;
+
+	return 0;
+}
+
+static void dlb2_dir_port_configure_pp(struct dlb2_hw *hw,
+				       struct dlb2_hw_domain *domain,
+				       struct dlb2_dir_pq_pair *port,
+				       bool vdev_req,
+				       unsigned int vdev_id)
+{
+	u32 reg = 0;
+
+	DLB2_BITS_SET(reg, domain->id.phys_id, DLB2_SYS_DIR_PP2VAS_VAS);
+	DLB2_CSR_WR(hw, DLB2_SYS_DIR_PP2VAS(port->id.phys_id), reg);
+
+	if (vdev_req) {
+		unsigned int offs;
+		u32 virt_id;
+
+		/*
+		 * DLB uses producer port address bits 17:12 to determine the
+		 * producer port ID. In Scalable IOV mode, PP accesses come
+		 * through the PF MMIO window for the physical producer port,
+		 * so for translation purposes the virtual and physical port
+		 * IDs are equal.
+		 */
+		if (hw->virt_mode == DLB2_VIRT_SRIOV)
+			virt_id = port->id.virt_id;
+		else
+			virt_id = port->id.phys_id;
+
+		reg = 0;
+		DLB2_BITS_SET(reg, port->id.phys_id, DLB2_SYS_VF_DIR_VPP2PP_PP);
+		offs = vdev_id * DLB2_MAX_NUM_DIR_PORTS(hw->ver) + virt_id;
+		DLB2_CSR_WR(hw, DLB2_SYS_VF_DIR_VPP2PP(offs), reg);
+
+		reg = 0;
+		DLB2_BITS_SET(reg, vdev_id, DLB2_SYS_DIR_PP2VDEV_VDEV);
+		DLB2_CSR_WR(hw, DLB2_SYS_DIR_PP2VDEV(port->id.phys_id), reg);
+
+		reg = 0;
+		DLB2_BIT_SET(reg, DLB2_SYS_VF_DIR_VPP_V_VPP_V);
+		DLB2_CSR_WR(hw, DLB2_SYS_VF_DIR_VPP_V(offs), reg);
+	}
+
+	reg = 0;
+	DLB2_BIT_SET(reg, DLB2_SYS_DIR_PP_V_PP_V);
+	DLB2_CSR_WR(hw, DLB2_SYS_DIR_PP_V(port->id.phys_id), reg);
+}
+
+static int dlb2_dir_port_configure_cq(struct dlb2_hw *hw,
+				      struct dlb2_hw_domain *domain,
+				      struct dlb2_dir_pq_pair *port,
+				      uintptr_t cq_dma_base,
+				      struct dlb2_create_dir_port_args *args,
+				      bool vdev_req,
+				      unsigned int vdev_id)
+{
+	u32 reg = 0;
+	u32 ds = 0;
+
+	/* The CQ address is 64B-aligned, and the DLB only wants bits [63:6] */
+	DLB2_BITS_SET(reg, cq_dma_base >> 6, DLB2_SYS_DIR_CQ_ADDR_L_ADDR_L);
+	DLB2_CSR_WR(hw, DLB2_SYS_DIR_CQ_ADDR_L(port->id.phys_id), reg);
+
+	reg = cq_dma_base >> 32;
+	DLB2_CSR_WR(hw, DLB2_SYS_DIR_CQ_ADDR_U(port->id.phys_id), reg);
+
+	/*
+	 * 'ro' == relaxed ordering. This setting allows DLB2 to write
+	 * cache lines out-of-order (but QEs within a cache line are always
+	 * updated in-order).
+	 */
+	reg = 0;
+	DLB2_BITS_SET(reg, vdev_id, DLB2_SYS_DIR_CQ2VF_PF_RO_VF);
+	DLB2_BITS_SET(reg, !vdev_req && (hw->virt_mode != DLB2_VIRT_SIOV),
+		 DLB2_SYS_DIR_CQ2VF_PF_RO_IS_PF);
+	DLB2_BIT_SET(reg, DLB2_SYS_DIR_CQ2VF_PF_RO_RO);
+
+	DLB2_CSR_WR(hw, DLB2_SYS_DIR_CQ2VF_PF_RO(port->id.phys_id), reg);
+
+	if (args->cq_depth <= 8) {
+		ds = 1;
+	} else if (args->cq_depth == 16) {
+		ds = 2;
+	} else if (args->cq_depth == 32) {
+		ds = 3;
+	} else if (args->cq_depth == 64) {
+		ds = 4;
+	} else if (args->cq_depth == 128) {
+		ds = 5;
+	} else if (args->cq_depth == 256) {
+		ds = 6;
+	} else if (args->cq_depth == 512) {
+		ds = 7;
+	} else if (args->cq_depth == 1024) {
+		ds = 8;
+	} else {
+		DLB2_HW_ERR(hw,
+			    "[%s():%d] Internal error: invalid CQ depth\n",
+			    __func__, __LINE__);
+		return -EFAULT;
+	}
+
+	reg = 0;
+	DLB2_BITS_SET(reg, ds,
+		      DLB2_CHP_DIR_CQ_TKN_DEPTH_SEL_TOKEN_DEPTH_SELECT);
+	DLB2_CSR_WR(hw,
+		    DLB2_CHP_DIR_CQ_TKN_DEPTH_SEL(hw->ver, port->id.phys_id),
+		    reg);
+
+	/*
+	 * To support CQs with depth less than 8, program the token count
+	 * register with a non-zero initial value. Operations such as domain
+	 * reset must take this initial value into account when quiescing the
+	 * CQ.
+	 */
+	port->init_tkn_cnt = 0;
+
+	if (args->cq_depth < 8) {
+		reg = 0;
+		port->init_tkn_cnt = 8 - args->cq_depth;
+
+		DLB2_BITS_SET(reg, port->init_tkn_cnt,
+			      DLB2_LSP_CQ_DIR_TKN_CNT_COUNT);
+		DLB2_CSR_WR(hw,
+			    DLB2_LSP_CQ_DIR_TKN_CNT(hw->ver, port->id.phys_id),
+			    reg);
+	} else {
+		DLB2_CSR_WR(hw,
+			    DLB2_LSP_CQ_DIR_TKN_CNT(hw->ver, port->id.phys_id),
+			    DLB2_LSP_CQ_DIR_TKN_CNT_RST);
+	}
+
+	reg = 0;
+	DLB2_BITS_SET(reg, ds,
+		      DLB2_LSP_CQ_DIR_TKN_DEPTH_SEL_DSI_TOKEN_DEPTH_SELECT_V2);
+	DLB2_CSR_WR(hw,
+		    DLB2_LSP_CQ_DIR_TKN_DEPTH_SEL_DSI(hw->ver,
+						      port->id.phys_id),
+		    reg);
+
+	/* Reset the CQ write pointer */
+	DLB2_CSR_WR(hw,
+		    DLB2_CHP_DIR_CQ_WPTR(hw->ver, port->id.phys_id),
+		    DLB2_CHP_DIR_CQ_WPTR_RST);
+
+	/* Virtualize the PPID */
+	reg = 0;
+	DLB2_CSR_WR(hw, DLB2_SYS_DIR_CQ_FMT(port->id.phys_id), reg);
+
+	/*
+	 * Address translation (AT) settings: 0: untranslated, 2: translated
+	 * (see ATS spec regarding Address Type field for more details)
+	 */
+	if (hw->ver == DLB2_HW_V2) {
+		reg = 0;
+		DLB2_CSR_WR(hw, DLB2_SYS_DIR_CQ_AT(port->id.phys_id), reg);
+	}
+
+	if (vdev_req && hw->virt_mode == DLB2_VIRT_SIOV) {
+		DLB2_BITS_SET(reg, hw->pasid[vdev_id],
+			      DLB2_SYS_DIR_CQ_PASID_PASID);
+		DLB2_BIT_SET(reg, DLB2_SYS_DIR_CQ_PASID_FMT2);
+	}
+
+	DLB2_CSR_WR(hw, DLB2_SYS_DIR_CQ_PASID(hw->ver, port->id.phys_id), reg);
+
+	reg = 0;
+	DLB2_BITS_SET(reg, domain->id.phys_id, DLB2_CHP_DIR_CQ2VAS_CQ2VAS);
+	DLB2_CSR_WR(hw, DLB2_CHP_DIR_CQ2VAS(hw->ver, port->id.phys_id), reg);
+
+	return 0;
+}
+
+static int dlb2_configure_dir_port(struct dlb2_hw *hw,
+				   struct dlb2_hw_domain *domain,
+				   struct dlb2_dir_pq_pair *port,
+				   uintptr_t cq_dma_base,
+				   struct dlb2_create_dir_port_args *args,
+				   bool vdev_req,
+				   unsigned int vdev_id)
+{
+	int ret;
+
+	ret = dlb2_dir_port_configure_cq(hw,
+					 domain,
+					 port,
+					 cq_dma_base,
+					 args,
+					 vdev_req,
+					 vdev_id);
+
+	if (ret)
+		return ret;
+
+	dlb2_dir_port_configure_pp(hw,
+				   domain,
+				   port,
+				   vdev_req,
+				   vdev_id);
+
+	dlb2_dir_port_cq_enable(hw, port);
+
+	port->enabled = true;
+
+	port->port_configured = true;
+
+	return 0;
+}
+
+static int dlb2_ldb_port_map_qid_static(struct dlb2_hw *hw,
+					struct dlb2_ldb_port *p,
+					struct dlb2_ldb_queue *q,
+					u8 priority)
+{
+	enum dlb2_qid_map_state state;
+	u32 lsp_qid2cq2;
+	u32 lsp_qid2cq;
+	u32 atm_qid2cq;
+	u32 cq2priov;
+	u32 cq2qid;
+	int i;
+
+	/* Look for a pending or already mapped slot, else an unused slot */
+	if (!dlb2_port_find_slot_queue(p, DLB2_QUEUE_MAP_IN_PROG, q, &i) &&
+	    !dlb2_port_find_slot_queue(p, DLB2_QUEUE_MAPPED, q, &i) &&
+	    !dlb2_port_find_slot(p, DLB2_QUEUE_UNMAPPED, &i)) {
+		DLB2_HW_ERR(hw,
+			    "[%s():%d] Internal error: CQ has no available QID mapping slots\n",
+			    __func__, __LINE__);
+		return -EFAULT;
+	}
+
+	/* Read-modify-write the priority and valid bit register */
+	cq2priov = DLB2_CSR_RD(hw, DLB2_LSP_CQ2PRIOV(hw->ver, p->id.phys_id));
+
+	cq2priov |= (1 << (i + DLB2_LSP_CQ2PRIOV_V_LOC)) & DLB2_LSP_CQ2PRIOV_V;
+	cq2priov |= ((priority & 0x7) << (i + DLB2_LSP_CQ2PRIOV_PRIO_LOC) * 3)
+		    & DLB2_LSP_CQ2PRIOV_PRIO;
+
+	DLB2_CSR_WR(hw, DLB2_LSP_CQ2PRIOV(hw->ver, p->id.phys_id), cq2priov);
+
+	/* Read-modify-write the QID map register */
+	if (i < 4)
+		cq2qid = DLB2_CSR_RD(hw, DLB2_LSP_CQ2QID0(hw->ver,
+							  p->id.phys_id));
+	else
+		cq2qid = DLB2_CSR_RD(hw, DLB2_LSP_CQ2QID1(hw->ver,
+							  p->id.phys_id));
+
+	if (i == 0 || i == 4)
+		DLB2_BITS_SET(cq2qid, q->id.phys_id, DLB2_LSP_CQ2QID0_QID_P0);
+	if (i == 1 || i == 5)
+		DLB2_BITS_SET(cq2qid, q->id.phys_id, DLB2_LSP_CQ2QID0_QID_P1);
+	if (i == 2 || i == 6)
+		DLB2_BITS_SET(cq2qid, q->id.phys_id, DLB2_LSP_CQ2QID0_QID_P2);
+	if (i == 3 || i == 7)
+		DLB2_BITS_SET(cq2qid, q->id.phys_id, DLB2_LSP_CQ2QID0_QID_P3);
+
+	if (i < 4)
+		DLB2_CSR_WR(hw,
+			    DLB2_LSP_CQ2QID0(hw->ver, p->id.phys_id), cq2qid);
+	else
+		DLB2_CSR_WR(hw,
+			    DLB2_LSP_CQ2QID1(hw->ver, p->id.phys_id), cq2qid);
+
+	atm_qid2cq = DLB2_CSR_RD(hw,
+				 DLB2_ATM_QID2CQIDIX(q->id.phys_id,
+						p->id.phys_id / 4));
+
+	lsp_qid2cq = DLB2_CSR_RD(hw,
+				 DLB2_LSP_QID2CQIDIX(hw->ver, q->id.phys_id,
+						p->id.phys_id / 4));
+
+	lsp_qid2cq2 = DLB2_CSR_RD(hw,
+				  DLB2_LSP_QID2CQIDIX2(hw->ver, q->id.phys_id,
+						  p->id.phys_id / 4));
+
+	switch (p->id.phys_id % 4) {
+	case 0:
+		DLB2_BIT_SET(atm_qid2cq,
+			     1 << (i + DLB2_ATM_QID2CQIDIX_00_CQ_P0_LOC));
+		DLB2_BIT_SET(lsp_qid2cq,
+			     1 << (i + DLB2_LSP_QID2CQIDIX_00_CQ_P0_LOC));
+		DLB2_BIT_SET(lsp_qid2cq2,
+			     1 << (i + DLB2_LSP_QID2CQIDIX2_00_CQ_P0_LOC));
+		break;
+
+	case 1:
+		DLB2_BIT_SET(atm_qid2cq,
+			     1 << (i + DLB2_ATM_QID2CQIDIX_00_CQ_P1_LOC));
+		DLB2_BIT_SET(lsp_qid2cq,
+			     1 << (i + DLB2_LSP_QID2CQIDIX_00_CQ_P1_LOC));
+		DLB2_BIT_SET(lsp_qid2cq2,
+			     1 << (i + DLB2_LSP_QID2CQIDIX2_00_CQ_P1_LOC));
+		break;
+
+	case 2:
+		DLB2_BIT_SET(atm_qid2cq,
+			     1 << (i + DLB2_ATM_QID2CQIDIX_00_CQ_P2_LOC));
+		DLB2_BIT_SET(lsp_qid2cq,
+			     1 << (i + DLB2_LSP_QID2CQIDIX_00_CQ_P2_LOC));
+		DLB2_BIT_SET(lsp_qid2cq2,
+			     1 << (i + DLB2_LSP_QID2CQIDIX2_00_CQ_P2_LOC));
+		break;
+
+	case 3:
+		DLB2_BIT_SET(atm_qid2cq,
+			     1 << (i + DLB2_ATM_QID2CQIDIX_00_CQ_P3_LOC));
+		DLB2_BIT_SET(lsp_qid2cq,
+			     1 << (i + DLB2_LSP_QID2CQIDIX_00_CQ_P3_LOC));
+		DLB2_BIT_SET(lsp_qid2cq2,
+			     1 << (i + DLB2_LSP_QID2CQIDIX2_00_CQ_P3_LOC));
+		break;
+	}
+
+	DLB2_CSR_WR(hw,
+		    DLB2_ATM_QID2CQIDIX(q->id.phys_id, p->id.phys_id / 4),
+		    atm_qid2cq);
+
+	DLB2_CSR_WR(hw,
+		    DLB2_LSP_QID2CQIDIX(hw->ver,
+					q->id.phys_id, p->id.phys_id / 4),
+		    lsp_qid2cq);
+
+	DLB2_CSR_WR(hw,
+		    DLB2_LSP_QID2CQIDIX2(hw->ver,
+					 q->id.phys_id, p->id.phys_id / 4),
+		    lsp_qid2cq2);
+
+	dlb2_flush_csr(hw);
+
+	p->qid_map[i].qid = q->id.phys_id;
+	p->qid_map[i].priority = priority;
+
+	state = DLB2_QUEUE_MAPPED;
+
+	return dlb2_port_slot_state_transition(hw, p, q, i, state);
+}
+
+static void dlb2_ldb_port_change_qid_priority(struct dlb2_hw *hw,
+					      struct dlb2_ldb_port *port,
+					      int slot,
+					      struct dlb2_map_qid_args *args)
+{
+	u32 cq2priov;
+
+	/* Read-modify-write the priority and valid bit register */
+	cq2priov = DLB2_CSR_RD(hw,
+			       DLB2_LSP_CQ2PRIOV(hw->ver, port->id.phys_id));
+
+	cq2priov |= (1 << (slot + DLB2_LSP_CQ2PRIOV_V_LOC)) &
+		    DLB2_LSP_CQ2PRIOV_V;
+	cq2priov |= ((args->priority & 0x7) << slot * 3) &
+		    DLB2_LSP_CQ2PRIOV_PRIO;
+
+	DLB2_CSR_WR(hw, DLB2_LSP_CQ2PRIOV(hw->ver, port->id.phys_id), cq2priov);
+
+	dlb2_flush_csr(hw);
+
+	port->qid_map[slot].priority = args->priority;
+}
+
+static int dlb2_ldb_port_set_has_work_bits(struct dlb2_hw *hw,
+					   struct dlb2_ldb_port *port,
+					   struct dlb2_ldb_queue *queue,
+					   int slot)
+{
+	u32 ctrl = 0;
+	u32 active;
+	u32 enq;
+
+	/* Set the atomic scheduling haswork bit */
+	active = DLB2_CSR_RD(hw, DLB2_LSP_QID_AQED_ACTIVE_CNT(hw->ver,
+							 queue->id.phys_id));
+
+	DLB2_BITS_SET(ctrl, port->id.phys_id, DLB2_LSP_LDB_SCHED_CTRL_CQ);
+	DLB2_BITS_SET(ctrl, slot, DLB2_LSP_LDB_SCHED_CTRL_QIDIX);
+	DLB2_BIT_SET(ctrl, DLB2_LSP_LDB_SCHED_CTRL_VALUE);
+	DLB2_BITS_SET(ctrl,
+		      DLB2_BITS_GET(active,
+				    DLB2_LSP_QID_AQED_ACTIVE_CNT_COUNT) > 0,
+				    DLB2_LSP_LDB_SCHED_CTRL_RLIST_HASWORK_V);
+
+	/* Set the non-atomic scheduling haswork bit */
+	DLB2_CSR_WR(hw, DLB2_LSP_LDB_SCHED_CTRL(hw->ver), ctrl);
+
+	enq = DLB2_CSR_RD(hw,
+			  DLB2_LSP_QID_LDB_ENQUEUE_CNT(hw->ver,
+						       queue->id.phys_id));
+
+	memset(&ctrl, 0, sizeof(ctrl));
+
+	DLB2_BITS_SET(ctrl, port->id.phys_id, DLB2_LSP_LDB_SCHED_CTRL_CQ);
+	DLB2_BITS_SET(ctrl, slot, DLB2_LSP_LDB_SCHED_CTRL_QIDIX);
+	DLB2_BIT_SET(ctrl, DLB2_LSP_LDB_SCHED_CTRL_VALUE);
+	DLB2_BITS_SET(ctrl,
+		      DLB2_BITS_GET(enq,
+				    DLB2_LSP_QID_LDB_ENQUEUE_CNT_COUNT) > 0,
+		      DLB2_LSP_LDB_SCHED_CTRL_NALB_HASWORK_V);
+
+	DLB2_CSR_WR(hw, DLB2_LSP_LDB_SCHED_CTRL(hw->ver), ctrl);
+
+	dlb2_flush_csr(hw);
+
+	return 0;
+}
+
+static void dlb2_ldb_port_clear_has_work_bits(struct dlb2_hw *hw,
+					      struct dlb2_ldb_port *port,
+					      u8 slot)
+{
+	u32 ctrl = 0;
+
+	DLB2_BITS_SET(ctrl, port->id.phys_id, DLB2_LSP_LDB_SCHED_CTRL_CQ);
+	DLB2_BITS_SET(ctrl, slot, DLB2_LSP_LDB_SCHED_CTRL_QIDIX);
+	DLB2_BIT_SET(ctrl, DLB2_LSP_LDB_SCHED_CTRL_RLIST_HASWORK_V);
+
+	DLB2_CSR_WR(hw, DLB2_LSP_LDB_SCHED_CTRL(hw->ver), ctrl);
+
+	memset(&ctrl, 0, sizeof(ctrl));
+
+	DLB2_BITS_SET(ctrl, port->id.phys_id, DLB2_LSP_LDB_SCHED_CTRL_CQ);
+	DLB2_BITS_SET(ctrl, slot, DLB2_LSP_LDB_SCHED_CTRL_QIDIX);
+	DLB2_BIT_SET(ctrl, DLB2_LSP_LDB_SCHED_CTRL_NALB_HASWORK_V);
+
+	DLB2_CSR_WR(hw, DLB2_LSP_LDB_SCHED_CTRL(hw->ver), ctrl);
+
+	dlb2_flush_csr(hw);
+}
+
+static void dlb2_ldb_port_clear_queue_if_status(struct dlb2_hw *hw,
+						struct dlb2_ldb_port *port,
+						int slot)
+{
+	u32 ctrl = 0;
+
+	DLB2_BITS_SET(ctrl, port->id.phys_id, DLB2_LSP_LDB_SCHED_CTRL_CQ);
+	DLB2_BITS_SET(ctrl, slot, DLB2_LSP_LDB_SCHED_CTRL_QIDIX);
+	DLB2_BIT_SET(ctrl, DLB2_LSP_LDB_SCHED_CTRL_INFLIGHT_OK_V);
+
+	DLB2_CSR_WR(hw, DLB2_LSP_LDB_SCHED_CTRL(hw->ver), ctrl);
+
+	dlb2_flush_csr(hw);
+}
+
+static void dlb2_ldb_port_set_queue_if_status(struct dlb2_hw *hw,
 					      struct dlb2_ldb_port *port,
 					      int slot)
 {
-	u32 ctrl = 0;
+	u32 ctrl = 0;
+
+	DLB2_BITS_SET(ctrl, port->id.phys_id, DLB2_LSP_LDB_SCHED_CTRL_CQ);
+	DLB2_BITS_SET(ctrl, slot, DLB2_LSP_LDB_SCHED_CTRL_QIDIX);
+	DLB2_BIT_SET(ctrl, DLB2_LSP_LDB_SCHED_CTRL_VALUE);
+	DLB2_BIT_SET(ctrl, DLB2_LSP_LDB_SCHED_CTRL_INFLIGHT_OK_V);
+
+	DLB2_CSR_WR(hw, DLB2_LSP_LDB_SCHED_CTRL(hw->ver), ctrl);
+
+	dlb2_flush_csr(hw);
+}
+
+static void dlb2_ldb_queue_set_inflight_limit(struct dlb2_hw *hw,
+					      struct dlb2_ldb_queue *queue)
+{
+	u32 infl_lim = 0;
+
+	DLB2_BITS_SET(infl_lim, queue->num_qid_inflights,
+		 DLB2_LSP_QID_LDB_INFL_LIM_LIMIT);
+
+	DLB2_CSR_WR(hw, DLB2_LSP_QID_LDB_INFL_LIM(hw->ver, queue->id.phys_id),
+		    infl_lim);
+}
+
+static void dlb2_ldb_queue_clear_inflight_limit(struct dlb2_hw *hw,
+						struct dlb2_ldb_queue *queue)
+{
+	DLB2_CSR_WR(hw,
+		    DLB2_LSP_QID_LDB_INFL_LIM(hw->ver, queue->id.phys_id),
+		    DLB2_LSP_QID_LDB_INFL_LIM_RST);
+}
+
+/*
+ * dlb2_ldb_queue_{enable, disable}_mapped_cqs() don't operate exactly as
+ * their function names imply, and should only be called by the dynamic CQ
+ * mapping code.
+ */
+static void dlb2_ldb_queue_disable_mapped_cqs(struct dlb2_hw *hw,
+					      struct dlb2_hw_domain *domain,
+					      struct dlb2_ldb_queue *queue)
+{
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_ldb_port *port;
+	int slot, i;
+
+	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
+		DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i], port, iter) {
+			enum dlb2_qid_map_state state = DLB2_QUEUE_MAPPED;
+
+			if (!dlb2_port_find_slot_queue(port, state,
+						       queue, &slot))
+				continue;
+
+			if (port->enabled)
+				dlb2_ldb_port_cq_disable(hw, port);
+		}
+	}
+}
+
+static void dlb2_ldb_queue_enable_mapped_cqs(struct dlb2_hw *hw,
+					     struct dlb2_hw_domain *domain,
+					     struct dlb2_ldb_queue *queue)
+{
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_ldb_port *port;
+	int slot, i;
+
+	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
+		DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i], port, iter) {
+			enum dlb2_qid_map_state state = DLB2_QUEUE_MAPPED;
+
+			if (!dlb2_port_find_slot_queue(port, state,
+						       queue, &slot))
+				continue;
+
+			if (port->enabled)
+				dlb2_ldb_port_cq_enable(hw, port);
+		}
+	}
+}
+
+static int dlb2_ldb_port_finish_map_qid_dynamic(struct dlb2_hw *hw,
+						struct dlb2_hw_domain *domain,
+						struct dlb2_ldb_port *port,
+						struct dlb2_ldb_queue *queue)
+{
+	struct dlb2_list_entry *iter __attribute__((unused));
+	enum dlb2_qid_map_state state;
+	int slot, ret, i;
+	u32 infl_cnt;
+	u8 prio;
+
+	infl_cnt = DLB2_CSR_RD(hw,
+			       DLB2_LSP_QID_LDB_INFL_CNT(hw->ver,
+						    queue->id.phys_id));
+
+	if (DLB2_BITS_GET(infl_cnt, DLB2_LSP_QID_LDB_INFL_CNT_COUNT)) {
+		DLB2_HW_ERR(hw,
+			    "[%s()] Internal error: non-zero QID inflight count\n",
+			    __func__);
+		return -EINVAL;
+	}
+
+	/*
+	 * Static map the port and set its corresponding has_work bits.
+	 */
+	state = DLB2_QUEUE_MAP_IN_PROG;
+	if (!dlb2_port_find_slot_queue(port, state, queue, &slot))
+		return -EINVAL;
+
+	prio = port->qid_map[slot].priority;
+
+	/*
+	 * Update the CQ2QID, CQ2PRIOV, and QID2CQIDX registers, and
+	 * the port's qid_map state.
+	 */
+	ret = dlb2_ldb_port_map_qid_static(hw, port, queue, prio);
+	if (ret)
+		return ret;
+
+	ret = dlb2_ldb_port_set_has_work_bits(hw, port, queue, slot);
+	if (ret)
+		return ret;
+
+	/*
+	 * Ensure IF_status(cq,qid) is 0 before enabling the port to
+	 * prevent spurious schedules to cause the queue's inflight
+	 * count to increase.
+	 */
+	dlb2_ldb_port_clear_queue_if_status(hw, port, slot);
+
+	/* Reset the queue's inflight status */
+	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
+		DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i], port, iter) {
+			state = DLB2_QUEUE_MAPPED;
+			if (!dlb2_port_find_slot_queue(port, state,
+						       queue, &slot))
+				continue;
+
+			dlb2_ldb_port_set_queue_if_status(hw, port, slot);
+		}
+	}
+
+	dlb2_ldb_queue_set_inflight_limit(hw, queue);
+
+	/* Re-enable CQs mapped to this queue */
+	dlb2_ldb_queue_enable_mapped_cqs(hw, domain, queue);
+
+	/* If this queue has other mappings pending, clear its inflight limit */
+	if (queue->num_pending_additions > 0)
+		dlb2_ldb_queue_clear_inflight_limit(hw, queue);
+
+	return 0;
+}
+
+/**
+ * dlb2_ldb_port_map_qid_dynamic() - perform a "dynamic" QID->CQ mapping
+ * @hw: dlb2_hw handle for a particular device.
+ * @port: load-balanced port
+ * @queue: load-balanced queue
+ * @priority: queue servicing priority
+ *
+ * Returns 0 if the queue was mapped, 1 if the mapping is scheduled to occur
+ * at a later point, and <0 if an error occurred.
+ */
+static int dlb2_ldb_port_map_qid_dynamic(struct dlb2_hw *hw,
+					 struct dlb2_ldb_port *port,
+					 struct dlb2_ldb_queue *queue,
+					 u8 priority)
+{
+	enum dlb2_qid_map_state state;
+	struct dlb2_hw_domain *domain;
+	int domain_id, slot, ret;
+	u32 infl_cnt;
+
+	domain_id = port->domain_id.phys_id;
+
+	domain = dlb2_get_domain_from_id(hw, domain_id, false, 0);
+	if (!domain) {
+		DLB2_HW_ERR(hw,
+			    "[%s()] Internal error: unable to find domain %d\n",
+			    __func__, port->domain_id.phys_id);
+		return -EINVAL;
+	}
+
+	/*
+	 * Set the QID inflight limit to 0 to prevent further scheduling of the
+	 * queue.
+	 */
+	DLB2_CSR_WR(hw, DLB2_LSP_QID_LDB_INFL_LIM(hw->ver,
+						  queue->id.phys_id), 0);
+
+	if (!dlb2_port_find_slot(port, DLB2_QUEUE_UNMAPPED, &slot)) {
+		DLB2_HW_ERR(hw,
+			    "Internal error: No available unmapped slots\n");
+		return -EFAULT;
+	}
+
+	port->qid_map[slot].qid = queue->id.phys_id;
+	port->qid_map[slot].priority = priority;
+
+	state = DLB2_QUEUE_MAP_IN_PROG;
+	ret = dlb2_port_slot_state_transition(hw, port, queue, slot, state);
+	if (ret)
+		return ret;
+
+	infl_cnt = DLB2_CSR_RD(hw,
+			       DLB2_LSP_QID_LDB_INFL_CNT(hw->ver,
+						    queue->id.phys_id));
+
+	if (DLB2_BITS_GET(infl_cnt, DLB2_LSP_QID_LDB_INFL_CNT_COUNT)) {
+		/*
+		 * The queue is owed completions so it's not safe to map it
+		 * yet. Schedule a kernel thread to complete the mapping later,
+		 * once software has completed all the queue's inflight events.
+		 */
+		if (!os_worker_active(hw))
+			os_schedule_work(hw);
+
+		return 1;
+	}
+
+	/*
+	 * Disable the affected CQ, and the CQs already mapped to the QID,
+	 * before reading the QID's inflight count a second time. There is an
+	 * unlikely race in which the QID may schedule one more QE after we
+	 * read an inflight count of 0, and disabling the CQs guarantees that
+	 * the race will not occur after a re-read of the inflight count
+	 * register.
+	 */
+	if (port->enabled)
+		dlb2_ldb_port_cq_disable(hw, port);
+
+	dlb2_ldb_queue_disable_mapped_cqs(hw, domain, queue);
+
+	infl_cnt = DLB2_CSR_RD(hw,
+			       DLB2_LSP_QID_LDB_INFL_CNT(hw->ver,
+						    queue->id.phys_id));
+
+	if (DLB2_BITS_GET(infl_cnt, DLB2_LSP_QID_LDB_INFL_CNT_COUNT)) {
+		if (port->enabled)
+			dlb2_ldb_port_cq_enable(hw, port);
+
+		dlb2_ldb_queue_enable_mapped_cqs(hw, domain, queue);
+
+		/*
+		 * The queue is owed completions so it's not safe to map it
+		 * yet. Schedule a kernel thread to complete the mapping later,
+		 * once software has completed all the queue's inflight events.
+		 */
+		if (!os_worker_active(hw))
+			os_schedule_work(hw);
+
+		return 1;
+	}
+
+	return dlb2_ldb_port_finish_map_qid_dynamic(hw, domain, port, queue);
+}
+
+static int dlb2_ldb_port_map_qid(struct dlb2_hw *hw,
+				 struct dlb2_hw_domain *domain,
+				 struct dlb2_ldb_port *port,
+				 struct dlb2_ldb_queue *queue,
+				 u8 prio)
+{
+	if (domain->started)
+		return dlb2_ldb_port_map_qid_dynamic(hw, port, queue, prio);
+	else
+		return dlb2_ldb_port_map_qid_static(hw, port, queue, prio);
+}
+
+static int dlb2_ldb_port_unmap_qid(struct dlb2_hw *hw,
+				   struct dlb2_ldb_port *port,
+				   struct dlb2_ldb_queue *queue)
+{
+	enum dlb2_qid_map_state mapped, in_progress, pending_map, unmapped;
+	u32 lsp_qid2cq2;
+	u32 lsp_qid2cq;
+	u32 atm_qid2cq;
+	u32 cq2priov;
+	u32 queue_id;
+	u32 port_id;
+	int i;
+
+	/* Find the queue's slot */
+	mapped = DLB2_QUEUE_MAPPED;
+	in_progress = DLB2_QUEUE_UNMAP_IN_PROG;
+	pending_map = DLB2_QUEUE_UNMAP_IN_PROG_PENDING_MAP;
+
+	if (!dlb2_port_find_slot_queue(port, mapped, queue, &i) &&
+	    !dlb2_port_find_slot_queue(port, in_progress, queue, &i) &&
+	    !dlb2_port_find_slot_queue(port, pending_map, queue, &i)) {
+		DLB2_HW_ERR(hw,
+			    "[%s():%d] Internal error: QID %d isn't mapped\n",
+			    __func__, __LINE__, queue->id.phys_id);
+		return -EFAULT;
+	}
+
+	port_id = port->id.phys_id;
+	queue_id = queue->id.phys_id;
+
+	/* Read-modify-write the priority and valid bit register */
+	cq2priov = DLB2_CSR_RD(hw, DLB2_LSP_CQ2PRIOV(hw->ver, port_id));
+
+	cq2priov &= ~(1 << (i + DLB2_LSP_CQ2PRIOV_V_LOC));
+
+	DLB2_CSR_WR(hw, DLB2_LSP_CQ2PRIOV(hw->ver, port_id), cq2priov);
+
+	atm_qid2cq = DLB2_CSR_RD(hw, DLB2_ATM_QID2CQIDIX(queue_id,
+							 port_id / 4));
+
+	lsp_qid2cq = DLB2_CSR_RD(hw,
+				 DLB2_LSP_QID2CQIDIX(hw->ver,
+						queue_id, port_id / 4));
+
+	lsp_qid2cq2 = DLB2_CSR_RD(hw,
+				  DLB2_LSP_QID2CQIDIX2(hw->ver,
+						  queue_id, port_id / 4));
+
+	switch (port_id % 4) {
+	case 0:
+		atm_qid2cq &= ~(1 << (i + DLB2_ATM_QID2CQIDIX_00_CQ_P0_LOC));
+		lsp_qid2cq &= ~(1 << (i + DLB2_LSP_QID2CQIDIX_00_CQ_P0_LOC));
+		lsp_qid2cq2 &= ~(1 << (i + DLB2_LSP_QID2CQIDIX2_00_CQ_P0_LOC));
+		break;
+
+	case 1:
+		atm_qid2cq &= ~(1 << (i + DLB2_ATM_QID2CQIDIX_00_CQ_P1_LOC));
+		lsp_qid2cq &= ~(1 << (i + DLB2_LSP_QID2CQIDIX_00_CQ_P1_LOC));
+		lsp_qid2cq2 &= ~(1 << (i + DLB2_LSP_QID2CQIDIX2_00_CQ_P1_LOC));
+		break;
+
+	case 2:
+		atm_qid2cq &= ~(1 << (i + DLB2_ATM_QID2CQIDIX_00_CQ_P2_LOC));
+		lsp_qid2cq &= ~(1 << (i + DLB2_LSP_QID2CQIDIX_00_CQ_P2_LOC));
+		lsp_qid2cq2 &= ~(1 << (i + DLB2_LSP_QID2CQIDIX2_00_CQ_P2_LOC));
+		break;
+
+	case 3:
+		atm_qid2cq &= ~(1 << (i + DLB2_ATM_QID2CQIDIX_00_CQ_P3_LOC));
+		lsp_qid2cq &= ~(1 << (i + DLB2_LSP_QID2CQIDIX_00_CQ_P3_LOC));
+		lsp_qid2cq2 &= ~(1 << (i + DLB2_LSP_QID2CQIDIX2_00_CQ_P3_LOC));
+		break;
+	}
+
+	DLB2_CSR_WR(hw, DLB2_ATM_QID2CQIDIX(queue_id, port_id / 4), atm_qid2cq);
+
+	DLB2_CSR_WR(hw, DLB2_LSP_QID2CQIDIX(hw->ver, queue_id, port_id / 4),
+		    lsp_qid2cq);
+
+	DLB2_CSR_WR(hw, DLB2_LSP_QID2CQIDIX2(hw->ver, queue_id, port_id / 4),
+		    lsp_qid2cq2);
+
+	dlb2_flush_csr(hw);
+
+	unmapped = DLB2_QUEUE_UNMAPPED;
+
+	return dlb2_port_slot_state_transition(hw, port, queue, i, unmapped);
+}
+
+static void
+dlb2_log_create_sched_domain_args(struct dlb2_hw *hw,
+				  struct dlb2_create_sched_domain_args *args,
+				  bool vdev_req,
+				  unsigned int vdev_id)
+{
+	DLB2_HW_DBG(hw, "DLB2 create sched domain arguments:\n");
+	if (vdev_req)
+		DLB2_HW_DBG(hw, "(Request from vdev %d)\n", vdev_id);
+	DLB2_HW_DBG(hw, "\tNumber of LDB queues:          %d\n",
+		    args->num_ldb_queues);
+	DLB2_HW_DBG(hw, "\tNumber of LDB ports (any CoS): %d\n",
+		    args->num_ldb_ports);
+	DLB2_HW_DBG(hw, "\tNumber of LDB ports (CoS 0):   %d\n",
+		    args->num_cos_ldb_ports[0]);
+	DLB2_HW_DBG(hw, "\tNumber of LDB ports (CoS 1):   %d\n",
+		    args->num_cos_ldb_ports[1]);
+	DLB2_HW_DBG(hw, "\tNumber of LDB ports (CoS 2):   %d\n",
+		    args->num_cos_ldb_ports[2]);
+	DLB2_HW_DBG(hw, "\tNumber of LDB ports (CoS 3):   %d\n",
+		    args->num_cos_ldb_ports[3]);
+	DLB2_HW_DBG(hw, "\tStrict CoS allocation:         %d\n",
+		    args->cos_strict);
+	DLB2_HW_DBG(hw, "\tNumber of DIR ports:           %d\n",
+		    args->num_dir_ports);
+	DLB2_HW_DBG(hw, "\tNumber of ATM inflights:       %d\n",
+		    args->num_atomic_inflights);
+	DLB2_HW_DBG(hw, "\tNumber of hist list entries:   %d\n",
+		    args->num_hist_list_entries);
+	if (hw->ver == DLB2_HW_V2) {
+		DLB2_HW_DBG(hw, "\tNumber of LDB credits:         %d\n",
+			    args->num_ldb_credits);
+		DLB2_HW_DBG(hw, "\tNumber of DIR credits:         %d\n",
+			    args->num_dir_credits);
+	} else {
+		DLB2_HW_DBG(hw, "\tNumber of credits:         %d\n",
+			    args->num_credits);
+	}
+}
+
+/**
+ * dlb2_hw_create_sched_domain() - create a scheduling domain
+ * @hw: dlb2_hw handle for a particular device.
+ * @args: scheduling domain creation arguments.
+ * @resp: response structure.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
+ *
+ * This function creates a scheduling domain containing the resources specified
+ * in args. The individual resources (queues, ports, credits) can be configured
+ * after creating a scheduling domain.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ *
+ * Return:
+ * Returns 0 upon success, < 0 otherwise. If an error occurs, resp->status is
+ * assigned a detailed error code from enum dlb2_error. If successful, resp->id
+ * contains the domain ID.
+ *
+ * resp->id contains a virtual ID if vdev_req is true.
+ *
+ * Errors:
+ * EINVAL - A requested resource is unavailable, or the requested domain name
+ *	    is already in use.
+ * EFAULT - Internal error (resp->status not set).
+ */
+int dlb2_hw_create_sched_domain(struct dlb2_hw *hw,
+				struct dlb2_create_sched_domain_args *args,
+				struct dlb2_cmd_response *resp,
+				bool vdev_req,
+				unsigned int vdev_id)
+{
+	struct dlb2_function_resources *rsrcs;
+	struct dlb2_hw_domain *domain;
+	int ret;
+
+	rsrcs = (vdev_req) ? &hw->vdev[vdev_id] : &hw->pf;
+
+	dlb2_log_create_sched_domain_args(hw, args, vdev_req, vdev_id);
+
+	/*
+	 * Verify that hardware resources are available before attempting to
+	 * satisfy the request. This simplifies the error unwinding code.
+	 */
+	ret = dlb2_verify_create_sched_dom_args(rsrcs, args, resp, hw, &domain);
+	if (ret)
+		return ret;
+
+	dlb2_init_domain_rsrc_lists(domain);
+
+	ret = dlb2_domain_attach_resources(hw, rsrcs, domain, args, resp);
+	if (ret) {
+		DLB2_HW_ERR(hw,
+			    "[%s()] Internal error: failed to verify args.\n",
+			    __func__);
+
+		return ret;
+	}
+
+	dlb2_list_del(&rsrcs->avail_domains, &domain->func_list);
+
+	dlb2_list_add(&rsrcs->used_domains, &domain->func_list);
+
+	resp->id = (vdev_req) ? domain->id.virt_id : domain->id.phys_id;
+	resp->status = 0;
+
+	return 0;
+}
+
+static void
+dlb2_log_create_ldb_queue_args(struct dlb2_hw *hw,
+			       u32 domain_id,
+			       struct dlb2_create_ldb_queue_args *args,
+			       bool vdev_req,
+			       unsigned int vdev_id)
+{
+	DLB2_HW_DBG(hw, "DLB2 create load-balanced queue arguments:\n");
+	if (vdev_req)
+		DLB2_HW_DBG(hw, "(Request from vdev %d)\n", vdev_id);
+	DLB2_HW_DBG(hw, "\tDomain ID:                  %d\n",
+		    domain_id);
+	DLB2_HW_DBG(hw, "\tNumber of sequence numbers: %d\n",
+		    args->num_sequence_numbers);
+	DLB2_HW_DBG(hw, "\tNumber of QID inflights:    %d\n",
+		    args->num_qid_inflights);
+	DLB2_HW_DBG(hw, "\tNumber of ATM inflights:    %d\n",
+		    args->num_atomic_inflights);
+}
+
+/**
+ * dlb2_hw_create_ldb_queue() - create a load-balanced queue
+ * @hw: dlb2_hw handle for a particular device.
+ * @domain_id: domain ID.
+ * @args: queue creation arguments.
+ * @resp: response structure.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
+ *
+ * This function creates a load-balanced queue.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ *
+ * Return:
+ * Returns 0 upon success, < 0 otherwise. If an error occurs, resp->status is
+ * assigned a detailed error code from enum dlb2_error. If successful, resp->id
+ * contains the queue ID.
+ *
+ * resp->id contains a virtual ID if vdev_req is true.
+ *
+ * Errors:
+ * EINVAL - A requested resource is unavailable, the domain is not configured,
+ *	    the domain has already been started, or the requested queue name is
+ *	    already in use.
+ * EFAULT - Internal error (resp->status not set).
+ */
+int dlb2_hw_create_ldb_queue(struct dlb2_hw *hw,
+			     u32 domain_id,
+			     struct dlb2_create_ldb_queue_args *args,
+			     struct dlb2_cmd_response *resp,
+			     bool vdev_req,
+			     unsigned int vdev_id)
+{
+	struct dlb2_hw_domain *domain;
+	struct dlb2_ldb_queue *queue;
+	int ret;
+
+	dlb2_log_create_ldb_queue_args(hw, domain_id, args, vdev_req, vdev_id);
+
+	/*
+	 * Verify that hardware resources are available before attempting to
+	 * satisfy the request. This simplifies the error unwinding code.
+	 */
+	ret = dlb2_verify_create_ldb_queue_args(hw,
+						domain_id,
+						args,
+						resp,
+						vdev_req,
+						vdev_id,
+						&domain,
+						&queue);
+	if (ret)
+		return ret;
+
+	ret = dlb2_ldb_queue_attach_resources(hw, domain, queue, args);
+
+	if (ret) {
+		DLB2_HW_ERR(hw,
+			    "[%s():%d] Internal error: failed to attach the ldb queue resources\n",
+			    __func__, __LINE__);
+		return ret;
+	}
+
+	dlb2_configure_ldb_queue(hw, domain, queue, args, vdev_req, vdev_id);
+
+	queue->num_mappings = 0;
+
+	queue->configured = true;
+
+	/*
+	 * Configuration succeeded, so move the resource from the 'avail' to
+	 * the 'used' list.
+	 */
+	dlb2_list_del(&domain->avail_ldb_queues, &queue->domain_list);
+
+	dlb2_list_add(&domain->used_ldb_queues, &queue->domain_list);
+
+	resp->status = 0;
+	resp->id = (vdev_req) ? queue->id.virt_id : queue->id.phys_id;
+
+	return 0;
+}
+
+static void
+dlb2_log_create_dir_queue_args(struct dlb2_hw *hw,
+			       u32 domain_id,
+			       struct dlb2_create_dir_queue_args *args,
+			       bool vdev_req,
+			       unsigned int vdev_id)
+{
+	DLB2_HW_DBG(hw, "DLB2 create directed queue arguments:\n");
+	if (vdev_req)
+		DLB2_HW_DBG(hw, "(Request from vdev %d)\n", vdev_id);
+	DLB2_HW_DBG(hw, "\tDomain ID: %d\n", domain_id);
+	DLB2_HW_DBG(hw, "\tPort ID:   %d\n", args->port_id);
+}
+
+/**
+ * dlb2_hw_create_dir_queue() - create a directed queue
+ * @hw: dlb2_hw handle for a particular device.
+ * @domain_id: domain ID.
+ * @args: queue creation arguments.
+ * @resp: response structure.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
+ *
+ * This function creates a directed queue.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ *
+ * Return:
+ * Returns 0 upon success, < 0 otherwise. If an error occurs, resp->status is
+ * assigned a detailed error code from enum dlb2_error. If successful, resp->id
+ * contains the queue ID.
+ *
+ * resp->id contains a virtual ID if vdev_req is true.
+ *
+ * Errors:
+ * EINVAL - A requested resource is unavailable, the domain is not configured,
+ *	    or the domain has already been started.
+ * EFAULT - Internal error (resp->status not set).
+ */
+int dlb2_hw_create_dir_queue(struct dlb2_hw *hw,
+			     u32 domain_id,
+			     struct dlb2_create_dir_queue_args *args,
+			     struct dlb2_cmd_response *resp,
+			     bool vdev_req,
+			     unsigned int vdev_id)
+{
+	struct dlb2_dir_pq_pair *queue;
+	struct dlb2_hw_domain *domain;
+	int ret;
+
+	dlb2_log_create_dir_queue_args(hw, domain_id, args, vdev_req, vdev_id);
+
+	/*
+	 * Verify that hardware resources are available before attempting to
+	 * satisfy the request. This simplifies the error unwinding code.
+	 */
+	ret = dlb2_verify_create_dir_queue_args(hw,
+						domain_id,
+						args,
+						resp,
+						vdev_req,
+						vdev_id,
+						&domain,
+						&queue);
+	if (ret)
+		return ret;
+
+	dlb2_configure_dir_queue(hw, domain, queue, args, vdev_req, vdev_id);
+
+	/*
+	 * Configuration succeeded, so move the resource from the 'avail' to
+	 * the 'used' list (if it's not already there).
+	 */
+	if (args->port_id == -1) {
+		dlb2_list_del(&domain->avail_dir_pq_pairs,
+			      &queue->domain_list);
+
+		dlb2_list_add(&domain->used_dir_pq_pairs,
+			      &queue->domain_list);
+	}
+
+	resp->status = 0;
+
+	resp->id = (vdev_req) ? queue->id.virt_id : queue->id.phys_id;
+
+	return 0;
+}
+
+static void
+dlb2_log_create_ldb_port_args(struct dlb2_hw *hw,
+			      u32 domain_id,
+			      uintptr_t cq_dma_base,
+			      struct dlb2_create_ldb_port_args *args,
+			      bool vdev_req,
+			      unsigned int vdev_id)
+{
+	DLB2_HW_DBG(hw, "DLB2 create load-balanced port arguments:\n");
+	if (vdev_req)
+		DLB2_HW_DBG(hw, "(Request from vdev %d)\n", vdev_id);
+	DLB2_HW_DBG(hw, "\tDomain ID:                 %d\n",
+		    domain_id);
+	DLB2_HW_DBG(hw, "\tCQ depth:                  %d\n",
+		    args->cq_depth);
+	DLB2_HW_DBG(hw, "\tCQ hist list size:         %d\n",
+		    args->cq_history_list_size);
+	DLB2_HW_DBG(hw, "\tCQ base address:           0x%lx\n",
+		    cq_dma_base);
+	DLB2_HW_DBG(hw, "\tCoS ID:                    %u\n", args->cos_id);
+	DLB2_HW_DBG(hw, "\tStrict CoS allocation:     %u\n",
+		    args->cos_strict);
+}
+
+/**
+ * dlb2_hw_create_ldb_port() - create a load-balanced port
+ * @hw: dlb2_hw handle for a particular device.
+ * @domain_id: domain ID.
+ * @args: port creation arguments.
+ * @cq_dma_base: base address of the CQ memory. This can be a PA or an IOVA.
+ * @resp: response structure.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
+ *
+ * This function creates a load-balanced port.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ *
+ * Return:
+ * Returns 0 upon success, < 0 otherwise. If an error occurs, resp->status is
+ * assigned a detailed error code from enum dlb2_error. If successful, resp->id
+ * contains the port ID.
+ *
+ * resp->id contains a virtual ID if vdev_req is true.
+ *
+ * Errors:
+ * EINVAL - A requested resource is unavailable, a credit setting is invalid, a
+ *	    pointer address is not properly aligned, the domain is not
+ *	    configured, or the domain has already been started.
+ * EFAULT - Internal error (resp->status not set).
+ */
+int dlb2_hw_create_ldb_port(struct dlb2_hw *hw,
+			    u32 domain_id,
+			    struct dlb2_create_ldb_port_args *args,
+			    uintptr_t cq_dma_base,
+			    struct dlb2_cmd_response *resp,
+			    bool vdev_req,
+			    unsigned int vdev_id)
+{
+	struct dlb2_hw_domain *domain;
+	struct dlb2_ldb_port *port;
+	int ret, cos_id;
+
+	dlb2_log_create_ldb_port_args(hw,
+				      domain_id,
+				      cq_dma_base,
+				      args,
+				      vdev_req,
+				      vdev_id);
+
+	/*
+	 * Verify that hardware resources are available before attempting to
+	 * satisfy the request. This simplifies the error unwinding code.
+	 */
+	ret = dlb2_verify_create_ldb_port_args(hw,
+					       domain_id,
+					       cq_dma_base,
+					       args,
+					       resp,
+					       vdev_req,
+					       vdev_id,
+					       &domain,
+					       &port,
+					       &cos_id);
+	if (ret)
+		return ret;
+
+	ret = dlb2_configure_ldb_port(hw,
+				      domain,
+				      port,
+				      cq_dma_base,
+				      args,
+				      vdev_req,
+				      vdev_id);
+	if (ret)
+		return ret;
+
+	/*
+	 * Configuration succeeded, so move the resource from the 'avail' to
+	 * the 'used' list.
+	 */
+	dlb2_list_del(&domain->avail_ldb_ports[cos_id], &port->domain_list);
+
+	dlb2_list_add(&domain->used_ldb_ports[cos_id], &port->domain_list);
+
+	resp->status = 0;
+	resp->id = (vdev_req) ? port->id.virt_id : port->id.phys_id;
+
+	return 0;
+}
+
+static void
+dlb2_log_create_dir_port_args(struct dlb2_hw *hw,
+			      u32 domain_id,
+			      uintptr_t cq_dma_base,
+			      struct dlb2_create_dir_port_args *args,
+			      bool vdev_req,
+			      unsigned int vdev_id)
+{
+	DLB2_HW_DBG(hw, "DLB2 create directed port arguments:\n");
+	if (vdev_req)
+		DLB2_HW_DBG(hw, "(Request from vdev %d)\n", vdev_id);
+	DLB2_HW_DBG(hw, "\tDomain ID:                 %d\n",
+		    domain_id);
+	DLB2_HW_DBG(hw, "\tCQ depth:                  %d\n",
+		    args->cq_depth);
+	DLB2_HW_DBG(hw, "\tCQ base address:           0x%lx\n",
+		    cq_dma_base);
+}
+
+/**
+ * dlb2_hw_create_dir_port() - create a directed port
+ * @hw: dlb2_hw handle for a particular device.
+ * @domain_id: domain ID.
+ * @args: port creation arguments.
+ * @cq_dma_base: base address of the CQ memory. This can be a PA or an IOVA.
+ * @resp: response structure.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
+ *
+ * This function creates a directed port.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ *
+ * Return:
+ * Returns 0 upon success, < 0 otherwise. If an error occurs, resp->status is
+ * assigned a detailed error code from enum dlb2_error. If successful, resp->id
+ * contains the port ID.
+ *
+ * resp->id contains a virtual ID if vdev_req is true.
+ *
+ * Errors:
+ * EINVAL - A requested resource is unavailable, a credit setting is invalid, a
+ *	    pointer address is not properly aligned, the domain is not
+ *	    configured, or the domain has already been started.
+ * EFAULT - Internal error (resp->status not set).
+ */
+int dlb2_hw_create_dir_port(struct dlb2_hw *hw,
+			    u32 domain_id,
+			    struct dlb2_create_dir_port_args *args,
+			    uintptr_t cq_dma_base,
+			    struct dlb2_cmd_response *resp,
+			    bool vdev_req,
+			    unsigned int vdev_id)
+{
+	struct dlb2_dir_pq_pair *port;
+	struct dlb2_hw_domain *domain;
+	int ret;
+
+	dlb2_log_create_dir_port_args(hw,
+				      domain_id,
+				      cq_dma_base,
+				      args,
+				      vdev_req,
+				      vdev_id);
+
+	/*
+	 * Verify that hardware resources are available before attempting to
+	 * satisfy the request. This simplifies the error unwinding code.
+	 */
+	ret = dlb2_verify_create_dir_port_args(hw,
+					       domain_id,
+					       cq_dma_base,
+					       args,
+					       resp,
+					       vdev_req,
+					       vdev_id,
+					       &domain,
+					       &port);
+	if (ret)
+		return ret;
+
+	ret = dlb2_configure_dir_port(hw,
+				      domain,
+				      port,
+				      cq_dma_base,
+				      args,
+				      vdev_req,
+				      vdev_id);
+	if (ret)
+		return ret;
+
+	/*
+	 * Configuration succeeded, so move the resource from the 'avail' or
+	 * 'res' to the 'used' list (if it's not already there).
+	 */
+	if (args->queue_id == -1) {
+		struct dlb2_list_head *res = &domain->rsvd_dir_pq_pairs;
+		struct dlb2_list_head *avail = &domain->avail_dir_pq_pairs;
+
+		if ((args->is_producer && !dlb2_list_empty(res)) ||
+		    dlb2_list_empty(avail))
+			dlb2_list_del(res, &port->domain_list);
+		else
+			dlb2_list_del(avail, &port->domain_list);
+
+		dlb2_list_add(&domain->used_dir_pq_pairs, &port->domain_list);
+	}
+
+	resp->status = 0;
+	resp->id = (vdev_req) ? port->id.virt_id : port->id.phys_id;
+
+	return 0;
+}
+
+static void dlb2_log_start_domain(struct dlb2_hw *hw,
+				  u32 domain_id,
+				  bool vdev_req,
+				  unsigned int vdev_id)
+{
+	DLB2_HW_DBG(hw, "DLB2 start domain arguments:\n");
+	if (vdev_req)
+		DLB2_HW_DBG(hw, "(Request from vdev %d)\n", vdev_id);
+	DLB2_HW_DBG(hw, "\tDomain ID: %d\n", domain_id);
+}
+
+static int
+dlb2_hw_start_stop_domain(struct dlb2_hw *hw,
+		     u32 domain_id,
+		     bool start_domain,
+		     struct dlb2_cmd_response *resp,
+		     bool vdev_req,
+		     unsigned int vdev_id)
+{
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_dir_pq_pair *dir_queue;
+	struct dlb2_ldb_queue *ldb_queue;
+	struct dlb2_hw_domain *domain;
+	int ret;
+
+	dlb2_log_start_domain(hw, domain_id, vdev_req, vdev_id);
+
+	ret = dlb2_verify_start_stop_domain_args(hw,
+						 domain_id,
+						 start_domain,
+						 resp,
+						 vdev_req,
+						 vdev_id,
+						 &domain);
+	if (ret)
+		return ret;
+
+	/*
+	 * Enable load-balanced and directed queue write permissions for the
+	 * queues this domain owns. Without this, the DLB2 will drop all
+	 * incoming traffic to those queues.
+	 */
+	DLB2_DOM_LIST_FOR(domain->used_ldb_queues, ldb_queue, iter) {
+		u32 vasqid_v = 0;
+		unsigned int offs;
+
+		if (start_domain)
+			DLB2_BIT_SET(vasqid_v, DLB2_SYS_LDB_VASQID_V_VASQID_V);
+
+		offs = domain->id.phys_id * DLB2_MAX_NUM_LDB_QUEUES +
+			ldb_queue->id.phys_id;
+
+		DLB2_CSR_WR(hw, DLB2_SYS_LDB_VASQID_V(offs), vasqid_v);
+	}
+
+	DLB2_DOM_LIST_FOR(domain->used_dir_pq_pairs, dir_queue, iter) {
+		u32 vasqid_v = 0;
+		unsigned int offs;
+
+		if (start_domain)
+			DLB2_BIT_SET(vasqid_v, DLB2_SYS_DIR_VASQID_V_VASQID_V);
+
+		offs = domain->id.phys_id * DLB2_MAX_NUM_DIR_PORTS(hw->ver) +
+			dir_queue->id.phys_id;
+
+		DLB2_CSR_WR(hw, DLB2_SYS_DIR_VASQID_V(offs), vasqid_v);
+	}
+
+	dlb2_flush_csr(hw);
+
+	/* Return any pending tokens before stopping the domain. */
+	if (!start_domain) {
+		dlb2_domain_drain_ldb_cqs(hw, domain, false);
+		dlb2_domain_drain_dir_cqs(hw, domain, false);
+	}
+	domain->started = start_domain;
+
+	resp->status = 0;
+
+	return 0;
+}
+
+/**
+ * dlb2_hw_start_domain() - start a scheduling domain
+ * @hw: dlb2_hw handle for a particular device.
+ * @domain_id: domain ID.
+ * @arg: start domain arguments.
+ * @resp: response structure.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
+ *
+ * This function starts a scheduling domain, which allows applications to send
+ * traffic through it. Once a domain is started, its resources can no longer be
+ * configured (besides QID remapping and port enable/disable).
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ *
+ * Return:
+ * Returns 0 upon success, < 0 otherwise. If an error occurs, resp->status is
+ * assigned a detailed error code from enum dlb2_error.
+ *
+ * Errors:
+ * EINVAL - the domain is not configured, or the domain is already started.
+ */
+int
+dlb2_hw_start_domain(struct dlb2_hw *hw,
+		     u32 domain_id,
+		     __attribute((unused)) struct dlb2_start_domain_args *args,
+		     struct dlb2_cmd_response *resp,
+		     bool vdev_req,
+		     unsigned int vdev_id)
+{
+	return dlb2_hw_start_stop_domain(hw, domain_id, true, resp, vdev_req, vdev_id);
+}
+
+/**
+ * dlb2_hw_stop_domain() - stop a scheduling domain
+ * @hw: dlb2_hw handle for a particular device.
+ * @domain_id: domain ID.
+ * @arg: stop domain arguments.
+ * @resp: response structure.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
+ *
+ * This function stops a scheduling domain, which allows applications to send
+ * traffic through it. Once a domain is stoped, its resources can no longer be
+ * configured (besides QID remapping and port enable/disable).
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ *
+ * Return:
+ * Returns 0 upon success, < 0 otherwise. If an error occurs, resp->status is
+ * assigned a detailed error code from enum dlb2_error.
+ *
+ * Errors:
+ * EINVAL - the domain is not configured, or the domain is already stoped.
+ */
+int
+dlb2_hw_stop_domain(struct dlb2_hw *hw,
+		     u32 domain_id,
+		     __attribute((unused)) struct dlb2_stop_domain_args *args,
+		     struct dlb2_cmd_response *resp,
+		     bool vdev_req,
+		     unsigned int vdev_id)
+{
+	return dlb2_hw_start_stop_domain(hw, domain_id, false, resp, vdev_req, vdev_id);
+}
+
+static void
+dlb2_domain_finish_unmap_port_slot(struct dlb2_hw *hw,
+				   struct dlb2_hw_domain *domain,
+				   struct dlb2_ldb_port *port,
+				   int slot)
+{
+	enum dlb2_qid_map_state state;
+	struct dlb2_ldb_queue *queue;
+
+	queue = &hw->rsrcs.ldb_queues[port->qid_map[slot].qid];
+
+	state = port->qid_map[slot].state;
+
+	/* Update the QID2CQIDX and CQ2QID vectors */
+	dlb2_ldb_port_unmap_qid(hw, port, queue);
+
+	/*
+	 * Ensure the QID will not be serviced by this {CQ, slot} by clearing
+	 * the has_work bits
+	 */
+	dlb2_ldb_port_clear_has_work_bits(hw, port, slot);
+
+	/* Reset the {CQ, slot} to its default state */
+	dlb2_ldb_port_set_queue_if_status(hw, port, slot);
+
+	/* Re-enable the CQ if it wasn't manually disabled by the user */
+	if (port->enabled)
+		dlb2_ldb_port_cq_enable(hw, port);
+
+	/*
+	 * If there is a mapping that is pending this slot's removal, perform
+	 * the mapping now.
+	 */
+	if (state == DLB2_QUEUE_UNMAP_IN_PROG_PENDING_MAP) {
+		struct dlb2_ldb_port_qid_map *map;
+		struct dlb2_ldb_queue *map_queue;
+		u8 prio;
+
+		map = &port->qid_map[slot];
+
+		map->qid = map->pending_qid;
+		map->priority = map->pending_priority;
+
+		map_queue = &hw->rsrcs.ldb_queues[map->qid];
+		prio = map->priority;
+
+		dlb2_ldb_port_map_qid(hw, domain, port, map_queue, prio);
+	}
+}
+
+static bool dlb2_domain_finish_unmap_port(struct dlb2_hw *hw,
+					  struct dlb2_hw_domain *domain,
+					  struct dlb2_ldb_port *port)
+{
+	u32 infl_cnt;
+	int i;
+	const int max_iters = 1000;
+	const int iter_poll_us = 100;
+
+	if (port->num_pending_removals == 0)
+		return false;
+
+	/*
+	 * The unmap requires all the CQ's outstanding inflights to be
+	 * completed. Poll up to 100ms.
+	 */
+	for (i = 0; i < max_iters; i++) {
+		infl_cnt = DLB2_CSR_RD(hw, DLB2_LSP_CQ_LDB_INFL_CNT(hw->ver,
+						       port->id.phys_id));
 
-	DLB2_BITS_SET(ctrl, port->id.phys_id, DLB2_LSP_LDB_SCHED_CTRL_CQ);
-	DLB2_BITS_SET(ctrl, slot, DLB2_LSP_LDB_SCHED_CTRL_QIDIX);
-	DLB2_BIT_SET(ctrl, DLB2_LSP_LDB_SCHED_CTRL_VALUE);
-	DLB2_BIT_SET(ctrl, DLB2_LSP_LDB_SCHED_CTRL_INFLIGHT_OK_V);
+		if (DLB2_BITS_GET(infl_cnt,
+				  DLB2_LSP_CQ_LDB_INFL_CNT_COUNT) == 0)
+			break;
+		rte_delay_us_sleep(iter_poll_us);
+	}
 
-	DLB2_CSR_WR(hw, DLB2_LSP_LDB_SCHED_CTRL(hw->ver), ctrl);
+	if (DLB2_BITS_GET(infl_cnt, DLB2_LSP_CQ_LDB_INFL_CNT_COUNT) > 0)
+		return false;
 
-	dlb2_flush_csr(hw);
+	for (i = 0; i < DLB2_MAX_NUM_QIDS_PER_LDB_CQ; i++) {
+		struct dlb2_ldb_port_qid_map *map;
+
+		map = &port->qid_map[i];
+
+		if (map->state != DLB2_QUEUE_UNMAP_IN_PROG &&
+		    map->state != DLB2_QUEUE_UNMAP_IN_PROG_PENDING_MAP)
+			continue;
+
+		dlb2_domain_finish_unmap_port_slot(hw, domain, port, i);
+	}
+
+	return true;
 }
 
-static int dlb2_ldb_port_map_qid_static(struct dlb2_hw *hw,
-					struct dlb2_ldb_port *p,
-					struct dlb2_ldb_queue *q,
-					u8 priority)
+static unsigned int
+dlb2_domain_finish_unmap_qid_procedures(struct dlb2_hw *hw,
+					struct dlb2_hw_domain *domain)
 {
-	enum dlb2_qid_map_state state;
-	u32 lsp_qid2cq2;
-	u32 lsp_qid2cq;
-	u32 atm_qid2cq;
-	u32 cq2priov;
-	u32 cq2qid;
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_ldb_port *port;
 	int i;
 
-	/* Look for a pending or already mapped slot, else an unused slot */
-	if (!dlb2_port_find_slot_queue(p, DLB2_QUEUE_MAP_IN_PROG, q, &i) &&
-	    !dlb2_port_find_slot_queue(p, DLB2_QUEUE_MAPPED, q, &i) &&
-	    !dlb2_port_find_slot(p, DLB2_QUEUE_UNMAPPED, &i)) {
-		DLB2_HW_ERR(hw,
-			    "[%s():%d] Internal error: CQ has no available QID mapping slots\n",
-			    __func__, __LINE__);
-		return -EFAULT;
+	if (!domain->configured || domain->num_pending_removals == 0)
+		return 0;
+
+	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
+		DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i], port, iter)
+			dlb2_domain_finish_unmap_port(hw, domain, port);
 	}
 
-	/* Read-modify-write the priority and valid bit register */
-	cq2priov = DLB2_CSR_RD(hw, DLB2_LSP_CQ2PRIOV(hw->ver, p->id.phys_id));
+	return domain->num_pending_removals;
+}
 
-	cq2priov |= (1 << (i + DLB2_LSP_CQ2PRIOV_V_LOC)) & DLB2_LSP_CQ2PRIOV_V;
-	cq2priov |= ((priority & 0x7) << (i + DLB2_LSP_CQ2PRIOV_PRIO_LOC) * 3)
-		    & DLB2_LSP_CQ2PRIOV_PRIO;
+/**
+ * dlb2_finish_unmap_qid_procedures() - finish any pending unmap procedures
+ * @hw: dlb2_hw handle for a particular device.
+ *
+ * This function attempts to finish any outstanding unmap procedures.
+ * This function should be called by the kernel thread responsible for
+ * finishing map/unmap procedures.
+ *
+ * Return:
+ * Returns the number of procedures that weren't completed.
+ */
+unsigned int dlb2_finish_unmap_qid_procedures(struct dlb2_hw *hw)
+{
+	int i, num = 0;
 
-	DLB2_CSR_WR(hw, DLB2_LSP_CQ2PRIOV(hw->ver, p->id.phys_id), cq2priov);
+	/* Finish queue unmap jobs for any domain that needs it */
+	for (i = 0; i < DLB2_MAX_NUM_DOMAINS; i++) {
+		struct dlb2_hw_domain *domain = &hw->domains[i];
 
-	/* Read-modify-write the QID map register */
-	if (i < 4)
-		cq2qid = DLB2_CSR_RD(hw, DLB2_LSP_CQ2QID0(hw->ver,
-							  p->id.phys_id));
-	else
-		cq2qid = DLB2_CSR_RD(hw, DLB2_LSP_CQ2QID1(hw->ver,
-							  p->id.phys_id));
+		num += dlb2_domain_finish_unmap_qid_procedures(hw, domain);
+	}
 
-	if (i == 0 || i == 4)
-		DLB2_BITS_SET(cq2qid, q->id.phys_id, DLB2_LSP_CQ2QID0_QID_P0);
-	if (i == 1 || i == 5)
-		DLB2_BITS_SET(cq2qid, q->id.phys_id, DLB2_LSP_CQ2QID0_QID_P1);
-	if (i == 2 || i == 6)
-		DLB2_BITS_SET(cq2qid, q->id.phys_id, DLB2_LSP_CQ2QID0_QID_P2);
-	if (i == 3 || i == 7)
-		DLB2_BITS_SET(cq2qid, q->id.phys_id, DLB2_LSP_CQ2QID0_QID_P3);
+	return num;
+}
 
-	if (i < 4)
-		DLB2_CSR_WR(hw,
-			    DLB2_LSP_CQ2QID0(hw->ver, p->id.phys_id), cq2qid);
-	else
-		DLB2_CSR_WR(hw,
-			    DLB2_LSP_CQ2QID1(hw->ver, p->id.phys_id), cq2qid);
+static void dlb2_domain_finish_map_port(struct dlb2_hw *hw,
+					struct dlb2_hw_domain *domain,
+					struct dlb2_ldb_port *port)
+{
+	int i;
 
-	atm_qid2cq = DLB2_CSR_RD(hw,
-				 DLB2_ATM_QID2CQIDIX(q->id.phys_id,
-						p->id.phys_id / 4));
+	for (i = 0; i < DLB2_MAX_NUM_QIDS_PER_LDB_CQ; i++) {
+		u32 infl_cnt;
+		struct dlb2_ldb_queue *queue;
+		int qid;
 
-	lsp_qid2cq = DLB2_CSR_RD(hw,
-				 DLB2_LSP_QID2CQIDIX(hw->ver, q->id.phys_id,
-						p->id.phys_id / 4));
+		if (port->qid_map[i].state != DLB2_QUEUE_MAP_IN_PROG)
+			continue;
 
-	lsp_qid2cq2 = DLB2_CSR_RD(hw,
-				  DLB2_LSP_QID2CQIDIX2(hw->ver, q->id.phys_id,
-						  p->id.phys_id / 4));
+		qid = port->qid_map[i].qid;
 
-	switch (p->id.phys_id % 4) {
-	case 0:
-		DLB2_BIT_SET(atm_qid2cq,
-			     1 << (i + DLB2_ATM_QID2CQIDIX_00_CQ_P0_LOC));
-		DLB2_BIT_SET(lsp_qid2cq,
-			     1 << (i + DLB2_LSP_QID2CQIDIX_00_CQ_P0_LOC));
-		DLB2_BIT_SET(lsp_qid2cq2,
-			     1 << (i + DLB2_LSP_QID2CQIDIX2_00_CQ_P0_LOC));
-		break;
+		queue = dlb2_get_ldb_queue_from_id(hw, qid, false, 0);
 
-	case 1:
-		DLB2_BIT_SET(atm_qid2cq,
-			     1 << (i + DLB2_ATM_QID2CQIDIX_00_CQ_P1_LOC));
-		DLB2_BIT_SET(lsp_qid2cq,
-			     1 << (i + DLB2_LSP_QID2CQIDIX_00_CQ_P1_LOC));
-		DLB2_BIT_SET(lsp_qid2cq2,
-			     1 << (i + DLB2_LSP_QID2CQIDIX2_00_CQ_P1_LOC));
-		break;
+		if (!queue) {
+			DLB2_HW_ERR(hw,
+				    "[%s()] Internal error: unable to find queue %d\n",
+				    __func__, qid);
+			continue;
+		}
 
-	case 2:
-		DLB2_BIT_SET(atm_qid2cq,
-			     1 << (i + DLB2_ATM_QID2CQIDIX_00_CQ_P2_LOC));
-		DLB2_BIT_SET(lsp_qid2cq,
-			     1 << (i + DLB2_LSP_QID2CQIDIX_00_CQ_P2_LOC));
-		DLB2_BIT_SET(lsp_qid2cq2,
-			     1 << (i + DLB2_LSP_QID2CQIDIX2_00_CQ_P2_LOC));
-		break;
+		infl_cnt = DLB2_CSR_RD(hw,
+				       DLB2_LSP_QID_LDB_INFL_CNT(hw->ver, qid));
 
-	case 3:
-		DLB2_BIT_SET(atm_qid2cq,
-			     1 << (i + DLB2_ATM_QID2CQIDIX_00_CQ_P3_LOC));
-		DLB2_BIT_SET(lsp_qid2cq,
-			     1 << (i + DLB2_LSP_QID2CQIDIX_00_CQ_P3_LOC));
-		DLB2_BIT_SET(lsp_qid2cq2,
-			     1 << (i + DLB2_LSP_QID2CQIDIX2_00_CQ_P3_LOC));
-		break;
-	}
+		if (DLB2_BITS_GET(infl_cnt, DLB2_LSP_QID_LDB_INFL_CNT_COUNT))
+			continue;
 
-	DLB2_CSR_WR(hw,
-		    DLB2_ATM_QID2CQIDIX(q->id.phys_id, p->id.phys_id / 4),
-		    atm_qid2cq);
+		/*
+		 * Disable the affected CQ, and the CQs already mapped to the
+		 * QID, before reading the QID's inflight count a second time.
+		 * There is an unlikely race in which the QID may schedule one
+		 * more QE after we read an inflight count of 0, and disabling
+		 * the CQs guarantees that the race will not occur after a
+		 * re-read of the inflight count register.
+		 */
+		if (port->enabled)
+			dlb2_ldb_port_cq_disable(hw, port);
 
-	DLB2_CSR_WR(hw,
-		    DLB2_LSP_QID2CQIDIX(hw->ver,
-					q->id.phys_id, p->id.phys_id / 4),
-		    lsp_qid2cq);
+		dlb2_ldb_queue_disable_mapped_cqs(hw, domain, queue);
 
-	DLB2_CSR_WR(hw,
-		    DLB2_LSP_QID2CQIDIX2(hw->ver,
-					 q->id.phys_id, p->id.phys_id / 4),
-		    lsp_qid2cq2);
+		infl_cnt = DLB2_CSR_RD(hw,
+				       DLB2_LSP_QID_LDB_INFL_CNT(hw->ver, qid));
 
-	dlb2_flush_csr(hw);
+		if (DLB2_BITS_GET(infl_cnt, DLB2_LSP_QID_LDB_INFL_CNT_COUNT)) {
+			if (port->enabled)
+				dlb2_ldb_port_cq_enable(hw, port);
 
-	p->qid_map[i].qid = q->id.phys_id;
-	p->qid_map[i].priority = priority;
+			dlb2_ldb_queue_enable_mapped_cqs(hw, domain, queue);
 
-	state = DLB2_QUEUE_MAPPED;
+			continue;
+		}
 
-	return dlb2_port_slot_state_transition(hw, p, q, i, state);
+		dlb2_ldb_port_finish_map_qid_dynamic(hw, domain, port, queue);
+	}
 }
 
-static int dlb2_ldb_port_set_has_work_bits(struct dlb2_hw *hw,
-					   struct dlb2_ldb_port *port,
-					   struct dlb2_ldb_queue *queue,
-					   int slot)
+static unsigned int
+dlb2_domain_finish_map_qid_procedures(struct dlb2_hw *hw,
+				      struct dlb2_hw_domain *domain)
 {
-	u32 ctrl = 0;
-	u32 active;
-	u32 enq;
-
-	/* Set the atomic scheduling haswork bit */
-	active = DLB2_CSR_RD(hw, DLB2_LSP_QID_AQED_ACTIVE_CNT(hw->ver,
-							 queue->id.phys_id));
-
-	DLB2_BITS_SET(ctrl, port->id.phys_id, DLB2_LSP_LDB_SCHED_CTRL_CQ);
-	DLB2_BITS_SET(ctrl, slot, DLB2_LSP_LDB_SCHED_CTRL_QIDIX);
-	DLB2_BIT_SET(ctrl, DLB2_LSP_LDB_SCHED_CTRL_VALUE);
-	DLB2_BITS_SET(ctrl,
-		      DLB2_BITS_GET(active,
-				    DLB2_LSP_QID_AQED_ACTIVE_CNT_COUNT) > 0,
-				    DLB2_LSP_LDB_SCHED_CTRL_RLIST_HASWORK_V);
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_ldb_port *port;
+	int i;
 
-	/* Set the non-atomic scheduling haswork bit */
-	DLB2_CSR_WR(hw, DLB2_LSP_LDB_SCHED_CTRL(hw->ver), ctrl);
+	if (!domain->configured || domain->num_pending_additions == 0)
+		return 0;
 
-	enq = DLB2_CSR_RD(hw,
-			  DLB2_LSP_QID_LDB_ENQUEUE_CNT(hw->ver,
-						       queue->id.phys_id));
+	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
+		DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i], port, iter)
+			dlb2_domain_finish_map_port(hw, domain, port);
+	}
 
-	memset(&ctrl, 0, sizeof(ctrl));
+	return domain->num_pending_additions;
+}
 
-	DLB2_BITS_SET(ctrl, port->id.phys_id, DLB2_LSP_LDB_SCHED_CTRL_CQ);
-	DLB2_BITS_SET(ctrl, slot, DLB2_LSP_LDB_SCHED_CTRL_QIDIX);
-	DLB2_BIT_SET(ctrl, DLB2_LSP_LDB_SCHED_CTRL_VALUE);
-	DLB2_BITS_SET(ctrl,
-		      DLB2_BITS_GET(enq,
-				    DLB2_LSP_QID_LDB_ENQUEUE_CNT_COUNT) > 0,
-		      DLB2_LSP_LDB_SCHED_CTRL_NALB_HASWORK_V);
+/**
+ * dlb2_finish_map_qid_procedures() - finish any pending map procedures
+ * @hw: dlb2_hw handle for a particular device.
+ *
+ * This function attempts to finish any outstanding map procedures.
+ * This function should be called by the kernel thread responsible for
+ * finishing map/unmap procedures.
+ *
+ * Return:
+ * Returns the number of procedures that weren't completed.
+ */
+unsigned int dlb2_finish_map_qid_procedures(struct dlb2_hw *hw)
+{
+	int i, num = 0;
 
-	DLB2_CSR_WR(hw, DLB2_LSP_LDB_SCHED_CTRL(hw->ver), ctrl);
+	/* Finish queue map jobs for any domain that needs it */
+	for (i = 0; i < DLB2_MAX_NUM_DOMAINS; i++) {
+		struct dlb2_hw_domain *domain = &hw->domains[i];
 
-	dlb2_flush_csr(hw);
+		num += dlb2_domain_finish_map_qid_procedures(hw, domain);
+	}
 
-	return 0;
+	return num;
 }
 
-static void dlb2_ldb_port_clear_has_work_bits(struct dlb2_hw *hw,
-					      struct dlb2_ldb_port *port,
-					      u8 slot)
+static void dlb2_log_map_qid(struct dlb2_hw *hw,
+			     u32 domain_id,
+			     struct dlb2_map_qid_args *args,
+			     bool vdev_req,
+			     unsigned int vdev_id)
 {
-	u32 ctrl = 0;
-
-	DLB2_BITS_SET(ctrl, port->id.phys_id, DLB2_LSP_LDB_SCHED_CTRL_CQ);
-	DLB2_BITS_SET(ctrl, slot, DLB2_LSP_LDB_SCHED_CTRL_QIDIX);
-	DLB2_BIT_SET(ctrl, DLB2_LSP_LDB_SCHED_CTRL_RLIST_HASWORK_V);
+	DLB2_HW_DBG(hw, "DLB2 map QID arguments:\n");
+	if (vdev_req)
+		DLB2_HW_DBG(hw, "(Request from vdev %d)\n", vdev_id);
+	DLB2_HW_DBG(hw, "\tDomain ID: %d\n",
+		    domain_id);
+	DLB2_HW_DBG(hw, "\tPort ID:   %d\n",
+		    args->port_id);
+	DLB2_HW_DBG(hw, "\tQueue ID:  %d\n",
+		    args->qid);
+	DLB2_HW_DBG(hw, "\tPriority:  %d\n",
+		    args->priority);
+}
 
-	DLB2_CSR_WR(hw, DLB2_LSP_LDB_SCHED_CTRL(hw->ver), ctrl);
+/**
+ * dlb2_hw_map_qid() - map a load-balanced queue to a load-balanced port
+ * @hw: dlb2_hw handle for a particular device.
+ * @domain_id: domain ID.
+ * @args: map QID arguments.
+ * @resp: response structure.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
+ *
+ * This function configures the DLB to schedule QEs from the specified queue
+ * to the specified port. Each load-balanced port can be mapped to up to 8
+ * queues; each load-balanced queue can potentially map to all the
+ * load-balanced ports.
+ *
+ * A successful return does not necessarily mean the mapping was configured. If
+ * this function is unable to immediately map the queue to the port, it will
+ * add the requested operation to a per-port list of pending map/unmap
+ * operations, and (if it's not already running) launch a kernel thread that
+ * periodically attempts to process all pending operations. In a sense, this is
+ * an asynchronous function.
+ *
+ * This asynchronicity creates two views of the state of hardware: the actual
+ * hardware state and the requested state (as if every request completed
+ * immediately). If there are any pending map/unmap operations, the requested
+ * state will differ from the actual state. All validation is performed with
+ * respect to the pending state; for instance, if there are 8 pending map
+ * operations for port X, a request for a 9th will fail because a load-balanced
+ * port can only map up to 8 queues.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ *
+ * Return:
+ * Returns 0 upon success, < 0 otherwise. If an error occurs, resp->status is
+ * assigned a detailed error code from enum dlb2_error.
+ *
+ * Errors:
+ * EINVAL - A requested resource is unavailable, invalid port or queue ID, or
+ *	    the domain is not configured.
+ * EFAULT - Internal error (resp->status not set).
+ * EBUSY  - The requested port has outstanding detach operations.
+ */
+int dlb2_hw_map_qid(struct dlb2_hw *hw,
+		    u32 domain_id,
+		    struct dlb2_map_qid_args *args,
+		    struct dlb2_cmd_response *resp,
+		    bool vdev_req,
+		    unsigned int vdev_id)
+{
+	struct dlb2_hw_domain *domain;
+	struct dlb2_ldb_queue *queue;
+	enum dlb2_qid_map_state st;
+	struct dlb2_ldb_port *port;
+	int ret, i;
+	u8 prio;
 
-	memset(&ctrl, 0, sizeof(ctrl));
+	dlb2_log_map_qid(hw, domain_id, args, vdev_req, vdev_id);
 
-	DLB2_BITS_SET(ctrl, port->id.phys_id, DLB2_LSP_LDB_SCHED_CTRL_CQ);
-	DLB2_BITS_SET(ctrl, slot, DLB2_LSP_LDB_SCHED_CTRL_QIDIX);
-	DLB2_BIT_SET(ctrl, DLB2_LSP_LDB_SCHED_CTRL_NALB_HASWORK_V);
+	/*
+	 * Verify that hardware resources are available before attempting to
+	 * satisfy the request. This simplifies the error unwinding code.
+	 */
+	ret = dlb2_verify_map_qid_args(hw,
+				       domain_id,
+				       args,
+				       resp,
+				       vdev_req,
+				       vdev_id,
+				       &domain,
+				       &port,
+				       &queue);
+	if (ret)
+		return ret;
 
-	DLB2_CSR_WR(hw, DLB2_LSP_LDB_SCHED_CTRL(hw->ver), ctrl);
+	prio = args->priority;
 
-	dlb2_flush_csr(hw);
-}
+	/*
+	 * If there are any outstanding detach operations for this port,
+	 * attempt to complete them. This may be necessary to free up a QID
+	 * slot for this requested mapping.
+	 */
+	if (port->num_pending_removals) {
+		bool bool_ret;
+		bool_ret = dlb2_domain_finish_unmap_port(hw, domain, port);
+		if (!bool_ret)
+			return -EBUSY;
+	}
 
+	ret = dlb2_verify_map_qid_slot_available(port, queue, resp);
+	if (ret)
+		return ret;
 
-static void dlb2_ldb_queue_set_inflight_limit(struct dlb2_hw *hw,
-					      struct dlb2_ldb_queue *queue)
-{
-	u32 infl_lim = 0;
+	/* Hardware requires disabling the CQ before mapping QIDs. */
+	if (port->enabled)
+		dlb2_ldb_port_cq_disable(hw, port);
 
-	DLB2_BITS_SET(infl_lim, queue->num_qid_inflights,
-		 DLB2_LSP_QID_LDB_INFL_LIM_LIMIT);
+	/*
+	 * If this is only a priority change, don't perform the full QID->CQ
+	 * mapping procedure
+	 */
+	st = DLB2_QUEUE_MAPPED;
+	if (dlb2_port_find_slot_queue(port, st, queue, &i)) {
+		if (prio != port->qid_map[i].priority) {
+			dlb2_ldb_port_change_qid_priority(hw, port, i, args);
+			DLB2_HW_DBG(hw, "DLB2 map: priority change\n");
+		}
 
-	DLB2_CSR_WR(hw, DLB2_LSP_QID_LDB_INFL_LIM(hw->ver, queue->id.phys_id),
-		    infl_lim);
-}
+		st = DLB2_QUEUE_MAPPED;
+		ret = dlb2_port_slot_state_transition(hw, port, queue, i, st);
+		if (ret)
+			return ret;
 
-static void dlb2_ldb_queue_clear_inflight_limit(struct dlb2_hw *hw,
-						struct dlb2_ldb_queue *queue)
-{
-	DLB2_CSR_WR(hw,
-		    DLB2_LSP_QID_LDB_INFL_LIM(hw->ver, queue->id.phys_id),
-		    DLB2_LSP_QID_LDB_INFL_LIM_RST);
-}
+		goto map_qid_done;
+	}
 
-static int dlb2_ldb_port_finish_map_qid_dynamic(struct dlb2_hw *hw,
-						struct dlb2_hw_domain *domain,
-						struct dlb2_ldb_port *port,
-						struct dlb2_ldb_queue *queue)
-{
-	struct dlb2_list_entry *iter;
-	enum dlb2_qid_map_state state;
-	int slot, ret, i;
-	u32 infl_cnt;
-	u8 prio;
-	RTE_SET_USED(iter);
+	st = DLB2_QUEUE_UNMAP_IN_PROG;
+	if (dlb2_port_find_slot_queue(port, st, queue, &i)) {
+		if (prio != port->qid_map[i].priority) {
+			dlb2_ldb_port_change_qid_priority(hw, port, i, args);
+			DLB2_HW_DBG(hw, "DLB2 map: priority change\n");
+		}
 
-	infl_cnt = DLB2_CSR_RD(hw,
-			       DLB2_LSP_QID_LDB_INFL_CNT(hw->ver,
-						    queue->id.phys_id));
+		st = DLB2_QUEUE_MAPPED;
+		ret = dlb2_port_slot_state_transition(hw, port, queue, i, st);
+		if (ret)
+			return ret;
 
-	if (DLB2_BITS_GET(infl_cnt, DLB2_LSP_QID_LDB_INFL_CNT_COUNT)) {
-		DLB2_HW_ERR(hw,
-			    "[%s()] Internal error: non-zero QID inflight count\n",
-			    __func__);
-		return -EINVAL;
+		goto map_qid_done;
 	}
 
 	/*
-	 * Static map the port and set its corresponding has_work bits.
+	 * If this is a priority change on an in-progress mapping, don't
+	 * perform the full QID->CQ mapping procedure.
 	 */
-	state = DLB2_QUEUE_MAP_IN_PROG;
-	if (!dlb2_port_find_slot_queue(port, state, queue, &slot))
-		return -EINVAL;
+	st = DLB2_QUEUE_MAP_IN_PROG;
+	if (dlb2_port_find_slot_queue(port, st, queue, &i)) {
+		port->qid_map[i].priority = prio;
 
-	prio = port->qid_map[slot].priority;
+		DLB2_HW_DBG(hw, "DLB2 map: priority change only\n");
+
+		goto map_qid_done;
+	}
 
 	/*
-	 * Update the CQ2QID, CQ2PRIOV, and QID2CQIDX registers, and
-	 * the port's qid_map state.
+	 * If this is a priority change on a pending mapping, update the
+	 * pending priority
 	 */
-	ret = dlb2_ldb_port_map_qid_static(hw, port, queue, prio);
-	if (ret)
-		return ret;
+	if (dlb2_port_find_slot_with_pending_map_queue(port, queue, &i)) {
+		port->qid_map[i].pending_priority = prio;
 
-	ret = dlb2_ldb_port_set_has_work_bits(hw, port, queue, slot);
-	if (ret)
-		return ret;
+		DLB2_HW_DBG(hw, "DLB2 map: priority change only\n");
+
+		goto map_qid_done;
+	}
 
 	/*
-	 * Ensure IF_status(cq,qid) is 0 before enabling the port to
-	 * prevent spurious schedules to cause the queue's inflight
-	 * count to increase.
+	 * If all the CQ's slots are in use, then there's an unmap in progress
+	 * (guaranteed by dlb2_verify_map_qid_slot_available()), so add this
+	 * mapping to pending_map and return. When the removal is completed for
+	 * the slot's current occupant, this mapping will be performed.
 	 */
-	dlb2_ldb_port_clear_queue_if_status(hw, port, slot);
+	if (!dlb2_port_find_slot(port, DLB2_QUEUE_UNMAPPED, &i)) {
+		if (dlb2_port_find_slot(port, DLB2_QUEUE_UNMAP_IN_PROG, &i)) {
+			enum dlb2_qid_map_state new_st;
 
-	/* Reset the queue's inflight status */
-	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
-		DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i], port, iter) {
-			state = DLB2_QUEUE_MAPPED;
-			if (!dlb2_port_find_slot_queue(port, state,
-						       queue, &slot))
-				continue;
+			port->qid_map[i].pending_qid = queue->id.phys_id;
+			port->qid_map[i].pending_priority = prio;
+
+			new_st = DLB2_QUEUE_UNMAP_IN_PROG_PENDING_MAP;
+
+			ret = dlb2_port_slot_state_transition(hw, port, queue,
+							      i, new_st);
+			if (ret)
+				return ret;
 
-			dlb2_ldb_port_set_queue_if_status(hw, port, slot);
+			DLB2_HW_DBG(hw, "DLB2 map: map pending removal\n");
+
+			goto map_qid_done;
 		}
 	}
 
-	dlb2_ldb_queue_set_inflight_limit(hw, queue);
+	/*
+	 * If the domain has started, a special "dynamic" CQ->queue mapping
+	 * procedure is required in order to safely update the CQ<->QID tables.
+	 * The "static" procedure cannot be used when traffic is flowing,
+	 * because the CQ<->QID tables cannot be updated atomically and the
+	 * scheduler won't see the new mapping unless the queue's if_status
+	 * changes, which isn't guaranteed.
+	 */
+	ret = dlb2_ldb_port_map_qid(hw, domain, port, queue, prio);
 
-	/* Re-enable CQs mapped to this queue */
-	dlb2_ldb_queue_enable_mapped_cqs(hw, domain, queue);
+	/* If ret is less than zero, it's due to an internal error */
+	if (ret < 0)
+		return ret;
 
-	/* If this queue has other mappings pending, clear its inflight limit */
-	if (queue->num_pending_additions > 0)
-		dlb2_ldb_queue_clear_inflight_limit(hw, queue);
+map_qid_done:
+	/* Enable CQ only if linking is done and not delayed by starting OS thread */
+	if ((ret == 0) && port->enabled)
+		dlb2_ldb_port_cq_enable(hw, port);
+
+	resp->status = 0;
 
 	return 0;
 }
 
+static void dlb2_log_unmap_qid(struct dlb2_hw *hw,
+			       u32 domain_id,
+			       struct dlb2_unmap_qid_args *args,
+			       bool vdev_req,
+			       unsigned int vdev_id)
+{
+	DLB2_HW_DBG(hw, "DLB2 unmap QID arguments:\n");
+	if (vdev_req)
+		DLB2_HW_DBG(hw, "(Request from vdev %d)\n", vdev_id);
+	DLB2_HW_DBG(hw, "\tDomain ID: %d\n",
+		    domain_id);
+	DLB2_HW_DBG(hw, "\tPort ID:   %d\n",
+		    args->port_id);
+	DLB2_HW_DBG(hw, "\tQueue ID:  %d\n",
+		    args->qid);
+	if (args->qid < DLB2_MAX_NUM_LDB_QUEUES)
+		DLB2_HW_DBG(hw, "\tQueue's num mappings:  %d\n",
+			    hw->rsrcs.ldb_queues[args->qid].num_mappings);
+}
+
 /**
- * dlb2_ldb_port_map_qid_dynamic() - perform a "dynamic" QID->CQ mapping
+ * dlb2_hw_unmap_qid() - Unmap a load-balanced queue from a load-balanced port
  * @hw: dlb2_hw handle for a particular device.
- * @port: load-balanced port
- * @queue: load-balanced queue
- * @priority: queue servicing priority
+ * @domain_id: domain ID.
+ * @args: unmap QID arguments.
+ * @resp: response structure.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
  *
- * Returns 0 if the queue was mapped, 1 if the mapping is scheduled to occur
- * at a later point, and <0 if an error occurred.
+ * This function configures the DLB to stop scheduling QEs from the specified
+ * queue to the specified port.
+ *
+ * A successful return does not necessarily mean the mapping was removed. If
+ * this function is unable to immediately unmap the queue from the port, it
+ * will add the requested operation to a per-port list of pending map/unmap
+ * operations, and (if it's not already running) launch a kernel thread that
+ * periodically attempts to process all pending operations. See
+ * dlb2_hw_map_qid() for more details.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ *
+ * Return:
+ * Returns 0 upon success, < 0 otherwise. If an error occurs, resp->status is
+ * assigned a detailed error code from enum dlb2_error.
+ *
+ * Errors:
+ * EINVAL - A requested resource is unavailable, invalid port or queue ID, or
+ *	    the domain is not configured.
+ * EFAULT - Internal error (resp->status not set).
  */
-static int dlb2_ldb_port_map_qid_dynamic(struct dlb2_hw *hw,
-					 struct dlb2_ldb_port *port,
-					 struct dlb2_ldb_queue *queue,
-					 u8 priority)
+int dlb2_hw_unmap_qid(struct dlb2_hw *hw,
+		      u32 domain_id,
+		      struct dlb2_unmap_qid_args *args,
+		      struct dlb2_cmd_response *resp,
+		      bool vdev_req,
+		      unsigned int vdev_id)
 {
-	enum dlb2_qid_map_state state;
 	struct dlb2_hw_domain *domain;
-	int domain_id, slot, ret;
-	u32 infl_cnt;
-
-	domain_id = port->domain_id.phys_id;
+	struct dlb2_ldb_queue *queue;
+	enum dlb2_qid_map_state st;
+	struct dlb2_ldb_port *port;
+	bool unmap_complete;
+	int i, ret;
 
-	domain = dlb2_get_domain_from_id(hw, domain_id, false, 0);
-	if (domain == NULL) {
-		DLB2_HW_ERR(hw,
-			    "[%s()] Internal error: unable to find domain %d\n",
-			    __func__, port->domain_id.phys_id);
-		return -EINVAL;
-	}
+	dlb2_log_unmap_qid(hw, domain_id, args, vdev_req, vdev_id);
 
 	/*
-	 * Set the QID inflight limit to 0 to prevent further scheduling of the
-	 * queue.
+	 * Verify that hardware resources are available before attempting to
+	 * satisfy the request. This simplifies the error unwinding code.
 	 */
-	DLB2_CSR_WR(hw, DLB2_LSP_QID_LDB_INFL_LIM(hw->ver,
-						  queue->id.phys_id), 0);
-
-	if (!dlb2_port_find_slot(port, DLB2_QUEUE_UNMAPPED, &slot)) {
-		DLB2_HW_ERR(hw,
-			    "Internal error: No available unmapped slots\n");
-		return -EFAULT;
-	}
-
-	port->qid_map[slot].qid = queue->id.phys_id;
-	port->qid_map[slot].priority = priority;
-
-	state = DLB2_QUEUE_MAP_IN_PROG;
-	ret = dlb2_port_slot_state_transition(hw, port, queue, slot, state);
+	ret = dlb2_verify_unmap_qid_args(hw,
+					 domain_id,
+					 args,
+					 resp,
+					 vdev_req,
+					 vdev_id,
+					 &domain,
+					 &port,
+					 &queue);
 	if (ret)
 		return ret;
 
-	infl_cnt = DLB2_CSR_RD(hw,
-			       DLB2_LSP_QID_LDB_INFL_CNT(hw->ver,
-						    queue->id.phys_id));
-
-	if (DLB2_BITS_GET(infl_cnt, DLB2_LSP_QID_LDB_INFL_CNT_COUNT)) {
+	/*
+	 * If the queue hasn't been mapped yet, we need to update the slot's
+	 * state and re-enable the queue's inflights.
+	 */
+	st = DLB2_QUEUE_MAP_IN_PROG;
+	if (dlb2_port_find_slot_queue(port, st, queue, &i)) {
 		/*
-		 * The queue is owed completions so it's not safe to map it
-		 * yet. Schedule a kernel thread to complete the mapping later,
-		 * once software has completed all the queue's inflight events.
+		 * Since the in-progress map was aborted, re-enable the QID's
+		 * inflights.
 		 */
-		if (!os_worker_active(hw))
-			os_schedule_work(hw);
+		if (queue->num_pending_additions == 0)
+			dlb2_ldb_queue_set_inflight_limit(hw, queue);
 
-		return 1;
+		st = DLB2_QUEUE_UNMAPPED;
+		ret = dlb2_port_slot_state_transition(hw, port, queue, i, st);
+		if (ret)
+			return ret;
+
+		goto unmap_qid_done;
 	}
 
 	/*
-	 * Disable the affected CQ, and the CQs already mapped to the QID,
-	 * before reading the QID's inflight count a second time. There is an
-	 * unlikely race in which the QID may schedule one more QE after we
-	 * read an inflight count of 0, and disabling the CQs guarantees that
-	 * the race will not occur after a re-read of the inflight count
-	 * register.
+	 * If the queue mapping is on hold pending an unmap, we simply need to
+	 * update the slot's state.
 	 */
-	if (port->enabled)
-		dlb2_ldb_port_cq_disable(hw, port);
-
-	dlb2_ldb_queue_disable_mapped_cqs(hw, domain, queue);
-
-	infl_cnt = DLB2_CSR_RD(hw,
-			       DLB2_LSP_QID_LDB_INFL_CNT(hw->ver,
-						    queue->id.phys_id));
-
-	if (DLB2_BITS_GET(infl_cnt, DLB2_LSP_QID_LDB_INFL_CNT_COUNT)) {
-		if (port->enabled)
-			dlb2_ldb_port_cq_enable(hw, port);
-
-		dlb2_ldb_queue_enable_mapped_cqs(hw, domain, queue);
-
-		/*
-		 * The queue is owed completions so it's not safe to map it
-		 * yet. Schedule a kernel thread to complete the mapping later,
-		 * once software has completed all the queue's inflight events.
-		 */
-		if (!os_worker_active(hw))
-			os_schedule_work(hw);
+	if (dlb2_port_find_slot_with_pending_map_queue(port, queue, &i)) {
+		st = DLB2_QUEUE_UNMAP_IN_PROG;
+		ret = dlb2_port_slot_state_transition(hw, port, queue, i, st);
+		if (ret)
+			return ret;
 
-		return 1;
+		goto unmap_qid_done;
 	}
 
-	return dlb2_ldb_port_finish_map_qid_dynamic(hw, domain, port, queue);
-}
-
-static void dlb2_domain_finish_map_port(struct dlb2_hw *hw,
-					struct dlb2_hw_domain *domain,
-					struct dlb2_ldb_port *port)
-{
-	int i;
-
-	for (i = 0; i < DLB2_MAX_NUM_QIDS_PER_LDB_CQ; i++) {
-		u32 infl_cnt;
-		struct dlb2_ldb_queue *queue;
-		int qid;
-
-		if (port->qid_map[i].state != DLB2_QUEUE_MAP_IN_PROG)
-			continue;
-
-		qid = port->qid_map[i].qid;
-
-		queue = dlb2_get_ldb_queue_from_id(hw, qid, false, 0);
-
-		if (queue == NULL) {
-			DLB2_HW_ERR(hw,
-				    "[%s()] Internal error: unable to find queue %d\n",
-				    __func__, qid);
-			continue;
-		}
-
-		infl_cnt = DLB2_CSR_RD(hw,
-				       DLB2_LSP_QID_LDB_INFL_CNT(hw->ver, qid));
-
-		if (DLB2_BITS_GET(infl_cnt, DLB2_LSP_QID_LDB_INFL_CNT_COUNT))
-			continue;
+	st = DLB2_QUEUE_MAPPED;
+	if (!dlb2_port_find_slot_queue(port, st, queue, &i)) {
+		DLB2_HW_ERR(hw,
+			    "[%s()] Internal error: no available CQ slots\n",
+			    __func__);
+		return -EFAULT;
+	}
 
-		/*
-		 * Disable the affected CQ, and the CQs already mapped to the
-		 * QID, before reading the QID's inflight count a second time.
-		 * There is an unlikely race in which the QID may schedule one
-		 * more QE after we read an inflight count of 0, and disabling
-		 * the CQs guarantees that the race will not occur after a
-		 * re-read of the inflight count register.
-		 */
-		if (port->enabled)
-			dlb2_ldb_port_cq_disable(hw, port);
+	/*
+	 * QID->CQ mapping removal is an asychronous procedure. It requires
+	 * stopping the DLB2 from scheduling this CQ, draining all inflights
+	 * from the CQ, then unmapping the queue from the CQ. This function
+	 * simply marks the port as needing the queue unmapped, and (if
+	 * necessary) starts the unmapping worker thread.
+	 */
+	dlb2_ldb_port_cq_disable(hw, port);
 
-		dlb2_ldb_queue_disable_mapped_cqs(hw, domain, queue);
+	st = DLB2_QUEUE_UNMAP_IN_PROG;
+	ret = dlb2_port_slot_state_transition(hw, port, queue, i, st);
+	if (ret)
+		return ret;
 
-		infl_cnt = DLB2_CSR_RD(hw,
-				       DLB2_LSP_QID_LDB_INFL_CNT(hw->ver, qid));
+	/*
+	 * Attempt to finish the unmapping now, in case the port has no
+	 * outstanding inflights. If that's not the case, this will fail and
+	 * the unmapping will be completed at a later time.
+	 */
+	unmap_complete = dlb2_domain_finish_unmap_port(hw, domain, port);
 
-		if (DLB2_BITS_GET(infl_cnt, DLB2_LSP_QID_LDB_INFL_CNT_COUNT)) {
-			if (port->enabled)
-				dlb2_ldb_port_cq_enable(hw, port);
+	/*
+	 * If the unmapping couldn't complete immediately, launch the worker
+	 * thread (if it isn't already launched) to finish it later.
+	 */
+	if (!unmap_complete && !os_worker_active(hw))
+		os_schedule_work(hw);
 
-			dlb2_ldb_queue_enable_mapped_cqs(hw, domain, queue);
+unmap_qid_done:
+	resp->status = 0;
 
-			continue;
-		}
+	return 0;
+}
 
-		dlb2_ldb_port_finish_map_qid_dynamic(hw, domain, port, queue);
-	}
+static void dlb2_log_enable_port(struct dlb2_hw *hw,
+				 u32 domain_id,
+				 u32 port_id,
+				 bool vdev_req,
+				 unsigned int vdev_id)
+{
+	DLB2_HW_DBG(hw, "DLB2 enable port arguments:\n");
+	if (vdev_req)
+		DLB2_HW_DBG(hw, "(Request from vdev %d)\n", vdev_id);
+	DLB2_HW_DBG(hw, "\tDomain ID: %d\n",
+		    domain_id);
+	DLB2_HW_DBG(hw, "\tPort ID:   %d\n",
+		    port_id);
 }
 
-static unsigned int
-dlb2_domain_finish_map_qid_procedures(struct dlb2_hw *hw,
-				      struct dlb2_hw_domain *domain)
+/**
+ * dlb2_hw_enable_ldb_port() - enable a load-balanced port for scheduling
+ * @hw: dlb2_hw handle for a particular device.
+ * @domain_id: domain ID.
+ * @args: port enable arguments.
+ * @resp: response structure.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
+ *
+ * This function configures the DLB to schedule QEs to a load-balanced port.
+ * Ports are enabled by default.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ *
+ * Return:
+ * Returns 0 upon success, < 0 otherwise. If an error occurs, resp->status is
+ * assigned a detailed error code from enum dlb2_error.
+ *
+ * Errors:
+ * EINVAL - The port ID is invalid or the domain is not configured.
+ * EFAULT - Internal error (resp->status not set).
+ */
+int dlb2_hw_enable_ldb_port(struct dlb2_hw *hw,
+			    u32 domain_id,
+			    struct dlb2_enable_ldb_port_args *args,
+			    struct dlb2_cmd_response *resp,
+			    bool vdev_req,
+			    unsigned int vdev_id)
 {
-	struct dlb2_list_entry *iter;
+	struct dlb2_hw_domain *domain;
 	struct dlb2_ldb_port *port;
-	int i;
-	RTE_SET_USED(iter);
+	int ret;
 
-	if (!domain->configured || domain->num_pending_additions == 0)
-		return 0;
+	dlb2_log_enable_port(hw, domain_id, args->port_id, vdev_req, vdev_id);
 
-	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
-		DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i], port, iter)
-			dlb2_domain_finish_map_port(hw, domain, port);
+	/*
+	 * Verify that hardware resources are available before attempting to
+	 * satisfy the request. This simplifies the error unwinding code.
+	 */
+	ret = dlb2_verify_enable_ldb_port_args(hw,
+					       domain_id,
+					       args,
+					       resp,
+					       vdev_req,
+					       vdev_id,
+					       &domain,
+					       &port);
+	if (ret)
+		return ret;
+
+	if (!port->enabled) {
+		dlb2_ldb_port_cq_enable(hw, port);
+		port->enabled = true;
 	}
 
-	return domain->num_pending_additions;
+	resp->status = 0;
+
+	return 0;
 }
 
-static int dlb2_ldb_port_unmap_qid(struct dlb2_hw *hw,
-				   struct dlb2_ldb_port *port,
-				   struct dlb2_ldb_queue *queue)
+static void dlb2_log_disable_port(struct dlb2_hw *hw,
+				  u32 domain_id,
+				  u32 port_id,
+				  bool vdev_req,
+				  unsigned int vdev_id)
 {
-	enum dlb2_qid_map_state mapped, in_progress, pending_map, unmapped;
-	u32 lsp_qid2cq2;
-	u32 lsp_qid2cq;
-	u32 atm_qid2cq;
-	u32 cq2priov;
-	u32 queue_id;
-	u32 port_id;
-	int i;
-
-	/* Find the queue's slot */
-	mapped = DLB2_QUEUE_MAPPED;
-	in_progress = DLB2_QUEUE_UNMAP_IN_PROG;
-	pending_map = DLB2_QUEUE_UNMAP_IN_PROG_PENDING_MAP;
+	DLB2_HW_DBG(hw, "DLB2 disable port arguments:\n");
+	if (vdev_req)
+		DLB2_HW_DBG(hw, "(Request from vdev %d)\n", vdev_id);
+	DLB2_HW_DBG(hw, "\tDomain ID: %d\n",
+		    domain_id);
+	DLB2_HW_DBG(hw, "\tPort ID:   %d\n",
+		    port_id);
+}
 
-	if (!dlb2_port_find_slot_queue(port, mapped, queue, &i) &&
-	    !dlb2_port_find_slot_queue(port, in_progress, queue, &i) &&
-	    !dlb2_port_find_slot_queue(port, pending_map, queue, &i)) {
-		DLB2_HW_ERR(hw,
-			    "[%s():%d] Internal error: QID %d isn't mapped\n",
-			    __func__, __LINE__, queue->id.phys_id);
-		return -EFAULT;
-	}
+/**
+ * dlb2_hw_disable_ldb_port() - disable a load-balanced port for scheduling
+ * @hw: dlb2_hw handle for a particular device.
+ * @domain_id: domain ID.
+ * @args: port disable arguments.
+ * @resp: response structure.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
+ *
+ * This function configures the DLB to stop scheduling QEs to a load-balanced
+ * port. Ports are enabled by default.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ *
+ * Return:
+ * Returns 0 upon success, < 0 otherwise. If an error occurs, resp->status is
+ * assigned a detailed error code from enum dlb2_error.
+ *
+ * Errors:
+ * EINVAL - The port ID is invalid or the domain is not configured.
+ * EFAULT - Internal error (resp->status not set).
+ */
+int dlb2_hw_disable_ldb_port(struct dlb2_hw *hw,
+			     u32 domain_id,
+			     struct dlb2_disable_ldb_port_args *args,
+			     struct dlb2_cmd_response *resp,
+			     bool vdev_req,
+			     unsigned int vdev_id)
+{
+	struct dlb2_hw_domain *domain;
+	struct dlb2_ldb_port *port;
+	int ret;
 
-	port_id = port->id.phys_id;
-	queue_id = queue->id.phys_id;
+	dlb2_log_disable_port(hw, domain_id, args->port_id, vdev_req, vdev_id);
 
-	/* Read-modify-write the priority and valid bit register */
-	cq2priov = DLB2_CSR_RD(hw, DLB2_LSP_CQ2PRIOV(hw->ver, port_id));
+	/*
+	 * Verify that hardware resources are available before attempting to
+	 * satisfy the request. This simplifies the error unwinding code.
+	 */
+	ret = dlb2_verify_disable_ldb_port_args(hw,
+						domain_id,
+						args,
+						resp,
+						vdev_req,
+						vdev_id,
+						&domain,
+						&port);
+	if (ret)
+		return ret;
 
-	cq2priov &= ~(1 << (i + DLB2_LSP_CQ2PRIOV_V_LOC));
+	if (port->enabled) {
+		dlb2_ldb_port_cq_disable(hw, port);
+		port->enabled = false;
+	}
 
-	DLB2_CSR_WR(hw, DLB2_LSP_CQ2PRIOV(hw->ver, port_id), cq2priov);
+	resp->status = 0;
 
-	atm_qid2cq = DLB2_CSR_RD(hw, DLB2_ATM_QID2CQIDIX(queue_id,
-							 port_id / 4));
+	return 0;
+}
 
-	lsp_qid2cq = DLB2_CSR_RD(hw,
-				 DLB2_LSP_QID2CQIDIX(hw->ver,
-						queue_id, port_id / 4));
+/**
+ * dlb2_hw_enable_dir_port() - enable a directed port for scheduling
+ * @hw: dlb2_hw handle for a particular device.
+ * @domain_id: domain ID.
+ * @args: port enable arguments.
+ * @resp: response structure.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
+ *
+ * This function configures the DLB to schedule QEs to a directed port.
+ * Ports are enabled by default.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ *
+ * Return:
+ * Returns 0 upon success, < 0 otherwise. If an error occurs, resp->status is
+ * assigned a detailed error code from enum dlb2_error.
+ *
+ * Errors:
+ * EINVAL - The port ID is invalid or the domain is not configured.
+ * EFAULT - Internal error (resp->status not set).
+ */
+int dlb2_hw_enable_dir_port(struct dlb2_hw *hw,
+			    u32 domain_id,
+			    struct dlb2_enable_dir_port_args *args,
+			    struct dlb2_cmd_response *resp,
+			    bool vdev_req,
+			    unsigned int vdev_id)
+{
+	struct dlb2_dir_pq_pair *port;
+	struct dlb2_hw_domain *domain;
+	int ret;
 
-	lsp_qid2cq2 = DLB2_CSR_RD(hw,
-				  DLB2_LSP_QID2CQIDIX2(hw->ver,
-						  queue_id, port_id / 4));
+	dlb2_log_enable_port(hw, domain_id, args->port_id, vdev_req, vdev_id);
 
-	switch (port_id % 4) {
-	case 0:
-		atm_qid2cq &= ~(1 << (i + DLB2_ATM_QID2CQIDIX_00_CQ_P0_LOC));
-		lsp_qid2cq &= ~(1 << (i + DLB2_LSP_QID2CQIDIX_00_CQ_P0_LOC));
-		lsp_qid2cq2 &= ~(1 << (i + DLB2_LSP_QID2CQIDIX2_00_CQ_P0_LOC));
-		break;
+	/*
+	 * Verify that hardware resources are available before attempting to
+	 * satisfy the request. This simplifies the error unwinding code.
+	 */
+	ret = dlb2_verify_enable_dir_port_args(hw,
+					       domain_id,
+					       args,
+					       resp,
+					       vdev_req,
+					       vdev_id,
+					       &domain,
+					       &port);
+	if (ret)
+		return ret;
 
-	case 1:
-		atm_qid2cq &= ~(1 << (i + DLB2_ATM_QID2CQIDIX_00_CQ_P1_LOC));
-		lsp_qid2cq &= ~(1 << (i + DLB2_LSP_QID2CQIDIX_00_CQ_P1_LOC));
-		lsp_qid2cq2 &= ~(1 << (i + DLB2_LSP_QID2CQIDIX2_00_CQ_P1_LOC));
-		break;
+	if (!port->enabled) {
+		dlb2_dir_port_cq_enable(hw, port);
+		port->enabled = true;
+	}
 
-	case 2:
-		atm_qid2cq &= ~(1 << (i + DLB2_ATM_QID2CQIDIX_00_CQ_P2_LOC));
-		lsp_qid2cq &= ~(1 << (i + DLB2_LSP_QID2CQIDIX_00_CQ_P2_LOC));
-		lsp_qid2cq2 &= ~(1 << (i + DLB2_LSP_QID2CQIDIX2_00_CQ_P2_LOC));
-		break;
+	resp->status = 0;
 
-	case 3:
-		atm_qid2cq &= ~(1 << (i + DLB2_ATM_QID2CQIDIX_00_CQ_P3_LOC));
-		lsp_qid2cq &= ~(1 << (i + DLB2_LSP_QID2CQIDIX_00_CQ_P3_LOC));
-		lsp_qid2cq2 &= ~(1 << (i + DLB2_LSP_QID2CQIDIX2_00_CQ_P3_LOC));
-		break;
-	}
+	return 0;
+}
 
-	DLB2_CSR_WR(hw, DLB2_ATM_QID2CQIDIX(queue_id, port_id / 4), atm_qid2cq);
+/**
+ * dlb2_hw_disable_dir_port() - disable a directed port for scheduling
+ * @hw: dlb2_hw handle for a particular device.
+ * @domain_id: domain ID.
+ * @args: port disable arguments.
+ * @resp: response structure.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
+ *
+ * This function configures the DLB to stop scheduling QEs to a directed port.
+ * Ports are enabled by default.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ *
+ * Return:
+ * Returns 0 upon success, < 0 otherwise. If an error occurs, resp->status is
+ * assigned a detailed error code from enum dlb2_error.
+ *
+ * Errors:
+ * EINVAL - The port ID is invalid or the domain is not configured.
+ * EFAULT - Internal error (resp->status not set).
+ */
+int dlb2_hw_disable_dir_port(struct dlb2_hw *hw,
+			     u32 domain_id,
+			     struct dlb2_disable_dir_port_args *args,
+			     struct dlb2_cmd_response *resp,
+			     bool vdev_req,
+			     unsigned int vdev_id)
+{
+	struct dlb2_dir_pq_pair *port;
+	struct dlb2_hw_domain *domain;
+	int ret;
 
-	DLB2_CSR_WR(hw, DLB2_LSP_QID2CQIDIX(hw->ver, queue_id, port_id / 4),
-		    lsp_qid2cq);
+	dlb2_log_disable_port(hw, domain_id, args->port_id, vdev_req, vdev_id);
 
-	DLB2_CSR_WR(hw, DLB2_LSP_QID2CQIDIX2(hw->ver, queue_id, port_id / 4),
-		    lsp_qid2cq2);
+	/*
+	 * Verify that hardware resources are available before attempting to
+	 * satisfy the request. This simplifies the error unwinding code.
+	 */
+	ret = dlb2_verify_disable_dir_port_args(hw,
+						domain_id,
+						args,
+						resp,
+						vdev_req,
+						vdev_id,
+						&domain,
+						&port);
+	if (ret)
+		return ret;
 
-	dlb2_flush_csr(hw);
+	if (port->enabled) {
+		dlb2_dir_port_cq_disable(hw, port);
+		port->enabled = false;
+	}
 
-	unmapped = DLB2_QUEUE_UNMAPPED;
+	resp->status = 0;
 
-	return dlb2_port_slot_state_transition(hw, port, queue, i, unmapped);
+	return 0;
 }
 
-static int dlb2_ldb_port_map_qid(struct dlb2_hw *hw,
-				 struct dlb2_hw_domain *domain,
-				 struct dlb2_ldb_port *port,
-				 struct dlb2_ldb_queue *queue,
-				 u8 prio)
+/**
+ * dlb2_notify_vf() - send an alarm to a VF
+ * @hw: dlb2_hw handle for a particular device.
+ * @vf_id: VF ID
+ * @notification: notification
+ *
+ * This function sends a notification (as defined in dlb2_mbox.h) to a VF.
+ *
+ * Return:
+ * Returns 0 upon success, <0 if the VF doesn't ACK the PF->VF interrupt.
+ */
+int dlb2_notify_vf(struct dlb2_hw *hw,
+		   unsigned int vf_id,
+		   enum dlb2_mbox_vf_notification_type notification)
 {
-	if (domain->started)
-		return dlb2_ldb_port_map_qid_dynamic(hw, port, queue, prio);
-	else
-		return dlb2_ldb_port_map_qid_static(hw, port, queue, prio);
-}
+	struct dlb2_mbox_vf_notification_cmd_req req;
+	int ret, retry_cnt;
 
-static void
-dlb2_domain_finish_unmap_port_slot(struct dlb2_hw *hw,
-				   struct dlb2_hw_domain *domain,
-				   struct dlb2_ldb_port *port,
-				   int slot)
-{
-	enum dlb2_qid_map_state state;
-	struct dlb2_ldb_queue *queue;
+	req.hdr.type = DLB2_MBOX_VF_CMD_NOTIFICATION;
+	req.notification = notification;
 
-	queue = &hw->rsrcs.ldb_queues[port->qid_map[slot].qid];
+	ret = dlb2_pf_write_vf_mbox_req(hw, vf_id, &req, sizeof(req));
+	if (ret)
+		return ret;
 
-	state = port->qid_map[slot].state;
+	dlb2_send_async_pf_to_vdev_msg(hw, vf_id);
 
-	/* Update the QID2CQIDX and CQ2QID vectors */
-	dlb2_ldb_port_unmap_qid(hw, port, queue);
+	/* Timeout after 1 second of inactivity */
+	retry_cnt = 1000;
+	do {
+		if (dlb2_pf_to_vdev_complete(hw, vf_id))
+			break;
+		os_msleep(1);
+	} while (--retry_cnt);
 
-	/*
-	 * Ensure the QID will not be serviced by this {CQ, slot} by clearing
-	 * the has_work bits
-	 */
-	dlb2_ldb_port_clear_has_work_bits(hw, port, slot);
+	if (!retry_cnt) {
+		DLB2_HW_ERR(hw,
+			    "PF driver timed out waiting for mbox response\n");
+		return -ETIMEDOUT;
+	}
 
-	/* Reset the {CQ, slot} to its default state */
-	dlb2_ldb_port_set_queue_if_status(hw, port, slot);
+	/* No response data expected for notifications. */
 
-	/* Re-enable the CQ if it was not manually disabled by the user */
-	if (port->enabled)
-		dlb2_ldb_port_cq_enable(hw, port);
+	return 0;
+}
 
-	/*
-	 * If there is a mapping that is pending this slot's removal, perform
-	 * the mapping now.
-	 */
-	if (state == DLB2_QUEUE_UNMAP_IN_PROG_PENDING_MAP) {
-		struct dlb2_ldb_port_qid_map *map;
-		struct dlb2_ldb_queue *map_queue;
-		u8 prio;
+/**
+ * dlb2_vdev_in_use() - query whether a virtual device is in use
+ * @hw: dlb2_hw handle for a particular device.
+ * @id: virtual device ID
+ *
+ * This function sends a mailbox request to the vdev to query whether the vdev
+ * is in use.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ *
+ * Return:
+ * Returns 0 for false, 1 for true, and <0 if the mailbox request times out or
+ * an internal error occurs.
+ */
+int dlb2_vdev_in_use(struct dlb2_hw *hw, unsigned int id)
+{
+	struct dlb2_mbox_vf_in_use_cmd_resp resp;
+	struct dlb2_mbox_vf_in_use_cmd_req req;
+	int ret, retry_cnt;
 
-		map = &port->qid_map[slot];
+	req.hdr.type = DLB2_MBOX_VF_CMD_IN_USE;
 
-		map->qid = map->pending_qid;
-		map->priority = map->pending_priority;
+	ret = dlb2_pf_write_vf_mbox_req(hw, id, &req, sizeof(req));
+	if (ret)
+		return ret;
 
-		map_queue = &hw->rsrcs.ldb_queues[map->qid];
-		prio = map->priority;
+	dlb2_send_async_pf_to_vdev_msg(hw, id);
 
-		dlb2_ldb_port_map_qid(hw, domain, port, map_queue, prio);
+	/* Timeout after 1 second of inactivity */
+	retry_cnt = 1000;
+	do {
+		if (dlb2_pf_to_vdev_complete(hw, id))
+			break;
+		os_msleep(1);
+	} while (--retry_cnt);
+
+	if (!retry_cnt) {
+		DLB2_HW_ERR(hw,
+			    "PF driver timed out waiting for mbox response\n");
+		return -ETIMEDOUT;
 	}
-}
 
+	ret = dlb2_pf_read_vf_mbox_resp(hw, id, &resp, sizeof(resp));
+	if (ret)
+		return ret;
 
-static bool dlb2_domain_finish_unmap_port(struct dlb2_hw *hw,
-					  struct dlb2_hw_domain *domain,
-					  struct dlb2_ldb_port *port)
-{
-	u32 infl_cnt;
-	int i;
-	const int max_iters = 1000;
-	const int iter_poll_us = 100;
+	if (resp.hdr.status != DLB2_MBOX_ST_SUCCESS) {
+		DLB2_HW_ERR(hw,
+			    "[%s()]: failed with mailbox error: %s\n",
+			    __func__,
+			    dlb2_mbox_st_string(&resp.hdr));
 
-	if (port->num_pending_removals == 0)
-		return false;
+		return -1;
+	}
 
-	/*
-	 * The unmap requires all the CQ's outstanding inflights to be
-	 * completed. Poll up to 100ms.
-	 */
-	for (i = 0; i < max_iters; i++) {
-		infl_cnt = DLB2_CSR_RD(hw, DLB2_LSP_CQ_LDB_INFL_CNT(hw->ver,
-						       port->id.phys_id));
+	return resp.in_use;
+}
 
-		if (DLB2_BITS_GET(infl_cnt,
-				  DLB2_LSP_CQ_LDB_INFL_CNT_COUNT) == 0)
-			break;
-		rte_delay_us_sleep(iter_poll_us);
-	}
+static int dlb2_notify_vf_alarm(struct dlb2_hw *hw,
+				unsigned int vf_id,
+				u32 domain_id,
+				u32 alert_id,
+				u32 aux_alert_data)
+{
+	struct dlb2_mbox_vf_alert_cmd_req req;
+	int ret, retry_cnt;
 
-	if (DLB2_BITS_GET(infl_cnt, DLB2_LSP_CQ_LDB_INFL_CNT_COUNT) > 0)
-		return false;
+	req.hdr.type = DLB2_MBOX_VF_CMD_DOMAIN_ALERT;
+	req.domain_id = domain_id;
+	req.alert_id = alert_id;
+	req.aux_alert_data = aux_alert_data;
 
-	for (i = 0; i < DLB2_MAX_NUM_QIDS_PER_LDB_CQ; i++) {
-		struct dlb2_ldb_port_qid_map *map;
+	ret = dlb2_pf_write_vf_mbox_req(hw, vf_id, &req, sizeof(req));
+	if (ret)
+		return ret;
 
-		map = &port->qid_map[i];
+	dlb2_send_async_pf_to_vdev_msg(hw, vf_id);
 
-		if (map->state != DLB2_QUEUE_UNMAP_IN_PROG &&
-		    map->state != DLB2_QUEUE_UNMAP_IN_PROG_PENDING_MAP)
-			continue;
+	/* Timeout after 1 second of inactivity */
+	retry_cnt = 1000;
+	do {
+		if (dlb2_pf_to_vdev_complete(hw, vf_id))
+			break;
+		os_msleep(1);
+	} while (--retry_cnt);
 
-		dlb2_domain_finish_unmap_port_slot(hw, domain, port, i);
+	if (!retry_cnt) {
+		DLB2_HW_ERR(hw,
+			    "PF driver timed out waiting for mbox response\n");
+		return -ETIMEDOUT;
 	}
 
-	return true;
+	/* No response data expected for alarm notifications. */
+
+	return 0;
 }
 
-static unsigned int
-dlb2_domain_finish_unmap_qid_procedures(struct dlb2_hw *hw,
-					struct dlb2_hw_domain *domain)
+/**
+ * dlb2_set_msix_mode() - enable certain hardware alarm interrupts
+ * @hw: dlb2_hw handle for a particular device.
+ * @mode: MSI-X mode (DLB2_MSIX_MODE_PACKED or DLB2_MSIX_MODE_COMPRESSED)
+ *
+ * This function configures the hardware to use either packed or compressed
+ * mode. This function should not be called if using MSI interrupts.
+ */
+void dlb2_set_msix_mode(struct dlb2_hw *hw, int mode)
 {
-	struct dlb2_list_entry *iter;
-	struct dlb2_ldb_port *port;
-	int i;
-	RTE_SET_USED(iter);
-
-	if (!domain->configured || domain->num_pending_removals == 0)
-		return 0;
+	u32 msix_mode = 0;
 
-	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
-		DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i], port, iter)
-			dlb2_domain_finish_unmap_port(hw, domain, port);
-	}
+	DLB2_BITS_SET(msix_mode, mode, DLB2_SYS_MSIX_MODE_MODE_V2);
 
-	return domain->num_pending_removals;
+	DLB2_CSR_WR(hw, DLB2_SYS_MSIX_MODE, msix_mode);
 }
 
-static void dlb2_domain_disable_ldb_cqs(struct dlb2_hw *hw,
-					struct dlb2_hw_domain *domain)
+/**
+ * dlb2_configure_ldb_cq_interrupt() - configure load-balanced CQ for
+ *					interrupts
+ * @hw: dlb2_hw handle for a particular device.
+ * @port_id: load-balanced port ID.
+ * @vector: interrupt vector ID. Should be 0 for MSI or compressed MSI-X mode,
+ *	    else a value up to 64.
+ * @mode: interrupt type (DLB2_CQ_ISR_MODE_MSI or DLB2_CQ_ISR_MODE_MSIX)
+ * @vf: If the port is VF-owned, the VF's ID. This is used for translating the
+ *	virtual port ID to a physical port ID. Ignored if mode is not MSI.
+ * @owner_vf: the VF to route the interrupt to. Ignore if mode is not MSI.
+ * @threshold: the minimum CQ depth at which the interrupt can fire. Must be
+ *	greater than 0.
+ *
+ * This function configures the DLB registers for load-balanced CQ's
+ * interrupts. This doesn't enable the CQ's interrupt; that can be done with
+ * dlb2_arm_cq_interrupt() or through an interrupt arm QE.
+ *
+ * Return:
+ * Returns 0 upon success, < 0 otherwise.
+ *
+ * Errors:
+ * EINVAL - The port ID is invalid.
+ */
+int dlb2_configure_ldb_cq_interrupt(struct dlb2_hw *hw,
+				    int port_id,
+				    int vector,
+				    int mode,
+				    unsigned int vf,
+				    unsigned int owner_vf,
+				    u16 threshold)
 {
-	struct dlb2_list_entry *iter;
 	struct dlb2_ldb_port *port;
-	int i;
-	RTE_SET_USED(iter);
+	bool vdev_req;
+	u32 reg = 0;
 
-	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
-		DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i], port, iter) {
-			port->enabled = false;
+	vdev_req = (mode == DLB2_CQ_ISR_MODE_MSI ||
+		    mode == DLB2_CQ_ISR_MODE_ADI);
 
-			dlb2_ldb_port_cq_disable(hw, port);
-		}
+	port = dlb2_get_ldb_port_from_id(hw, port_id, vdev_req, vf);
+	if (!port) {
+		DLB2_HW_ERR(hw,
+			    "[%s()]: Internal error: failed to enable LDB CQ int\n\tport_id: %u, vdev_req: %u, vdev: %u\n",
+			    __func__, port_id, vdev_req, vf);
+		return -EINVAL;
 	}
-}
-
-
-static void dlb2_log_reset_domain(struct dlb2_hw *hw,
-				  u32 domain_id,
-				  bool vdev_req,
-				  unsigned int vdev_id)
-{
-	DLB2_HW_DBG(hw, "DLB2 reset domain:\n");
-	if (vdev_req)
-		DLB2_HW_DBG(hw, "(Request from vdev %d)\n", vdev_id);
-	DLB2_HW_DBG(hw, "\tDomain ID: %d\n", domain_id);
-}
 
-static void dlb2_domain_disable_dir_vpps(struct dlb2_hw *hw,
-					 struct dlb2_hw_domain *domain,
-					 unsigned int vdev_id)
-{
-	struct dlb2_list_entry *iter;
-	struct dlb2_dir_pq_pair *port;
-	u32 vpp_v = 0;
-	RTE_SET_USED(iter);
+	/* Trigger the interrupt when threshold or more QEs arrive in the CQ */
+	DLB2_BITS_SET(reg, threshold - 1,
+		 DLB2_CHP_LDB_CQ_INT_DEPTH_THRSH_DEPTH_THRESHOLD);
+	DLB2_CSR_WR(hw, DLB2_CHP_LDB_CQ_INT_DEPTH_THRSH(hw->ver,
+						   port->id.phys_id), reg);
 
-	DLB2_DOM_LIST_FOR(domain->used_dir_pq_pairs, port, iter) {
-		unsigned int offs;
-		u32 virt_id;
+	reg = 0;
+	DLB2_BIT_SET(reg, DLB2_CHP_LDB_CQ_INT_ENB_EN_DEPTH);
+	DLB2_CSR_WR(hw,
+		    DLB2_CHP_LDB_CQ_INT_ENB(hw->ver, port->id.phys_id), reg);
 
-		if (hw->virt_mode == DLB2_VIRT_SRIOV)
-			virt_id = port->id.virt_id;
-		else
-			virt_id = port->id.phys_id;
+	reg = 0;
+	DLB2_BITS_SET(reg, vector, DLB2_SYS_LDB_CQ_ISR_VECTOR);
+	DLB2_BITS_SET(reg, owner_vf, DLB2_SYS_LDB_CQ_ISR_VF);
+	DLB2_BITS_SET(reg, mode, DLB2_SYS_LDB_CQ_ISR_EN_CODE);
 
-		offs = vdev_id * DLB2_MAX_NUM_DIR_PORTS(hw->ver) + virt_id;
+	DLB2_CSR_WR(hw, DLB2_SYS_LDB_CQ_ISR(port->id.phys_id), reg);
 
-		DLB2_CSR_WR(hw, DLB2_SYS_VF_DIR_VPP_V(offs), vpp_v);
-	}
+	return 0;
 }
 
-static void dlb2_domain_disable_ldb_vpps(struct dlb2_hw *hw,
-					 struct dlb2_hw_domain *domain,
-					 unsigned int vdev_id)
+/**
+ * dlb2_hw_ldb_cq_interrupt_enabled() - Check if the interrupt is enabled
+ * @hw: dlb2_hw handle for a particular device.
+ * @port_id: physical load-balanced port ID.
+ *
+ * This function returns whether the load-balanced CQ interrupt is enabled.
+ */
+int dlb2_hw_ldb_cq_interrupt_enabled(struct dlb2_hw *hw, int port_id)
 {
-	struct dlb2_list_entry *iter;
-	struct dlb2_ldb_port *port;
-	u32 vpp_v = 0;
-	int i;
-	RTE_SET_USED(iter);
-
-	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
-		DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i], port, iter) {
-			unsigned int offs;
-			u32 virt_id;
-
-			if (hw->virt_mode == DLB2_VIRT_SRIOV)
-				virt_id = port->id.virt_id;
-			else
-				virt_id = port->id.phys_id;
-
-			offs = vdev_id * DLB2_MAX_NUM_LDB_PORTS + virt_id;
+	u32 isr = 0;
 
-			DLB2_CSR_WR(hw, DLB2_SYS_VF_LDB_VPP_V(offs), vpp_v);
-		}
-	}
+	isr = DLB2_CSR_RD(hw, DLB2_SYS_LDB_CQ_ISR(port_id));
+
+	return DLB2_BITS_GET(isr, DLB2_SYS_LDB_CQ_ISR_EN_CODE) !=
+	       DLB2_CQ_ISR_MODE_DIS;
 }
 
-static void
-dlb2_domain_disable_ldb_port_interrupts(struct dlb2_hw *hw,
-					struct dlb2_hw_domain *domain)
+/**
+ * dlb2_hw_ldb_cq_interrupt_set_mode() - Program the CQ interrupt mode
+ * @hw: dlb2_hw handle for a particular device.
+ * @port_id: physical load-balanced port ID.
+ * @mode: interrupt type (DLB2_CQ_ISR_MODE_{DIS, MSI, MSIX, ADI})
+ *
+ * This function can be used to disable (MODE_DIS) and re-enable the
+ * load-balanced CQ's interrupt. It should only be called after the interrupt
+ * has been configured with dlb2_configure_ldb_cq_interrupt().
+ */
+void dlb2_hw_ldb_cq_interrupt_set_mode(struct dlb2_hw *hw,
+				       int port_id,
+				       int mode)
 {
-	struct dlb2_list_entry *iter;
-	struct dlb2_ldb_port *port;
-	u32 int_en = 0;
-	u32 wd_en = 0;
-	int i;
-	RTE_SET_USED(iter);
+	u32 isr = 0;
 
-	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
-		DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i], port, iter) {
-			DLB2_CSR_WR(hw,
-				    DLB2_CHP_LDB_CQ_INT_ENB(hw->ver,
-						       port->id.phys_id),
-				    int_en);
+	isr = DLB2_CSR_RD(hw, DLB2_SYS_LDB_CQ_ISR(port_id));
 
-			DLB2_CSR_WR(hw,
-				    DLB2_CHP_LDB_CQ_WD_ENB(hw->ver,
-						      port->id.phys_id),
-				    wd_en);
-		}
-	}
+	DLB2_BITS_SET(isr, mode, DLB2_SYS_LDB_CQ_ISR_EN_CODE);
+
+	DLB2_CSR_WR(hw, DLB2_SYS_LDB_CQ_ISR(port_id), isr);
 }
 
-static void
-dlb2_domain_disable_dir_port_interrupts(struct dlb2_hw *hw,
-					struct dlb2_hw_domain *domain)
+/**
+ * dlb2_configure_dir_cq_interrupt() - configure directed CQ for interrupts
+ * @hw: dlb2_hw handle for a particular device.
+ * @port_id: load-balanced port ID.
+ * @vector: interrupt vector ID. Should be 0 for MSI or compressed MSI-X mode,
+ *	    else a value up to 64.
+ * @mode: interrupt type (DLB2_CQ_ISR_MODE_MSI or DLB2_CQ_ISR_MODE_MSIX)
+ * @vf: If the port is VF-owned, the VF's ID. This is used for translating the
+ *	virtual port ID to a physical port ID. Ignored if mode is not MSI.
+ * @owner_vf: the VF to route the interrupt to. Ignore if mode is not MSI.
+ * @threshold: the minimum CQ depth at which the interrupt can fire. Must be
+ *	greater than 0.
+ *
+ * This function configures the DLB registers for directed CQ's interrupts.
+ * This doesn't enable the CQ's interrupt; that can be done with
+ * dlb2_arm_cq_interrupt() or through an interrupt arm QE.
+ *
+ * Return:
+ * Returns 0 upon success, < 0 otherwise.
+ *
+ * Errors:
+ * EINVAL - The port ID is invalid.
+ */
+int dlb2_configure_dir_cq_interrupt(struct dlb2_hw *hw,
+				    int port_id,
+				    int vector,
+				    int mode,
+				    unsigned int vf,
+				    unsigned int owner_vf,
+				    u16 threshold)
 {
-	struct dlb2_list_entry *iter;
 	struct dlb2_dir_pq_pair *port;
-	u32 int_en = 0;
-	u32 wd_en = 0;
-	RTE_SET_USED(iter);
+	bool vdev_req;
+	u32 reg = 0;
 
-	DLB2_DOM_LIST_FOR(domain->used_dir_pq_pairs, port, iter) {
-		DLB2_CSR_WR(hw,
-			    DLB2_CHP_DIR_CQ_INT_ENB(hw->ver, port->id.phys_id),
-			    int_en);
+	vdev_req = (mode == DLB2_CQ_ISR_MODE_MSI ||
+		    mode == DLB2_CQ_ISR_MODE_ADI);
 
-		DLB2_CSR_WR(hw,
-			    DLB2_CHP_DIR_CQ_WD_ENB(hw->ver, port->id.phys_id),
-			    wd_en);
+	port = dlb2_get_dir_pq_from_id(hw, port_id, vdev_req, vf);
+	if (!port) {
+		DLB2_HW_ERR(hw,
+			    "[%s()]: Internal error: failed to enable DIR CQ int\n\tport_id: %u, vdev_req: %u, vdev: %u\n",
+			    __func__, port_id, vdev_req, vf);
+		return -EINVAL;
 	}
-}
 
-static void
-dlb2_domain_disable_ldb_queue_write_perms(struct dlb2_hw *hw,
-					  struct dlb2_hw_domain *domain)
-{
-	int domain_offset = domain->id.phys_id * DLB2_MAX_NUM_LDB_QUEUES;
-	struct dlb2_list_entry *iter;
-	struct dlb2_ldb_queue *queue;
-	RTE_SET_USED(iter);
+	/* Trigger the interrupt when threshold or more QEs arrive in the CQ */
+	DLB2_BITS_SET(reg, threshold - 1,
+		 DLB2_CHP_DIR_CQ_INT_DEPTH_THRSH_DEPTH_THRESHOLD);
+	DLB2_CSR_WR(hw, DLB2_CHP_DIR_CQ_INT_DEPTH_THRSH(hw->ver,
+						   port->id.phys_id), reg);
 
-	DLB2_DOM_LIST_FOR(domain->used_ldb_queues, queue, iter) {
-		int idx = domain_offset + queue->id.phys_id;
+	reg = 0;
+	DLB2_BIT_SET(reg, DLB2_CHP_DIR_CQ_INT_ENB_EN_DEPTH);
+	DLB2_CSR_WR(hw,
+		    DLB2_CHP_DIR_CQ_INT_ENB(hw->ver, port->id.phys_id), reg);
 
-		DLB2_CSR_WR(hw, DLB2_SYS_LDB_VASQID_V(idx), 0);
+	reg = 0;
+	DLB2_BITS_SET(reg, vector, DLB2_SYS_DIR_CQ_ISR_VECTOR);
+	DLB2_BITS_SET(reg, owner_vf, DLB2_SYS_DIR_CQ_ISR_VF);
+	DLB2_BITS_SET(reg, mode, DLB2_SYS_DIR_CQ_ISR_EN_CODE);
 
-		if (queue->id.vdev_owned) {
-			DLB2_CSR_WR(hw,
-				    DLB2_SYS_LDB_QID2VQID(queue->id.phys_id),
-				    0);
+	DLB2_CSR_WR(hw, DLB2_SYS_DIR_CQ_ISR(port->id.phys_id), reg);
 
-			idx = queue->id.vdev_id * DLB2_MAX_NUM_LDB_QUEUES +
-				queue->id.virt_id;
+	return 0;
+}
 
-			DLB2_CSR_WR(hw, DLB2_SYS_VF_LDB_VQID_V(idx), 0);
+/**
+ * dlb2_hw_dir_cq_interrupt_enabled() - Check if the interrupt is enabled
+ * @hw: dlb2_hw handle for a particular device.
+ * @port_id: physical load-balanced port ID.
+ *
+ * This function returns whether the load-balanced CQ interrupt is enabled.
+ */
+int dlb2_hw_dir_cq_interrupt_enabled(struct dlb2_hw *hw, int port_id)
+{
+	u32 isr = 0;
 
-			DLB2_CSR_WR(hw, DLB2_SYS_VF_LDB_VQID2QID(idx), 0);
-		}
-	}
+	isr = DLB2_CSR_RD(hw, DLB2_SYS_DIR_CQ_ISR(port_id));
+
+	return DLB2_BITS_GET(isr, DLB2_SYS_DIR_CQ_ISR_EN_CODE) !=
+	       DLB2_CQ_ISR_MODE_DIS;
 }
 
-static void
-dlb2_domain_disable_dir_queue_write_perms(struct dlb2_hw *hw,
-					  struct dlb2_hw_domain *domain)
+/**
+ * dlb2_hw_dir_cq_interrupt_set_mode() - Program the CQ interrupt mode
+ * @hw: dlb2_hw handle for a particular device.
+ * @port_id: physical directed port ID.
+ * @mode: interrupt type (DLB2_CQ_ISR_MODE_{DIS, MSI, MSIX, ADI})
+ *
+ * This function can be used to disable (MODE_DIS) and re-enable the
+ * directed CQ's interrupt. It should only be called after the interrupt
+ * has been configured with dlb2_configure_dir_cq_interrupt().
+ */
+void dlb2_hw_dir_cq_interrupt_set_mode(struct dlb2_hw *hw,
+				       int port_id,
+				       int mode)
 {
-	struct dlb2_list_entry *iter;
-	struct dlb2_dir_pq_pair *queue;
-	unsigned long max_ports;
-	int domain_offset;
-	RTE_SET_USED(iter);
+	u32 isr = 0;
 
-	max_ports = DLB2_MAX_NUM_DIR_PORTS(hw->ver);
+	isr = DLB2_CSR_RD(hw, DLB2_SYS_DIR_CQ_ISR(port_id));
 
-	domain_offset = domain->id.phys_id * max_ports;
+	DLB2_BITS_SET(isr, mode, DLB2_SYS_DIR_CQ_ISR_EN_CODE);
 
-	DLB2_DOM_LIST_FOR(domain->used_dir_pq_pairs, queue, iter) {
-		int idx = domain_offset + queue->id.phys_id;
+	DLB2_CSR_WR(hw, DLB2_SYS_DIR_CQ_ISR(port_id), isr);
+}
 
-		DLB2_CSR_WR(hw, DLB2_SYS_DIR_VASQID_V(idx), 0);
+/**
+ * dlb2_arm_cq_interrupt() - arm a CQ's interrupt
+ * @hw: dlb2_hw handle for a particular device.
+ * @port_id: port ID
+ * @is_ldb: true for load-balanced port, false for a directed port
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
+ *
+ * This function arms the CQ's interrupt. The CQ must be configured prior to
+ * calling this function.
+ *
+ * The function does no parameter validation; that is the caller's
+ * responsibility.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ *
+ * Return: returns 0 upon success, <0 otherwise.
+ *
+ * EINVAL - Invalid port ID.
+ */
+int dlb2_arm_cq_interrupt(struct dlb2_hw *hw,
+			  int port_id,
+			  bool is_ldb,
+			  bool vdev_req,
+			  unsigned int vdev_id)
+{
+	u32 val;
+	u32 reg;
 
-		if (queue->id.vdev_owned) {
-			idx = queue->id.vdev_id * max_ports + queue->id.virt_id;
+	if (vdev_req && is_ldb) {
+		struct dlb2_ldb_port *ldb_port;
 
-			DLB2_CSR_WR(hw, DLB2_SYS_VF_DIR_VQID_V(idx), 0);
+		ldb_port = dlb2_get_ldb_port_from_id(hw, port_id,
+						     true, vdev_id);
 
-			DLB2_CSR_WR(hw, DLB2_SYS_VF_DIR_VQID2QID(idx), 0);
-		}
-	}
-}
+		if (!ldb_port || !ldb_port->configured)
+			return -EINVAL;
 
-static void dlb2_domain_disable_ldb_seq_checks(struct dlb2_hw *hw,
-					       struct dlb2_hw_domain *domain)
-{
-	struct dlb2_list_entry *iter;
-	struct dlb2_ldb_port *port;
-	u32 chk_en = 0;
-	int i;
-	RTE_SET_USED(iter);
+		port_id = ldb_port->id.phys_id;
+	} else if (vdev_req && !is_ldb) {
+		struct dlb2_dir_pq_pair *dir_port;
 
-	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
-		DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i], port, iter) {
-			DLB2_CSR_WR(hw,
-				    DLB2_CHP_SN_CHK_ENBL(hw->ver,
-							 port->id.phys_id),
-				    chk_en);
-		}
+		dir_port = dlb2_get_dir_pq_from_id(hw, port_id, true, vdev_id);
+
+		if (!dir_port || !dir_port->port_configured)
+			return -EINVAL;
+
+		port_id = dir_port->id.phys_id;
 	}
-}
 
-static int dlb2_domain_wait_for_ldb_cqs_to_empty(struct dlb2_hw *hw,
-						 struct dlb2_hw_domain *domain)
-{
-	struct dlb2_list_entry *iter;
-	struct dlb2_ldb_port *port;
-	int i;
-	RTE_SET_USED(iter);
+	val = 1 << (port_id % 32);
 
-	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
-		DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i], port, iter) {
-			int j;
+	if (is_ldb && port_id < 32)
+		reg = DLB2_CHP_LDB_CQ_INTR_ARMED0(hw->ver);
+	else if (is_ldb && port_id < 64)
+		reg = DLB2_CHP_LDB_CQ_INTR_ARMED1(hw->ver);
+	else if (!is_ldb && port_id < 32)
+		reg = DLB2_CHP_DIR_CQ_INTR_ARMED0(hw->ver);
+	else
+		reg = DLB2_CHP_DIR_CQ_INTR_ARMED1(hw->ver);
 
-			for (j = 0; j < DLB2_MAX_CQ_COMP_CHECK_LOOPS; j++) {
-				if (dlb2_ldb_cq_inflight_count(hw, port) == 0)
-					break;
-			}
+	DLB2_CSR_WR(hw, reg, val);
 
-			if (j == DLB2_MAX_CQ_COMP_CHECK_LOOPS) {
-				DLB2_HW_ERR(hw,
-					    "[%s()] Internal error: failed to flush load-balanced port %d's completions.\n",
-					    __func__, port->id.phys_id);
-				return -EFAULT;
-			}
-		}
-	}
+	dlb2_flush_csr(hw);
 
 	return 0;
 }
 
-static void dlb2_domain_disable_dir_cqs(struct dlb2_hw *hw,
-					struct dlb2_hw_domain *domain)
+/**
+ * dlb2_read_compressed_cq_intr_status() - read compressed CQ interrupt status
+ * @hw: dlb2_hw handle for a particular device.
+ * @ldb_interrupts: 2-entry array of u32 bitmaps
+ * @dir_interrupts: 4-entry array of u32 bitmaps
+ *
+ * This function can be called from a compressed CQ interrupt handler to
+ * determine which CQ interrupts have fired. The caller should take appropriate
+ * (such as waking threads blocked on a CQ's interrupt) then ack the interrupts
+ * with dlb2_ack_compressed_cq_intr().
+ */
+void dlb2_read_compressed_cq_intr_status(struct dlb2_hw *hw,
+					 u32 *ldb_interrupts,
+					 u32 *dir_interrupts)
 {
-	struct dlb2_list_entry *iter;
-	struct dlb2_dir_pq_pair *port;
-	RTE_SET_USED(iter);
+	/* Read every CQ's interrupt status */
 
-	DLB2_DOM_LIST_FOR(domain->used_dir_pq_pairs, port, iter) {
-		port->enabled = false;
+	ldb_interrupts[0] = DLB2_CSR_RD(hw, DLB2_SYS_LDB_CQ_31_0_OCC_INT_STS);
+	ldb_interrupts[1] = DLB2_CSR_RD(hw, DLB2_SYS_LDB_CQ_63_32_OCC_INT_STS);
 
-		dlb2_dir_port_cq_disable(hw, port);
-	}
+	dir_interrupts[0] = DLB2_CSR_RD(hw, DLB2_SYS_DIR_CQ_31_0_OCC_INT_STS);
+	dir_interrupts[1] = DLB2_CSR_RD(hw, DLB2_SYS_DIR_CQ_63_32_OCC_INT_STS);
 }
 
-static void
-dlb2_domain_disable_dir_producer_ports(struct dlb2_hw *hw,
-				       struct dlb2_hw_domain *domain)
+/**
+ * dlb2_ack_msix_interrupt() - Ack an MSI-X interrupt
+ * @hw: dlb2_hw handle for a particular device.
+ * @vector: interrupt vector.
+ *
+ * Note: Only needed for PF service interrupts (vector 0). CQ interrupts are
+ * acked in dlb2_ack_compressed_cq_intr().
+ */
+void dlb2_ack_msix_interrupt(struct dlb2_hw *hw, int vector)
 {
-	struct dlb2_list_entry *iter;
-	struct dlb2_dir_pq_pair *port;
-	u32 pp_v = 0;
-	RTE_SET_USED(iter);
+	u32 ack = 0;
 
-	DLB2_DOM_LIST_FOR(domain->used_dir_pq_pairs, port, iter) {
-		DLB2_CSR_WR(hw,
-			    DLB2_SYS_DIR_PP_V(port->id.phys_id),
-			    pp_v);
+	switch (vector) {
+	case 0:
+		DLB2_BIT_SET(ack, DLB2_SYS_MSIX_ACK_MSIX_0_ACK);
+		break;
+	case 1:
+		DLB2_BIT_SET(ack, DLB2_SYS_MSIX_ACK_MSIX_1_ACK);
+		/*
+		 * CSSY-1650
+		 * workaround h/w bug for lost MSI-X interrupts
+		 *
+		 * The recommended workaround for acknowledging
+		 * vector 1 interrupts is :
+		 *   1: set   MSI-X mask
+		 *   2: set   MSIX_PASSTHROUGH
+		 *   3: clear MSIX_ACK
+		 *   4: clear MSIX_PASSTHROUGH
+		 *   5: clear MSI-X mask
+		 *
+		 * The MSIX-ACK (step 3) is cleared for all vectors
+		 * below. We handle steps 1 & 2 for vector 1 here.
+		 *
+		 * The bitfields for MSIX_ACK and MSIX_PASSTHRU are
+		 * defined the same, so we just use the MSIX_ACK
+		 * value when writing to PASSTHRU.
+		 */
+
+		/* set MSI-X mask and passthrough for vector 1 */
+		DLB2_FUNC_WR(hw, DLB2_MSIX_VECTOR_CTRL(1), 1);
+		DLB2_CSR_WR(hw, DLB2_SYS_MSIX_PASSTHRU, ack);
+		break;
 	}
-}
 
-static void
-dlb2_domain_disable_ldb_producer_ports(struct dlb2_hw *hw,
-				       struct dlb2_hw_domain *domain)
-{
-	struct dlb2_list_entry *iter;
-	struct dlb2_ldb_port *port;
-	u32 pp_v = 0;
-	int i;
-	RTE_SET_USED(iter);
+	/* clear MSIX_ACK (write one to clear) */
+	DLB2_CSR_WR(hw, DLB2_SYS_MSIX_ACK, ack);
 
-	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
-		DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i], port, iter) {
-			DLB2_CSR_WR(hw,
-				    DLB2_SYS_LDB_PP_V(port->id.phys_id),
-				    pp_v);
-		}
+	if (vector == 1) {
+		/*
+		 * finish up steps 4 & 5 of the workaround -
+		 * clear pasthrough and mask
+		 */
+		DLB2_CSR_WR(hw, DLB2_SYS_MSIX_PASSTHRU, 0);
+		DLB2_FUNC_WR(hw, DLB2_MSIX_VECTOR_CTRL(1), 0);
 	}
+
+	dlb2_flush_csr(hw);
 }
 
-static int dlb2_domain_verify_reset_success(struct dlb2_hw *hw,
-					    struct dlb2_hw_domain *domain)
+/**
+ * dlb2_ack_compressed_cq_intr() - ack compressed CQ interrupts
+ * @hw: dlb2_hw handle for a particular device.
+ * @ldb_interrupts: 2-entry array of u32 bitmaps
+ * @dir_interrupts: 4-entry array of u32 bitmaps
+ *
+ * This function ACKs compressed CQ interrupts. Its arguments should be the
+ * same ones passed to dlb2_read_compressed_cq_intr_status().
+ */
+void dlb2_ack_compressed_cq_intr(struct dlb2_hw *hw,
+				 u32 *ldb_interrupts,
+				 u32 *dir_interrupts)
 {
-	struct dlb2_list_entry *iter;
-	struct dlb2_dir_pq_pair *dir_port;
-	struct dlb2_ldb_port *ldb_port;
-	struct dlb2_ldb_queue *queue;
-	int i;
-	RTE_SET_USED(iter);
-
-	/*
-	 * Confirm that all the domain's queue's inflight counts and AQED
-	 * active counts are 0.
-	 */
-	DLB2_DOM_LIST_FOR(domain->used_ldb_queues, queue, iter) {
-		if (!dlb2_ldb_queue_is_empty(hw, queue)) {
-			DLB2_HW_ERR(hw,
-				    "[%s()] Internal error: failed to empty ldb queue %d\n",
-				    __func__, queue->id.phys_id);
-			return -EFAULT;
-		}
-	}
+	/* Write back the status regs to ack the interrupts */
+	if (ldb_interrupts[0])
+		DLB2_CSR_WR(hw,
+			    DLB2_SYS_LDB_CQ_31_0_OCC_INT_STS,
+			    ldb_interrupts[0]);
+	if (ldb_interrupts[1])
+		DLB2_CSR_WR(hw,
+			    DLB2_SYS_LDB_CQ_63_32_OCC_INT_STS,
+			    ldb_interrupts[1]);
 
-	/* Confirm that all the domain's CQs inflight and token counts are 0. */
-	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
-		DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i], ldb_port, iter) {
-			if (dlb2_ldb_cq_inflight_count(hw, ldb_port) ||
-			    dlb2_ldb_cq_token_count(hw, ldb_port)) {
-				DLB2_HW_ERR(hw,
-					    "[%s()] Internal error: failed to empty ldb port %d\n",
-					    __func__, ldb_port->id.phys_id);
-				return -EFAULT;
-			}
-		}
-	}
+	if (dir_interrupts[0])
+		DLB2_CSR_WR(hw,
+			    DLB2_SYS_DIR_CQ_31_0_OCC_INT_STS,
+			    dir_interrupts[0]);
+	if (dir_interrupts[1])
+		DLB2_CSR_WR(hw,
+			    DLB2_SYS_DIR_CQ_63_32_OCC_INT_STS,
+			    dir_interrupts[1]);
 
-	DLB2_DOM_LIST_FOR(domain->used_dir_pq_pairs, dir_port, iter) {
-		if (!dlb2_dir_queue_is_empty(hw, dir_port)) {
-			DLB2_HW_ERR(hw,
-				    "[%s()] Internal error: failed to empty dir queue %d\n",
-				    __func__, dir_port->id.phys_id);
-			return -EFAULT;
-		}
+	dlb2_ack_msix_interrupt(hw, DLB2_PF_COMPRESSED_MODE_CQ_VECTOR_ID);
+}
 
-		if (dlb2_dir_cq_token_count(hw, dir_port)) {
-			DLB2_HW_ERR(hw,
-				    "[%s()] Internal error: failed to empty dir port %d\n",
-				    __func__, dir_port->id.phys_id);
-			return -EFAULT;
-		}
-	}
+/**
+ * dlb2_read_vf_intr_status() - read the VF interrupt status register
+ * @hw: dlb2_hw handle for a particular device.
+ *
+ * This function can be called from a VF's interrupt handler to determine
+ * which interrupts have fired. The first 31 bits correspond to CQ interrupt
+ * vectors, and the final bit is for the PF->VF mailbox interrupt vector.
+ *
+ * Return:
+ * Returns a bit vector indicating which interrupt vectors are active.
+ */
+u32 dlb2_read_vf_intr_status(struct dlb2_hw *hw)
+{
+	return DLB2_FUNC_RD(hw, DLB2_VF_VF_MSI_ISR);
+}
 
-	return 0;
+/**
+ * dlb2_ack_vf_intr_status() - ack VF interrupts
+ * @hw: dlb2_hw handle for a particular device.
+ * @interrupts: 32-bit bitmap
+ *
+ * This function ACKs a VF's interrupts. Its interrupts argument should be the
+ * value returned by dlb2_read_vf_intr_status().
+ */
+void dlb2_ack_vf_intr_status(struct dlb2_hw *hw, u32 interrupts)
+{
+	DLB2_FUNC_WR(hw, DLB2_VF_VF_MSI_ISR, interrupts);
 }
 
-static void __dlb2_domain_reset_ldb_port_registers(struct dlb2_hw *hw,
-						   struct dlb2_ldb_port *port)
+/**
+ * dlb2_ack_vf_msi_intr() - ack VF MSI interrupt
+ * @hw: dlb2_hw handle for a particular device.
+ * @interrupts: 32-bit bitmap
+ *
+ * This function clears the VF's MSI interrupt pending register. Its interrupts
+ * argument should be contain the MSI vectors to ACK. For example, if MSI MME
+ * is in mode 0, then one bit 0 should ever be set.
+ */
+void dlb2_ack_vf_msi_intr(struct dlb2_hw *hw, u32 interrupts)
 {
-	DLB2_CSR_WR(hw,
-		    DLB2_SYS_LDB_PP2VAS(port->id.phys_id),
-		    DLB2_SYS_LDB_PP2VAS_RST);
+	DLB2_FUNC_WR(hw, DLB2_VF_VF_MSI_ISR_PEND, interrupts);
+}
 
-	DLB2_CSR_WR(hw,
-		    DLB2_CHP_LDB_CQ2VAS(hw->ver, port->id.phys_id),
-		    DLB2_CHP_LDB_CQ2VAS_RST);
+/**
+ * dlb2_ack_pf_mbox_int() - ack PF->VF mailbox interrupt
+ * @hw: dlb2_hw handle for a particular device.
+ *
+ * When done processing the PF mailbox request, this function unsets
+ * the PF's mailbox ISR register.
+ */
+void dlb2_ack_pf_mbox_int(struct dlb2_hw *hw)
+{
+	u32 isr = 0;
 
-	DLB2_CSR_WR(hw,
-		    DLB2_SYS_LDB_PP2VDEV(port->id.phys_id),
-		    DLB2_SYS_LDB_PP2VDEV_RST);
+	if (hw->virt_mode == DLB2_VIRT_SIOV)
+		DLB2_BITS_CLR(isr, DLB2_VF_PF2VF_MAILBOX_ISR_PF_ISR);
+	else
+		DLB2_BIT_SET(isr, DLB2_VF_PF2VF_MAILBOX_ISR_PF_ISR);
 
-	if (port->id.vdev_owned) {
-		unsigned int offs;
-		u32 virt_id;
+	DLB2_FUNC_WR(hw, DLB2_VF_PF2VF_MAILBOX_ISR, isr);
+}
 
-		/*
-		 * DLB uses producer port address bits 17:12 to determine the
-		 * producer port ID. In Scalable IOV mode, PP accesses come
-		 * through the PF MMIO window for the physical producer port,
-		 * so for translation purposes the virtual and physical port
-		 * IDs are equal.
-		 */
-		if (hw->virt_mode == DLB2_VIRT_SRIOV)
-			virt_id = port->id.virt_id;
-		else
-			virt_id = port->id.phys_id;
+/**
+ * dlb2_enable_ingress_error_alarms() - enable ingress error alarm interrupts
+ * @hw: dlb2_hw handle for a particular device.
+ */
+void dlb2_enable_ingress_error_alarms(struct dlb2_hw *hw)
+{
+	u32 en;
 
-		offs = port->id.vdev_id * DLB2_MAX_NUM_LDB_PORTS + virt_id;
+	en = DLB2_CSR_RD(hw, DLB2_SYS_INGRESS_ALARM_ENBL);
 
-		DLB2_CSR_WR(hw,
-			    DLB2_SYS_VF_LDB_VPP2PP(offs),
-			    DLB2_SYS_VF_LDB_VPP2PP_RST);
+	DLB2_BIT_SET(en, DLB2_SYS_INGRESS_ALARM_ENBL_ILLEGAL_HCW);
+	DLB2_BIT_SET(en, DLB2_SYS_INGRESS_ALARM_ENBL_ILLEGAL_PP);
+	DLB2_BIT_SET(en, DLB2_SYS_INGRESS_ALARM_ENBL_ILLEGAL_PASID);
+	DLB2_BIT_SET(en, DLB2_SYS_INGRESS_ALARM_ENBL_ILLEGAL_QID);
+	DLB2_BIT_SET(en, DLB2_SYS_INGRESS_ALARM_ENBL_DISABLED_QID);
+	DLB2_BIT_SET(en, DLB2_SYS_INGRESS_ALARM_ENBL_ILLEGAL_LDB_QID_CFG);
 
-		DLB2_CSR_WR(hw,
-			    DLB2_SYS_VF_LDB_VPP_V(offs),
-			    DLB2_SYS_VF_LDB_VPP_V_RST);
-	}
+	DLB2_CSR_WR(hw, DLB2_SYS_INGRESS_ALARM_ENBL, en);
+}
 
-	DLB2_CSR_WR(hw,
-		    DLB2_SYS_LDB_PP_V(port->id.phys_id),
-		    DLB2_SYS_LDB_PP_V_RST);
+/**
+ * dlb2_disable_ingress_error_alarms() - disable ingress error alarm interrupts
+ * @hw: dlb2_hw handle for a particular device.
+ */
+void dlb2_disable_ingress_error_alarms(struct dlb2_hw *hw)
+{
+	u32 en;
+
+	en = DLB2_CSR_RD(hw, DLB2_SYS_INGRESS_ALARM_ENBL);
+
+	DLB2_BITS_CLR(en, DLB2_SYS_INGRESS_ALARM_ENBL_ILLEGAL_HCW);
+	DLB2_BITS_CLR(en, DLB2_SYS_INGRESS_ALARM_ENBL_ILLEGAL_PP);
+	DLB2_BITS_CLR(en, DLB2_SYS_INGRESS_ALARM_ENBL_ILLEGAL_PASID);
+	DLB2_BITS_CLR(en, DLB2_SYS_INGRESS_ALARM_ENBL_ILLEGAL_QID);
+	DLB2_BITS_CLR(en, DLB2_SYS_INGRESS_ALARM_ENBL_DISABLED_QID);
+	DLB2_BITS_CLR(en, DLB2_SYS_INGRESS_ALARM_ENBL_ILLEGAL_LDB_QID_CFG);
+
+	DLB2_CSR_WR(hw, DLB2_SYS_INGRESS_ALARM_ENBL, en);
+}
+
+static void dlb2_log_alarm_syndrome(struct dlb2_hw *hw,
+				    const char *str,
+				    u32 synd)
+{
+	DLB2_HW_ERR(hw, "%s:\n", str);
+	DLB2_HW_ERR(hw, "\tsyndrome: 0x%x\n", DLB2_SYND(SYNDROME));
+	DLB2_HW_ERR(hw, "\trtype:    0x%x\n", DLB2_SYND(RTYPE));
+	DLB2_HW_ERR(hw, "\talarm:    0x%x\n", DLB2_SYND(ALARM));
+	DLB2_HW_ERR(hw, "\tcwd:      0x%x\n", DLB2_SYND(CWD));
+	DLB2_HW_ERR(hw, "\tvf_pf_mb: 0x%x\n", DLB2_SYND(VF_PF_MB));
+	DLB2_HW_ERR(hw, "\tcls:      0x%x\n", DLB2_SYND(CLS));
+	DLB2_HW_ERR(hw, "\taid:      0x%x\n", DLB2_SYND(AID));
+	DLB2_HW_ERR(hw, "\tunit:     0x%x\n", DLB2_SYND(UNIT));
+	DLB2_HW_ERR(hw, "\tsource:   0x%x\n", DLB2_SYND(SOURCE));
+	DLB2_HW_ERR(hw, "\tmore:     0x%x\n", DLB2_SYND(MORE));
+	DLB2_HW_ERR(hw, "\tvalid:    0x%x\n", DLB2_SYND(VALID));
+}
+
+/* Note: this array's contents must match dlb2_alert_id() */
+static const char dlb2_alert_strings[NUM_DLB2_DOMAIN_ALERTS][128] = {
+	[DLB2_DOMAIN_ALERT_PP_ILLEGAL_ENQ] = "Illegal enqueue",
+	[DLB2_DOMAIN_ALERT_PP_EXCESS_TOKEN_POPS] = "Excess token pops",
+	[DLB2_DOMAIN_ALERT_ILLEGAL_HCW] = "Illegal HCW",
+	[DLB2_DOMAIN_ALERT_ILLEGAL_QID] = "Illegal QID",
+	[DLB2_DOMAIN_ALERT_DISABLED_QID] = "Disabled QID",
+};
+
+static void dlb2_log_pf_vf_syndrome(struct dlb2_hw *hw,
+				    const char *str,
+				    u32 synd0,
+				    u32 synd1,
+				    u32 synd2,
+				    u32 alert_id)
+{
+	DLB2_HW_ERR(hw, "%s:\n", str);
+	if (alert_id < NUM_DLB2_DOMAIN_ALERTS)
+		DLB2_HW_ERR(hw, "Alert: %s\n", dlb2_alert_strings[alert_id]);
+	DLB2_HW_ERR(hw, "\tsyndrome:     0x%x\n", DLB2_SYND0(SYNDROME));
+	DLB2_HW_ERR(hw, "\trtype:        0x%x\n", DLB2_SYND0(RTYPE));
+	DLB2_HW_ERR(hw, "\tis_ldb:       0x%x\n", DLB2_SYND0(IS_LDB));
+	DLB2_HW_ERR(hw, "\tcls:          0x%x\n", DLB2_SYND0(CLS));
+	DLB2_HW_ERR(hw, "\taid:          0x%x\n", DLB2_SYND0(AID));
+	DLB2_HW_ERR(hw, "\tunit:         0x%x\n", DLB2_SYND0(UNIT));
+	DLB2_HW_ERR(hw, "\tsource:       0x%x\n", DLB2_SYND0(SOURCE));
+	DLB2_HW_ERR(hw, "\tmore:         0x%x\n", DLB2_SYND0(MORE));
+	DLB2_HW_ERR(hw, "\tvalid:        0x%x\n", DLB2_SYND0(VALID));
+	DLB2_HW_ERR(hw, "\tdsi:          0x%x\n", DLB2_SYND1(DSI));
+	DLB2_HW_ERR(hw, "\tqid:          0x%x\n", DLB2_SYND1(QID));
+	DLB2_HW_ERR(hw, "\tqtype:        0x%x\n", DLB2_SYND1(QTYPE));
+	DLB2_HW_ERR(hw, "\tqpri:         0x%x\n", DLB2_SYND1(QPRI));
+	DLB2_HW_ERR(hw, "\tmsg_type:     0x%x\n", DLB2_SYND1(MSG_TYPE));
+	DLB2_HW_ERR(hw, "\tlock_id:      0x%x\n", DLB2_SYND2(LOCK_ID));
+	DLB2_HW_ERR(hw, "\tmeas:         0x%x\n", DLB2_SYND2(MEAS));
+	DLB2_HW_ERR(hw, "\tdebug:        0x%x\n", DLB2_SYND2(DEBUG));
+	DLB2_HW_ERR(hw, "\tcq_pop:       0x%x\n", DLB2_SYND2(CQ_POP));
+	DLB2_HW_ERR(hw, "\tqe_uhl:       0x%x\n", DLB2_SYND2(QE_UHL));
+	DLB2_HW_ERR(hw, "\tqe_orsp:      0x%x\n", DLB2_SYND2(QE_ORSP));
+	DLB2_HW_ERR(hw, "\tqe_valid:     0x%x\n", DLB2_SYND2(QE_VALID));
+	DLB2_HW_ERR(hw, "\tcq_int_rearm: 0x%x\n", DLB2_SYND2(CQ_INT_REARM));
+	DLB2_HW_ERR(hw, "\tdsi_error:    0x%x\n", DLB2_SYND2(DSI_ERROR));
+}
+
+static void dlb2_clear_syndrome_register(struct dlb2_hw *hw, u32 offset)
+{
+	u32 synd = 0;
+
+	DLB2_BIT_SET(synd, DLB2_SYS_ALARM_HW_SYND_VALID);
+	DLB2_BIT_SET(synd, DLB2_SYS_ALARM_HW_SYND_MORE);
+
+	DLB2_CSR_WR(hw, offset, synd);
+}
 
-	DLB2_CSR_WR(hw,
-		    DLB2_LSP_CQ_LDB_DSBL(hw->ver, port->id.phys_id),
-		    DLB2_LSP_CQ_LDB_DSBL_RST);
+/**
+ * dlb2_process_alarm_interrupt() - process an alarm interrupt
+ * @hw: dlb2_hw handle for a particular device.
+ *
+ * This function reads and logs the alarm syndrome, then acks the interrupt.
+ * This function should be called from the alarm interrupt handler when
+ * interrupt vector DLB2_INT_ALARM fires.
+ */
+void dlb2_process_alarm_interrupt(struct dlb2_hw *hw)
+{
+	u32 synd;
 
-	DLB2_CSR_WR(hw,
-		    DLB2_CHP_LDB_CQ_DEPTH(hw->ver, port->id.phys_id),
-		    DLB2_CHP_LDB_CQ_DEPTH_RST);
+	DLB2_HW_DBG(hw, "Processing alarm interrupt\n");
 
-	if (hw->ver != DLB2_HW_V2)
-		DLB2_CSR_WR(hw,
-			    DLB2_LSP_CFG_CQ_LDB_WU_LIMIT(port->id.phys_id),
-			    DLB2_LSP_CFG_CQ_LDB_WU_LIMIT_RST);
+	synd = DLB2_CSR_RD(hw, DLB2_SYS_ALARM_HW_SYND);
 
-	DLB2_CSR_WR(hw,
-		    DLB2_LSP_CQ_LDB_INFL_LIM(hw->ver, port->id.phys_id),
-		    DLB2_LSP_CQ_LDB_INFL_LIM_RST);
+	dlb2_log_alarm_syndrome(hw, "HW alarm syndrome", synd);
 
-	DLB2_CSR_WR(hw,
-		    DLB2_CHP_HIST_LIST_LIM(hw->ver, port->id.phys_id),
-		    DLB2_CHP_HIST_LIST_LIM_RST);
+	dlb2_clear_syndrome_register(hw, DLB2_SYS_ALARM_HW_SYND);
+}
 
-	DLB2_CSR_WR(hw,
-		    DLB2_CHP_HIST_LIST_BASE(hw->ver, port->id.phys_id),
-		    DLB2_CHP_HIST_LIST_BASE_RST);
+static u32 dlb2_hw_read_vf_to_pf_int_bitvec(struct dlb2_hw *hw)
+{
+	/*
+	 * The PF has one VF->PF MBOX ISR register per VF space, but they all
+	 * alias to the same physical register.
+	 */
+	return DLB2_FUNC_RD(hw, DLB2_PF_VF2PF_MAILBOX_ISR(0));
+}
 
-	DLB2_CSR_WR(hw,
-		    DLB2_CHP_HIST_LIST_POP_PTR(hw->ver, port->id.phys_id),
-		    DLB2_CHP_HIST_LIST_POP_PTR_RST);
+static u32 dlb2_sw_read_vdev_to_pf_int_bitvec(struct dlb2_hw *hw)
+{
+	u32 bitvec = 0;
+	int i;
 
-	DLB2_CSR_WR(hw,
-		    DLB2_CHP_HIST_LIST_PUSH_PTR(hw->ver, port->id.phys_id),
-		    DLB2_CHP_HIST_LIST_PUSH_PTR_RST);
+	for (i = 0; i < DLB2_MAX_NUM_VDEVS; i++) {
+		if (*hw->mbox[i].vdev_to_pf.isr_in_progress)
+			bitvec |= (1 << i);
+	}
 
-	DLB2_CSR_WR(hw,
-		    DLB2_CHP_LDB_CQ_INT_DEPTH_THRSH(hw->ver, port->id.phys_id),
-		    DLB2_CHP_LDB_CQ_INT_DEPTH_THRSH_RST);
+	return bitvec;
+}
 
-	DLB2_CSR_WR(hw,
-		    DLB2_CHP_LDB_CQ_TMR_THRSH(hw->ver, port->id.phys_id),
-		    DLB2_CHP_LDB_CQ_TMR_THRSH_RST);
+/**
+ * dlb2_read_vdev_to_pf_int_bitvec() - return a bit vector of all requesting
+ *					vdevs
+ * @hw: dlb2_hw handle for a particular device.
+ *
+ * When the vdev->PF ISR fires, this function can be called to determine which
+ * vdev(s) are requesting service. This bitvector must be passed to
+ * dlb2_ack_vdev_to_pf_int() when processing is complete for all requesting
+ * vdevs.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ *
+ * Return:
+ * Returns a bit vector indicating which VFs (0-15) have requested service.
+ */
+u32 dlb2_read_vdev_to_pf_int_bitvec(struct dlb2_hw *hw)
+{
+	if (hw->virt_mode == DLB2_VIRT_SIOV)
+		return dlb2_sw_read_vdev_to_pf_int_bitvec(hw);
+	else
+		return dlb2_hw_read_vf_to_pf_int_bitvec(hw);
+}
 
-	DLB2_CSR_WR(hw,
-		    DLB2_CHP_LDB_CQ_INT_ENB(hw->ver, port->id.phys_id),
-		    DLB2_CHP_LDB_CQ_INT_ENB_RST);
+static void dlb2_hw_ack_vf_mbox_int(struct dlb2_hw *hw, u32 bitvec)
+{
+	/*
+	 * The PF has one VF->PF MBOX ISR register per VF space, but
+	 * they all alias to the same physical register.
+	 */
+	DLB2_FUNC_WR(hw, DLB2_PF_VF2PF_MAILBOX_ISR(0), bitvec);
+}
 
-	DLB2_CSR_WR(hw,
-		    DLB2_SYS_LDB_CQ_ISR(port->id.phys_id),
-		    DLB2_SYS_LDB_CQ_ISR_RST);
+static void dlb2_sw_ack_vdev_mbox_int(struct dlb2_hw *hw, u32 bitvec)
+{
+	int i;
 
-	DLB2_CSR_WR(hw,
-		    DLB2_LSP_CQ_LDB_TKN_DEPTH_SEL(hw->ver, port->id.phys_id),
-		    DLB2_LSP_CQ_LDB_TKN_DEPTH_SEL_RST);
+	for (i = 0; i < DLB2_MAX_NUM_VDEVS; i++) {
+		if ((bitvec & (1 << i)) == 0)
+			continue;
 
-	DLB2_CSR_WR(hw,
-		    DLB2_CHP_LDB_CQ_TKN_DEPTH_SEL(hw->ver, port->id.phys_id),
-		    DLB2_CHP_LDB_CQ_TKN_DEPTH_SEL_RST);
+		*hw->mbox[i].vdev_to_pf.isr_in_progress = 0;
+	}
+}
 
-	DLB2_CSR_WR(hw,
-		    DLB2_CHP_LDB_CQ_WPTR(hw->ver, port->id.phys_id),
-		    DLB2_CHP_LDB_CQ_WPTR_RST);
+/**
+ * dlb2_ack_vdev_mbox_int() - ack processed vdev->PF mailbox interrupt
+ * @hw: dlb2_hw handle for a particular device.
+ * @bitvec: bit vector returned by dlb2_read_vdev_to_pf_int_bitvec()
+ *
+ * When done processing all VF mailbox requests, this function unsets the VF's
+ * mailbox ISR register.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ */
+void dlb2_ack_vdev_mbox_int(struct dlb2_hw *hw, u32 bitvec)
+{
+	if (hw->virt_mode == DLB2_VIRT_SIOV)
+		dlb2_sw_ack_vdev_mbox_int(hw, bitvec);
+	else
+		dlb2_hw_ack_vf_mbox_int(hw, bitvec);
+}
 
-	DLB2_CSR_WR(hw,
-		    DLB2_LSP_CQ_LDB_TKN_CNT(hw->ver, port->id.phys_id),
-		    DLB2_LSP_CQ_LDB_TKN_CNT_RST);
+/**
+ * dlb2_read_vf_flr_int_bitvec() - return a bit vector of all VFs requesting
+ *				    FLR
+ * @hw: dlb2_hw handle for a particular device.
+ *
+ * When the VF FLR ISR fires, this function can be called to determine which
+ * VF(s) are requesting FLRs. This bitvector must passed to
+ * dlb2_ack_vf_flr_int() when processing is complete for all requesting VFs.
+ *
+ * Return:
+ * Returns a bit vector indicating which VFs (0-15) have requested FLRs.
+ */
+u32 dlb2_read_vf_flr_int_bitvec(struct dlb2_hw *hw)
+{
+	/*
+	 * The PF has one VF->PF FLR ISR register per VF space, but they all
+	 * alias to the same physical register.
+	 */
+	return DLB2_FUNC_RD(hw, DLB2_PF_VF2PF_FLR_ISR(0));
+}
 
-	DLB2_CSR_WR(hw,
-		    DLB2_SYS_LDB_CQ_ADDR_L(port->id.phys_id),
-		    DLB2_SYS_LDB_CQ_ADDR_L_RST);
+/**
+ * dlb2_ack_vf_flr_int() - ack processed VF<->PF interrupt(s)
+ * @hw: dlb2_hw handle for a particular device.
+ * @bitvec: bit vector returned by dlb2_read_vf_flr_int_bitvec()
+ *
+ * When done processing all VF FLR requests, this function unsets the VF's FLR
+ * ISR register.
+ */
+void dlb2_ack_vf_flr_int(struct dlb2_hw *hw, u32 bitvec)
+{
+	u32 dis = 0;
+	int i;
 
-	DLB2_CSR_WR(hw,
-		    DLB2_SYS_LDB_CQ_ADDR_U(port->id.phys_id),
-		    DLB2_SYS_LDB_CQ_ADDR_U_RST);
+	if (!bitvec)
+		return;
 
-	if (hw->ver == DLB2_HW_V2)
-		DLB2_CSR_WR(hw,
-			    DLB2_SYS_LDB_CQ_AT(port->id.phys_id),
-			    DLB2_SYS_LDB_CQ_AT_RST);
+	/* Re-enable access to the VF BAR */
+	for (i = 0; i < DLB2_MAX_NUM_VDEVS; i++) {
+		if (!(bitvec & (1 << i)))
+			continue;
 
-	DLB2_CSR_WR(hw,
-		    DLB2_SYS_LDB_CQ_PASID(hw->ver, port->id.phys_id),
-		    DLB2_SYS_LDB_CQ_PASID_RST);
+		DLB2_CSR_WR(hw, DLB2_IOSF_FUNC_VF_BAR_DSBL(i), dis);
+	}
 
-	DLB2_CSR_WR(hw,
-		    DLB2_SYS_LDB_CQ2VF_PF_RO(port->id.phys_id),
-		    DLB2_SYS_LDB_CQ2VF_PF_RO_RST);
+	/* Notify the VF driver that the reset has completed */
+	DLB2_FUNC_WR(hw, DLB2_PF_VF_RESET_IN_PROGRESS(0), bitvec);
 
-	DLB2_CSR_WR(hw,
-		    DLB2_LSP_CQ_LDB_TOT_SCH_CNTL(hw->ver, port->id.phys_id),
-		    DLB2_LSP_CQ_LDB_TOT_SCH_CNTL_RST);
+	/* Mark the FLR ISR as complete */
+	DLB2_FUNC_WR(hw, DLB2_PF_VF2PF_FLR_ISR(0), bitvec);
+}
 
-	DLB2_CSR_WR(hw,
-		    DLB2_LSP_CQ_LDB_TOT_SCH_CNTH(hw->ver, port->id.phys_id),
-		    DLB2_LSP_CQ_LDB_TOT_SCH_CNTH_RST);
+/**
+ * dlb2_ack_vdev_to_pf_int() - ack processed VF mbox and FLR interrupt(s)
+ * @hw: dlb2_hw handle for a particular device.
+ * @mbox_bitvec: bit vector returned by dlb2_read_vdev_to_pf_int_bitvec()
+ * @flr_bitvec: bit vector returned by dlb2_read_vf_flr_int_bitvec()
+ *
+ * When done processing all VF requests, this function communicates to the
+ * hardware that processing is complete.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ */
+void dlb2_ack_vdev_to_pf_int(struct dlb2_hw *hw,
+			     u32 mbox_bitvec,
+			     u32 flr_bitvec)
+{
+	int i;
 
-	DLB2_CSR_WR(hw,
-		    DLB2_LSP_CQ2QID0(hw->ver, port->id.phys_id),
-		    DLB2_LSP_CQ2QID0_RST);
+	/* If using Scalable IOV, this is a noop */
+	if (hw->virt_mode == DLB2_VIRT_SIOV)
+		return;
 
-	DLB2_CSR_WR(hw,
-		    DLB2_LSP_CQ2QID1(hw->ver, port->id.phys_id),
-		    DLB2_LSP_CQ2QID1_RST);
+	for (i = 0; i < DLB2_MAX_NUM_VDEVS; i++) {
+		u32 isr = 0;
 
-	DLB2_CSR_WR(hw,
-		    DLB2_LSP_CQ2PRIOV(hw->ver, port->id.phys_id),
-		    DLB2_LSP_CQ2PRIOV_RST);
+		if (!((mbox_bitvec & (1 << i)) || (flr_bitvec & (1 << i))))
+			continue;
+
+		/* Unset the VF's ISR pending bit */
+		DLB2_BIT_SET(isr, DLB2_PF_VF2PF_ISR_PEND_ISR_PEND);
+		DLB2_FUNC_WR(hw, DLB2_PF_VF2PF_ISR_PEND(i), isr);
+	}
 }
 
-static void dlb2_domain_reset_ldb_port_registers(struct dlb2_hw *hw,
-						 struct dlb2_hw_domain *domain)
+/**
+ * dlb2_process_wdt_interrupt() - process watchdog timer interrupts
+ * @hw: dlb2_hw handle for a particular device.
+ *
+ * This function reads the watchdog timer interrupt cause registers to
+ * determine which port(s) had a watchdog timeout, and notifies the
+ * application(s) that own the port(s).
+ */
+void dlb2_process_wdt_interrupt(struct dlb2_hw *hw)
 {
-	struct dlb2_list_entry *iter;
-	struct dlb2_ldb_port *port;
-	int i;
-	RTE_SET_USED(iter);
+	u32 alert_id = DLB2_DOMAIN_ALERT_CQ_WATCHDOG_TIMEOUT;
+	u32 dwdto_0, dwdto_1;
+	u32 lwdto_0, lwdto_1;
+	int i, ret;
+
+	dwdto_0 = DLB2_CSR_RD(hw, DLB2_CHP_CFG_DIR_WDTO_0(hw->ver));
+	dwdto_1 = DLB2_CSR_RD(hw, DLB2_CHP_CFG_DIR_WDTO_1(hw->ver));
+	lwdto_0 = DLB2_CSR_RD(hw, DLB2_CHP_CFG_LDB_WDTO_0(hw->ver));
+	lwdto_1 = DLB2_CSR_RD(hw, DLB2_CHP_CFG_LDB_WDTO_1(hw->ver));
+
+	if (dwdto_0 == 0xFFFFFFFF &&
+	    dwdto_1 == 0xFFFFFFFF &&
+	    lwdto_0 == 0xFFFFFFFF &&
+	    lwdto_1 == 0xFFFFFFFF)
+		return;
+
+	/* Alert applications for affected directed ports */
+	for (i = 0; i < DLB2_MAX_NUM_DIR_PORTS(hw->ver); i++) {
+		struct dlb2_dir_pq_pair *port;
+		int idx = i % 32;
+
+		if (i < 32 && !(dwdto_0 & (1 << idx)))
+			continue;
+		if (i >= 32 && !(dwdto_1 & (1 << idx)))
+			continue;
 
-	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
-		DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i], port, iter)
-			__dlb2_domain_reset_ldb_port_registers(hw, port);
+		port = dlb2_get_dir_pq_from_id(hw, i, false, 0);
+		if (!port) {
+			DLB2_HW_ERR(hw,
+				    "[%s()]: Internal error: unable to find DIR port %u\n",
+				    __func__, i);
+			return;
+		}
+
+		if (port->id.vdev_owned)
+			ret = dlb2_notify_vf_alarm(hw,
+						   port->id.vdev_id,
+						   port->domain_id.virt_id,
+						   alert_id,
+						   port->id.virt_id);
+		else
+			ret = os_notify_user_space(hw,
+						   port->domain_id.phys_id,
+						   alert_id,
+						   i);
+		if (ret)
+			DLB2_HW_ERR(hw,
+				    "[%s()] Internal error: failed to notify\n",
+				    __func__);
 	}
-}
 
-static void
-__dlb2_domain_reset_dir_port_registers(struct dlb2_hw *hw,
-				       struct dlb2_dir_pq_pair *port)
-{
-	u32 reg = 0;
+	/* Alert applications for affected load-balanced ports */
+	for (i = 0; i < DLB2_MAX_NUM_LDB_PORTS; i++) {
+		struct dlb2_ldb_port *port;
+		int idx = i % 32;
 
-	DLB2_CSR_WR(hw,
-		    DLB2_CHP_DIR_CQ2VAS(hw->ver, port->id.phys_id),
-		    DLB2_CHP_DIR_CQ2VAS_RST);
+		if (i < 32 && !(lwdto_0 & (1 << idx)))
+			continue;
+		if (i >= 32 && !(lwdto_1 & (1 << idx)))
+			continue;
 
-	DLB2_CSR_WR(hw,
-		    DLB2_LSP_CQ_DIR_DSBL(hw->ver, port->id.phys_id),
-		    DLB2_LSP_CQ_DIR_DSBL_RST);
+		port = dlb2_get_ldb_port_from_id(hw, i, false, 0);
+		if (!port) {
+			DLB2_HW_ERR(hw,
+				    "[%s()]: Internal error: unable to find LDB port %u\n",
+				    __func__, i);
+			return;
+		}
 
-	DLB2_BIT_SET(reg, DLB2_SYS_WB_DIR_CQ_STATE_CQ_OPT_CLR);
+		/* aux_alert_data[8] is 1 to indicate a load-balanced port */
+		if (port->id.vdev_owned)
+			ret = dlb2_notify_vf_alarm(hw,
+						   port->id.vdev_id,
+						   port->domain_id.virt_id,
+						   alert_id,
+						   (1 << 8) | port->id.virt_id);
+		else
+			ret = os_notify_user_space(hw,
+						   port->domain_id.phys_id,
+						   alert_id,
+						   (1 << 8) | i);
+		if (ret)
+			DLB2_HW_ERR(hw,
+				    "[%s()] Internal error: failed to notify\n",
+				    __func__);
+	}
 
-	if (hw->ver == DLB2_HW_V2)
-		DLB2_CSR_WR(hw, DLB2_SYS_DIR_CQ_OPT_CLR, port->id.phys_id);
-	else
-		DLB2_CSR_WR(hw,
-			    DLB2_SYS_WB_DIR_CQ_STATE(port->id.phys_id), reg);
+	/* Clear watchdog timeout flag(s) (W1CLR) */
+	DLB2_CSR_WR(hw, DLB2_CHP_CFG_DIR_WDTO_0(hw->ver), dwdto_0);
+	DLB2_CSR_WR(hw, DLB2_CHP_CFG_DIR_WDTO_1(hw->ver), dwdto_1);
+	DLB2_CSR_WR(hw, DLB2_CHP_CFG_LDB_WDTO_0(hw->ver), lwdto_0);
+	DLB2_CSR_WR(hw, DLB2_CHP_CFG_LDB_WDTO_1(hw->ver), lwdto_1);
 
-	DLB2_CSR_WR(hw,
-		    DLB2_CHP_DIR_CQ_DEPTH(hw->ver, port->id.phys_id),
-		    DLB2_CHP_DIR_CQ_DEPTH_RST);
+	dlb2_flush_csr(hw);
 
-	DLB2_CSR_WR(hw,
-		    DLB2_CHP_DIR_CQ_INT_DEPTH_THRSH(hw->ver, port->id.phys_id),
-		    DLB2_CHP_DIR_CQ_INT_DEPTH_THRSH_RST);
+	/* Re-enable watchdog timeout(s) (W1CLR) */
+	DLB2_CSR_WR(hw, DLB2_CHP_CFG_DIR_WD_DISABLE0(hw->ver), dwdto_0);
+	DLB2_CSR_WR(hw, DLB2_CHP_CFG_DIR_WD_DISABLE1(hw->ver), dwdto_1);
+	DLB2_CSR_WR(hw, DLB2_CHP_CFG_LDB_WD_DISABLE0(hw->ver), lwdto_0);
+	DLB2_CSR_WR(hw, DLB2_CHP_CFG_LDB_WD_DISABLE1(hw->ver), lwdto_1);
+}
 
-	DLB2_CSR_WR(hw,
-		    DLB2_CHP_DIR_CQ_TMR_THRSH(hw->ver, port->id.phys_id),
-		    DLB2_CHP_DIR_CQ_TMR_THRSH_RST);
+static void dlb2_process_ingress_error(struct dlb2_hw *hw,
+				       u32 synd0,
+				       u32 alert_id,
+				       bool vf_error,
+				       unsigned int vf_id)
+{
+	struct dlb2_hw_domain *domain;
+	bool is_ldb;
+	u8 port_id;
+	int ret;
 
-	DLB2_CSR_WR(hw,
-		    DLB2_CHP_DIR_CQ_INT_ENB(hw->ver, port->id.phys_id),
-		    DLB2_CHP_DIR_CQ_INT_ENB_RST);
+	port_id = DLB2_SYND0(SYNDROME) & 0x7F;
+	if (DLB2_SYND0(SOURCE) == DLB2_ALARM_HW_SOURCE_SYS)
+		is_ldb = DLB2_SYND0(IS_LDB);
+	else
+		is_ldb = (DLB2_SYND0(SYNDROME) & 0x80) != 0;
 
-	DLB2_CSR_WR(hw,
-		    DLB2_SYS_DIR_CQ_ISR(port->id.phys_id),
-		    DLB2_SYS_DIR_CQ_ISR_RST);
+	/* Get the domain ID and, if it's a VF domain, the virtual port ID */
+	if (is_ldb) {
+		struct dlb2_ldb_port *port;
 
-	DLB2_CSR_WR(hw,
-		    DLB2_LSP_CQ_DIR_TKN_DEPTH_SEL_DSI(hw->ver,
-						      port->id.phys_id),
-		    DLB2_LSP_CQ_DIR_TKN_DEPTH_SEL_DSI_RST);
+		port = dlb2_get_ldb_port_from_id(hw, port_id, vf_error, vf_id);
+		if (!port) {
+			DLB2_HW_ERR(hw,
+				    "[%s()]: Internal error: unable to find LDB port\n\tport: %u, vf_error: %u, vf_id: %u\n",
+				    __func__, port_id, vf_error, vf_id);
+			return;
+		}
 
-	DLB2_CSR_WR(hw,
-		    DLB2_CHP_DIR_CQ_TKN_DEPTH_SEL(hw->ver, port->id.phys_id),
-		    DLB2_CHP_DIR_CQ_TKN_DEPTH_SEL_RST);
+		domain = &hw->domains[port->domain_id.phys_id];
+	} else {
+		struct dlb2_dir_pq_pair *port;
 
-	DLB2_CSR_WR(hw,
-		    DLB2_CHP_DIR_CQ_WPTR(hw->ver, port->id.phys_id),
-		    DLB2_CHP_DIR_CQ_WPTR_RST);
+		port = dlb2_get_dir_pq_from_id(hw, port_id, vf_error, vf_id);
+		if (!port) {
+			DLB2_HW_ERR(hw,
+				    "[%s()]: Internal error: unable to find DIR port\n\tport: %u, vf_error: %u, vf_id: %u\n",
+				    __func__, port_id, vf_error, vf_id);
+			return;
+		}
 
-	DLB2_CSR_WR(hw,
-		    DLB2_LSP_CQ_DIR_TKN_CNT(hw->ver, port->id.phys_id),
-		    DLB2_LSP_CQ_DIR_TKN_CNT_RST);
+		domain = &hw->domains[port->domain_id.phys_id];
+	}
 
-	DLB2_CSR_WR(hw,
-		    DLB2_SYS_DIR_CQ_ADDR_L(port->id.phys_id),
-		    DLB2_SYS_DIR_CQ_ADDR_L_RST);
+	if (vf_error)
+		ret = dlb2_notify_vf_alarm(hw,
+					   vf_id,
+					   domain->id.virt_id,
+					   alert_id,
+					   (is_ldb << 8) | port_id);
+	else
+		ret = os_notify_user_space(hw,
+					   domain->id.phys_id,
+					   alert_id,
+					   (is_ldb << 8) | port_id);
+	if (ret)
+		DLB2_HW_ERR(hw,
+			    "[%s()] Internal error: failed to notify\n",
+			    __func__);
+}
 
-	DLB2_CSR_WR(hw,
-		    DLB2_SYS_DIR_CQ_ADDR_U(port->id.phys_id),
-		    DLB2_SYS_DIR_CQ_ADDR_U_RST);
+static u32 dlb2_alert_id(u32 synd0)
+{
+	if (DLB2_SYND0(UNIT) == DLB2_ALARM_HW_UNIT_CHP &&
+	    DLB2_SYND0(AID) == DLB2_ALARM_HW_CHP_AID_ILLEGAL_ENQ)
+		return DLB2_DOMAIN_ALERT_PP_ILLEGAL_ENQ;
+	else if (DLB2_SYND0(UNIT) == DLB2_ALARM_HW_UNIT_CHP &&
+		 DLB2_SYND0(AID) == DLB2_ALARM_HW_CHP_AID_EXCESS_TOKEN_POPS)
+		return DLB2_DOMAIN_ALERT_PP_EXCESS_TOKEN_POPS;
+	else if (DLB2_SYND0(SOURCE) == DLB2_ALARM_HW_SOURCE_SYS &&
+		 DLB2_SYND0(AID) == DLB2_ALARM_SYS_AID_ILLEGAL_HCW)
+		return DLB2_DOMAIN_ALERT_ILLEGAL_HCW;
+	else if (DLB2_SYND0(SOURCE) == DLB2_ALARM_HW_SOURCE_SYS &&
+		 DLB2_SYND0(AID) == DLB2_ALARM_SYS_AID_ILLEGAL_QID)
+		return DLB2_DOMAIN_ALERT_ILLEGAL_QID;
+	else if (DLB2_SYND0(SOURCE) == DLB2_ALARM_HW_SOURCE_SYS &&
+		 DLB2_SYND0(AID) == DLB2_ALARM_SYS_AID_DISABLED_QID)
+		return DLB2_DOMAIN_ALERT_DISABLED_QID;
+	else
+		return NUM_DLB2_DOMAIN_ALERTS;
+}
 
-	DLB2_CSR_WR(hw,
-		    DLB2_SYS_DIR_CQ_AT(port->id.phys_id),
-		    DLB2_SYS_DIR_CQ_AT_RST);
+/**
+ * dlb2_process_ingress_error_interrupt() - process ingress error interrupts
+ * @hw: dlb2_hw handle for a particular device.
+ *
+ * This function reads the alarm syndrome, logs it, notifies user-space, and
+ * acks the interrupt. This function should be called from the alarm interrupt
+ * handler when interrupt vector DLB2_INT_INGRESS_ERROR fires.
+ *
+ * Return:
+ * Returns true if an ingress error interrupt occurred, false otherwise
+ */
+bool dlb2_process_ingress_error_interrupt(struct dlb2_hw *hw)
+{
+	u32 synd0, synd1, synd2;
+	u32 alert_id;
+	bool valid;
+	int i;
 
-	if (hw->ver == DLB2_HW_V2)
-		DLB2_CSR_WR(hw,
-			    DLB2_SYS_DIR_CQ_AT(port->id.phys_id),
-			    DLB2_SYS_DIR_CQ_AT_RST);
+	synd0 = DLB2_CSR_RD(hw, DLB2_SYS_ALARM_PF_SYND0);
 
-	DLB2_CSR_WR(hw,
-		    DLB2_SYS_DIR_CQ_PASID(hw->ver, port->id.phys_id),
-		    DLB2_SYS_DIR_CQ_PASID_RST);
+	valid = DLB2_SYND0(VALID);
 
-	DLB2_CSR_WR(hw,
-		    DLB2_SYS_DIR_CQ_FMT(port->id.phys_id),
-		    DLB2_SYS_DIR_CQ_FMT_RST);
+	if (valid) {
+		synd1 = DLB2_CSR_RD(hw, DLB2_SYS_ALARM_PF_SYND1);
+		synd2 = DLB2_CSR_RD(hw, DLB2_SYS_ALARM_PF_SYND2);
 
-	DLB2_CSR_WR(hw,
-		    DLB2_SYS_DIR_CQ2VF_PF_RO(port->id.phys_id),
-		    DLB2_SYS_DIR_CQ2VF_PF_RO_RST);
+		alert_id = dlb2_alert_id(synd0);
 
-	DLB2_CSR_WR(hw,
-		    DLB2_LSP_CQ_DIR_TOT_SCH_CNTL(hw->ver, port->id.phys_id),
-		    DLB2_LSP_CQ_DIR_TOT_SCH_CNTL_RST);
+		dlb2_log_pf_vf_syndrome(hw,
+					"PF Ingress error alarm",
+					synd0, synd1, synd2, alert_id);
 
-	DLB2_CSR_WR(hw,
-		    DLB2_LSP_CQ_DIR_TOT_SCH_CNTH(hw->ver, port->id.phys_id),
-		    DLB2_LSP_CQ_DIR_TOT_SCH_CNTH_RST);
+		dlb2_clear_syndrome_register(hw, DLB2_SYS_ALARM_PF_SYND0);
 
-	DLB2_CSR_WR(hw,
-		    DLB2_SYS_DIR_PP2VAS(port->id.phys_id),
-		    DLB2_SYS_DIR_PP2VAS_RST);
+		dlb2_process_ingress_error(hw, synd0, alert_id, false, 0);
+	}
 
-	DLB2_CSR_WR(hw,
-		    DLB2_CHP_DIR_CQ2VAS(hw->ver, port->id.phys_id),
-		    DLB2_CHP_DIR_CQ2VAS_RST);
+	for (i = 0; i < DLB2_MAX_NUM_VDEVS; i++) {
+		synd0 = DLB2_CSR_RD(hw, DLB2_SYS_ALARM_VF_SYND0(i));
 
-	DLB2_CSR_WR(hw,
-		    DLB2_SYS_DIR_PP2VDEV(port->id.phys_id),
-		    DLB2_SYS_DIR_PP2VDEV_RST);
+		valid |= DLB2_SYND0(VALID);
 
-	if (port->id.vdev_owned) {
-		unsigned int offs;
-		u32 virt_id;
+		if (!DLB2_SYND0(VALID))
+			continue;
 
-		/*
-		 * DLB uses producer port address bits 17:12 to determine the
-		 * producer port ID. In Scalable IOV mode, PP accesses come
-		 * through the PF MMIO window for the physical producer port,
-		 * so for translation purposes the virtual and physical port
-		 * IDs are equal.
-		 */
-		if (hw->virt_mode == DLB2_VIRT_SRIOV)
-			virt_id = port->id.virt_id;
-		else
-			virt_id = port->id.phys_id;
+		synd1 = DLB2_CSR_RD(hw, DLB2_SYS_ALARM_VF_SYND1(i));
+		synd2 = DLB2_CSR_RD(hw, DLB2_SYS_ALARM_VF_SYND2(i));
 
-		offs = port->id.vdev_id * DLB2_MAX_NUM_DIR_PORTS(hw->ver) +
-			virt_id;
+		alert_id = dlb2_alert_id(synd0);
 
-		DLB2_CSR_WR(hw,
-			    DLB2_SYS_VF_DIR_VPP2PP(offs),
-			    DLB2_SYS_VF_DIR_VPP2PP_RST);
+		dlb2_log_pf_vf_syndrome(hw,
+					"VF Ingress error alarm",
+					synd0, synd1, synd2, alert_id);
 
-		DLB2_CSR_WR(hw,
-			    DLB2_SYS_VF_DIR_VPP_V(offs),
-			    DLB2_SYS_VF_DIR_VPP_V_RST);
+		dlb2_clear_syndrome_register(hw, DLB2_SYS_ALARM_VF_SYND0(i));
+
+		dlb2_process_ingress_error(hw, synd0, alert_id, true, i);
 	}
 
-	DLB2_CSR_WR(hw,
-		    DLB2_SYS_DIR_PP_V(port->id.phys_id),
-		    DLB2_SYS_DIR_PP_V_RST);
+	return valid;
 }
 
-static void dlb2_domain_reset_dir_port_registers(struct dlb2_hw *hw,
-						 struct dlb2_hw_domain *domain)
+/**
+ * dlb2_get_group_sequence_numbers() - return a group's number of SNs per queue
+ * @hw: dlb2_hw handle for a particular device.
+ * @group_id: sequence number group ID.
+ *
+ * This function returns the configured number of sequence numbers per queue
+ * for the specified group.
+ *
+ * Return:
+ * Returns -EINVAL if group_id is invalid, else the group's SNs per queue.
+ */
+int dlb2_get_group_sequence_numbers(struct dlb2_hw *hw, u32 group_id)
 {
-	struct dlb2_list_entry *iter;
-	struct dlb2_dir_pq_pair *port;
-	RTE_SET_USED(iter);
+	if (group_id >= DLB2_MAX_NUM_SEQUENCE_NUMBER_GROUPS)
+		return -EINVAL;
 
-	DLB2_DOM_LIST_FOR(domain->used_dir_pq_pairs, port, iter)
-		__dlb2_domain_reset_dir_port_registers(hw, port);
+	return hw->rsrcs.sn_groups[group_id].sequence_numbers_per_queue;
 }
 
-static void dlb2_domain_reset_ldb_queue_registers(struct dlb2_hw *hw,
-						  struct dlb2_hw_domain *domain)
+/**
+ * dlb2_get_group_sequence_number_occupancy() - return a group's in-use slots
+ * @hw: dlb2_hw handle for a particular device.
+ * @group_id: sequence number group ID.
+ *
+ * This function returns the group's number of in-use slots (i.e. load-balanced
+ * queues using the specified group).
+ *
+ * Return:
+ * Returns -EINVAL if group_id is invalid, else the group's SNs per queue.
+ */
+int dlb2_get_group_sequence_number_occupancy(struct dlb2_hw *hw, u32 group_id)
 {
-	struct dlb2_list_entry *iter;
-	struct dlb2_ldb_queue *queue;
-	RTE_SET_USED(iter);
-
-	DLB2_DOM_LIST_FOR(domain->used_ldb_queues, queue, iter) {
-		unsigned int queue_id = queue->id.phys_id;
-		int i;
+	if (group_id >= DLB2_MAX_NUM_SEQUENCE_NUMBER_GROUPS)
+		return -EINVAL;
 
-		DLB2_CSR_WR(hw,
-			    DLB2_LSP_QID_NALDB_TOT_ENQ_CNTL(hw->ver, queue_id),
-			    DLB2_LSP_QID_NALDB_TOT_ENQ_CNTL_RST);
+	return dlb2_sn_group_used_slots(&hw->rsrcs.sn_groups[group_id]);
+}
 
-		DLB2_CSR_WR(hw,
-			    DLB2_LSP_QID_NALDB_TOT_ENQ_CNTH(hw->ver, queue_id),
-			    DLB2_LSP_QID_NALDB_TOT_ENQ_CNTH_RST);
+static void dlb2_log_set_group_sequence_numbers(struct dlb2_hw *hw,
+						u32 group_id,
+						u32 val)
+{
+	DLB2_HW_DBG(hw, "DLB2 set group sequence numbers:\n");
+	DLB2_HW_DBG(hw, "\tGroup ID: %u\n", group_id);
+	DLB2_HW_DBG(hw, "\tValue:    %u\n", val);
+}
 
-		DLB2_CSR_WR(hw,
-			    DLB2_LSP_QID_ATM_TOT_ENQ_CNTL(hw->ver, queue_id),
-			    DLB2_LSP_QID_ATM_TOT_ENQ_CNTL_RST);
+/**
+ * dlb2_set_group_sequence_numbers() - assign a group's number of SNs per queue
+ * @hw: dlb2_hw handle for a particular device.
+ * @group_id: sequence number group ID.
+ * @val: requested amount of sequence numbers per queue.
+ *
+ * This function configures the group's number of sequence numbers per queue.
+ * val can be a power-of-two between 32 and 1024, inclusive. This setting can
+ * be configured until the first ordered load-balanced queue is configured, at
+ * which point the configuration is locked.
+ *
+ * Return:
+ * Returns 0 upon success; -EINVAL if group_id or val is invalid, -EPERM if an
+ * ordered queue is configured.
+ */
+int dlb2_set_group_sequence_numbers(struct dlb2_hw *hw,
+				    u32 group_id,
+				    u32 val)
+{
+	const u32 valid_allocations[] = {64, 128, 256, 512, 1024};
+	struct dlb2_sn_group *group;
+	u32 sn_mode = 0;
+	int mode;
 
-		DLB2_CSR_WR(hw,
-			    DLB2_LSP_QID_ATM_TOT_ENQ_CNTH(hw->ver, queue_id),
-			    DLB2_LSP_QID_ATM_TOT_ENQ_CNTH_RST);
+	if (group_id >= DLB2_MAX_NUM_SEQUENCE_NUMBER_GROUPS)
+		return -EINVAL;
 
-		DLB2_CSR_WR(hw,
-			    DLB2_LSP_QID_NALDB_MAX_DEPTH(hw->ver, queue_id),
-			    DLB2_LSP_QID_NALDB_MAX_DEPTH_RST);
+	group = &hw->rsrcs.sn_groups[group_id];
 
-		DLB2_CSR_WR(hw,
-			    DLB2_LSP_QID_LDB_INFL_LIM(hw->ver, queue_id),
-			    DLB2_LSP_QID_LDB_INFL_LIM_RST);
+	/*
+	 * Once the first load-balanced queue using an SN group is configured,
+	 * the group cannot be changed.
+	 */
+	if (group->slot_use_bitmap != 0)
+		return -EPERM;
 
-		DLB2_CSR_WR(hw,
-			    DLB2_LSP_QID_AQED_ACTIVE_LIM(hw->ver, queue_id),
-			    DLB2_LSP_QID_AQED_ACTIVE_LIM_RST);
+	for (mode = 0; mode < DLB2_MAX_NUM_SEQUENCE_NUMBER_MODES; mode++)
+		if (val == valid_allocations[mode])
+			break;
 
-		DLB2_CSR_WR(hw,
-			    DLB2_LSP_QID_ATM_DEPTH_THRSH(hw->ver, queue_id),
-			    DLB2_LSP_QID_ATM_DEPTH_THRSH_RST);
+	if (mode == DLB2_MAX_NUM_SEQUENCE_NUMBER_MODES)
+		return -EINVAL;
 
-		DLB2_CSR_WR(hw,
-			    DLB2_LSP_QID_NALDB_DEPTH_THRSH(hw->ver, queue_id),
-			    DLB2_LSP_QID_NALDB_DEPTH_THRSH_RST);
+	group->mode = mode;
+	group->sequence_numbers_per_queue = val;
 
-		DLB2_CSR_WR(hw,
-			    DLB2_SYS_LDB_QID_ITS(queue_id),
-			    DLB2_SYS_LDB_QID_ITS_RST);
+	DLB2_BITS_SET(sn_mode, hw->rsrcs.sn_groups[0].mode,
+		 DLB2_RO_GRP_SN_MODE_SN_MODE_0);
+	DLB2_BITS_SET(sn_mode, hw->rsrcs.sn_groups[1].mode,
+		 DLB2_RO_GRP_SN_MODE_SN_MODE_1);
 
-		DLB2_CSR_WR(hw,
-			    DLB2_CHP_ORD_QID_SN(hw->ver, queue_id),
-			    DLB2_CHP_ORD_QID_SN_RST);
+	DLB2_CSR_WR(hw, DLB2_RO_GRP_SN_MODE(hw->ver), sn_mode);
 
-		DLB2_CSR_WR(hw,
-			    DLB2_CHP_ORD_QID_SN_MAP(hw->ver, queue_id),
-			    DLB2_CHP_ORD_QID_SN_MAP_RST);
+	dlb2_log_set_group_sequence_numbers(hw, group_id, val);
 
-		DLB2_CSR_WR(hw,
-			    DLB2_SYS_LDB_QID_V(queue_id),
-			    DLB2_SYS_LDB_QID_V_RST);
+	return 0;
+}
 
-		DLB2_CSR_WR(hw,
-			    DLB2_SYS_LDB_QID_CFG_V(queue_id),
-			    DLB2_SYS_LDB_QID_CFG_V_RST);
+static u32 dlb2_ldb_cq_inflight_count(struct dlb2_hw *hw,
+				      struct dlb2_ldb_port *port)
+{
+	u32 cnt;
 
-		if (queue->sn_cfg_valid) {
-			u32 offs[2];
+	cnt = DLB2_CSR_RD(hw,
+			  DLB2_LSP_CQ_LDB_INFL_CNT(hw->ver, port->id.phys_id));
 
-			offs[0] = DLB2_RO_GRP_0_SLT_SHFT(hw->ver,
-							 queue->sn_slot);
-			offs[1] = DLB2_RO_GRP_1_SLT_SHFT(hw->ver,
-							 queue->sn_slot);
+	return DLB2_BITS_GET(cnt, DLB2_LSP_CQ_LDB_INFL_CNT_COUNT);
+}
 
-			DLB2_CSR_WR(hw,
-				    offs[queue->sn_group],
-				    DLB2_RO_GRP_0_SLT_SHFT_RST);
-		}
+static u32 dlb2_ldb_cq_token_count(struct dlb2_hw *hw,
+				   struct dlb2_ldb_port *port)
+{
+	u32 cnt;
 
-		for (i = 0; i < DLB2_LSP_QID2CQIDIX_NUM; i++) {
-			DLB2_CSR_WR(hw,
-				    DLB2_LSP_QID2CQIDIX(hw->ver, queue_id, i),
-				    DLB2_LSP_QID2CQIDIX_00_RST);
+	cnt = DLB2_CSR_RD(hw,
+			  DLB2_LSP_CQ_LDB_TKN_CNT(hw->ver, port->id.phys_id));
 
-			DLB2_CSR_WR(hw,
-				    DLB2_LSP_QID2CQIDIX2(hw->ver, queue_id, i),
-				    DLB2_LSP_QID2CQIDIX2_00_RST);
+	/*
+	 * Account for the initial token count, which is used in order to
+	 * provide a CQ with depth less than 8.
+	 */
 
-			DLB2_CSR_WR(hw,
-				    DLB2_ATM_QID2CQIDIX(queue_id, i),
-				    DLB2_ATM_QID2CQIDIX_00_RST);
-		}
-	}
+	return DLB2_BITS_GET(cnt, DLB2_LSP_CQ_LDB_TKN_CNT_TOKEN_COUNT) -
+		port->init_tkn_cnt;
 }
 
-static void dlb2_domain_reset_dir_queue_registers(struct dlb2_hw *hw,
-						  struct dlb2_hw_domain *domain)
+static int dlb2_drain_ldb_cq(struct dlb2_hw *hw, struct dlb2_ldb_port *port)
 {
-	struct dlb2_list_entry *iter;
-	struct dlb2_dir_pq_pair *queue;
-	RTE_SET_USED(iter);
+	u32 infl_cnt, tkn_cnt;
+	unsigned int i;
 
-	DLB2_DOM_LIST_FOR(domain->used_dir_pq_pairs, queue, iter) {
-		DLB2_CSR_WR(hw,
-			    DLB2_LSP_QID_DIR_MAX_DEPTH(hw->ver,
-						       queue->id.phys_id),
-			    DLB2_LSP_QID_DIR_MAX_DEPTH_RST);
+	infl_cnt = dlb2_ldb_cq_inflight_count(hw, port);
+	tkn_cnt = dlb2_ldb_cq_token_count(hw, port);
 
-		DLB2_CSR_WR(hw,
-			    DLB2_LSP_QID_DIR_TOT_ENQ_CNTL(hw->ver,
-							  queue->id.phys_id),
-			    DLB2_LSP_QID_DIR_TOT_ENQ_CNTL_RST);
+	if (infl_cnt || tkn_cnt) {
+		struct dlb2_hcw hcw_mem[8], *hcw;
+		void __iomem *pp_addr;
 
-		DLB2_CSR_WR(hw,
-			    DLB2_LSP_QID_DIR_TOT_ENQ_CNTH(hw->ver,
-							  queue->id.phys_id),
-			    DLB2_LSP_QID_DIR_TOT_ENQ_CNTH_RST);
+		pp_addr = os_map_producer_port(hw, port->id.phys_id, true);
 
-		DLB2_CSR_WR(hw,
-			    DLB2_LSP_QID_DIR_DEPTH_THRSH(hw->ver,
-							 queue->id.phys_id),
-			    DLB2_LSP_QID_DIR_DEPTH_THRSH_RST);
+		/* Point hcw to a 64B-aligned location */
+		hcw = (struct dlb2_hcw *)((uintptr_t)&hcw_mem[4] & ~0x3F);
 
-		DLB2_CSR_WR(hw,
-			    DLB2_SYS_DIR_QID_ITS(queue->id.phys_id),
-			    DLB2_SYS_DIR_QID_ITS_RST);
+		/*
+		 * Program the first HCW for a completion and token return and
+		 * the other HCWs as NOOPS
+		 */
 
-		DLB2_CSR_WR(hw,
-			    DLB2_SYS_DIR_QID_V(queue->id.phys_id),
-			    DLB2_SYS_DIR_QID_V_RST);
-	}
-}
+		memset(hcw, 0, 4 * sizeof(*hcw));
+		hcw->qe_comp = (infl_cnt > 0);
+		hcw->cq_token = (tkn_cnt > 0);
+		hcw->lock_id = tkn_cnt - 1;
 
+		/* Return tokens in the first HCW */
+		os_enqueue_four_hcws(hw, hcw, pp_addr);
 
+		hcw->cq_token = 0;
 
+		/* Issue remaining completions (if any) */
+		for (i = 1; i < infl_cnt; i++)
+			os_enqueue_four_hcws(hw, hcw, pp_addr);
 
+		os_fence_hcw(hw, pp_addr);
 
-static void dlb2_domain_reset_registers(struct dlb2_hw *hw,
-					struct dlb2_hw_domain *domain)
-{
-	dlb2_domain_reset_ldb_port_registers(hw, domain);
+		os_unmap_producer_port(hw, pp_addr);
+	}
 
-	dlb2_domain_reset_dir_port_registers(hw, domain);
+	return tkn_cnt;
+}
 
-	dlb2_domain_reset_ldb_queue_registers(hw, domain);
+static int dlb2_domain_wait_for_ldb_cqs_to_empty(struct dlb2_hw *hw,
+						 struct dlb2_hw_domain *domain)
+{
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_ldb_port *port;
+	int i;
 
-	dlb2_domain_reset_dir_queue_registers(hw, domain);
+	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
+		DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i], port, iter) {
+			int j;
 
-	if (hw->ver == DLB2_HW_V2) {
-		DLB2_CSR_WR(hw,
-			    DLB2_CHP_CFG_LDB_VAS_CRD(domain->id.phys_id),
-			    DLB2_CHP_CFG_LDB_VAS_CRD_RST);
+			for (j = 0; j < DLB2_MAX_CQ_COMP_CHECK_LOOPS; j++) {
+				if (dlb2_ldb_cq_inflight_count(hw, port) == 0)
+					break;
+			}
 
-		DLB2_CSR_WR(hw,
-			    DLB2_CHP_CFG_DIR_VAS_CRD(domain->id.phys_id),
-			    DLB2_CHP_CFG_DIR_VAS_CRD_RST);
-	} else
-		DLB2_CSR_WR(hw,
-			    DLB2_CHP_CFG_VAS_CRD(domain->id.phys_id),
-			    DLB2_CHP_CFG_VAS_CRD_RST);
+			if (j == DLB2_MAX_CQ_COMP_CHECK_LOOPS) {
+				DLB2_HW_ERR(hw,
+					    "[%s()] Internal error: failed to flush load-balanced port %d's completions.\n",
+					    __func__, port->id.phys_id);
+				return -EFAULT;
+			}
+		}
+	}
+
+	return 0;
 }
 
 static int dlb2_domain_reset_software_state(struct dlb2_hw *hw,
 					    struct dlb2_hw_domain *domain)
 {
-	struct dlb2_dir_pq_pair *tmp_dir_port;
-	struct dlb2_ldb_queue *tmp_ldb_queue;
-	struct dlb2_ldb_port *tmp_ldb_port;
-	struct dlb2_list_entry *iter1;
-	struct dlb2_list_entry *iter2;
+	struct dlb2_dir_pq_pair *tmp_dir_port __attribute__((unused));
+	struct dlb2_ldb_queue *tmp_ldb_queue __attribute__((unused));
+	struct dlb2_ldb_port *tmp_ldb_port __attribute__((unused));
+	struct dlb2_list_entry *iter1 __attribute__((unused));
+	struct dlb2_list_entry *iter2 __attribute__((unused));
 	struct dlb2_function_resources *rsrcs;
 	struct dlb2_dir_pq_pair *dir_port;
 	struct dlb2_ldb_queue *ldb_queue;
 	struct dlb2_ldb_port *ldb_port;
 	struct dlb2_list_head *list;
 	int ret, i;
-	RTE_SET_USED(tmp_dir_port);
-	RTE_SET_USED(tmp_ldb_queue);
-	RTE_SET_USED(tmp_ldb_port);
-	RTE_SET_USED(iter1);
-	RTE_SET_USED(iter2);
 
 	rsrcs = domain->parent_func;
 
@@ -3613,7 +7446,7 @@ static int dlb2_domain_reset_software_state(struct dlb2_hw *hw,
 				    domain->total_hist_list_entries);
 	if (ret) {
 		DLB2_HW_ERR(hw,
-			    "[%s()] Internal error: domain hist list base does not match the function's bitmap.\n",
+			    "[%s()] Internal error: domain hist list base doesn't match the function's bitmap.\n",
 			    __func__);
 		return ret;
 	}
@@ -3654,599 +7487,545 @@ static int dlb2_domain_reset_software_state(struct dlb2_hw *hw,
 	return 0;
 }
 
-static int dlb2_domain_drain_unmapped_queue(struct dlb2_hw *hw,
-					    struct dlb2_hw_domain *domain,
-					    struct dlb2_ldb_queue *queue)
-{
-	struct dlb2_ldb_port *port = NULL;
-	int ret, i;
-
-	/* If a domain has LDB queues, it must have LDB ports */
-	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
-		port = DLB2_DOM_LIST_HEAD(domain->used_ldb_ports[i],
-					  typeof(*port));
-		if (port)
-			break;
-	}
-
-	if (port == NULL) {
-		DLB2_HW_ERR(hw,
-			    "[%s()] Internal error: No configured LDB ports\n",
-			    __func__);
-		return -EFAULT;
-	}
-
-	/* If necessary, free up a QID slot in this CQ */
-	if (port->num_mappings == DLB2_MAX_NUM_QIDS_PER_LDB_CQ) {
-		struct dlb2_ldb_queue *mapped_queue;
-
-		mapped_queue = &hw->rsrcs.ldb_queues[port->qid_map[0].qid];
-
-		ret = dlb2_ldb_port_unmap_qid(hw, port, mapped_queue);
-		if (ret)
-			return ret;
-	}
-
-	ret = dlb2_ldb_port_map_qid_dynamic(hw, port, queue, 0);
-	if (ret)
-		return ret;
-
-	return dlb2_domain_drain_mapped_queues(hw, domain);
-}
-
-static int dlb2_domain_drain_unmapped_queues(struct dlb2_hw *hw,
-					     struct dlb2_hw_domain *domain)
+/**
+ * dlb2_resource_reset() - reset in-use resources to their initial state
+ * @hw: dlb2_hw handle for a particular device.
+ *
+ * This function resets in-use resources, and makes them available for use.
+ * All resources go back to their owning function, whether a PF or a VF.
+ */
+void dlb2_resource_reset(struct dlb2_hw *hw)
 {
-	struct dlb2_list_entry *iter;
-	struct dlb2_ldb_queue *queue;
-	int ret;
-	RTE_SET_USED(iter);
-
-	/* If the domain hasn't been started, there's no traffic to drain */
-	if (!domain->started)
-		return 0;
-
-	/*
-	 * Pre-condition: the unattached queue must not have any outstanding
-	 * completions. This is ensured by calling dlb2_domain_drain_ldb_cqs()
-	 * prior to this in dlb2_domain_drain_mapped_queues().
-	 */
-	DLB2_DOM_LIST_FOR(domain->used_ldb_queues, queue, iter) {
-		if (queue->num_mappings != 0 ||
-		    dlb2_ldb_queue_is_empty(hw, queue))
-			continue;
+	struct dlb2_hw_domain *domain, *next __attribute__((unused));
+	struct dlb2_list_entry *iter1 __attribute__((unused));
+	struct dlb2_list_entry *iter2 __attribute__((unused));
+	int i;
 
-		ret = dlb2_domain_drain_unmapped_queue(hw, domain, queue);
-		if (ret)
-			return ret;
+	for (i = 0; i < DLB2_MAX_NUM_VDEVS; i++) {
+		DLB2_FUNC_LIST_FOR_SAFE(hw->vdev[i].used_domains, domain,
+					next, iter1, iter2)
+			dlb2_domain_reset_software_state(hw, domain);
 	}
 
-	return 0;
+	DLB2_FUNC_LIST_FOR_SAFE(hw->pf.used_domains, domain,
+				next, iter1, iter2)
+		dlb2_domain_reset_software_state(hw, domain);
+}
+
+static u32 dlb2_dir_queue_depth(struct dlb2_hw *hw,
+				struct dlb2_dir_pq_pair *queue)
+{
+	u32 cnt;
+
+	cnt = DLB2_CSR_RD(hw, DLB2_LSP_QID_DIR_ENQUEUE_CNT(hw->ver,
+						      queue->id.phys_id));
+
+	return DLB2_BITS_GET(cnt, DLB2_LSP_QID_DIR_ENQUEUE_CNT_COUNT);
+}
+
+static bool dlb2_dir_queue_is_empty(struct dlb2_hw *hw,
+				    struct dlb2_dir_pq_pair *queue)
+{
+	return dlb2_dir_queue_depth(hw, queue) == 0;
+}
+
+static void dlb2_log_get_dir_queue_depth(struct dlb2_hw *hw,
+					 u32 domain_id,
+					 u32 queue_id,
+					 bool vdev_req,
+					 unsigned int vf_id)
+{
+	DLB2_HW_DBG(hw, "DLB get directed queue depth:\n");
+	if (vdev_req)
+		DLB2_HW_DBG(hw, "(Request from VF %d)\n", vf_id);
+	DLB2_HW_DBG(hw, "\tDomain ID: %d\n", domain_id);
+	DLB2_HW_DBG(hw, "\tQueue ID: %d\n", queue_id);
 }
 
 /**
- * dlb2_reset_domain() - reset a scheduling domain
+ * dlb2_hw_get_dir_queue_depth() - returns the depth of a directed queue
  * @hw: dlb2_hw handle for a particular device.
  * @domain_id: domain ID.
+ * @args: queue depth args
+ * @resp: response structure.
  * @vdev_req: indicates whether this request came from a vdev.
  * @vdev_id: If vdev_req is true, this contains the vdev's ID.
  *
- * This function resets and frees a DLB 2.0 scheduling domain and its associated
- * resources.
- *
- * Pre-condition: the driver must ensure software has stopped sending QEs
- * through this domain's producer ports before invoking this function, or
- * undefined behavior will result.
+ * This function returns the depth of a directed queue.
  *
  * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
  * device.
  *
  * Return:
- * Returns 0 upon success, -1 otherwise.
+ * Returns 0 upon success, < 0 otherwise. If an error occurs, resp->status is
+ * assigned a detailed error code from enum dlb2_error. If successful, resp->id
+ * contains the depth.
  *
- * EINVAL - Invalid domain ID, or the domain is not configured.
- * EFAULT - Internal error. (Possibly caused if software is the pre-condition
- *	    is not met.)
- * ETIMEDOUT - Hardware component didn't reset in the expected time.
+ * Errors:
+ * EINVAL - Invalid domain ID or queue ID.
  */
-int dlb2_reset_domain(struct dlb2_hw *hw,
-		      u32 domain_id,
-		      bool vdev_req,
-		      unsigned int vdev_id)
+int dlb2_hw_get_dir_queue_depth(struct dlb2_hw *hw,
+				u32 domain_id,
+				struct dlb2_get_dir_queue_depth_args *args,
+				struct dlb2_cmd_response *resp,
+				bool vdev_req,
+				unsigned int vdev_id)
 {
+	struct dlb2_dir_pq_pair *queue;
 	struct dlb2_hw_domain *domain;
-	int ret;
+	int id;
 
-	dlb2_log_reset_domain(hw, domain_id, vdev_req, vdev_id);
+	id = domain_id;
 
-	domain = dlb2_get_domain_from_id(hw, domain_id, vdev_req, vdev_id);
+	dlb2_log_get_dir_queue_depth(hw, domain_id, args->queue_id,
+				     vdev_req, vdev_id);
 
-	if (domain == NULL || !domain->configured)
+	domain = dlb2_get_domain_from_id(hw, id, vdev_req, vdev_id);
+	if (!domain) {
+		resp->status = DLB2_ST_INVALID_DOMAIN_ID;
 		return -EINVAL;
-
-	/* Disable VPPs */
-	if (vdev_req) {
-		dlb2_domain_disable_dir_vpps(hw, domain, vdev_id);
-
-		dlb2_domain_disable_ldb_vpps(hw, domain, vdev_id);
 	}
 
-	/* Disable CQ interrupts */
-	dlb2_domain_disable_dir_port_interrupts(hw, domain);
-
-	dlb2_domain_disable_ldb_port_interrupts(hw, domain);
-
-	/*
-	 * For each queue owned by this domain, disable its write permissions to
-	 * cause any traffic sent to it to be dropped. Well-behaved software
-	 * should not be sending QEs at this point.
-	 */
-	dlb2_domain_disable_dir_queue_write_perms(hw, domain);
-
-	dlb2_domain_disable_ldb_queue_write_perms(hw, domain);
-
-	/* Turn off completion tracking on all the domain's PPs. */
-	dlb2_domain_disable_ldb_seq_checks(hw, domain);
-
-	/*
-	 * Disable the LDB CQs and drain them in order to complete the map and
-	 * unmap procedures, which require zero CQ inflights and zero QID
-	 * inflights respectively.
-	 */
-	dlb2_domain_disable_ldb_cqs(hw, domain);
-
-	dlb2_domain_drain_ldb_cqs(hw, domain, false);
-
-	ret = dlb2_domain_wait_for_ldb_cqs_to_empty(hw, domain);
-	if (ret)
-		return ret;
-
-	ret = dlb2_domain_finish_unmap_qid_procedures(hw, domain);
-	if (ret)
-		return ret;
-
-	ret = dlb2_domain_finish_map_qid_procedures(hw, domain);
-	if (ret)
-		return ret;
-
-	/* Re-enable the CQs in order to drain the mapped queues. */
-	dlb2_domain_enable_ldb_cqs(hw, domain);
-
-	ret = dlb2_domain_drain_mapped_queues(hw, domain);
-	if (ret)
-		return ret;
-
-	ret = dlb2_domain_drain_unmapped_queues(hw, domain);
-	if (ret)
-		return ret;
-
-	/* Done draining LDB QEs, so disable the CQs. */
-	dlb2_domain_disable_ldb_cqs(hw, domain);
-
-	dlb2_domain_drain_dir_queues(hw, domain);
-
-	/* Done draining DIR QEs, so disable the CQs. */
-	dlb2_domain_disable_dir_cqs(hw, domain);
-
-	/* Disable PPs */
-	dlb2_domain_disable_dir_producer_ports(hw, domain);
-
-	dlb2_domain_disable_ldb_producer_ports(hw, domain);
+	id = args->queue_id;
 
-	ret = dlb2_domain_verify_reset_success(hw, domain);
-	if (ret)
-		return ret;
+	queue = dlb2_get_domain_used_dir_pq(hw, id, vdev_req, domain);
+	if (!queue) {
+		resp->status = DLB2_ST_INVALID_QID;
+		return -EINVAL;
+	}
 
-	/* Reset the QID and port state. */
-	dlb2_domain_reset_registers(hw, domain);
+	resp->id = dlb2_dir_queue_depth(hw, queue);
 
-	/* Hardware reset complete. Reset the domain's software state */
-	return dlb2_domain_reset_software_state(hw, domain);
+	return 0;
 }
 
 static void
-dlb2_log_create_ldb_queue_args(struct dlb2_hw *hw,
-			       u32 domain_id,
-			       struct dlb2_create_ldb_queue_args *args,
-			       bool vdev_req,
-			       unsigned int vdev_id)
+dlb2_log_pending_port_unmaps_args(struct dlb2_hw *hw,
+				  struct dlb2_pending_port_unmaps_args *args,
+				  bool vdev_req,
+				  unsigned int vdev_id)
 {
-	DLB2_HW_DBG(hw, "DLB2 create load-balanced queue arguments:\n");
+	DLB2_HW_DBG(hw, "DLB unmaps in progress arguments:\n");
 	if (vdev_req)
-		DLB2_HW_DBG(hw, "(Request from vdev %d)\n", vdev_id);
-	DLB2_HW_DBG(hw, "\tDomain ID:                  %d\n",
-		    domain_id);
-	DLB2_HW_DBG(hw, "\tNumber of sequence numbers: %d\n",
-		    args->num_sequence_numbers);
-	DLB2_HW_DBG(hw, "\tNumber of QID inflights:    %d\n",
-		    args->num_qid_inflights);
-	DLB2_HW_DBG(hw, "\tNumber of ATM inflights:    %d\n",
-		    args->num_atomic_inflights);
+		DLB2_HW_DBG(hw, "(Request from VF %d)\n", vdev_id);
+	DLB2_HW_DBG(hw, "\tPort ID: %d\n", args->port_id);
 }
 
-static int
-dlb2_ldb_queue_attach_to_sn_group(struct dlb2_hw *hw,
-				  struct dlb2_ldb_queue *queue,
-				  struct dlb2_create_ldb_queue_args *args)
+/**
+ * dlb2_hw_pending_port_unmaps() - returns the number of unmap operations in
+ *	progress.
+ * @hw: dlb2_hw handle for a particular device.
+ * @domain_id: domain ID.
+ * @args: number of unmaps in progress args
+ * @resp: response structure.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
+ *
+ * Return:
+ * Returns 0 upon success, < 0 otherwise. If an error occurs, resp->status is
+ * assigned a detailed error code from enum dlb2_error. If successful, resp->id
+ * contains the number of unmaps in progress.
+ *
+ * Errors:
+ * EINVAL - Invalid port ID.
+ */
+int dlb2_hw_pending_port_unmaps(struct dlb2_hw *hw,
+				u32 domain_id,
+				struct dlb2_pending_port_unmaps_args *args,
+				struct dlb2_cmd_response *resp,
+				bool vdev_req,
+				unsigned int vdev_id)
 {
-	int slot = -1;
-	int i;
-
-	queue->sn_cfg_valid = false;
+	struct dlb2_hw_domain *domain;
+	struct dlb2_ldb_port *port;
 
-	if (args->num_sequence_numbers == 0)
-		return 0;
+	dlb2_log_pending_port_unmaps_args(hw, args, vdev_req, vdev_id);
 
-	for (i = 0; i < DLB2_MAX_NUM_SEQUENCE_NUMBER_GROUPS; i++) {
-		struct dlb2_sn_group *group = &hw->rsrcs.sn_groups[i];
+	domain = dlb2_get_domain_from_id(hw, domain_id, vdev_req, vdev_id);
 
-		if (group->sequence_numbers_per_queue ==
-		    args->num_sequence_numbers &&
-		    !dlb2_sn_group_full(group)) {
-			slot = dlb2_sn_group_alloc_slot(group);
-			if (slot >= 0)
-				break;
-		}
+	if (!domain) {
+		resp->status = DLB2_ST_INVALID_DOMAIN_ID;
+		return -EINVAL;
 	}
 
-	if (slot == -1) {
-		DLB2_HW_ERR(hw,
-			    "[%s():%d] Internal error: no sequence number slots available\n",
-			    __func__, __LINE__);
-		return -EFAULT;
+	port = dlb2_get_domain_used_ldb_port(args->port_id, vdev_req, domain);
+	if (!port || !port->configured) {
+		resp->status = DLB2_ST_INVALID_PORT_ID;
+		return -EINVAL;
 	}
 
-	queue->sn_cfg_valid = true;
-	queue->sn_group = i;
-	queue->sn_slot = slot;
-	return 0;
+	resp->id = port->num_pending_removals;
+
+	return 0;
+}
+
+static u32 dlb2_ldb_queue_depth(struct dlb2_hw *hw,
+				struct dlb2_ldb_queue *queue)
+{
+	u32 aqed, ldb, atm;
+
+	aqed = DLB2_CSR_RD(hw, DLB2_LSP_QID_AQED_ACTIVE_CNT(hw->ver,
+						       queue->id.phys_id));
+	ldb = DLB2_CSR_RD(hw, DLB2_LSP_QID_LDB_ENQUEUE_CNT(hw->ver,
+						      queue->id.phys_id));
+	atm = DLB2_CSR_RD(hw,
+			  DLB2_LSP_QID_ATM_ACTIVE(hw->ver, queue->id.phys_id));
+
+	return DLB2_BITS_GET(aqed, DLB2_LSP_QID_AQED_ACTIVE_CNT_COUNT)
+	       + DLB2_BITS_GET(ldb, DLB2_LSP_QID_LDB_ENQUEUE_CNT_COUNT)
+	       + DLB2_BITS_GET(atm, DLB2_LSP_QID_ATM_ACTIVE_COUNT);
+}
+
+static bool dlb2_ldb_queue_is_empty(struct dlb2_hw *hw,
+				    struct dlb2_ldb_queue *queue)
+{
+	return dlb2_ldb_queue_depth(hw, queue) == 0;
 }
 
-static int
-dlb2_verify_create_ldb_queue_args(struct dlb2_hw *hw,
-				  u32 domain_id,
-				  struct dlb2_create_ldb_queue_args *args,
-				  struct dlb2_cmd_response *resp,
-				  bool vdev_req,
-				  unsigned int vdev_id,
-				  struct dlb2_hw_domain **out_domain,
-				  struct dlb2_ldb_queue **out_queue)
+static void dlb2_log_get_ldb_queue_depth(struct dlb2_hw *hw,
+					 u32 domain_id,
+					 u32 queue_id,
+					 bool vdev_req,
+					 unsigned int vf_id)
+{
+	DLB2_HW_DBG(hw, "DLB get load-balanced queue depth:\n");
+	if (vdev_req)
+		DLB2_HW_DBG(hw, "(Request from VF %d)\n", vf_id);
+	DLB2_HW_DBG(hw, "\tDomain ID: %d\n", domain_id);
+	DLB2_HW_DBG(hw, "\tQueue ID: %d\n", queue_id);
+}
+
+/**
+ * dlb2_hw_get_ldb_queue_depth() - returns the depth of a load-balanced queue
+ * @hw: dlb2_hw handle for a particular device.
+ * @domain_id: domain ID.
+ * @args: queue depth args
+ * @resp: response structure.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
+ *
+ * This function returns the depth of a load-balanced queue.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ *
+ * Return:
+ * Returns 0 upon success, < 0 otherwise. If an error occurs, resp->status is
+ * assigned a detailed error code from enum dlb2_error. If successful, resp->id
+ * contains the depth.
+ *
+ * Errors:
+ * EINVAL - Invalid domain ID or queue ID.
+ */
+int dlb2_hw_get_ldb_queue_depth(struct dlb2_hw *hw,
+				u32 domain_id,
+				struct dlb2_get_ldb_queue_depth_args *args,
+				struct dlb2_cmd_response *resp,
+				bool vdev_req,
+				unsigned int vdev_id)
 {
 	struct dlb2_hw_domain *domain;
 	struct dlb2_ldb_queue *queue;
-	int i;
 
-	domain = dlb2_get_domain_from_id(hw, domain_id, vdev_req, vdev_id);
+	dlb2_log_get_ldb_queue_depth(hw, domain_id, args->queue_id,
+				     vdev_req, vdev_id);
 
+	domain = dlb2_get_domain_from_id(hw, domain_id, vdev_req, vdev_id);
 	if (!domain) {
 		resp->status = DLB2_ST_INVALID_DOMAIN_ID;
 		return -EINVAL;
 	}
 
-	if (!domain->configured) {
-		resp->status = DLB2_ST_DOMAIN_NOT_CONFIGURED;
+	queue = dlb2_get_domain_ldb_queue(args->queue_id, vdev_req, domain);
+	if (!queue) {
+		resp->status = DLB2_ST_INVALID_QID;
 		return -EINVAL;
 	}
 
-	if (domain->started) {
-		resp->status = DLB2_ST_DOMAIN_STARTED;
-		return -EINVAL;
-	}
+	resp->id = dlb2_ldb_queue_depth(hw, queue);
 
-	queue = DLB2_DOM_LIST_HEAD(domain->avail_ldb_queues, typeof(*queue));
-	if (!queue) {
-		resp->status = DLB2_ST_LDB_QUEUES_UNAVAILABLE;
-		return -EINVAL;
-	}
+	return 0;
+}
 
-	if (args->num_sequence_numbers) {
-		for (i = 0; i < DLB2_MAX_NUM_SEQUENCE_NUMBER_GROUPS; i++) {
-			struct dlb2_sn_group *group = &hw->rsrcs.sn_groups[i];
+static void __dlb2_domain_reset_ldb_port_registers(struct dlb2_hw *hw,
+						   struct dlb2_ldb_port *port)
+{
+	DLB2_CSR_WR(hw,
+		    DLB2_SYS_LDB_PP2VAS(port->id.phys_id),
+		    DLB2_SYS_LDB_PP2VAS_RST);
 
-			if (group->sequence_numbers_per_queue ==
-			    args->num_sequence_numbers &&
-			    !dlb2_sn_group_full(group))
-				break;
-		}
+	DLB2_CSR_WR(hw,
+		    DLB2_CHP_LDB_CQ2VAS(hw->ver, port->id.phys_id),
+		    DLB2_CHP_LDB_CQ2VAS_RST);
 
-		if (i == DLB2_MAX_NUM_SEQUENCE_NUMBER_GROUPS) {
-			resp->status = DLB2_ST_SEQUENCE_NUMBERS_UNAVAILABLE;
-			return -EINVAL;
-		}
-	}
+	DLB2_CSR_WR(hw,
+		    DLB2_SYS_LDB_PP2VDEV(port->id.phys_id),
+		    DLB2_SYS_LDB_PP2VDEV_RST);
 
-	if (args->num_qid_inflights < 1 || args->num_qid_inflights > 2048) {
-		resp->status = DLB2_ST_INVALID_QID_INFLIGHT_ALLOCATION;
-		return -EINVAL;
-	}
+	if (port->id.vdev_owned) {
+		unsigned int offs;
+		u32 virt_id;
 
-	/* Inflights must be <= number of sequence numbers if ordered */
-	if (args->num_sequence_numbers != 0 &&
-	    args->num_qid_inflights > args->num_sequence_numbers) {
-		resp->status = DLB2_ST_INVALID_QID_INFLIGHT_ALLOCATION;
-		return -EINVAL;
-	}
+		/*
+		 * DLB uses producer port address bits 17:12 to determine the
+		 * producer port ID. In Scalable IOV mode, PP accesses come
+		 * through the PF MMIO window for the physical producer port,
+		 * so for translation purposes the virtual and physical port
+		 * IDs are equal.
+		 */
+		if (hw->virt_mode == DLB2_VIRT_SRIOV)
+			virt_id = port->id.virt_id;
+		else
+			virt_id = port->id.phys_id;
 
-	if (domain->num_avail_aqed_entries < args->num_atomic_inflights) {
-		resp->status = DLB2_ST_ATOMIC_INFLIGHTS_UNAVAILABLE;
-		return -EINVAL;
+		offs = port->id.vdev_id * DLB2_MAX_NUM_LDB_PORTS + virt_id;
+
+		DLB2_CSR_WR(hw,
+			    DLB2_SYS_VF_LDB_VPP2PP(offs),
+			    DLB2_SYS_VF_LDB_VPP2PP_RST);
+
+		DLB2_CSR_WR(hw,
+			    DLB2_SYS_VF_LDB_VPP_V(offs),
+			    DLB2_SYS_VF_LDB_VPP_V_RST);
 	}
 
-	if (args->num_atomic_inflights &&
-	    args->lock_id_comp_level != 0 &&
-	    args->lock_id_comp_level != 64 &&
-	    args->lock_id_comp_level != 128 &&
-	    args->lock_id_comp_level != 256 &&
-	    args->lock_id_comp_level != 512 &&
-	    args->lock_id_comp_level != 1024 &&
-	    args->lock_id_comp_level != 2048 &&
-	    args->lock_id_comp_level != 4096 &&
-	    args->lock_id_comp_level != 65536) {
-		resp->status = DLB2_ST_INVALID_LOCK_ID_COMP_LEVEL;
-		return -EINVAL;
+	DLB2_CSR_WR(hw,
+		    DLB2_SYS_LDB_PP_V(port->id.phys_id),
+		    DLB2_SYS_LDB_PP_V_RST);
+
+	DLB2_CSR_WR(hw,
+		    DLB2_LSP_CQ_LDB_DSBL(hw->ver, port->id.phys_id),
+		    DLB2_LSP_CQ_LDB_DSBL_RST);
+
+	DLB2_CSR_WR(hw,
+		    DLB2_CHP_LDB_CQ_DEPTH(hw->ver, port->id.phys_id),
+		    DLB2_CHP_LDB_CQ_DEPTH_RST);
+
+	if (hw->ver != DLB2_HW_V2) {
+		DLB2_CSR_WR(hw,
+			    DLB2_LSP_CFG_CQ_LDB_WU_LIMIT(port->id.phys_id),
+			    DLB2_LSP_CFG_CQ_LDB_WU_LIMIT_RST);
+		DLB2_CSR_WR(hw,
+			    DLB2_LSP_CQ_LDB_INFL_THRESH(port->id.phys_id),
+			    DLB2_LSP_CQ_LDB_INFL_THRESH_RST);
 	}
 
-	*out_domain = domain;
-	*out_queue = queue;
+	DLB2_CSR_WR(hw,
+		    DLB2_LSP_CQ_LDB_INFL_LIM(hw->ver, port->id.phys_id),
+		    DLB2_LSP_CQ_LDB_INFL_LIM_RST);
 
-	return 0;
-}
+	DLB2_CSR_WR(hw,
+		    DLB2_CHP_HIST_LIST_LIM(hw->ver, port->id.phys_id),
+		    DLB2_CHP_HIST_LIST_LIM_RST);
 
-static int
-dlb2_ldb_queue_attach_resources(struct dlb2_hw *hw,
-				struct dlb2_hw_domain *domain,
-				struct dlb2_ldb_queue *queue,
-				struct dlb2_create_ldb_queue_args *args)
-{
-	int ret;
-	ret = dlb2_ldb_queue_attach_to_sn_group(hw, queue, args);
-	if (ret)
-		return ret;
+	DLB2_CSR_WR(hw,
+		    DLB2_CHP_HIST_LIST_BASE(hw->ver, port->id.phys_id),
+		    DLB2_CHP_HIST_LIST_BASE_RST);
 
-	/* Attach QID inflights */
-	queue->num_qid_inflights = args->num_qid_inflights;
+	DLB2_CSR_WR(hw,
+		    DLB2_CHP_HIST_LIST_POP_PTR(hw->ver, port->id.phys_id),
+		    DLB2_CHP_HIST_LIST_POP_PTR_RST);
 
-	/* Attach atomic inflights */
-	queue->aqed_limit = args->num_atomic_inflights;
+	DLB2_CSR_WR(hw,
+		    DLB2_CHP_HIST_LIST_PUSH_PTR(hw->ver, port->id.phys_id),
+		    DLB2_CHP_HIST_LIST_PUSH_PTR_RST);
 
-	domain->num_avail_aqed_entries -= args->num_atomic_inflights;
-	domain->num_used_aqed_entries += args->num_atomic_inflights;
+	DLB2_CSR_WR(hw,
+		    DLB2_CHP_LDB_CQ_INT_DEPTH_THRSH(hw->ver, port->id.phys_id),
+		    DLB2_CHP_LDB_CQ_INT_DEPTH_THRSH_RST);
 
-	return 0;
-}
+	DLB2_CSR_WR(hw,
+		    DLB2_CHP_LDB_CQ_TMR_THRSH(hw->ver, port->id.phys_id),
+		    DLB2_CHP_LDB_CQ_TMR_THRSH_RST);
 
-static void dlb2_configure_ldb_queue(struct dlb2_hw *hw,
-				     struct dlb2_hw_domain *domain,
-				     struct dlb2_ldb_queue *queue,
-				     struct dlb2_create_ldb_queue_args *args,
-				     bool vdev_req,
-				     unsigned int vdev_id)
-{
-	struct dlb2_sn_group *sn_group;
-	unsigned int offs;
-	u32 reg = 0;
-	u32 alimit;
+	DLB2_CSR_WR(hw,
+		    DLB2_CHP_LDB_CQ_INT_ENB(hw->ver, port->id.phys_id),
+		    DLB2_CHP_LDB_CQ_INT_ENB_RST);
+
+	DLB2_CSR_WR(hw,
+		    DLB2_SYS_LDB_CQ_ISR(port->id.phys_id),
+		    DLB2_SYS_LDB_CQ_ISR_RST);
+
+	DLB2_CSR_WR(hw,
+		    DLB2_LSP_CQ_LDB_TKN_DEPTH_SEL(hw->ver, port->id.phys_id),
+		    DLB2_LSP_CQ_LDB_TKN_DEPTH_SEL_RST);
+
+	DLB2_CSR_WR(hw,
+		    DLB2_CHP_LDB_CQ_TKN_DEPTH_SEL(hw->ver, port->id.phys_id),
+		    DLB2_CHP_LDB_CQ_TKN_DEPTH_SEL_RST);
+
+	DLB2_CSR_WR(hw,
+		    DLB2_CHP_LDB_CQ_WPTR(hw->ver, port->id.phys_id),
+		    DLB2_CHP_LDB_CQ_WPTR_RST);
+
+	DLB2_CSR_WR(hw,
+		    DLB2_LSP_CQ_LDB_TKN_CNT(hw->ver, port->id.phys_id),
+		    DLB2_LSP_CQ_LDB_TKN_CNT_RST);
+
+	DLB2_CSR_WR(hw,
+		    DLB2_SYS_LDB_CQ_ADDR_L(port->id.phys_id),
+		    DLB2_SYS_LDB_CQ_ADDR_L_RST);
+
+	DLB2_CSR_WR(hw,
+		    DLB2_SYS_LDB_CQ_ADDR_U(port->id.phys_id),
+		    DLB2_SYS_LDB_CQ_ADDR_U_RST);
 
-	/* QID write permissions are turned on when the domain is started */
-	offs = domain->id.phys_id * DLB2_MAX_NUM_LDB_QUEUES + queue->id.phys_id;
+	if (hw->ver == DLB2_HW_V2)
+		DLB2_CSR_WR(hw,
+			    DLB2_SYS_LDB_CQ_AT(port->id.phys_id),
+			    DLB2_SYS_LDB_CQ_AT_RST);
 
-	DLB2_CSR_WR(hw, DLB2_SYS_LDB_VASQID_V(offs), reg);
+	DLB2_CSR_WR(hw,
+		    DLB2_SYS_LDB_CQ_PASID(hw->ver, port->id.phys_id),
+		    DLB2_SYS_LDB_CQ_PASID_RST);
 
-	/*
-	 * Unordered QIDs get 4K inflights, ordered get as many as the number
-	 * of sequence numbers.
-	 */
-	DLB2_BITS_SET(reg, args->num_qid_inflights,
-		      DLB2_LSP_QID_LDB_INFL_LIM_LIMIT);
-	DLB2_CSR_WR(hw, DLB2_LSP_QID_LDB_INFL_LIM(hw->ver,
-						  queue->id.phys_id), reg);
+	DLB2_CSR_WR(hw,
+		    DLB2_SYS_LDB_CQ2VF_PF_RO(port->id.phys_id),
+		    DLB2_SYS_LDB_CQ2VF_PF_RO_RST);
 
-	alimit = queue->aqed_limit;
+	DLB2_CSR_WR(hw,
+		    DLB2_LSP_CQ_LDB_TOT_SCH_CNTL(hw->ver, port->id.phys_id),
+		    DLB2_LSP_CQ_LDB_TOT_SCH_CNTL_RST);
 
-	if (alimit > DLB2_MAX_NUM_AQED_ENTRIES)
-		alimit = DLB2_MAX_NUM_AQED_ENTRIES;
+	DLB2_CSR_WR(hw,
+		    DLB2_LSP_CQ_LDB_TOT_SCH_CNTH(hw->ver, port->id.phys_id),
+		    DLB2_LSP_CQ_LDB_TOT_SCH_CNTH_RST);
 
-	reg = 0;
-	DLB2_BITS_SET(reg, alimit, DLB2_LSP_QID_AQED_ACTIVE_LIM_LIMIT);
 	DLB2_CSR_WR(hw,
-		    DLB2_LSP_QID_AQED_ACTIVE_LIM(hw->ver,
-						 queue->id.phys_id), reg);
+		    DLB2_LSP_CQ2QID0(hw->ver, port->id.phys_id),
+		    DLB2_LSP_CQ2QID0_RST);
 
-	reg = 0;
-	switch (args->lock_id_comp_level) {
-	case 64:
-		DLB2_BITS_SET(reg, 1, DLB2_AQED_QID_HID_WIDTH_COMPRESS_CODE);
-		break;
-	case 128:
-		DLB2_BITS_SET(reg, 2, DLB2_AQED_QID_HID_WIDTH_COMPRESS_CODE);
-		break;
-	case 256:
-		DLB2_BITS_SET(reg, 3, DLB2_AQED_QID_HID_WIDTH_COMPRESS_CODE);
-		break;
-	case 512:
-		DLB2_BITS_SET(reg, 4, DLB2_AQED_QID_HID_WIDTH_COMPRESS_CODE);
-		break;
-	case 1024:
-		DLB2_BITS_SET(reg, 5, DLB2_AQED_QID_HID_WIDTH_COMPRESS_CODE);
-		break;
-	case 2048:
-		DLB2_BITS_SET(reg, 6, DLB2_AQED_QID_HID_WIDTH_COMPRESS_CODE);
-		break;
-	case 4096:
-		DLB2_BITS_SET(reg, 7, DLB2_AQED_QID_HID_WIDTH_COMPRESS_CODE);
-		break;
-	default:
-		/* No compression by default */
-		break;
-	}
+	DLB2_CSR_WR(hw,
+		    DLB2_LSP_CQ2QID1(hw->ver, port->id.phys_id),
+		    DLB2_LSP_CQ2QID1_RST);
 
-	DLB2_CSR_WR(hw, DLB2_AQED_QID_HID_WIDTH(queue->id.phys_id), reg);
+	DLB2_CSR_WR(hw,
+		    DLB2_LSP_CQ2PRIOV(hw->ver, port->id.phys_id),
+		    DLB2_LSP_CQ2PRIOV_RST);
+}
 
-	reg = 0;
-	/* Don't timestamp QEs that pass through this queue */
-	DLB2_CSR_WR(hw, DLB2_SYS_LDB_QID_ITS(queue->id.phys_id), reg);
+static void dlb2_domain_reset_ldb_port_registers(struct dlb2_hw *hw,
+						 struct dlb2_hw_domain *domain)
+{
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_ldb_port *port;
+	int i;
+
+	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
+		DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i], port, iter)
+			__dlb2_domain_reset_ldb_port_registers(hw, port);
+	}
+}
+
+static void
+__dlb2_domain_reset_dir_port_registers(struct dlb2_hw *hw,
+				       struct dlb2_dir_pq_pair *port)
+{
+	u32 reg = 0;
 
-	DLB2_BITS_SET(reg, args->depth_threshold,
-		      DLB2_LSP_QID_ATM_DEPTH_THRSH_THRESH);
 	DLB2_CSR_WR(hw,
-		    DLB2_LSP_QID_ATM_DEPTH_THRSH(hw->ver,
-						 queue->id.phys_id), reg);
+		    DLB2_CHP_DIR_CQ2VAS(hw->ver, port->id.phys_id),
+		    DLB2_CHP_DIR_CQ2VAS_RST);
 
-	reg = 0;
-	DLB2_BITS_SET(reg, args->depth_threshold,
-		      DLB2_LSP_QID_NALDB_DEPTH_THRSH_THRESH);
 	DLB2_CSR_WR(hw,
-		    DLB2_LSP_QID_NALDB_DEPTH_THRSH(hw->ver, queue->id.phys_id),
-		    reg);
+		    DLB2_LSP_CQ_DIR_DSBL(hw->ver, port->id.phys_id),
+		    DLB2_LSP_CQ_DIR_DSBL_RST);
 
-	/*
-	 * This register limits the number of inflight flows a queue can have
-	 * at one time.  It has an upper bound of 2048, but can be
-	 * over-subscribed. 512 is chosen so that a single queue does not use
-	 * the entire atomic storage, but can use a substantial portion if
-	 * needed.
-	 */
-	reg = 0;
-	DLB2_BITS_SET(reg, 512, DLB2_AQED_QID_FID_LIM_QID_FID_LIMIT);
-	DLB2_CSR_WR(hw, DLB2_AQED_QID_FID_LIM(queue->id.phys_id), reg);
+	DLB2_BIT_SET(reg, DLB2_SYS_WB_DIR_CQ_STATE_CQ_OPT_CLR);
 
-	/* Configure SNs */
-	reg = 0;
-	sn_group = &hw->rsrcs.sn_groups[queue->sn_group];
-	DLB2_BITS_SET(reg, sn_group->mode, DLB2_CHP_ORD_QID_SN_MAP_MODE);
-	DLB2_BITS_SET(reg, queue->sn_slot, DLB2_CHP_ORD_QID_SN_MAP_SLOT);
-	DLB2_BITS_SET(reg, sn_group->id, DLB2_CHP_ORD_QID_SN_MAP_GRP);
+	if (hw->ver == DLB2_HW_V2)
+		DLB2_CSR_WR(hw, DLB2_SYS_DIR_CQ_OPT_CLR, port->id.phys_id);
+	else
+		DLB2_CSR_WR(hw,
+			    DLB2_SYS_WB_DIR_CQ_STATE(port->id.phys_id), reg);
 
 	DLB2_CSR_WR(hw,
-		    DLB2_CHP_ORD_QID_SN_MAP(hw->ver, queue->id.phys_id), reg);
-
-	reg = 0;
-	DLB2_BITS_SET(reg, (args->num_sequence_numbers != 0),
-		 DLB2_SYS_LDB_QID_CFG_V_SN_CFG_V);
-	DLB2_BITS_SET(reg, (args->num_atomic_inflights != 0),
-		 DLB2_SYS_LDB_QID_CFG_V_FID_CFG_V);
+		    DLB2_CHP_DIR_CQ_DEPTH(hw->ver, port->id.phys_id),
+		    DLB2_CHP_DIR_CQ_DEPTH_RST);
 
-	DLB2_CSR_WR(hw, DLB2_SYS_LDB_QID_CFG_V(queue->id.phys_id), reg);
+	DLB2_CSR_WR(hw,
+		    DLB2_CHP_DIR_CQ_INT_DEPTH_THRSH(hw->ver, port->id.phys_id),
+		    DLB2_CHP_DIR_CQ_INT_DEPTH_THRSH_RST);
 
-	if (vdev_req) {
-		offs = vdev_id * DLB2_MAX_NUM_LDB_QUEUES + queue->id.virt_id;
+	DLB2_CSR_WR(hw,
+		    DLB2_CHP_DIR_CQ_TMR_THRSH(hw->ver, port->id.phys_id),
+		    DLB2_CHP_DIR_CQ_TMR_THRSH_RST);
 
-		reg = 0;
-		DLB2_BIT_SET(reg, DLB2_SYS_VF_LDB_VQID_V_VQID_V);
-		DLB2_CSR_WR(hw, DLB2_SYS_VF_LDB_VQID_V(offs), reg);
+	DLB2_CSR_WR(hw,
+		    DLB2_CHP_DIR_CQ_INT_ENB(hw->ver, port->id.phys_id),
+		    DLB2_CHP_DIR_CQ_INT_ENB_RST);
 
-		reg = 0;
-		DLB2_BITS_SET(reg, queue->id.phys_id,
-			      DLB2_SYS_VF_LDB_VQID2QID_QID);
-		DLB2_CSR_WR(hw, DLB2_SYS_VF_LDB_VQID2QID(offs), reg);
+	DLB2_CSR_WR(hw,
+		    DLB2_SYS_DIR_CQ_ISR(port->id.phys_id),
+		    DLB2_SYS_DIR_CQ_ISR_RST);
 
-		reg = 0;
-		DLB2_BITS_SET(reg, queue->id.virt_id,
-			      DLB2_SYS_LDB_QID2VQID_VQID);
-		DLB2_CSR_WR(hw, DLB2_SYS_LDB_QID2VQID(queue->id.phys_id), reg);
-	}
+	DLB2_CSR_WR(hw,
+		    DLB2_LSP_CQ_DIR_TKN_DEPTH_SEL_DSI(hw->ver,
+						      port->id.phys_id),
+		    DLB2_LSP_CQ_DIR_TKN_DEPTH_SEL_DSI_RST);
 
-	reg = 0;
-	DLB2_BIT_SET(reg, DLB2_SYS_LDB_QID_V_QID_V);
-	DLB2_CSR_WR(hw, DLB2_SYS_LDB_QID_V(queue->id.phys_id), reg);
-}
+	DLB2_CSR_WR(hw,
+		    DLB2_CHP_DIR_CQ_TKN_DEPTH_SEL(hw->ver, port->id.phys_id),
+		    DLB2_CHP_DIR_CQ_TKN_DEPTH_SEL_RST);
 
-/**
- * dlb2_hw_create_ldb_queue() - create a load-balanced queue
- * @hw: dlb2_hw handle for a particular device.
- * @domain_id: domain ID.
- * @args: queue creation arguments.
- * @resp: response structure.
- * @vdev_req: indicates whether this request came from a vdev.
- * @vdev_id: If vdev_req is true, this contains the vdev's ID.
- *
- * This function creates a load-balanced queue.
- *
- * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
- * device.
- *
- * Return:
- * Returns 0 upon success, < 0 otherwise. If an error occurs, resp->status is
- * assigned a detailed error code from enum dlb2_error. If successful, resp->id
- * contains the queue ID.
- *
- * resp->id contains a virtual ID if vdev_req is true.
- *
- * Errors:
- * EINVAL - A requested resource is unavailable, the domain is not configured,
- *	    the domain has already been started, or the requested queue name is
- *	    already in use.
- * EFAULT - Internal error (resp->status not set).
- */
-int dlb2_hw_create_ldb_queue(struct dlb2_hw *hw,
-			     u32 domain_id,
-			     struct dlb2_create_ldb_queue_args *args,
-			     struct dlb2_cmd_response *resp,
-			     bool vdev_req,
-			     unsigned int vdev_id)
-{
-	struct dlb2_hw_domain *domain;
-	struct dlb2_ldb_queue *queue;
-	int ret;
+	DLB2_CSR_WR(hw,
+		    DLB2_CHP_DIR_CQ_WPTR(hw->ver, port->id.phys_id),
+		    DLB2_CHP_DIR_CQ_WPTR_RST);
 
-	dlb2_log_create_ldb_queue_args(hw, domain_id, args, vdev_req, vdev_id);
+	DLB2_CSR_WR(hw,
+		    DLB2_LSP_CQ_DIR_TKN_CNT(hw->ver, port->id.phys_id),
+		    DLB2_LSP_CQ_DIR_TKN_CNT_RST);
 
-	/*
-	 * Verify that hardware resources are available before attempting to
-	 * satisfy the request. This simplifies the error unwinding code.
-	 */
-	ret = dlb2_verify_create_ldb_queue_args(hw,
-						domain_id,
-						args,
-						resp,
-						vdev_req,
-						vdev_id,
-						&domain,
-						&queue);
-	if (ret)
-		return ret;
+	DLB2_CSR_WR(hw,
+		    DLB2_SYS_DIR_CQ_ADDR_L(port->id.phys_id),
+		    DLB2_SYS_DIR_CQ_ADDR_L_RST);
 
-	ret = dlb2_ldb_queue_attach_resources(hw, domain, queue, args);
+	DLB2_CSR_WR(hw,
+		    DLB2_SYS_DIR_CQ_ADDR_U(port->id.phys_id),
+		    DLB2_SYS_DIR_CQ_ADDR_U_RST);
 
-	if (ret) {
-		DLB2_HW_ERR(hw,
-			    "[%s():%d] Internal error: failed to attach the ldb queue resources\n",
-			    __func__, __LINE__);
-		return ret;
-	}
+	DLB2_CSR_WR(hw,
+		    DLB2_SYS_DIR_CQ_AT(port->id.phys_id),
+		    DLB2_SYS_DIR_CQ_AT_RST);
 
-	dlb2_configure_ldb_queue(hw, domain, queue, args, vdev_req, vdev_id);
+	if (hw->ver == DLB2_HW_V2)
+		DLB2_CSR_WR(hw,
+			    DLB2_SYS_DIR_CQ_AT(port->id.phys_id),
+			    DLB2_SYS_DIR_CQ_AT_RST);
 
-	queue->num_mappings = 0;
+	DLB2_CSR_WR(hw,
+		    DLB2_SYS_DIR_CQ_PASID(hw->ver, port->id.phys_id),
+		    DLB2_SYS_DIR_CQ_PASID_RST);
 
-	queue->configured = true;
+	DLB2_CSR_WR(hw,
+		    DLB2_SYS_DIR_CQ_FMT(port->id.phys_id),
+		    DLB2_SYS_DIR_CQ_FMT_RST);
 
-	/*
-	 * Configuration succeeded, so move the resource from the 'avail' to
-	 * the 'used' list.
-	 */
-	dlb2_list_del(&domain->avail_ldb_queues, &queue->domain_list);
+	DLB2_CSR_WR(hw,
+		    DLB2_SYS_DIR_CQ2VF_PF_RO(port->id.phys_id),
+		    DLB2_SYS_DIR_CQ2VF_PF_RO_RST);
 
-	dlb2_list_add(&domain->used_ldb_queues, &queue->domain_list);
+	DLB2_CSR_WR(hw,
+		    DLB2_LSP_CQ_DIR_TOT_SCH_CNTL(hw->ver, port->id.phys_id),
+		    DLB2_LSP_CQ_DIR_TOT_SCH_CNTL_RST);
 
-	resp->status = 0;
-	resp->id = (vdev_req) ? queue->id.virt_id : queue->id.phys_id;
+	DLB2_CSR_WR(hw,
+		    DLB2_LSP_CQ_DIR_TOT_SCH_CNTH(hw->ver, port->id.phys_id),
+		    DLB2_LSP_CQ_DIR_TOT_SCH_CNTH_RST);
 
-	return 0;
-}
+	DLB2_CSR_WR(hw,
+		    DLB2_SYS_DIR_PP2VAS(port->id.phys_id),
+		    DLB2_SYS_DIR_PP2VAS_RST);
 
-static void dlb2_ldb_port_configure_pp(struct dlb2_hw *hw,
-				       struct dlb2_hw_domain *domain,
-				       struct dlb2_ldb_port *port,
-				       bool vdev_req,
-				       unsigned int vdev_id)
-{
-	u32 reg = 0;
+	DLB2_CSR_WR(hw,
+		    DLB2_CHP_DIR_CQ2VAS(hw->ver, port->id.phys_id),
+		    DLB2_CHP_DIR_CQ2VAS_RST);
 
-	DLB2_BITS_SET(reg, domain->id.phys_id, DLB2_SYS_LDB_PP2VAS_VAS);
-	DLB2_CSR_WR(hw, DLB2_SYS_LDB_PP2VAS(port->id.phys_id), reg);
+	DLB2_CSR_WR(hw,
+		    DLB2_SYS_DIR_PP2VDEV(port->id.phys_id),
+		    DLB2_SYS_DIR_PP2VDEV_RST);
 
-	if (vdev_req) {
+	if (port->id.vdev_owned) {
 		unsigned int offs;
 		u32 virt_id;
 
@@ -4262,2270 +8041,2542 @@ static void dlb2_ldb_port_configure_pp(struct dlb2_hw *hw,
 		else
 			virt_id = port->id.phys_id;
 
-		reg = 0;
-		DLB2_BITS_SET(reg, port->id.phys_id, DLB2_SYS_VF_LDB_VPP2PP_PP);
-		offs = vdev_id * DLB2_MAX_NUM_LDB_PORTS + virt_id;
-		DLB2_CSR_WR(hw, DLB2_SYS_VF_LDB_VPP2PP(offs), reg);
+		offs = port->id.vdev_id * DLB2_MAX_NUM_DIR_PORTS(hw->ver) +
+			virt_id;
 
-		reg = 0;
-		DLB2_BITS_SET(reg, vdev_id, DLB2_SYS_LDB_PP2VDEV_VDEV);
-		DLB2_CSR_WR(hw, DLB2_SYS_LDB_PP2VDEV(port->id.phys_id), reg);
+		DLB2_CSR_WR(hw,
+			    DLB2_SYS_VF_DIR_VPP2PP(offs),
+			    DLB2_SYS_VF_DIR_VPP2PP_RST);
 
-		reg = 0;
-		DLB2_BIT_SET(reg, DLB2_SYS_VF_LDB_VPP_V_VPP_V);
-		DLB2_CSR_WR(hw, DLB2_SYS_VF_LDB_VPP_V(offs), reg);
+		DLB2_CSR_WR(hw,
+			    DLB2_SYS_VF_DIR_VPP_V(offs),
+			    DLB2_SYS_VF_DIR_VPP_V_RST);
 	}
 
-	reg = 0;
-	DLB2_BIT_SET(reg, DLB2_SYS_LDB_PP_V_PP_V);
-	DLB2_CSR_WR(hw, DLB2_SYS_LDB_PP_V(port->id.phys_id), reg);
+	DLB2_CSR_WR(hw,
+		    DLB2_SYS_DIR_PP_V(port->id.phys_id),
+		    DLB2_SYS_DIR_PP_V_RST);
 }
 
-static int dlb2_ldb_port_configure_cq(struct dlb2_hw *hw,
-				      struct dlb2_hw_domain *domain,
-				      struct dlb2_ldb_port *port,
-				      uintptr_t cq_dma_base,
-				      struct dlb2_create_ldb_port_args *args,
-				      bool vdev_req,
-				      unsigned int vdev_id)
+static void dlb2_domain_reset_dir_port_registers(struct dlb2_hw *hw,
+						 struct dlb2_hw_domain *domain)
 {
-	u32 hl_base = 0;
-	u32 reg = 0;
-	u32 ds = 0;
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_dir_pq_pair *port;
 
-	/* The CQ address is 64B-aligned, and the DLB only wants bits [63:6] */
-	DLB2_BITS_SET(reg, cq_dma_base >> 6, DLB2_SYS_LDB_CQ_ADDR_L_ADDR_L);
-	DLB2_CSR_WR(hw, DLB2_SYS_LDB_CQ_ADDR_L(port->id.phys_id), reg);
+	DLB2_DOM_LIST_FOR(domain->used_dir_pq_pairs, port, iter)
+		__dlb2_domain_reset_dir_port_registers(hw, port);
+}
 
-	reg = cq_dma_base >> 32;
-	DLB2_CSR_WR(hw, DLB2_SYS_LDB_CQ_ADDR_U(port->id.phys_id), reg);
+static void dlb2_domain_reset_ldb_queue_registers(struct dlb2_hw *hw,
+						  struct dlb2_hw_domain *domain)
+{
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_ldb_queue *queue;
 
-	/*
-	 * 'ro' == relaxed ordering. This setting allows DLB2 to write
-	 * cache lines out-of-order (but QEs within a cache line are always
-	 * updated in-order).
-	 */
-	reg = 0;
-	DLB2_BITS_SET(reg, vdev_id, DLB2_SYS_LDB_CQ2VF_PF_RO_VF);
-	DLB2_BITS_SET(reg,
-		 !vdev_req && (hw->virt_mode != DLB2_VIRT_SIOV),
-		 DLB2_SYS_LDB_CQ2VF_PF_RO_IS_PF);
-	DLB2_BIT_SET(reg, DLB2_SYS_LDB_CQ2VF_PF_RO_RO);
+	DLB2_DOM_LIST_FOR(domain->used_ldb_queues, queue, iter) {
+		unsigned int queue_id = queue->id.phys_id;
+		int i;
 
-	DLB2_CSR_WR(hw, DLB2_SYS_LDB_CQ2VF_PF_RO(port->id.phys_id), reg);
+		DLB2_CSR_WR(hw,
+			    DLB2_LSP_QID_NALDB_TOT_ENQ_CNTL(hw->ver, queue_id),
+			    DLB2_LSP_QID_NALDB_TOT_ENQ_CNTL_RST);
 
-	port->cq_depth = args->cq_depth;
+		DLB2_CSR_WR(hw,
+			    DLB2_LSP_QID_NALDB_TOT_ENQ_CNTH(hw->ver, queue_id),
+			    DLB2_LSP_QID_NALDB_TOT_ENQ_CNTH_RST);
 
-	if (args->cq_depth <= 8) {
-		ds = 1;
-	} else if (args->cq_depth == 16) {
-		ds = 2;
-	} else if (args->cq_depth == 32) {
-		ds = 3;
-	} else if (args->cq_depth == 64) {
-		ds = 4;
-	} else if (args->cq_depth == 128) {
-		ds = 5;
-	} else if (args->cq_depth == 256) {
-		ds = 6;
-	} else if (args->cq_depth == 512) {
-		ds = 7;
-	} else if (args->cq_depth == 1024) {
-		ds = 8;
-	} else {
-		DLB2_HW_ERR(hw,
-			    "[%s():%d] Internal error: invalid CQ depth\n",
-			    __func__, __LINE__);
-		return -EFAULT;
-	}
+		DLB2_CSR_WR(hw,
+			    DLB2_LSP_QID_ATM_TOT_ENQ_CNTL(hw->ver, queue_id),
+			    DLB2_LSP_QID_ATM_TOT_ENQ_CNTL_RST);
 
-	reg = 0;
-	DLB2_BITS_SET(reg, ds,
-		      DLB2_CHP_LDB_CQ_TKN_DEPTH_SEL_TOKEN_DEPTH_SELECT);
-	DLB2_CSR_WR(hw,
-		    DLB2_CHP_LDB_CQ_TKN_DEPTH_SEL(hw->ver, port->id.phys_id),
-		    reg);
+		DLB2_CSR_WR(hw,
+			    DLB2_LSP_QID_ATM_TOT_ENQ_CNTH(hw->ver, queue_id),
+			    DLB2_LSP_QID_ATM_TOT_ENQ_CNTH_RST);
 
-	/*
-	 * To support CQs with depth less than 8, program the token count
-	 * register with a non-zero initial value. Operations such as domain
-	 * reset must take this initial value into account when quiescing the
-	 * CQ.
-	 */
-	port->init_tkn_cnt = 0;
+		DLB2_CSR_WR(hw,
+			    DLB2_LSP_QID_NALDB_MAX_DEPTH(hw->ver, queue_id),
+			    DLB2_LSP_QID_NALDB_MAX_DEPTH_RST);
 
-	if (args->cq_depth < 8) {
-		reg = 0;
-		port->init_tkn_cnt = 8 - args->cq_depth;
+		DLB2_CSR_WR(hw,
+			    DLB2_LSP_QID_LDB_INFL_LIM(hw->ver, queue_id),
+			    DLB2_LSP_QID_LDB_INFL_LIM_RST);
 
-		DLB2_BITS_SET(reg,
-			      port->init_tkn_cnt,
-			      DLB2_LSP_CQ_LDB_TKN_CNT_TOKEN_COUNT);
 		DLB2_CSR_WR(hw,
-			    DLB2_LSP_CQ_LDB_TKN_CNT(hw->ver, port->id.phys_id),
-			    reg);
-	} else {
+			    DLB2_LSP_QID_AQED_ACTIVE_LIM(hw->ver, queue_id),
+			    DLB2_LSP_QID_AQED_ACTIVE_LIM_RST);
+
 		DLB2_CSR_WR(hw,
-			    DLB2_LSP_CQ_LDB_TKN_CNT(hw->ver, port->id.phys_id),
-			    DLB2_LSP_CQ_LDB_TKN_CNT_RST);
-	}
+			    DLB2_LSP_QID_ATM_DEPTH_THRSH(hw->ver, queue_id),
+			    DLB2_LSP_QID_ATM_DEPTH_THRSH_RST);
 
-	reg = 0;
-	DLB2_BITS_SET(reg, ds,
-		      DLB2_LSP_CQ_LDB_TKN_DEPTH_SEL_TOKEN_DEPTH_SELECT_V2);
-	DLB2_CSR_WR(hw,
-		    DLB2_LSP_CQ_LDB_TKN_DEPTH_SEL(hw->ver, port->id.phys_id),
-		    reg);
+		DLB2_CSR_WR(hw,
+			    DLB2_LSP_QID_NALDB_DEPTH_THRSH(hw->ver, queue_id),
+			    DLB2_LSP_QID_NALDB_DEPTH_THRSH_RST);
 
-	/* Reset the CQ write pointer */
-	DLB2_CSR_WR(hw,
-		    DLB2_CHP_LDB_CQ_WPTR(hw->ver, port->id.phys_id),
-		    DLB2_CHP_LDB_CQ_WPTR_RST);
+		DLB2_CSR_WR(hw,
+			    DLB2_SYS_LDB_QID_ITS(queue_id),
+			    DLB2_SYS_LDB_QID_ITS_RST);
 
-	reg = 0;
-	DLB2_BITS_SET(reg,
-		      port->hist_list_entry_limit - 1,
-		      DLB2_CHP_HIST_LIST_LIM_LIMIT);
-	DLB2_CSR_WR(hw, DLB2_CHP_HIST_LIST_LIM(hw->ver, port->id.phys_id), reg);
+		DLB2_CSR_WR(hw,
+			    DLB2_CHP_ORD_QID_SN(hw->ver, queue_id),
+			    DLB2_CHP_ORD_QID_SN_RST);
 
-	DLB2_BITS_SET(hl_base, port->hist_list_entry_base,
-		      DLB2_CHP_HIST_LIST_BASE_BASE);
-	DLB2_CSR_WR(hw,
-		    DLB2_CHP_HIST_LIST_BASE(hw->ver, port->id.phys_id),
-		    hl_base);
+		DLB2_CSR_WR(hw,
+			    DLB2_CHP_ORD_QID_SN_MAP(hw->ver, queue_id),
+			    DLB2_CHP_ORD_QID_SN_MAP_RST);
 
-	/*
-	 * The inflight limit sets a cap on the number of QEs for which this CQ
-	 * can owe completions at one time.
-	 */
-	reg = 0;
-	DLB2_BITS_SET(reg, args->cq_history_list_size,
-		      DLB2_LSP_CQ_LDB_INFL_LIM_LIMIT);
-	DLB2_CSR_WR(hw, DLB2_LSP_CQ_LDB_INFL_LIM(hw->ver, port->id.phys_id),
-		    reg);
+		DLB2_CSR_WR(hw,
+			    DLB2_SYS_LDB_QID_V(queue_id),
+			    DLB2_SYS_LDB_QID_V_RST);
 
-	reg = 0;
-	DLB2_BITS_SET(reg, DLB2_BITS_GET(hl_base, DLB2_CHP_HIST_LIST_BASE_BASE),
-		      DLB2_CHP_HIST_LIST_PUSH_PTR_PUSH_PTR);
-	DLB2_CSR_WR(hw, DLB2_CHP_HIST_LIST_PUSH_PTR(hw->ver, port->id.phys_id),
-		    reg);
+		DLB2_CSR_WR(hw,
+			    DLB2_SYS_LDB_QID_CFG_V(queue_id),
+			    DLB2_SYS_LDB_QID_CFG_V_RST);
 
-	reg = 0;
-	DLB2_BITS_SET(reg, DLB2_BITS_GET(hl_base, DLB2_CHP_HIST_LIST_BASE_BASE),
-		      DLB2_CHP_HIST_LIST_POP_PTR_POP_PTR);
-	DLB2_CSR_WR(hw, DLB2_CHP_HIST_LIST_POP_PTR(hw->ver, port->id.phys_id),
-		    reg);
+		if (queue->sn_cfg_valid) {
+			u32 offs[2];
 
-	/*
-	 * Address translation (AT) settings: 0: untranslated, 2: translated
-	 * (see ATS spec regarding Address Type field for more details)
-	 */
+			offs[0] = DLB2_RO_GRP_0_SLT_SHFT(hw->ver,
+							 queue->sn_slot);
+			offs[1] = DLB2_RO_GRP_1_SLT_SHFT(hw->ver,
+							 queue->sn_slot);
+
+			DLB2_CSR_WR(hw,
+				    offs[queue->sn_group],
+				    DLB2_RO_GRP_0_SLT_SHFT_RST);
+		}
+
+		for (i = 0; i < DLB2_LSP_QID2CQIDIX_NUM; i++) {
+			DLB2_CSR_WR(hw,
+				    DLB2_LSP_QID2CQIDIX(hw->ver, queue_id, i),
+				    DLB2_LSP_QID2CQIDIX_00_RST);
+
+			DLB2_CSR_WR(hw,
+				    DLB2_LSP_QID2CQIDIX2(hw->ver, queue_id, i),
+				    DLB2_LSP_QID2CQIDIX2_00_RST);
 
-	if (hw->ver == DLB2_HW_V2) {
-		reg = 0;
-		DLB2_CSR_WR(hw, DLB2_SYS_LDB_CQ_AT(port->id.phys_id), reg);
+			DLB2_CSR_WR(hw,
+				    DLB2_ATM_QID2CQIDIX(queue_id, i),
+				    DLB2_ATM_QID2CQIDIX_00_RST);
+		}
 	}
+}
 
-	if (vdev_req && hw->virt_mode == DLB2_VIRT_SIOV) {
-		reg = 0;
-		DLB2_BITS_SET(reg, hw->pasid[vdev_id],
-			      DLB2_SYS_LDB_CQ_PASID_PASID);
-		DLB2_BIT_SET(reg, DLB2_SYS_LDB_CQ_PASID_FMT2);
-	}
+static void dlb2_domain_reset_dir_queue_registers(struct dlb2_hw *hw,
+						  struct dlb2_hw_domain *domain)
+{
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_dir_pq_pair *queue;
 
-	DLB2_CSR_WR(hw, DLB2_SYS_LDB_CQ_PASID(hw->ver, port->id.phys_id), reg);
+	DLB2_DOM_LIST_FOR(domain->used_dir_pq_pairs, queue, iter) {
+		DLB2_CSR_WR(hw,
+			    DLB2_LSP_QID_DIR_MAX_DEPTH(hw->ver,
+						       queue->id.phys_id),
+			    DLB2_LSP_QID_DIR_MAX_DEPTH_RST);
 
-	reg = 0;
-	DLB2_BITS_SET(reg, domain->id.phys_id, DLB2_CHP_LDB_CQ2VAS_CQ2VAS);
-	DLB2_CSR_WR(hw, DLB2_CHP_LDB_CQ2VAS(hw->ver, port->id.phys_id), reg);
+		DLB2_CSR_WR(hw,
+			    DLB2_LSP_QID_DIR_TOT_ENQ_CNTL(hw->ver,
+							  queue->id.phys_id),
+			    DLB2_LSP_QID_DIR_TOT_ENQ_CNTL_RST);
 
-	/* Disable the port's QID mappings */
-	reg = 0;
-	DLB2_CSR_WR(hw, DLB2_LSP_CQ2PRIOV(hw->ver, port->id.phys_id), reg);
+		DLB2_CSR_WR(hw,
+			    DLB2_LSP_QID_DIR_TOT_ENQ_CNTH(hw->ver,
+							  queue->id.phys_id),
+			    DLB2_LSP_QID_DIR_TOT_ENQ_CNTH_RST);
 
-	return 0;
-}
+		DLB2_CSR_WR(hw,
+			    DLB2_LSP_QID_DIR_DEPTH_THRSH(hw->ver,
+							 queue->id.phys_id),
+			    DLB2_LSP_QID_DIR_DEPTH_THRSH_RST);
 
-static bool
-dlb2_cq_depth_is_valid(u32 depth)
-{
-	if (depth != 1 && depth != 2 &&
-	    depth != 4 && depth != 8 &&
-	    depth != 16 && depth != 32 &&
-	    depth != 64 && depth != 128 &&
-	    depth != 256 && depth != 512 &&
-	    depth != 1024)
-		return false;
+		DLB2_CSR_WR(hw,
+			    DLB2_SYS_DIR_QID_ITS(queue->id.phys_id),
+			    DLB2_SYS_DIR_QID_ITS_RST);
 
-	return true;
+		DLB2_CSR_WR(hw,
+			    DLB2_SYS_DIR_QID_V(queue->id.phys_id),
+			    DLB2_SYS_DIR_QID_V_RST);
+	}
 }
 
-static int dlb2_configure_ldb_port(struct dlb2_hw *hw,
-				   struct dlb2_hw_domain *domain,
-				   struct dlb2_ldb_port *port,
-				   uintptr_t cq_dma_base,
-				   struct dlb2_create_ldb_port_args *args,
-				   bool vdev_req,
-				   unsigned int vdev_id)
+static u32 dlb2_dir_cq_token_count(struct dlb2_hw *hw,
+				   struct dlb2_dir_pq_pair *port)
 {
-	int ret, i;
+	u32 cnt;
 
-	port->hist_list_entry_base = domain->hist_list_entry_base +
-				     domain->hist_list_entry_offset;
-	port->hist_list_entry_limit = port->hist_list_entry_base +
-				      args->cq_history_list_size;
+	cnt = DLB2_CSR_RD(hw,
+			  DLB2_LSP_CQ_DIR_TKN_CNT(hw->ver, port->id.phys_id));
 
-	domain->hist_list_entry_offset += args->cq_history_list_size;
-	domain->avail_hist_list_entries -= args->cq_history_list_size;
+	/*
+	 * Account for the initial token count, which is used in order to
+	 * provide a CQ with depth less than 8.
+	 */
 
-	ret = dlb2_ldb_port_configure_cq(hw,
-					 domain,
-					 port,
-					 cq_dma_base,
-					 args,
-					 vdev_req,
-					 vdev_id);
-	if (ret)
-		return ret;
+	return DLB2_BITS_GET(cnt, DLB2_LSP_CQ_DIR_TKN_CNT_COUNT) -
+	       port->init_tkn_cnt;
+}
 
-	dlb2_ldb_port_configure_pp(hw,
-				   domain,
-				   port,
-				   vdev_req,
-				   vdev_id);
+static int dlb2_domain_verify_reset_success(struct dlb2_hw *hw,
+					    struct dlb2_hw_domain *domain)
+{
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_dir_pq_pair *dir_port;
+	struct dlb2_ldb_port *ldb_port;
+	struct dlb2_ldb_queue *queue;
+	int i;
 
-	dlb2_ldb_port_cq_enable(hw, port);
+	/*
+	 * Confirm that all the domain's queue's inflight counts and AQED
+	 * active counts are 0.
+	 */
+	DLB2_DOM_LIST_FOR(domain->used_ldb_queues, queue, iter) {
+		if (!dlb2_ldb_queue_is_empty(hw, queue)) {
+			DLB2_HW_ERR(hw,
+				    "[%s()] Internal error: failed to empty ldb queue %d\n",
+				    __func__, queue->id.phys_id);
+			return -EFAULT;
+		}
+	}
 
-	for (i = 0; i < DLB2_MAX_NUM_QIDS_PER_LDB_CQ; i++)
-		port->qid_map[i].state = DLB2_QUEUE_UNMAPPED;
-	port->num_mappings = 0;
+	/* Confirm that all the domain's CQs inflight and token counts are 0. */
+	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
+		DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i], ldb_port, iter) {
+			if (dlb2_ldb_cq_inflight_count(hw, ldb_port) ||
+			    dlb2_ldb_cq_token_count(hw, ldb_port)) {
+				DLB2_HW_ERR(hw,
+					    "[%s()] Internal error: failed to empty ldb port %d\n",
+					    __func__, ldb_port->id.phys_id);
+				return -EFAULT;
+			}
+		}
+	}
 
-	port->enabled = true;
+	DLB2_DOM_LIST_FOR(domain->used_dir_pq_pairs, dir_port, iter) {
+		if (!dlb2_dir_queue_is_empty(hw, dir_port)) {
+			DLB2_HW_ERR(hw,
+				    "[%s()] Internal error: failed to empty dir queue %d\n",
+				    __func__, dir_port->id.phys_id);
+			return -EFAULT;
+		}
 
-	port->configured = true;
+		if (dlb2_dir_cq_token_count(hw, dir_port)) {
+			DLB2_HW_ERR(hw,
+				    "[%s()] Internal error: failed to empty dir port %d\n",
+				    __func__, dir_port->id.phys_id);
+			return -EFAULT;
+		}
+	}
 
 	return 0;
 }
 
-static void
-dlb2_log_create_ldb_port_args(struct dlb2_hw *hw,
-			      u32 domain_id,
-			      uintptr_t cq_dma_base,
-			      struct dlb2_create_ldb_port_args *args,
-			      bool vdev_req,
-			      unsigned int vdev_id)
+static void dlb2_domain_reset_registers(struct dlb2_hw *hw,
+					struct dlb2_hw_domain *domain)
 {
-	DLB2_HW_DBG(hw, "DLB2 create load-balanced port arguments:\n");
-	if (vdev_req)
-		DLB2_HW_DBG(hw, "(Request from vdev %d)\n", vdev_id);
-	DLB2_HW_DBG(hw, "\tDomain ID:                 %d\n",
-		    domain_id);
-	DLB2_HW_DBG(hw, "\tCQ depth:                  %d\n",
-		    args->cq_depth);
-	DLB2_HW_DBG(hw, "\tCQ hist list size:         %d\n",
-		    args->cq_history_list_size);
-	DLB2_HW_DBG(hw, "\tCQ base address:           0x%lx\n",
-		    cq_dma_base);
-	DLB2_HW_DBG(hw, "\tCoS ID:                    %u\n", args->cos_id);
-	DLB2_HW_DBG(hw, "\tStrict CoS allocation:     %u\n",
-		    args->cos_strict);
-}
+	dlb2_domain_reset_ldb_port_registers(hw, domain);
 
-static int
-dlb2_verify_create_ldb_port_args(struct dlb2_hw *hw,
-				 u32 domain_id,
-				 uintptr_t cq_dma_base,
-				 struct dlb2_create_ldb_port_args *args,
-				 struct dlb2_cmd_response *resp,
-				 bool vdev_req,
-				 unsigned int vdev_id,
-				 struct dlb2_hw_domain **out_domain,
-				 struct dlb2_ldb_port **out_port,
-				 int *out_cos_id)
-{
-	struct dlb2_hw_domain *domain;
-	struct dlb2_ldb_port *port;
-	int i, id;
+	dlb2_domain_reset_dir_port_registers(hw, domain);
 
-	domain = dlb2_get_domain_from_id(hw, domain_id, vdev_req, vdev_id);
+	dlb2_domain_reset_ldb_queue_registers(hw, domain);
 
-	if (!domain) {
-		resp->status = DLB2_ST_INVALID_DOMAIN_ID;
-		return -EINVAL;
-	}
+	dlb2_domain_reset_dir_queue_registers(hw, domain);
 
-	if (!domain->configured) {
-		resp->status = DLB2_ST_DOMAIN_NOT_CONFIGURED;
-		return -EINVAL;
-	}
+	if (hw->ver == DLB2_HW_V2) {
+		DLB2_CSR_WR(hw,
+			    DLB2_CHP_CFG_LDB_VAS_CRD(domain->id.phys_id),
+			    DLB2_CHP_CFG_LDB_VAS_CRD_RST);
 
-	if (domain->started) {
-		resp->status = DLB2_ST_DOMAIN_STARTED;
-		return -EINVAL;
-	}
+		DLB2_CSR_WR(hw,
+			    DLB2_CHP_CFG_DIR_VAS_CRD(domain->id.phys_id),
+			    DLB2_CHP_CFG_DIR_VAS_CRD_RST);
+	} else
+		DLB2_CSR_WR(hw,
+			    DLB2_CHP_CFG_VAS_CRD(domain->id.phys_id),
+			    DLB2_CHP_CFG_VAS_CRD_RST);
+}
 
-	if (args->cos_id >= DLB2_NUM_COS_DOMAINS &&
-	    (args->cos_id != DLB2_COS_DEFAULT || args->cos_strict)) {
-		resp->status = DLB2_ST_INVALID_COS_ID;
-		return -EINVAL;
-	}
+static int dlb2_domain_drain_ldb_cqs(struct dlb2_hw *hw,
+				      struct dlb2_hw_domain *domain,
+				      bool toggle_port)
+{
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_ldb_port *port;
+	int drain_cnt = 0;
+	int i;
 
-	if (args->cos_strict) {
-		id = args->cos_id;
-		port = DLB2_DOM_LIST_HEAD(domain->avail_ldb_ports[id],
-					  typeof(*port));
-	} else {
-		for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
-			if (args->cos_id == DLB2_COS_DEFAULT) {
-				/* Allocate from best performing cos */
-				u32 cos_idx = i + DLB2_MAX_NUM_LDB_PORTS;
-				id = hw->ldb_pp_allocations[cos_idx];
-			} else {
-				id = (args->cos_id + i) % DLB2_NUM_COS_DOMAINS;
-			}
+	/* If the domain hasn't been started, there's no traffic to drain */
+	if (!domain->started)
+		return 0;
+
+	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
+		DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i], port, iter) {
+			if (toggle_port)
+				dlb2_ldb_port_cq_disable(hw, port);
 
-			port = DLB2_DOM_LIST_HEAD(domain->avail_ldb_ports[id],
-						  typeof(*port));
-			if (port)
-				break;
+			drain_cnt += dlb2_drain_ldb_cq(hw, port);
+
+			if (toggle_port)
+				dlb2_ldb_port_cq_enable(hw, port);
 		}
 	}
 
-	if (!port) {
-		resp->status = DLB2_ST_LDB_PORTS_UNAVAILABLE;
-		return -EINVAL;
-	}
+	return drain_cnt;
+}
 
-	DLB2_LOG_INFO(": LDB: cos=%d port:%d\n", id, port->id.phys_id);
+static bool dlb2_domain_mapped_queues_empty(struct dlb2_hw *hw,
+					    struct dlb2_hw_domain *domain)
+{
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_ldb_queue *queue;
 
-	/* Check cache-line alignment */
-	if ((cq_dma_base & 0x3F) != 0) {
-		resp->status = DLB2_ST_INVALID_CQ_VIRT_ADDR;
-		return -EINVAL;
+	DLB2_DOM_LIST_FOR(domain->used_ldb_queues, queue, iter) {
+		if (queue->num_mappings == 0)
+			continue;
+
+		if (!dlb2_ldb_queue_is_empty(hw, queue))
+			return false;
 	}
 
-	if (!dlb2_cq_depth_is_valid(args->cq_depth)) {
-		resp->status = DLB2_ST_INVALID_CQ_DEPTH;
-		return -EINVAL;
+	return true;
+}
+
+static int dlb2_domain_drain_mapped_queues(struct dlb2_hw *hw,
+					   struct dlb2_hw_domain *domain)
+{
+	int i;
+
+	/* If the domain hasn't been started, there's no traffic to drain */
+	if (!domain->started)
+		return 0;
+
+	if (domain->num_pending_removals > 0) {
+		DLB2_HW_ERR(hw,
+			    "[%s()] Internal error: failed to unmap domain queues\n",
+			    __func__);
+		return -EFAULT;
 	}
 
-	/* The history list size must be >= 1 */
-	if (!args->cq_history_list_size) {
-		resp->status = DLB2_ST_INVALID_HIST_LIST_DEPTH;
-		return -EINVAL;
+	for (i = 0; i < DLB2_MAX_QID_EMPTY_CHECK_LOOPS(hw->ver); i++) {
+		int drain_cnt;
+
+		drain_cnt = dlb2_domain_drain_ldb_cqs(hw, domain, false);
+
+		if (dlb2_domain_mapped_queues_empty(hw, domain))
+			break;
+
+		/*
+		 * Allow time for DLB to schedule QEs before draining
+		 * the CQs again.
+		 */
+		if (!drain_cnt)
+			rte_delay_us(1);
 	}
 
-	if (args->cq_history_list_size > domain->avail_hist_list_entries) {
-		resp->status = DLB2_ST_HIST_LIST_ENTRIES_UNAVAILABLE;
-		return -EINVAL;
+	if (i == DLB2_MAX_QID_EMPTY_CHECK_LOOPS(hw->ver)) {
+		DLB2_HW_ERR(hw,
+			    "[%s()] Internal error: failed to empty queues\n",
+			    __func__);
+		return -EFAULT;
 	}
 
-	*out_domain = domain;
-	*out_port = port;
-	*out_cos_id = id;
+	/*
+	 * Drain the CQs one more time. For the queues to go empty, they would
+	 * have scheduled one or more QEs.
+	 */
+	dlb2_domain_drain_ldb_cqs(hw, domain, true);
 
 	return 0;
 }
 
-/**
- * dlb2_hw_create_ldb_port() - create a load-balanced port
- * @hw: dlb2_hw handle for a particular device.
- * @domain_id: domain ID.
- * @args: port creation arguments.
- * @cq_dma_base: base address of the CQ memory. This can be a PA or an IOVA.
- * @resp: response structure.
- * @vdev_req: indicates whether this request came from a vdev.
- * @vdev_id: If vdev_req is true, this contains the vdev's ID.
- *
- * This function creates a load-balanced port.
- *
- * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
- * device.
- *
- * Return:
- * Returns 0 upon success, < 0 otherwise. If an error occurs, resp->status is
- * assigned a detailed error code from enum dlb2_error. If successful, resp->id
- * contains the port ID.
- *
- * resp->id contains a virtual ID if vdev_req is true.
- *
- * Errors:
- * EINVAL - A requested resource is unavailable, a credit setting is invalid, a
- *	    pointer address is not properly aligned, the domain is not
- *	    configured, or the domain has already been started.
- * EFAULT - Internal error (resp->status not set).
- */
-int dlb2_hw_create_ldb_port(struct dlb2_hw *hw,
-			    u32 domain_id,
-			    struct dlb2_create_ldb_port_args *args,
-			    uintptr_t cq_dma_base,
-			    struct dlb2_cmd_response *resp,
-			    bool vdev_req,
-			    unsigned int vdev_id)
+static int dlb2_domain_drain_unmapped_queue(struct dlb2_hw *hw,
+					    struct dlb2_hw_domain *domain,
+					    struct dlb2_ldb_queue *queue)
 {
-	struct dlb2_hw_domain *domain;
-	struct dlb2_ldb_port *port;
-	int ret, cos_id;
+	struct dlb2_ldb_port *port = NULL;
+	int ret, i;
 
-	dlb2_log_create_ldb_port_args(hw,
-				      domain_id,
-				      cq_dma_base,
-				      args,
-				      vdev_req,
-				      vdev_id);
+	/* If a domain has LDB queues, it must have LDB ports */
+	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
+		port = DLB2_DOM_LIST_HEAD(domain->used_ldb_ports[i],
+					  typeof(*port));
+		if (port)
+			break;
+	}
 
-	/*
-	 * Verify that hardware resources are available before attempting to
-	 * satisfy the request. This simplifies the error unwinding code.
-	 */
-	ret = dlb2_verify_create_ldb_port_args(hw,
-					       domain_id,
-					       cq_dma_base,
-					       args,
-					       resp,
-					       vdev_req,
-					       vdev_id,
-					       &domain,
-					       &port,
-					       &cos_id);
-	if (ret)
-		return ret;
+	if (!port) {
+		DLB2_HW_ERR(hw,
+			    "[%s()] Internal error: No configured LDB ports\n",
+			    __func__);
+		return -EFAULT;
+	}
 
-	ret = dlb2_configure_ldb_port(hw,
-				      domain,
-				      port,
-				      cq_dma_base,
-				      args,
-				      vdev_req,
-				      vdev_id);
-	if (ret)
-		return ret;
+	/* If necessary, free up a QID slot in this CQ */
+	if (port->num_mappings == DLB2_MAX_NUM_QIDS_PER_LDB_CQ) {
+		struct dlb2_ldb_queue *mapped_queue;
 
-	/*
-	 * Configuration succeeded, so move the resource from the 'avail' to
-	 * the 'used' list.
-	 */
-	dlb2_list_del(&domain->avail_ldb_ports[cos_id], &port->domain_list);
+		mapped_queue = &hw->rsrcs.ldb_queues[port->qid_map[0].qid];
 
-	dlb2_list_add(&domain->used_ldb_ports[cos_id], &port->domain_list);
+		ret = dlb2_ldb_port_unmap_qid(hw, port, mapped_queue);
+		if (ret)
+			return ret;
+	}
 
-	resp->status = 0;
-	resp->id = (vdev_req) ? port->id.virt_id : port->id.phys_id;
+	ret = dlb2_ldb_port_map_qid_dynamic(hw, port, queue, 0);
+	if (ret)
+		return ret;
 
-	return 0;
+	return dlb2_domain_drain_mapped_queues(hw, domain);
 }
 
-static void
-dlb2_log_create_dir_port_args(struct dlb2_hw *hw,
-			      u32 domain_id,
-			      uintptr_t cq_dma_base,
-			      struct dlb2_create_dir_port_args *args,
-			      bool vdev_req,
-			      unsigned int vdev_id)
+static int dlb2_domain_drain_unmapped_queues(struct dlb2_hw *hw,
+					     struct dlb2_hw_domain *domain)
 {
-	DLB2_HW_DBG(hw, "DLB2 create directed port arguments:\n");
-	if (vdev_req)
-		DLB2_HW_DBG(hw, "(Request from vdev %d)\n", vdev_id);
-	DLB2_HW_DBG(hw, "\tDomain ID:                 %d\n",
-		    domain_id);
-	DLB2_HW_DBG(hw, "\tCQ depth:                  %d\n",
-		    args->cq_depth);
-	DLB2_HW_DBG(hw, "\tCQ base address:           0x%lx\n",
-		    cq_dma_base);
-}
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_ldb_queue *queue;
+	int ret;
 
-static struct dlb2_dir_pq_pair *
-dlb2_get_domain_used_dir_pq(struct dlb2_hw *hw,
-			    u32 id,
-			    bool vdev_req,
-			    struct dlb2_hw_domain *domain)
-{
-	struct dlb2_list_entry *iter;
-	struct dlb2_dir_pq_pair *port;
-	RTE_SET_USED(iter);
+	/* If the domain hasn't been started, there's no traffic to drain */
+	if (!domain->started)
+		return 0;
 
-	if (id >= DLB2_MAX_NUM_DIR_PORTS(hw->ver))
-		return NULL;
+	/*
+	 * Pre-condition: the unattached queue must not have any outstanding
+	 * completions. This is ensured by calling dlb2_domain_drain_ldb_cqs()
+	 * prior to this in dlb2_domain_drain_mapped_queues().
+	 */
+	DLB2_DOM_LIST_FOR(domain->used_ldb_queues, queue, iter) {
+		if (queue->num_mappings != 0 ||
+		    dlb2_ldb_queue_is_empty(hw, queue))
+			continue;
 
-	DLB2_DOM_LIST_FOR(domain->used_dir_pq_pairs, port, iter) {
-		if ((!vdev_req && port->id.phys_id == id) ||
-		    (vdev_req && port->id.virt_id == id))
-			return port;
+		ret = dlb2_domain_drain_unmapped_queue(hw, domain, queue);
+		if (ret)
+			return ret;
 	}
 
-	return NULL;
+	return 0;
 }
 
-static int
-dlb2_verify_create_dir_port_args(struct dlb2_hw *hw,
-				 u32 domain_id,
-				 uintptr_t cq_dma_base,
-				 struct dlb2_create_dir_port_args *args,
-				 struct dlb2_cmd_response *resp,
-				 bool vdev_req,
-				 unsigned int vdev_id,
-				 struct dlb2_hw_domain **out_domain,
-				 struct dlb2_dir_pq_pair **out_port)
+static int dlb2_drain_dir_cq(struct dlb2_hw *hw,
+			      struct dlb2_dir_pq_pair *port)
 {
-	struct dlb2_hw_domain *domain;
-	struct dlb2_dir_pq_pair *pq;
-
-	domain = dlb2_get_domain_from_id(hw, domain_id, vdev_req, vdev_id);
+	unsigned int port_id = port->id.phys_id;
+	u32 cnt;
 
-	if (!domain) {
-		resp->status = DLB2_ST_INVALID_DOMAIN_ID;
-		return -EINVAL;
-	}
+	/* Return any outstanding tokens */
+	cnt = dlb2_dir_cq_token_count(hw, port);
 
-	if (!domain->configured) {
-		resp->status = DLB2_ST_DOMAIN_NOT_CONFIGURED;
-		return -EINVAL;
-	}
+	if (cnt != 0) {
+		struct dlb2_hcw hcw_mem[8], *hcw;
+		void __iomem *pp_addr;
 
-	if (domain->started) {
-		resp->status = DLB2_ST_DOMAIN_STARTED;
-		return -EINVAL;
-	}
+		pp_addr = os_map_producer_port(hw, port_id, false);
 
-	if (args->queue_id != -1) {
-		/*
-		 * If the user claims the queue is already configured, validate
-		 * the queue ID, its domain, and whether the queue is
-		 * configured.
-		 */
-		pq = dlb2_get_domain_used_dir_pq(hw,
-						 args->queue_id,
-						 vdev_req,
-						 domain);
+		/* Point hcw to a 64B-aligned location */
+		hcw = (struct dlb2_hcw *)((uintptr_t)&hcw_mem[4] & ~0x3F);
 
-		if (!pq || pq->domain_id.phys_id != domain->id.phys_id ||
-		    !pq->queue_configured) {
-			resp->status = DLB2_ST_INVALID_DIR_QUEUE_ID;
-			return -EINVAL;
-		}
-	} else {
 		/*
-		 * If the port's queue is not configured, validate that a free
-		 * port-queue pair is available.
-		 * First try the 'res' list if the port is producer OR if
-		 * 'avail' list is empty else fall back to 'avail' list
+		 * Program the first HCW for a batch token return and
+		 * the rest as NOOPS
 		 */
-		if (!dlb2_list_empty(&domain->rsvd_dir_pq_pairs) &&
-		    (args->is_producer ||
-		     dlb2_list_empty(&domain->avail_dir_pq_pairs)))
-			pq = DLB2_DOM_LIST_HEAD(domain->rsvd_dir_pq_pairs,
-						typeof(*pq));
-		else
-			pq = DLB2_DOM_LIST_HEAD(domain->avail_dir_pq_pairs,
-						typeof(*pq));
-
-		if (!pq) {
-			resp->status = DLB2_ST_DIR_PORTS_UNAVAILABLE;
-			return -EINVAL;
-		}
-		DLB2_LOG_INFO(": DIR: port:%d is_producer=%d\n",
-			      pq->id.phys_id, args->is_producer);
+		memset(hcw, 0, 4 * sizeof(*hcw));
+		hcw->cq_token = 1;
+		hcw->lock_id = cnt - 1;
 
-	}
+		os_enqueue_four_hcws(hw, hcw, pp_addr);
 
-	/* Check cache-line alignment */
-	if ((cq_dma_base & 0x3F) != 0) {
-		resp->status = DLB2_ST_INVALID_CQ_VIRT_ADDR;
-		return -EINVAL;
-	}
+		os_fence_hcw(hw, pp_addr);
 
-	if (!dlb2_cq_depth_is_valid(args->cq_depth)) {
-		resp->status = DLB2_ST_INVALID_CQ_DEPTH;
-		return -EINVAL;
+		os_unmap_producer_port(hw, pp_addr);
 	}
 
-	*out_domain = domain;
-	*out_port = pq;
-
-	return 0;
+	return cnt;
 }
 
-static void dlb2_dir_port_configure_pp(struct dlb2_hw *hw,
-				       struct dlb2_hw_domain *domain,
-				       struct dlb2_dir_pq_pair *port,
-				       bool vdev_req,
-				       unsigned int vdev_id)
+static int dlb2_domain_drain_dir_cqs(struct dlb2_hw *hw,
+				     struct dlb2_hw_domain *domain,
+				     bool toggle_port)
 {
-	u32 reg = 0;
-
-	DLB2_BITS_SET(reg, domain->id.phys_id, DLB2_SYS_DIR_PP2VAS_VAS);
-	DLB2_CSR_WR(hw, DLB2_SYS_DIR_PP2VAS(port->id.phys_id), reg);
-
-	if (vdev_req) {
-		unsigned int offs;
-		u32 virt_id;
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_dir_pq_pair *port;
+	int drain_cnt = 0;
 
+	DLB2_DOM_LIST_FOR(domain->used_dir_pq_pairs, port, iter) {
 		/*
-		 * DLB uses producer port address bits 17:12 to determine the
-		 * producer port ID. In Scalable IOV mode, PP accesses come
-		 * through the PF MMIO window for the physical producer port,
-		 * so for translation purposes the virtual and physical port
-		 * IDs are equal.
+		 * Can't drain a port if it's not configured, and there's
+		 * nothing to drain if its queue is unconfigured.
 		 */
-		if (hw->virt_mode == DLB2_VIRT_SRIOV)
-			virt_id = port->id.virt_id;
-		else
-			virt_id = port->id.phys_id;
+		if (!port->port_configured || !port->queue_configured)
+			continue;
 
-		reg = 0;
-		DLB2_BITS_SET(reg, port->id.phys_id, DLB2_SYS_VF_DIR_VPP2PP_PP);
-		offs = vdev_id * DLB2_MAX_NUM_DIR_PORTS(hw->ver) + virt_id;
-		DLB2_CSR_WR(hw, DLB2_SYS_VF_DIR_VPP2PP(offs), reg);
+		if (toggle_port)
+			dlb2_dir_port_cq_disable(hw, port);
 
-		reg = 0;
-		DLB2_BITS_SET(reg, vdev_id, DLB2_SYS_DIR_PP2VDEV_VDEV);
-		DLB2_CSR_WR(hw, DLB2_SYS_DIR_PP2VDEV(port->id.phys_id), reg);
+		drain_cnt += dlb2_drain_dir_cq(hw, port);
 
-		reg = 0;
-		DLB2_BIT_SET(reg, DLB2_SYS_VF_DIR_VPP_V_VPP_V);
-		DLB2_CSR_WR(hw, DLB2_SYS_VF_DIR_VPP_V(offs), reg);
+		if (toggle_port)
+			dlb2_dir_port_cq_enable(hw, port);
 	}
 
-	reg = 0;
-	DLB2_BIT_SET(reg, DLB2_SYS_DIR_PP_V_PP_V);
-	DLB2_CSR_WR(hw, DLB2_SYS_DIR_PP_V(port->id.phys_id), reg);
+	return drain_cnt;
 }
 
-static int dlb2_dir_port_configure_cq(struct dlb2_hw *hw,
-				      struct dlb2_hw_domain *domain,
-				      struct dlb2_dir_pq_pair *port,
-				      uintptr_t cq_dma_base,
-				      struct dlb2_create_dir_port_args *args,
-				      bool vdev_req,
-				      unsigned int vdev_id)
+static bool dlb2_domain_dir_queues_empty(struct dlb2_hw *hw,
+					 struct dlb2_hw_domain *domain)
 {
-	u32 reg = 0;
-	u32 ds = 0;
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_dir_pq_pair *queue;
 
-	/* The CQ address is 64B-aligned, and the DLB only wants bits [63:6] */
-	DLB2_BITS_SET(reg, cq_dma_base >> 6, DLB2_SYS_DIR_CQ_ADDR_L_ADDR_L);
-	DLB2_CSR_WR(hw, DLB2_SYS_DIR_CQ_ADDR_L(port->id.phys_id), reg);
+	DLB2_DOM_LIST_FOR(domain->used_dir_pq_pairs, queue, iter) {
+		if (!dlb2_dir_queue_is_empty(hw, queue))
+			return false;
+	}
 
-	reg = cq_dma_base >> 32;
-	DLB2_CSR_WR(hw, DLB2_SYS_DIR_CQ_ADDR_U(port->id.phys_id), reg);
+	return true;
+}
 
-	/*
-	 * 'ro' == relaxed ordering. This setting allows DLB2 to write
-	 * cache lines out-of-order (but QEs within a cache line are always
-	 * updated in-order).
-	 */
-	reg = 0;
-	DLB2_BITS_SET(reg, vdev_id, DLB2_SYS_DIR_CQ2VF_PF_RO_VF);
-	DLB2_BITS_SET(reg, !vdev_req && (hw->virt_mode != DLB2_VIRT_SIOV),
-		 DLB2_SYS_DIR_CQ2VF_PF_RO_IS_PF);
-	DLB2_BIT_SET(reg, DLB2_SYS_DIR_CQ2VF_PF_RO_RO);
+static int dlb2_domain_drain_dir_queues(struct dlb2_hw *hw,
+					struct dlb2_hw_domain *domain)
+{
+	int i;
 
-	DLB2_CSR_WR(hw, DLB2_SYS_DIR_CQ2VF_PF_RO(port->id.phys_id), reg);
+	/* If the domain hasn't been started, there's no traffic to drain */
+	if (!domain->started)
+		return 0;
 
-	if (args->cq_depth <= 8) {
-		ds = 1;
-	} else if (args->cq_depth == 16) {
-		ds = 2;
-	} else if (args->cq_depth == 32) {
-		ds = 3;
-	} else if (args->cq_depth == 64) {
-		ds = 4;
-	} else if (args->cq_depth == 128) {
-		ds = 5;
-	} else if (args->cq_depth == 256) {
-		ds = 6;
-	} else if (args->cq_depth == 512) {
-		ds = 7;
-	} else if (args->cq_depth == 1024) {
-		ds = 8;
-	} else {
+	for (i = 0; i < DLB2_MAX_QID_EMPTY_CHECK_LOOPS(hw->ver); i++) {
+		int drain_cnt;
+
+		drain_cnt = dlb2_domain_drain_dir_cqs(hw, domain, false);
+
+		if (dlb2_domain_dir_queues_empty(hw, domain))
+			break;
+
+		/*
+		 * Allow time for DLB to schedule QEs before draining
+		 * the CQs again.
+		 */
+		if (!drain_cnt)
+			rte_delay_us(1);
+	}
+
+	if (i == DLB2_MAX_QID_EMPTY_CHECK_LOOPS(hw->ver)) {
 		DLB2_HW_ERR(hw,
-			    "[%s():%d] Internal error: invalid CQ depth\n",
-			    __func__, __LINE__);
+			    "[%s()] Internal error: failed to empty queues\n",
+			    __func__);
 		return -EFAULT;
 	}
 
-	reg = 0;
-	DLB2_BITS_SET(reg, ds,
-		      DLB2_CHP_DIR_CQ_TKN_DEPTH_SEL_TOKEN_DEPTH_SELECT);
-	DLB2_CSR_WR(hw,
-		    DLB2_CHP_DIR_CQ_TKN_DEPTH_SEL(hw->ver, port->id.phys_id),
-		    reg);
-
 	/*
-	 * To support CQs with depth less than 8, program the token count
-	 * register with a non-zero initial value. Operations such as domain
-	 * reset must take this initial value into account when quiescing the
-	 * CQ.
+	 * Drain the CQs one more time. For the queues to go empty, they would
+	 * have scheduled one or more QEs.
 	 */
-	port->init_tkn_cnt = 0;
+	dlb2_domain_drain_dir_cqs(hw, domain, true);
 
-	if (args->cq_depth < 8) {
-		reg = 0;
-		port->init_tkn_cnt = 8 - args->cq_depth;
+	return 0;
+}
 
-		DLB2_BITS_SET(reg, port->init_tkn_cnt,
-			      DLB2_LSP_CQ_DIR_TKN_CNT_COUNT);
-		DLB2_CSR_WR(hw,
-			    DLB2_LSP_CQ_DIR_TKN_CNT(hw->ver, port->id.phys_id),
-			    reg);
-	} else {
+static void
+dlb2_domain_disable_dir_producer_ports(struct dlb2_hw *hw,
+				       struct dlb2_hw_domain *domain)
+{
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_dir_pq_pair *port;
+	u32 pp_v = 0;
+
+	DLB2_DOM_LIST_FOR(domain->used_dir_pq_pairs, port, iter) {
 		DLB2_CSR_WR(hw,
-			    DLB2_LSP_CQ_DIR_TKN_CNT(hw->ver, port->id.phys_id),
-			    DLB2_LSP_CQ_DIR_TKN_CNT_RST);
+			    DLB2_SYS_DIR_PP_V(port->id.phys_id),
+			    pp_v);
+	}
+}
+
+static void
+dlb2_domain_disable_ldb_producer_ports(struct dlb2_hw *hw,
+				       struct dlb2_hw_domain *domain)
+{
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_ldb_port *port;
+	u32 pp_v = 0;
+	int i;
+
+	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
+		DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i], port, iter) {
+			DLB2_CSR_WR(hw,
+				    DLB2_SYS_LDB_PP_V(port->id.phys_id),
+				    pp_v);
+		}
 	}
+}
 
-	reg = 0;
-	DLB2_BITS_SET(reg, ds,
-		      DLB2_LSP_CQ_DIR_TKN_DEPTH_SEL_DSI_TOKEN_DEPTH_SELECT_V2);
-	DLB2_CSR_WR(hw,
-		    DLB2_LSP_CQ_DIR_TKN_DEPTH_SEL_DSI(hw->ver,
-						      port->id.phys_id),
-		    reg);
+static void dlb2_domain_disable_dir_vpps(struct dlb2_hw *hw,
+					 struct dlb2_hw_domain *domain,
+					 unsigned int vdev_id)
+{
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_dir_pq_pair *port;
+	u32 vpp_v = 0;
 
-	/* Reset the CQ write pointer */
-	DLB2_CSR_WR(hw,
-		    DLB2_CHP_DIR_CQ_WPTR(hw->ver, port->id.phys_id),
-		    DLB2_CHP_DIR_CQ_WPTR_RST);
+	DLB2_DOM_LIST_FOR(domain->used_dir_pq_pairs, port, iter) {
+		unsigned int offs;
+		u32 virt_id;
 
-	/* Virtualize the PPID */
-	reg = 0;
-	DLB2_CSR_WR(hw, DLB2_SYS_DIR_CQ_FMT(port->id.phys_id), reg);
+		if (hw->virt_mode == DLB2_VIRT_SRIOV)
+			virt_id = port->id.virt_id;
+		else
+			virt_id = port->id.phys_id;
 
-	/*
-	 * Address translation (AT) settings: 0: untranslated, 2: translated
-	 * (see ATS spec regarding Address Type field for more details)
-	 */
-	if (hw->ver == DLB2_HW_V2) {
-		reg = 0;
-		DLB2_CSR_WR(hw, DLB2_SYS_DIR_CQ_AT(port->id.phys_id), reg);
-	}
+		offs = vdev_id * DLB2_MAX_NUM_DIR_PORTS(hw->ver) + virt_id;
 
-	if (vdev_req && hw->virt_mode == DLB2_VIRT_SIOV) {
-		DLB2_BITS_SET(reg, hw->pasid[vdev_id],
-			      DLB2_SYS_DIR_CQ_PASID_PASID);
-		DLB2_BIT_SET(reg, DLB2_SYS_DIR_CQ_PASID_FMT2);
+		DLB2_CSR_WR(hw, DLB2_SYS_VF_DIR_VPP_V(offs), vpp_v);
 	}
+}
 
-	DLB2_CSR_WR(hw, DLB2_SYS_DIR_CQ_PASID(hw->ver, port->id.phys_id), reg);
+static void dlb2_domain_disable_ldb_vpps(struct dlb2_hw *hw,
+					 struct dlb2_hw_domain *domain,
+					 unsigned int vdev_id)
+{
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_ldb_port *port;
+	u32 vpp_v = 0;
+	int i;
 
-	reg = 0;
-	DLB2_BITS_SET(reg, domain->id.phys_id, DLB2_CHP_DIR_CQ2VAS_CQ2VAS);
-	DLB2_CSR_WR(hw, DLB2_CHP_DIR_CQ2VAS(hw->ver, port->id.phys_id), reg);
+	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
+		DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i], port, iter) {
+			unsigned int offs;
+			u32 virt_id;
 
-	return 0;
+			if (hw->virt_mode == DLB2_VIRT_SRIOV)
+				virt_id = port->id.virt_id;
+			else
+				virt_id = port->id.phys_id;
+
+			offs = vdev_id * DLB2_MAX_NUM_LDB_PORTS + virt_id;
+
+			DLB2_CSR_WR(hw, DLB2_SYS_VF_LDB_VPP_V(offs), vpp_v);
+		}
+	}
 }
 
-static int dlb2_configure_dir_port(struct dlb2_hw *hw,
-				   struct dlb2_hw_domain *domain,
-				   struct dlb2_dir_pq_pair *port,
-				   uintptr_t cq_dma_base,
-				   struct dlb2_create_dir_port_args *args,
-				   bool vdev_req,
-				   unsigned int vdev_id)
+static void dlb2_domain_disable_ldb_seq_checks(struct dlb2_hw *hw,
+					       struct dlb2_hw_domain *domain)
 {
-	int ret;
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_ldb_port *port;
+	u32 chk_en = 0;
+	int i;
 
-	ret = dlb2_dir_port_configure_cq(hw,
-					 domain,
-					 port,
-					 cq_dma_base,
-					 args,
-					 vdev_req,
-					 vdev_id);
+	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
+		DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i], port, iter) {
+			DLB2_CSR_WR(hw,
+				    DLB2_CHP_SN_CHK_ENBL(hw->ver,
+							 port->id.phys_id),
+				    chk_en);
+		}
+	}
+}
 
-	if (ret)
-		return ret;
+static void
+dlb2_domain_disable_ldb_port_interrupts(struct dlb2_hw *hw,
+					struct dlb2_hw_domain *domain)
+{
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_ldb_port *port;
+	u32 int_en = 0;
+	u32 wd_en = 0;
+	int i;
 
-	dlb2_dir_port_configure_pp(hw,
-				   domain,
-				   port,
-				   vdev_req,
-				   vdev_id);
+	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
+		DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i], port, iter) {
+			DLB2_CSR_WR(hw,
+				    DLB2_CHP_LDB_CQ_INT_ENB(hw->ver,
+						       port->id.phys_id),
+				    int_en);
 
-	dlb2_dir_port_cq_enable(hw, port);
+			DLB2_CSR_WR(hw,
+				    DLB2_CHP_LDB_CQ_WD_ENB(hw->ver,
+						      port->id.phys_id),
+				    wd_en);
+		}
+	}
+}
 
-	port->enabled = true;
+static void
+dlb2_domain_disable_dir_port_interrupts(struct dlb2_hw *hw,
+					struct dlb2_hw_domain *domain)
+{
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_dir_pq_pair *port;
+	u32 int_en = 0;
+	u32 wd_en = 0;
 
-	port->port_configured = true;
+	DLB2_DOM_LIST_FOR(domain->used_dir_pq_pairs, port, iter) {
+		DLB2_CSR_WR(hw,
+			    DLB2_CHP_DIR_CQ_INT_ENB(hw->ver, port->id.phys_id),
+			    int_en);
 
-	return 0;
+		DLB2_CSR_WR(hw,
+			    DLB2_CHP_DIR_CQ_WD_ENB(hw->ver, port->id.phys_id),
+			    wd_en);
+	}
 }
 
-/**
- * dlb2_hw_create_dir_port() - create a directed port
- * @hw: dlb2_hw handle for a particular device.
- * @domain_id: domain ID.
- * @args: port creation arguments.
- * @cq_dma_base: base address of the CQ memory. This can be a PA or an IOVA.
- * @resp: response structure.
- * @vdev_req: indicates whether this request came from a vdev.
- * @vdev_id: If vdev_req is true, this contains the vdev's ID.
- *
- * This function creates a directed port.
- *
- * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
- * device.
- *
- * Return:
- * Returns 0 upon success, < 0 otherwise. If an error occurs, resp->status is
- * assigned a detailed error code from enum dlb2_error. If successful, resp->id
- * contains the port ID.
- *
- * resp->id contains a virtual ID if vdev_req is true.
- *
- * Errors:
- * EINVAL - A requested resource is unavailable, a credit setting is invalid, a
- *	    pointer address is not properly aligned, the domain is not
- *	    configured, or the domain has already been started.
- * EFAULT - Internal error (resp->status not set).
- */
-int dlb2_hw_create_dir_port(struct dlb2_hw *hw,
-			    u32 domain_id,
-			    struct dlb2_create_dir_port_args *args,
-			    uintptr_t cq_dma_base,
-			    struct dlb2_cmd_response *resp,
-			    bool vdev_req,
-			    unsigned int vdev_id)
+static void
+dlb2_domain_disable_ldb_queue_write_perms(struct dlb2_hw *hw,
+					  struct dlb2_hw_domain *domain)
 {
-	struct dlb2_dir_pq_pair *port;
-	struct dlb2_hw_domain *domain;
-	int ret;
+	int domain_offset = domain->id.phys_id * DLB2_MAX_NUM_LDB_QUEUES;
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_ldb_queue *queue;
 
-	dlb2_log_create_dir_port_args(hw,
-				      domain_id,
-				      cq_dma_base,
-				      args,
-				      vdev_req,
-				      vdev_id);
+	DLB2_DOM_LIST_FOR(domain->used_ldb_queues, queue, iter) {
+		int idx = domain_offset + queue->id.phys_id;
 
-	/*
-	 * Verify that hardware resources are available before attempting to
-	 * satisfy the request. This simplifies the error unwinding code.
-	 */
-	ret = dlb2_verify_create_dir_port_args(hw,
-					       domain_id,
-					       cq_dma_base,
-					       args,
-					       resp,
-					       vdev_req,
-					       vdev_id,
-					       &domain,
-					       &port);
-	if (ret)
-		return ret;
+		DLB2_CSR_WR(hw, DLB2_SYS_LDB_VASQID_V(idx), 0);
 
-	ret = dlb2_configure_dir_port(hw,
-				      domain,
-				      port,
-				      cq_dma_base,
-				      args,
-				      vdev_req,
-				      vdev_id);
-	if (ret)
-		return ret;
+		if (queue->id.vdev_owned) {
+			DLB2_CSR_WR(hw,
+				    DLB2_SYS_LDB_QID2VQID(queue->id.phys_id),
+				    0);
 
-	/*
-	 * Configuration succeeded, so move the resource from the 'avail' or
-	 * 'res' to the 'used' list (if it's not already there).
-	 */
-	if (args->queue_id == -1) {
-		struct dlb2_list_head *res = &domain->rsvd_dir_pq_pairs;
-		struct dlb2_list_head *avail = &domain->avail_dir_pq_pairs;
+			idx = queue->id.vdev_id * DLB2_MAX_NUM_LDB_QUEUES +
+				queue->id.virt_id;
 
-		if ((args->is_producer && !dlb2_list_empty(res)) ||
-		     dlb2_list_empty(avail))
-			dlb2_list_del(res, &port->domain_list);
-		else
-			dlb2_list_del(avail, &port->domain_list);
+			DLB2_CSR_WR(hw, DLB2_SYS_VF_LDB_VQID_V(idx), 0);
 
-		dlb2_list_add(&domain->used_dir_pq_pairs, &port->domain_list);
+			DLB2_CSR_WR(hw, DLB2_SYS_VF_LDB_VQID2QID(idx), 0);
+		}
 	}
-
-	resp->status = 0;
-	resp->id = (vdev_req) ? port->id.virt_id : port->id.phys_id;
-
-	return 0;
 }
 
-static void dlb2_configure_dir_queue(struct dlb2_hw *hw,
-				     struct dlb2_hw_domain *domain,
-				     struct dlb2_dir_pq_pair *queue,
-				     struct dlb2_create_dir_queue_args *args,
-				     bool vdev_req,
-				     unsigned int vdev_id)
+static void
+dlb2_domain_disable_dir_queue_write_perms(struct dlb2_hw *hw,
+					  struct dlb2_hw_domain *domain)
 {
-	unsigned int offs;
-	u32 reg = 0;
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_dir_pq_pair *queue;
+	unsigned long max_ports;
+	int domain_offset;
 
-	/* QID write permissions are turned on when the domain is started */
-	offs = domain->id.phys_id * DLB2_MAX_NUM_DIR_QUEUES(hw->ver) +
-		queue->id.phys_id;
+	max_ports = DLB2_MAX_NUM_DIR_PORTS(hw->ver);
 
-	DLB2_CSR_WR(hw, DLB2_SYS_DIR_VASQID_V(offs), reg);
+	domain_offset = domain->id.phys_id * max_ports;
 
-	/* Don't timestamp QEs that pass through this queue */
-	DLB2_CSR_WR(hw, DLB2_SYS_DIR_QID_ITS(queue->id.phys_id), reg);
+	DLB2_DOM_LIST_FOR(domain->used_dir_pq_pairs, queue, iter) {
+		int idx = domain_offset + queue->id.phys_id;
 
-	reg = 0;
-	DLB2_BITS_SET(reg, args->depth_threshold,
-		      DLB2_LSP_QID_DIR_DEPTH_THRSH_THRESH);
-	DLB2_CSR_WR(hw,
-		    DLB2_LSP_QID_DIR_DEPTH_THRSH(hw->ver, queue->id.phys_id),
-		    reg);
+		DLB2_CSR_WR(hw, DLB2_SYS_DIR_VASQID_V(idx), 0);
 
-	if (vdev_req) {
-		offs = vdev_id * DLB2_MAX_NUM_DIR_QUEUES(hw->ver) +
-			queue->id.virt_id;
+		if (queue->id.vdev_owned) {
+			idx = queue->id.vdev_id * max_ports + queue->id.virt_id;
 
-		reg = 0;
-		DLB2_BIT_SET(reg, DLB2_SYS_VF_DIR_VQID_V_VQID_V);
-		DLB2_CSR_WR(hw, DLB2_SYS_VF_DIR_VQID_V(offs), reg);
+			DLB2_CSR_WR(hw, DLB2_SYS_VF_DIR_VQID_V(idx), 0);
 
-		reg = 0;
-		DLB2_BITS_SET(reg, queue->id.phys_id,
-			      DLB2_SYS_VF_DIR_VQID2QID_QID);
-		DLB2_CSR_WR(hw, DLB2_SYS_VF_DIR_VQID2QID(offs), reg);
+			DLB2_CSR_WR(hw, DLB2_SYS_VF_DIR_VQID2QID(idx), 0);
+		}
 	}
-
-	reg = 0;
-	DLB2_BIT_SET(reg, DLB2_SYS_DIR_QID_V_QID_V);
-	DLB2_CSR_WR(hw, DLB2_SYS_DIR_QID_V(queue->id.phys_id), reg);
-
-	queue->queue_configured = true;
 }
 
-static void
-dlb2_log_create_dir_queue_args(struct dlb2_hw *hw,
-			       u32 domain_id,
-			       struct dlb2_create_dir_queue_args *args,
-			       bool vdev_req,
-			       unsigned int vdev_id)
+static void dlb2_domain_disable_dir_cqs(struct dlb2_hw *hw,
+					struct dlb2_hw_domain *domain)
 {
-	DLB2_HW_DBG(hw, "DLB2 create directed queue arguments:\n");
-	if (vdev_req)
-		DLB2_HW_DBG(hw, "(Request from vdev %d)\n", vdev_id);
-	DLB2_HW_DBG(hw, "\tDomain ID: %d\n", domain_id);
-	DLB2_HW_DBG(hw, "\tPort ID:   %d\n", args->port_id);
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_dir_pq_pair *port;
+
+	DLB2_DOM_LIST_FOR(domain->used_dir_pq_pairs, port, iter) {
+		port->enabled = false;
+
+		dlb2_dir_port_cq_disable(hw, port);
+	}
 }
 
-static int
-dlb2_verify_create_dir_queue_args(struct dlb2_hw *hw,
-				  u32 domain_id,
-				  struct dlb2_create_dir_queue_args *args,
-				  struct dlb2_cmd_response *resp,
-				  bool vdev_req,
-				  unsigned int vdev_id,
-				  struct dlb2_hw_domain **out_domain,
-				  struct dlb2_dir_pq_pair **out_queue)
+static void dlb2_domain_disable_ldb_cqs(struct dlb2_hw *hw,
+					struct dlb2_hw_domain *domain)
 {
-	struct dlb2_hw_domain *domain;
-	struct dlb2_dir_pq_pair *pq;
-
-	domain = dlb2_get_domain_from_id(hw, domain_id, vdev_req, vdev_id);
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_ldb_port *port;
+	int i;
 
-	if (!domain) {
-		resp->status = DLB2_ST_INVALID_DOMAIN_ID;
-		return -EINVAL;
-	}
+	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
+		DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i], port, iter) {
+			port->enabled = false;
 
-	if (!domain->configured) {
-		resp->status = DLB2_ST_DOMAIN_NOT_CONFIGURED;
-		return -EINVAL;
+			dlb2_ldb_port_cq_disable(hw, port);
+		}
 	}
+}
 
-	if (domain->started) {
-		resp->status = DLB2_ST_DOMAIN_STARTED;
-		return -EINVAL;
-	}
+static void dlb2_domain_enable_ldb_cqs(struct dlb2_hw *hw,
+				       struct dlb2_hw_domain *domain)
+{
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_ldb_port *port;
+	int i;
 
-	/*
-	 * If the user claims the port is already configured, validate the port
-	 * ID, its domain, and whether the port is configured.
-	 */
-	if (args->port_id != -1) {
-		pq = dlb2_get_domain_used_dir_pq(hw,
-						 args->port_id,
-						 vdev_req,
-						 domain);
+	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
+		DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i], port, iter) {
+			port->enabled = true;
 
-		if (!pq || pq->domain_id.phys_id != domain->id.phys_id ||
-		    !pq->port_configured) {
-			resp->status = DLB2_ST_INVALID_PORT_ID;
-			return -EINVAL;
-		}
-	} else {
-		/*
-		 * If the queue's port is not configured, validate that a free
-		 * port-queue pair is available.
-		 */
-		pq = DLB2_DOM_LIST_HEAD(domain->avail_dir_pq_pairs,
-					typeof(*pq));
-		if (!pq) {
-			resp->status = DLB2_ST_DIR_QUEUES_UNAVAILABLE;
-			return -EINVAL;
+			dlb2_ldb_port_cq_enable(hw, port);
 		}
 	}
+}
 
-	*out_domain = domain;
-	*out_queue = pq;
-
-	return 0;
+static void dlb2_log_reset_domain(struct dlb2_hw *hw,
+				  u32 domain_id,
+				  bool vdev_req,
+				  unsigned int vdev_id)
+{
+	DLB2_HW_DBG(hw, "DLB2 reset domain:\n");
+	if (vdev_req)
+		DLB2_HW_DBG(hw, "(Request from vdev %d)\n", vdev_id);
+	DLB2_HW_DBG(hw, "\tDomain ID: %d\n", domain_id);
 }
 
 /**
- * dlb2_hw_create_dir_queue() - create a directed queue
+ * dlb2_reset_domain() - reset a scheduling domain
  * @hw: dlb2_hw handle for a particular device.
  * @domain_id: domain ID.
- * @args: queue creation arguments.
- * @resp: response structure.
  * @vdev_req: indicates whether this request came from a vdev.
  * @vdev_id: If vdev_req is true, this contains the vdev's ID.
  *
- * This function creates a directed queue.
+ * This function resets and frees a DLB 2.0 scheduling domain and its associated
+ * resources.
+ *
+ * Pre-condition: the driver must ensure software has stopped sending QEs
+ * through this domain's producer ports before invoking this function, or
+ * undefined behavior will result.
  *
  * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
  * device.
  *
  * Return:
- * Returns 0 upon success, < 0 otherwise. If an error occurs, resp->status is
- * assigned a detailed error code from enum dlb2_error. If successful, resp->id
- * contains the queue ID.
- *
- * resp->id contains a virtual ID if vdev_req is true.
+ * Returns 0 upon success, -1 otherwise.
  *
- * Errors:
- * EINVAL - A requested resource is unavailable, the domain is not configured,
- *	    or the domain has already been started.
- * EFAULT - Internal error (resp->status not set).
+ * EINVAL - Invalid domain ID, or the domain is not configured.
+ * EFAULT - Internal error. (Possibly caused if software is the pre-condition
+ *	    is not met.)
+ * ETIMEDOUT - Hardware component didn't reset in the expected time.
  */
-int dlb2_hw_create_dir_queue(struct dlb2_hw *hw,
-			     u32 domain_id,
-			     struct dlb2_create_dir_queue_args *args,
-			     struct dlb2_cmd_response *resp,
-			     bool vdev_req,
-			     unsigned int vdev_id)
+int dlb2_reset_domain(struct dlb2_hw *hw,
+		      u32 domain_id,
+		      bool vdev_req,
+		      unsigned int vdev_id)
 {
-	struct dlb2_dir_pq_pair *queue;
 	struct dlb2_hw_domain *domain;
 	int ret;
 
-	dlb2_log_create_dir_queue_args(hw, domain_id, args, vdev_req, vdev_id);
+	dlb2_log_reset_domain(hw, domain_id, vdev_req, vdev_id);
 
-	/*
-	 * Verify that hardware resources are available before attempting to
-	 * satisfy the request. This simplifies the error unwinding code.
-	 */
-	ret = dlb2_verify_create_dir_queue_args(hw,
-						domain_id,
-						args,
-						resp,
-						vdev_req,
-						vdev_id,
-						&domain,
-						&queue);
-	if (ret)
-		return ret;
+	domain = dlb2_get_domain_from_id(hw, domain_id, vdev_req, vdev_id);
 
-	dlb2_configure_dir_queue(hw, domain, queue, args, vdev_req, vdev_id);
+	if (!domain || !domain->configured)
+		return -EINVAL;
+
+	/* Disable VPPs */
+	if (vdev_req) {
+		dlb2_domain_disable_dir_vpps(hw, domain, vdev_id);
+
+		dlb2_domain_disable_ldb_vpps(hw, domain, vdev_id);
+	}
+
+	/* Disable CQ interrupts */
+	dlb2_domain_disable_dir_port_interrupts(hw, domain);
+
+	dlb2_domain_disable_ldb_port_interrupts(hw, domain);
 
 	/*
-	 * Configuration succeeded, so move the resource from the 'avail' to
-	 * the 'used' list (if it's not already there).
+	 * For each queue owned by this domain, disable its write permissions to
+	 * cause any traffic sent to it to be dropped. Well-behaved software
+	 * should not be sending QEs at this point.
 	 */
-	if (args->port_id == -1) {
-		dlb2_list_del(&domain->avail_dir_pq_pairs,
-			      &queue->domain_list);
+	dlb2_domain_disable_dir_queue_write_perms(hw, domain);
 
-		dlb2_list_add(&domain->used_dir_pq_pairs,
-			      &queue->domain_list);
-	}
+	dlb2_domain_disable_ldb_queue_write_perms(hw, domain);
 
-	resp->status = 0;
+	/* Turn off completion tracking on all the domain's PPs. */
+	dlb2_domain_disable_ldb_seq_checks(hw, domain);
 
-	resp->id = (vdev_req) ? queue->id.virt_id : queue->id.phys_id;
+	/*
+	 * Disable the LDB CQs and drain them in order to complete the map and
+	 * unmap procedures, which require zero CQ inflights and zero QID
+	 * inflights respectively.
+	 */
+	dlb2_domain_disable_ldb_cqs(hw, domain);
 
-	return 0;
-}
+	dlb2_domain_drain_ldb_cqs(hw, domain, false);
 
-static bool
-dlb2_port_find_slot_with_pending_map_queue(struct dlb2_ldb_port *port,
-					   struct dlb2_ldb_queue *queue,
-					   int *slot)
-{
-	int i;
+	ret = dlb2_domain_wait_for_ldb_cqs_to_empty(hw, domain);
+	if (ret)
+		return ret;
 
-	for (i = 0; i < DLB2_MAX_NUM_QIDS_PER_LDB_CQ; i++) {
-		struct dlb2_ldb_port_qid_map *map = &port->qid_map[i];
+	ret = dlb2_domain_finish_unmap_qid_procedures(hw, domain);
+	if (ret)
+		return ret;
+
+	ret = dlb2_domain_finish_map_qid_procedures(hw, domain);
+	if (ret)
+		return ret;
 
-		if (map->state == DLB2_QUEUE_UNMAP_IN_PROG_PENDING_MAP &&
-		    map->pending_qid == queue->id.phys_id)
-			break;
-	}
+	/* Re-enable the CQs in order to drain the mapped queues. */
+	dlb2_domain_enable_ldb_cqs(hw, domain);
 
-	*slot = i;
+	ret = dlb2_domain_drain_mapped_queues(hw, domain);
+	if (ret)
+		return ret;
 
-	return (i < DLB2_MAX_NUM_QIDS_PER_LDB_CQ);
-}
+	ret = dlb2_domain_drain_unmapped_queues(hw, domain);
+	if (ret)
+		return ret;
 
-static int dlb2_verify_map_qid_slot_available(struct dlb2_ldb_port *port,
-					      struct dlb2_ldb_queue *queue,
-					      struct dlb2_cmd_response *resp)
-{
-	enum dlb2_qid_map_state state;
-	int i;
+	/* Done draining LDB QEs, so disable the CQs. */
+	dlb2_domain_disable_ldb_cqs(hw, domain);
 
-	/* Unused slot available? */
-	if (port->num_mappings < DLB2_MAX_NUM_QIDS_PER_LDB_CQ)
-		return 0;
+	dlb2_domain_drain_dir_queues(hw, domain);
 
-	/*
-	 * If the queue is already mapped (from the application's perspective),
-	 * this is simply a priority update.
-	 */
-	state = DLB2_QUEUE_MAPPED;
-	if (dlb2_port_find_slot_queue(port, state, queue, &i))
-		return 0;
+	/* Done draining DIR QEs, so disable the CQs. */
+	dlb2_domain_disable_dir_cqs(hw, domain);
 
-	state = DLB2_QUEUE_MAP_IN_PROG;
-	if (dlb2_port_find_slot_queue(port, state, queue, &i))
-		return 0;
+	/* Disable PPs */
+	dlb2_domain_disable_dir_producer_ports(hw, domain);
 
-	if (dlb2_port_find_slot_with_pending_map_queue(port, queue, &i))
-		return 0;
+	dlb2_domain_disable_ldb_producer_ports(hw, domain);
 
-	/*
-	 * If the slot contains an unmap in progress, it's considered
-	 * available.
-	 */
-	state = DLB2_QUEUE_UNMAP_IN_PROG;
-	if (dlb2_port_find_slot(port, state, &i))
-		return 0;
+	ret = dlb2_domain_verify_reset_success(hw, domain);
+	if (ret)
+		return ret;
 
-	state = DLB2_QUEUE_UNMAPPED;
-	if (dlb2_port_find_slot(port, state, &i))
-		return 0;
+	/* Reset the QID and port state. */
+	dlb2_domain_reset_registers(hw, domain);
 
-	resp->status = DLB2_ST_NO_QID_SLOTS_AVAILABLE;
-	return -EINVAL;
+	/* Hardware reset complete. Reset the domain's software state */
+	return dlb2_domain_reset_software_state(hw, domain);
 }
 
-static struct dlb2_ldb_queue *
-dlb2_get_domain_ldb_queue(u32 id,
-			  bool vdev_req,
-			  struct dlb2_hw_domain *domain)
+/**
+ * dlb2_reset_vdev() - reset the hardware owned by a virtual device
+ * @hw: dlb2_hw handle for a particular device.
+ * @id: virtual device ID
+ *
+ * This function resets the hardware owned by a vdev, by resetting the vdev's
+ * domains one by one.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ */
+int dlb2_reset_vdev(struct dlb2_hw *hw, unsigned int id)
 {
-	struct dlb2_list_entry *iter;
-	struct dlb2_ldb_queue *queue;
-	RTE_SET_USED(iter);
+	struct dlb2_hw_domain *domain, *next __attribute__((unused));
+	struct dlb2_list_entry *it1 __attribute__((unused));
+	struct dlb2_list_entry *it2 __attribute__((unused));
+	struct dlb2_function_resources *rsrcs;
 
-	if (id >= DLB2_MAX_NUM_LDB_QUEUES)
-		return NULL;
+	if (id >= DLB2_MAX_NUM_VDEVS) {
+		DLB2_HW_ERR(hw,
+			    "[%s()] Internal error: invalid vdev ID %d\n",
+			    __func__, id);
+		return -1;
+	}
 
-	DLB2_DOM_LIST_FOR(domain->used_ldb_queues, queue, iter) {
-		if ((!vdev_req && queue->id.phys_id == id) ||
-		    (vdev_req && queue->id.virt_id == id))
-			return queue;
+	rsrcs = &hw->vdev[id];
+
+	DLB2_FUNC_LIST_FOR_SAFE(rsrcs->used_domains, domain, next, it1, it2) {
+		int ret = dlb2_reset_domain(hw,
+					    domain->id.virt_id,
+					    true,
+					    id);
+		if (ret)
+			return ret;
 	}
 
-	return NULL;
+	return 0;
 }
 
-static struct dlb2_ldb_port *
-dlb2_get_domain_used_ldb_port(u32 id,
-			      bool vdev_req,
-			      struct dlb2_hw_domain *domain)
+/**
+ * dlb2_ldb_port_owned_by_domain() - query whether a port is owned by a domain
+ * @hw: dlb2_hw handle for a particular device.
+ * @domain_id: domain ID.
+ * @port_id: port ID.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
+ *
+ * This function returns whether a load-balanced port is owned by a specified
+ * domain.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ *
+ * Return:
+ * Returns 0 if false, 1 if true, <0 otherwise.
+ *
+ * EINVAL - Invalid domain or port ID, or the domain is not configured.
+ */
+int dlb2_ldb_port_owned_by_domain(struct dlb2_hw *hw,
+				  u32 domain_id,
+				  u32 port_id,
+				  bool vdev_req,
+				  unsigned int vdev_id)
 {
-	struct dlb2_list_entry *iter;
+	struct dlb2_hw_domain *domain;
 	struct dlb2_ldb_port *port;
-	int i;
-	RTE_SET_USED(iter);
 
-	if (id >= DLB2_MAX_NUM_LDB_PORTS)
-		return NULL;
+	if (vdev_req && vdev_id >= DLB2_MAX_NUM_VDEVS)
+		return -EINVAL;
 
-	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
-		DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i], port, iter) {
-			if ((!vdev_req && port->id.phys_id == id) ||
-			    (vdev_req && port->id.virt_id == id))
-				return port;
-		}
+	domain = dlb2_get_domain_from_id(hw, domain_id, vdev_req, vdev_id);
 
-		DLB2_DOM_LIST_FOR(domain->avail_ldb_ports[i], port, iter) {
-			if ((!vdev_req && port->id.phys_id == id) ||
-			    (vdev_req && port->id.virt_id == id))
-				return port;
-		}
-	}
+	if (!domain || !domain->configured)
+		return -EINVAL;
 
-	return NULL;
+	port = dlb2_get_domain_ldb_port(port_id, vdev_req, domain);
+
+	if (!port)
+		return -EINVAL;
+
+	return port->domain_id.phys_id == domain->id.phys_id;
 }
 
-static void dlb2_ldb_port_change_qid_priority(struct dlb2_hw *hw,
-					      struct dlb2_ldb_port *port,
-					      int slot,
-					      struct dlb2_map_qid_args *args)
+/**
+ * dlb2_dir_port_owned_by_domain() - query whether a port is owned by a domain
+ * @hw: dlb2_hw handle for a particular device.
+ * @domain_id: domain ID.
+ * @port_id: port ID.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
+ *
+ * This function returns whether a directed port is owned by a specified
+ * domain.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ *
+ * Return:
+ * Returns 0 if false, 1 if true, <0 otherwise.
+ *
+ * EINVAL - Invalid domain or port ID, or the domain is not configured.
+ */
+int dlb2_dir_port_owned_by_domain(struct dlb2_hw *hw,
+				  u32 domain_id,
+				  u32 port_id,
+				  bool vdev_req,
+				  unsigned int vdev_id)
 {
-	u32 cq2priov;
+	struct dlb2_dir_pq_pair *port;
+	struct dlb2_hw_domain *domain;
 
-	/* Read-modify-write the priority and valid bit register */
-	cq2priov = DLB2_CSR_RD(hw,
-			       DLB2_LSP_CQ2PRIOV(hw->ver, port->id.phys_id));
+	if (vdev_req && vdev_id >= DLB2_MAX_NUM_VDEVS)
+		return -EINVAL;
 
-	cq2priov |= (1 << (slot + DLB2_LSP_CQ2PRIOV_V_LOC)) &
-		    DLB2_LSP_CQ2PRIOV_V;
-	cq2priov |= ((args->priority & 0x7) << slot * 3) &
-		    DLB2_LSP_CQ2PRIOV_PRIO;
+	domain = dlb2_get_domain_from_id(hw, domain_id, vdev_req, vdev_id);
 
-	DLB2_CSR_WR(hw, DLB2_LSP_CQ2PRIOV(hw->ver, port->id.phys_id), cq2priov);
+	if (!domain || !domain->configured)
+		return -EINVAL;
 
-	dlb2_flush_csr(hw);
+	port = dlb2_get_domain_dir_pq(hw, port_id, vdev_req, domain);
 
-	port->qid_map[slot].priority = args->priority;
+	if (!port)
+		return -EINVAL;
+
+	return port->domain_id.phys_id == domain->id.phys_id;
 }
 
-static int dlb2_verify_map_qid_args(struct dlb2_hw *hw,
-				    u32 domain_id,
-				    struct dlb2_map_qid_args *args,
-				    struct dlb2_cmd_response *resp,
-				    bool vdev_req,
-				    unsigned int vdev_id,
-				    struct dlb2_hw_domain **out_domain,
-				    struct dlb2_ldb_port **out_port,
-				    struct dlb2_ldb_queue **out_queue)
+static inline bool dlb2_ldb_port_owned_by_vf(struct dlb2_hw *hw,
+					     u32 vdev_id,
+					     u32 port_id)
 {
-	struct dlb2_hw_domain *domain;
-	struct dlb2_ldb_queue *queue;
-	struct dlb2_ldb_port *port;
-	int id;
+	return (hw->rsrcs.ldb_ports[port_id].id.vdev_owned &&
+		hw->rsrcs.ldb_ports[port_id].id.vdev_id == vdev_id);
+}
 
-	domain = dlb2_get_domain_from_id(hw, domain_id, vdev_req, vdev_id);
+static inline bool dlb2_dir_port_owned_by_vf(struct dlb2_hw *hw,
+					     u32 vdev_id,
+					     u32 port_id)
+{
+	return (hw->rsrcs.dir_pq_pairs[port_id].id.vdev_owned &&
+		hw->rsrcs.dir_pq_pairs[port_id].id.vdev_id == vdev_id);
+}
 
-	if (!domain) {
-		resp->status = DLB2_ST_INVALID_DOMAIN_ID;
-		return -EINVAL;
-	}
+/**
+ * dlb2_hw_get_num_resources() - query the PCI function's available resources
+ * @hw: dlb2_hw handle for a particular device.
+ * @arg: pointer to resource counts.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
+ *
+ * This function returns the number of available resources for the PF or for a
+ * VF.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ *
+ * Return:
+ * Returns 0 upon success, -EINVAL if vdev_req is true and vdev_id is
+ * invalid.
+ */
+int dlb2_hw_get_num_resources(struct dlb2_hw *hw,
+			      struct dlb2_get_num_resources_args *arg,
+			      bool vdev_req,
+			      unsigned int vdev_id)
+{
+	struct dlb2_function_resources *rsrcs;
+	struct dlb2_bitmap *map;
+	int i;
 
-	if (!domain->configured) {
-		resp->status = DLB2_ST_DOMAIN_NOT_CONFIGURED;
+	if (vdev_req && vdev_id >= DLB2_MAX_NUM_VDEVS)
 		return -EINVAL;
-	}
 
-	id = args->port_id;
+	if (vdev_req)
+		rsrcs = &hw->vdev[vdev_id];
+	else
+		rsrcs = &hw->pf;
 
-	port = dlb2_get_domain_used_ldb_port(id, vdev_req, domain);
+	arg->num_sched_domains = rsrcs->num_avail_domains;
 
-	if (!port || !port->configured) {
-		resp->status = DLB2_ST_INVALID_PORT_ID;
-		return -EINVAL;
-	}
+	arg->num_ldb_queues = rsrcs->num_avail_ldb_queues;
 
-	if (args->priority >= DLB2_QID_PRIORITIES) {
-		resp->status = DLB2_ST_INVALID_PRIORITY;
-		return -EINVAL;
-	}
+	arg->num_ldb_ports = 0;
+	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++)
+		arg->num_ldb_ports += rsrcs->num_avail_ldb_ports[i];
 
-	queue = dlb2_get_domain_ldb_queue(args->qid, vdev_req, domain);
+	arg->num_cos_ldb_ports[0] = rsrcs->num_avail_ldb_ports[0];
+	arg->num_cos_ldb_ports[1] = rsrcs->num_avail_ldb_ports[1];
+	arg->num_cos_ldb_ports[2] = rsrcs->num_avail_ldb_ports[2];
+	arg->num_cos_ldb_ports[3] = rsrcs->num_avail_ldb_ports[3];
 
-	if (!queue || !queue->configured) {
-		resp->status = DLB2_ST_INVALID_QID;
-		return -EINVAL;
-	}
+	arg->num_dir_ports = rsrcs->num_avail_dir_pq_pairs;
 
-	if (queue->domain_id.phys_id != domain->id.phys_id) {
-		resp->status = DLB2_ST_INVALID_QID;
-		return -EINVAL;
-	}
+	arg->num_atomic_inflights = rsrcs->num_avail_aqed_entries;
 
-	if (port->domain_id.phys_id != domain->id.phys_id) {
-		resp->status = DLB2_ST_INVALID_PORT_ID;
-		return -EINVAL;
-	}
+	map = rsrcs->avail_hist_list_entries;
 
-	*out_domain = domain;
-	*out_queue = queue;
-	*out_port = port;
+	arg->num_hist_list_entries = dlb2_bitmap_count(map);
 
-	return 0;
-}
+	arg->max_contiguous_hist_list_entries =
+		dlb2_bitmap_longest_set_range(map);
 
-static void dlb2_log_map_qid(struct dlb2_hw *hw,
-			     u32 domain_id,
-			     struct dlb2_map_qid_args *args,
-			     bool vdev_req,
-			     unsigned int vdev_id)
-{
-	DLB2_HW_DBG(hw, "DLB2 map QID arguments:\n");
-	if (vdev_req)
-		DLB2_HW_DBG(hw, "(Request from vdev %d)\n", vdev_id);
-	DLB2_HW_DBG(hw, "\tDomain ID: %d\n",
-		    domain_id);
-	DLB2_HW_DBG(hw, "\tPort ID:   %d\n",
-		    args->port_id);
-	DLB2_HW_DBG(hw, "\tQueue ID:  %d\n",
-		    args->qid);
-	DLB2_HW_DBG(hw, "\tPriority:  %d\n",
-		    args->priority);
+	if (hw->ver == DLB2_HW_V2) {
+		arg->num_ldb_credits = rsrcs->num_avail_qed_entries;
+		arg->num_dir_credits = rsrcs->num_avail_dqed_entries;
+	} else {
+		arg->num_credits = rsrcs->num_avail_entries;
+	}
+	return 0;
 }
 
 /**
- * dlb2_hw_map_qid() - map a load-balanced queue to a load-balanced port
+ * dlb2_hw_get_num_used_resources() - query the PCI function's used resources
  * @hw: dlb2_hw handle for a particular device.
- * @domain_id: domain ID.
- * @args: map QID arguments.
- * @resp: response structure.
+ * @arg: pointer to resource counts.
  * @vdev_req: indicates whether this request came from a vdev.
  * @vdev_id: If vdev_req is true, this contains the vdev's ID.
  *
- * This function configures the DLB to schedule QEs from the specified queue
- * to the specified port. Each load-balanced port can be mapped to up to 8
- * queues; each load-balanced queue can potentially map to all the
- * load-balanced ports.
- *
- * A successful return does not necessarily mean the mapping was configured. If
- * this function is unable to immediately map the queue to the port, it will
- * add the requested operation to a per-port list of pending map/unmap
- * operations, and (if it's not already running) launch a kernel thread that
- * periodically attempts to process all pending operations. In a sense, this is
- * an asynchronous function.
- *
- * This asynchronicity creates two views of the state of hardware: the actual
- * hardware state and the requested state (as if every request completed
- * immediately). If there are any pending map/unmap operations, the requested
- * state will differ from the actual state. All validation is performed with
- * respect to the pending state; for instance, if there are 8 pending map
- * operations for port X, a request for a 9th will fail because a load-balanced
- * port can only map up to 8 queues.
+ * This function returns the number of resources in use by the PF or a VF. It
+ * fills in the fields that args points to, except the following:
+ * - max_contiguous_atomic_inflights
+ * - max_contiguous_hist_list_entries
+ * - max_contiguous_ldb_credits
+ * - max_contiguous_dir_credits
  *
  * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
  * device.
  *
  * Return:
- * Returns 0 upon success, < 0 otherwise. If an error occurs, resp->status is
- * assigned a detailed error code from enum dlb2_error.
- *
- * Errors:
- * EINVAL - A requested resource is unavailable, invalid port or queue ID, or
- *	    the domain is not configured.
- * EFAULT - Internal error (resp->status not set).
- * EBUSY  - The requested port has outstanding detach operations.
+ * Returns 0 upon success, -EINVAL if vdev_req is true and vdev_id is
+ * invalid.
  */
-int dlb2_hw_map_qid(struct dlb2_hw *hw,
-		    u32 domain_id,
-		    struct dlb2_map_qid_args *args,
-		    struct dlb2_cmd_response *resp,
-		    bool vdev_req,
-		    unsigned int vdev_id)
+int dlb2_hw_get_num_used_resources(struct dlb2_hw *hw,
+				   struct dlb2_get_num_resources_args *arg,
+				   bool vdev_req,
+				   unsigned int vdev_id)
 {
+	struct dlb2_list_entry *iter1 __attribute__((unused));
+	struct dlb2_list_entry *iter2 __attribute__((unused));
+	struct dlb2_function_resources *rsrcs;
 	struct dlb2_hw_domain *domain;
-	struct dlb2_ldb_queue *queue;
-	enum dlb2_qid_map_state st;
-	struct dlb2_ldb_port *port;
-	int ret, i;
-	u8 prio;
-
-	dlb2_log_map_qid(hw, domain_id, args, vdev_req, vdev_id);
-
-	/*
-	 * Verify that hardware resources are available before attempting to
-	 * satisfy the request. This simplifies the error unwinding code.
-	 */
-	ret = dlb2_verify_map_qid_args(hw,
-				       domain_id,
-				       args,
-				       resp,
-				       vdev_req,
-				       vdev_id,
-				       &domain,
-				       &port,
-				       &queue);
-	if (ret)
-		return ret;
 
-	prio = args->priority;
+	if (vdev_req && vdev_id >= DLB2_MAX_NUM_VDEVS)
+		return -EINVAL;
 
-	/*
-	 * If there are any outstanding detach operations for this port,
-	 * attempt to complete them. This may be necessary to free up a QID
-	 * slot for this requested mapping.
-	 */
-	if (port->num_pending_removals) {
-		bool bool_ret;
-		bool_ret = dlb2_domain_finish_unmap_port(hw, domain, port);
-		if (!bool_ret)
-			return -EBUSY;
-	}
+	rsrcs = (vdev_req) ? &hw->vdev[vdev_id] : &hw->pf;
 
-	ret = dlb2_verify_map_qid_slot_available(port, queue, resp);
-	if (ret)
-		return ret;
+	memset(arg, 0, sizeof(*arg));
 
-	/* Hardware requires disabling the CQ before mapping QIDs. */
-	if (port->enabled)
-		dlb2_ldb_port_cq_disable(hw, port);
+	DLB2_FUNC_LIST_FOR(rsrcs->used_domains, domain, iter1) {
+		struct dlb2_ldb_queue *queue;
+		struct dlb2_ldb_port *ldb_port;
+		struct dlb2_dir_pq_pair *dir_port;
+		int i;
 
-	/*
-	 * If this is only a priority change, don't perform the full QID->CQ
-	 * mapping procedure
-	 */
-	st = DLB2_QUEUE_MAPPED;
-	if (dlb2_port_find_slot_queue(port, st, queue, &i)) {
-		if (prio != port->qid_map[i].priority) {
-			dlb2_ldb_port_change_qid_priority(hw, port, i, args);
-			DLB2_HW_DBG(hw, "DLB2 map: priority change\n");
-		}
+		arg->num_sched_domains++;
 
-		st = DLB2_QUEUE_MAPPED;
-		ret = dlb2_port_slot_state_transition(hw, port, queue, i, st);
-		if (ret)
-			return ret;
+		arg->num_atomic_inflights += domain->num_used_aqed_entries;
 
-		goto map_qid_done;
-	}
+		DLB2_DOM_LIST_FOR(domain->used_ldb_queues, queue, iter2)
+			arg->num_ldb_queues++;
+		DLB2_DOM_LIST_FOR(domain->avail_ldb_queues, queue, iter2)
+			arg->num_ldb_queues++;
 
-	st = DLB2_QUEUE_UNMAP_IN_PROG;
-	if (dlb2_port_find_slot_queue(port, st, queue, &i)) {
-		if (prio != port->qid_map[i].priority) {
-			dlb2_ldb_port_change_qid_priority(hw, port, i, args);
-			DLB2_HW_DBG(hw, "DLB2 map: priority change\n");
+		for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++) {
+			DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i],
+					  ldb_port, iter2)
+				arg->num_ldb_ports++;
+			DLB2_DOM_LIST_FOR(domain->avail_ldb_ports[i],
+					  ldb_port, iter2)
+				arg->num_ldb_ports++;
+
+			DLB2_DOM_LIST_FOR(domain->used_ldb_ports[i],
+					  ldb_port, iter2)
+				arg->num_cos_ldb_ports[i]++;
+			DLB2_DOM_LIST_FOR(domain->avail_ldb_ports[i],
+					  ldb_port, iter2)
+				arg->num_cos_ldb_ports[i]++;
 		}
 
-		st = DLB2_QUEUE_MAPPED;
-		ret = dlb2_port_slot_state_transition(hw, port, queue, i, st);
-		if (ret)
-			return ret;
+		DLB2_DOM_LIST_FOR(domain->used_dir_pq_pairs, dir_port, iter2)
+			arg->num_dir_ports++;
+		DLB2_DOM_LIST_FOR(domain->avail_dir_pq_pairs, dir_port, iter2)
+			arg->num_dir_ports++;
 
-		goto map_qid_done;
+		if (hw->ver == DLB2_HW_V2_5) {
+			arg->num_credits += domain->num_credits;
+		} else {
+			arg->num_ldb_credits += domain->num_ldb_credits;
+			arg->num_dir_credits += domain->num_dir_credits;
+		}
+		arg->num_hist_list_entries += domain->total_hist_list_entries;
 	}
 
-	/*
-	 * If this is a priority change on an in-progress mapping, don't
-	 * perform the full QID->CQ mapping procedure.
-	 */
-	st = DLB2_QUEUE_MAP_IN_PROG;
-	if (dlb2_port_find_slot_queue(port, st, queue, &i)) {
-		port->qid_map[i].priority = prio;
+	return 0;
+}
 
-		DLB2_HW_DBG(hw, "DLB2 map: priority change only\n");
+static void dlb2_hw_send_async_pf_to_vf_msg(struct dlb2_hw *hw,
+					    unsigned int vf_id)
+{
+	u32 isr = 0;
 
-		goto map_qid_done;
+	switch (vf_id) {
+	case 0:
+		DLB2_BIT_SET(isr, DLB2_PF_PF2VF_MAILBOX_ISR_VF0_ISR);
+		break;
+	case 1:
+		DLB2_BIT_SET(isr, DLB2_PF_PF2VF_MAILBOX_ISR_VF1_ISR);
+		break;
+	case 2:
+		DLB2_BIT_SET(isr, DLB2_PF_PF2VF_MAILBOX_ISR_VF2_ISR);
+		break;
+	case 3:
+		DLB2_BIT_SET(isr, DLB2_PF_PF2VF_MAILBOX_ISR_VF3_ISR);
+		break;
+	case 4:
+		DLB2_BIT_SET(isr, DLB2_PF_PF2VF_MAILBOX_ISR_VF4_ISR);
+		break;
+	case 5:
+		DLB2_BIT_SET(isr, DLB2_PF_PF2VF_MAILBOX_ISR_VF5_ISR);
+		break;
+	case 6:
+		DLB2_BIT_SET(isr, DLB2_PF_PF2VF_MAILBOX_ISR_VF6_ISR);
+		break;
+	case 7:
+		DLB2_BIT_SET(isr, DLB2_PF_PF2VF_MAILBOX_ISR_VF7_ISR);
+		break;
+	case 8:
+		DLB2_BIT_SET(isr, DLB2_PF_PF2VF_MAILBOX_ISR_VF8_ISR);
+		break;
+	case 9:
+		DLB2_BIT_SET(isr, DLB2_PF_PF2VF_MAILBOX_ISR_VF9_ISR);
+		break;
+	case 10:
+		DLB2_BIT_SET(isr, DLB2_PF_PF2VF_MAILBOX_ISR_VF10_ISR);
+		break;
+	case 11:
+		DLB2_BIT_SET(isr, DLB2_PF_PF2VF_MAILBOX_ISR_VF11_ISR);
+		break;
+	case 12:
+		DLB2_BIT_SET(isr, DLB2_PF_PF2VF_MAILBOX_ISR_VF12_ISR);
+		break;
+	case 13:
+		DLB2_BIT_SET(isr, DLB2_PF_PF2VF_MAILBOX_ISR_VF13_ISR);
+		break;
+	case 14:
+		DLB2_BIT_SET(isr, DLB2_PF_PF2VF_MAILBOX_ISR_VF14_ISR);
+		break;
+	case 15:
+		DLB2_BIT_SET(isr, DLB2_PF_PF2VF_MAILBOX_ISR_VF15_ISR);
+		break;
+	default:
+		break;
 	}
 
-	/*
-	 * If this is a priority change on a pending mapping, update the
-	 * pending priority
-	 */
-	if (dlb2_port_find_slot_with_pending_map_queue(port, queue, &i)) {
-		port->qid_map[i].pending_priority = prio;
+	DLB2_FUNC_WR(hw, DLB2_PF_PF2VF_MAILBOX_ISR(0), isr);
+}
 
-		DLB2_HW_DBG(hw, "DLB2 map: priority change only\n");
+static void dlb2_sw_send_async_pf_to_vdev_msg(struct dlb2_hw *hw,
+					      unsigned int vdev_id)
+{
+	void *arg = hw->mbox[vdev_id].pf_to_vdev_inject_arg;
 
-		goto map_qid_done;
-	}
+	/* Set the ISR in progress bit. The vdev driver will clear it. */
+	*hw->mbox[vdev_id].pf_to_vdev.isr_in_progress = 1;
 
-	/*
-	 * If all the CQ's slots are in use, then there's an unmap in progress
-	 * (guaranteed by dlb2_verify_map_qid_slot_available()), so add this
-	 * mapping to pending_map and return. When the removal is completed for
-	 * the slot's current occupant, this mapping will be performed.
-	 */
-	if (!dlb2_port_find_slot(port, DLB2_QUEUE_UNMAPPED, &i)) {
-		if (dlb2_port_find_slot(port, DLB2_QUEUE_UNMAP_IN_PROG, &i)) {
-			enum dlb2_qid_map_state new_st;
+	hw->mbox[vdev_id].pf_to_vdev_inject(arg);
+}
 
-			port->qid_map[i].pending_qid = queue->id.phys_id;
-			port->qid_map[i].pending_priority = prio;
+/**
+ * dlb2_send_async_pf_to_vdev_msg() - (PF only) send a mailbox message to a
+ *					vdev
+ * @hw: dlb2_hw handle for a particular device.
+ * @vdev_id: vdev ID.
+ *
+ * This function sends a PF->vdev mailbox message. It is asynchronous, so it
+ * returns once the message is sent but potentially before the vdev has
+ * processed the message. The caller must call dlb2_pf_to_vdev_complete() to
+ * determine when the vdev has finished processing the request.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ */
+void dlb2_send_async_pf_to_vdev_msg(struct dlb2_hw *hw, unsigned int vdev_id)
+{
+	if (hw->virt_mode == DLB2_VIRT_SIOV)
+		dlb2_sw_send_async_pf_to_vdev_msg(hw, vdev_id);
+	else
+		dlb2_hw_send_async_pf_to_vf_msg(hw, vdev_id);
+}
 
-			new_st = DLB2_QUEUE_UNMAP_IN_PROG_PENDING_MAP;
+static bool dlb2_hw_pf_to_vf_complete(struct dlb2_hw *hw, unsigned int vf_id)
+{
+	u32 isr;
 
-			ret = dlb2_port_slot_state_transition(hw, port, queue,
-							      i, new_st);
-			if (ret)
-				return ret;
+	isr = DLB2_FUNC_RD(hw, DLB2_PF_PF2VF_MAILBOX_ISR(vf_id));
 
-			DLB2_HW_DBG(hw, "DLB2 map: map pending removal\n");
+	return (isr & (1 << vf_id)) == 0;
+}
 
-			goto map_qid_done;
-		}
-	}
+static bool dlb2_sw_pf_to_vdev_complete(struct dlb2_hw *hw,
+					unsigned int vdev_id)
+{
+	return !(*hw->mbox[vdev_id].pf_to_vdev.isr_in_progress);
+}
 
-	/*
-	 * If the domain has started, a special "dynamic" CQ->queue mapping
-	 * procedure is required in order to safely update the CQ<->QID tables.
-	 * The "static" procedure cannot be used when traffic is flowing,
-	 * because the CQ<->QID tables cannot be updated atomically and the
-	 * scheduler won't see the new mapping unless the queue's if_status
-	 * changes, which isn't guaranteed.
-	 */
-	ret = dlb2_ldb_port_map_qid(hw, domain, port, queue, prio);
+/**
+ * dlb2_pf_to_vdev_complete() - check the status of an asynchronous mailbox
+ *			       request
+ * @hw: dlb2_hw handle for a particular device.
+ * @vdev_id: vdev ID.
+ *
+ * This function returns a boolean indicating whether the vdev has finished
+ * processing a PF->vdev mailbox request. It should only be called after
+ * sending an asynchronous request with dlb2_send_async_pf_to_vdev_msg().
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ */
+bool dlb2_pf_to_vdev_complete(struct dlb2_hw *hw, unsigned int vdev_id)
+{
+	if (hw->virt_mode == DLB2_VIRT_SIOV)
+		return dlb2_sw_pf_to_vdev_complete(hw, vdev_id);
+	else
+		return dlb2_hw_pf_to_vf_complete(hw, vdev_id);
+}
 
-	/* If ret is less than zero, it's due to an internal error */
-	if (ret < 0)
-		return ret;
+/**
+ * dlb2_send_async_vdev_to_pf_msg() - (vdev only) send a mailbox message to
+ *				       the PF
+ * @hw: dlb2_hw handle for a particular device.
+ *
+ * This function sends a VF->PF mailbox message. It is asynchronous, so it
+ * returns once the message is sent but potentially before the PF has processed
+ * the message. The caller must call dlb2_vdev_to_pf_complete() to determine
+ * when the PF has finished processing the request.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ */
+void dlb2_send_async_vdev_to_pf_msg(struct dlb2_hw *hw)
+{
+	u32 isr = 0;
+	u32 offs;
 
-map_qid_done:
-	if (port->enabled)
-		dlb2_ldb_port_cq_enable(hw, port);
+	if (hw->virt_mode == DLB2_VIRT_SIOV)
+		offs = DLB2_VF_SIOV_MBOX_ISR_TRIGGER;
+	else
+		offs = DLB2_VF_VF2PF_MAILBOX_ISR;
 
-	resp->status = 0;
+	DLB2_BIT_SET(isr, DLB2_VF_VF2PF_MAILBOX_ISR_ISR);
 
-	return 0;
+	DLB2_FUNC_WR(hw, offs, isr);
 }
 
-static void dlb2_log_unmap_qid(struct dlb2_hw *hw,
-			       u32 domain_id,
-			       struct dlb2_unmap_qid_args *args,
-			       bool vdev_req,
-			       unsigned int vdev_id)
+/**
+ * dlb2_vdev_to_pf_complete() - check the status of an asynchronous mailbox
+ *				 request
+ * @hw: dlb2_hw handle for a particular device.
+ *
+ * This function returns a boolean indicating whether the PF has finished
+ * processing a VF->PF mailbox request. It should only be called after sending
+ * an asynchronous request with dlb2_send_async_vdev_to_pf_msg().
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ */
+bool dlb2_vdev_to_pf_complete(struct dlb2_hw *hw)
 {
-	DLB2_HW_DBG(hw, "DLB2 unmap QID arguments:\n");
-	if (vdev_req)
-		DLB2_HW_DBG(hw, "(Request from vdev %d)\n", vdev_id);
-	DLB2_HW_DBG(hw, "\tDomain ID: %d\n",
-		    domain_id);
-	DLB2_HW_DBG(hw, "\tPort ID:   %d\n",
-		    args->port_id);
-	DLB2_HW_DBG(hw, "\tQueue ID:  %d\n",
-		    args->qid);
-	if (args->qid < DLB2_MAX_NUM_LDB_QUEUES)
-		DLB2_HW_DBG(hw, "\tQueue's num mappings:  %d\n",
-			    hw->rsrcs.ldb_queues[args->qid].num_mappings);
+	u32 isr;
+
+	isr = DLB2_FUNC_RD(hw, DLB2_VF_VF2PF_MAILBOX_ISR);
+
+	return (DLB2_BITS_GET(isr, DLB2_VF_VF2PF_MAILBOX_ISR_ISR) == 0);
 }
 
-static int dlb2_verify_unmap_qid_args(struct dlb2_hw *hw,
-				      u32 domain_id,
-				      struct dlb2_unmap_qid_args *args,
-				      struct dlb2_cmd_response *resp,
-				      bool vdev_req,
-				      unsigned int vdev_id,
-				      struct dlb2_hw_domain **out_domain,
-				      struct dlb2_ldb_port **out_port,
-				      struct dlb2_ldb_queue **out_queue)
+/**
+ * dlb2_vf_flr_complete() - check the status of a VF FLR
+ * @hw: dlb2_hw handle for a particular device.
+ *
+ * This function returns a boolean indicating whether the PF has finished
+ * executing the VF FLR. It should only be called after setting the VF's FLR
+ * bit.
+ */
+bool dlb2_vf_flr_complete(struct dlb2_hw *hw)
 {
-	enum dlb2_qid_map_state state;
-	struct dlb2_hw_domain *domain;
-	struct dlb2_ldb_queue *queue;
-	struct dlb2_ldb_port *port;
-	int slot;
-	int id;
+	u32 rip;
 
-	domain = dlb2_get_domain_from_id(hw, domain_id, vdev_req, vdev_id);
+	rip = DLB2_FUNC_RD(hw, DLB2_VF_VF_RESET_IN_PROGRESS);
 
-	if (!domain) {
-		resp->status = DLB2_ST_INVALID_DOMAIN_ID;
-		return -EINVAL;
-	}
+	return (DLB2_BITS_GET(rip,
+			      DLB2_VF_VF_RESET_IN_PROGRESS_RESET_IN_PROGRESS)
+		== 0);
+}
 
-	if (!domain->configured) {
-		resp->status = DLB2_ST_DOMAIN_NOT_CONFIGURED;
-		return -EINVAL;
-	}
+static u32 dlb2_read_vf2pf_mbox(struct dlb2_hw *hw,
+				unsigned int id,
+				int offs,
+				bool req)
+{
+	u32 idx = offs;
 
-	id = args->port_id;
+	if (req)
+		idx += DLB2_VF2PF_REQ_BASE_WORD;
+	else
+		idx += DLB2_VF2PF_RESP_BASE_WORD;
 
-	port = dlb2_get_domain_used_ldb_port(id, vdev_req, domain);
+	if (hw->virt_mode == DLB2_VIRT_SIOV)
+		return hw->mbox[id].vdev_to_pf.mbox[idx];
+	else
+		return DLB2_FUNC_RD(hw, DLB2_PF_VF2PF_MAILBOX(id, idx));
+}
 
-	if (!port || !port->configured) {
-		resp->status = DLB2_ST_INVALID_PORT_ID;
+/**
+ * dlb2_pf_read_vf_mbox_req() - (PF only) read a VF->PF mailbox request
+ * @hw: dlb2_hw handle for a particular device.
+ * @vdev_id: vdev ID.
+ * @data: pointer to message data.
+ * @len: size, in bytes, of the data array.
+ *
+ * This function copies one of the PF's VF->PF mailboxes into the array pointed
+ * to by data.
+ *
+ * Return:
+ * Returns 0 upon success, <0 otherwise.
+ *
+ * EINVAL - len >= DLB2_VF2PF_REQ_BYTES.
+ */
+int dlb2_pf_read_vf_mbox_req(struct dlb2_hw *hw,
+			     unsigned int vdev_id,
+			     void *data,
+			     int len)
+{
+	u32 buf[DLB2_VF2PF_REQ_BYTES / 4];
+	int num_words;
+	int i;
+
+	if (len > DLB2_VF2PF_REQ_BYTES) {
+		DLB2_HW_ERR(hw,
+			    "[%s()] len (%d) > VF->PF mailbox req size\n",
+			    __func__, len);
 		return -EINVAL;
 	}
 
-	if (port->domain_id.phys_id != domain->id.phys_id) {
-		resp->status = DLB2_ST_INVALID_PORT_ID;
+	if (len == 0) {
+		DLB2_HW_ERR(hw, "[%s()] invalid len (0)\n", __func__);
 		return -EINVAL;
 	}
 
-	queue = dlb2_get_domain_ldb_queue(args->qid, vdev_req, domain);
-
-	if (!queue || !queue->configured) {
-		DLB2_HW_ERR(hw, "[%s()] Can't unmap unconfigured queue %d\n",
-			    __func__, args->qid);
-		resp->status = DLB2_ST_INVALID_QID;
+	if (hw->virt_mode == DLB2_VIRT_SIOV &&
+	    !hw->mbox[vdev_id].vdev_to_pf.mbox) {
+		DLB2_HW_ERR(hw, "[%s()] No mailbox registered for vdev %d\n",
+			    __func__, vdev_id);
 		return -EINVAL;
 	}
 
 	/*
-	 * Verify that the port has the queue mapped. From the application's
-	 * perspective a queue is mapped if it is actually mapped, the map is
-	 * in progress, or the map is blocked pending an unmap.
+	 * Round up len to the nearest 4B boundary, since the mailbox registers
+	 * are 32b wide.
 	 */
-	state = DLB2_QUEUE_MAPPED;
-	if (dlb2_port_find_slot_queue(port, state, queue, &slot))
-		goto done;
-
-	state = DLB2_QUEUE_MAP_IN_PROG;
-	if (dlb2_port_find_slot_queue(port, state, queue, &slot))
-		goto done;
-
-	if (dlb2_port_find_slot_with_pending_map_queue(port, queue, &slot))
-		goto done;
+	num_words = len / 4;
+	if (len % 4 != 0)
+		num_words++;
 
-	resp->status = DLB2_ST_INVALID_QID;
-	return -EINVAL;
+	for (i = 0; i < num_words; i++)
+		buf[i] = dlb2_read_vf2pf_mbox(hw, vdev_id, i, true);
 
-done:
-	*out_domain = domain;
-	*out_port = port;
-	*out_queue = queue;
+	memcpy(data, buf, len);
 
 	return 0;
 }
 
 /**
- * dlb2_hw_unmap_qid() - Unmap a load-balanced queue from a load-balanced port
+ * dlb2_pf_read_vf_mbox_resp() - (PF only) read a VF->PF mailbox response
  * @hw: dlb2_hw handle for a particular device.
- * @domain_id: domain ID.
- * @args: unmap QID arguments.
- * @resp: response structure.
- * @vdev_req: indicates whether this request came from a vdev.
- * @vdev_id: If vdev_req is true, this contains the vdev's ID.
- *
- * This function configures the DLB to stop scheduling QEs from the specified
- * queue to the specified port.
+ * @vdev_id: vdev ID.
+ * @data: pointer to message data.
+ * @len: size, in bytes, of the data array.
  *
- * A successful return does not necessarily mean the mapping was removed. If
- * this function is unable to immediately unmap the queue from the port, it
- * will add the requested operation to a per-port list of pending map/unmap
- * operations, and (if it's not already running) launch a kernel thread that
- * periodically attempts to process all pending operations. See
- * dlb2_hw_map_qid() for more details.
- *
- * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
- * device.
+ * This function copies one of the PF's VF->PF mailboxes into the array pointed
+ * to by data.
  *
  * Return:
- * Returns 0 upon success, < 0 otherwise. If an error occurs, resp->status is
- * assigned a detailed error code from enum dlb2_error.
+ * Returns 0 upon success, <0 otherwise.
  *
- * Errors:
- * EINVAL - A requested resource is unavailable, invalid port or queue ID, or
- *	    the domain is not configured.
- * EFAULT - Internal error (resp->status not set).
+ * EINVAL - len >= DLB2_VF2PF_RESP_BYTES.
  */
-int dlb2_hw_unmap_qid(struct dlb2_hw *hw,
-		      u32 domain_id,
-		      struct dlb2_unmap_qid_args *args,
-		      struct dlb2_cmd_response *resp,
-		      bool vdev_req,
-		      unsigned int vdev_id)
+int dlb2_pf_read_vf_mbox_resp(struct dlb2_hw *hw,
+			      unsigned int vdev_id,
+			      void *data,
+			      int len)
 {
-	struct dlb2_hw_domain *domain;
-	struct dlb2_ldb_queue *queue;
-	enum dlb2_qid_map_state st;
-	struct dlb2_ldb_port *port;
-	bool unmap_complete;
-	int i, ret;
+	u32 buf[DLB2_VF2PF_RESP_BYTES / 4];
+	int num_words;
+	int i;
 
-	dlb2_log_unmap_qid(hw, domain_id, args, vdev_req, vdev_id);
+	if (len > DLB2_VF2PF_RESP_BYTES) {
+		DLB2_HW_ERR(hw,
+			    "[%s()] len (%d) > VF->PF mailbox resp size\n",
+			    __func__, len);
+		return -EINVAL;
+	}
 
 	/*
-	 * Verify that hardware resources are available before attempting to
-	 * satisfy the request. This simplifies the error unwinding code.
+	 * Round up len to the nearest 4B boundary, since the mailbox registers
+	 * are 32b wide.
 	 */
-	ret = dlb2_verify_unmap_qid_args(hw,
-					 domain_id,
-					 args,
-					 resp,
-					 vdev_req,
-					 vdev_id,
-					 &domain,
-					 &port,
-					 &queue);
-	if (ret)
-		return ret;
+	num_words = len / 4;
+	if (len % 4 != 0)
+		num_words++;
 
-	/*
-	 * If the queue hasn't been mapped yet, we need to update the slot's
-	 * state and re-enable the queue's inflights.
-	 */
-	st = DLB2_QUEUE_MAP_IN_PROG;
-	if (dlb2_port_find_slot_queue(port, st, queue, &i)) {
-		/*
-		 * Since the in-progress map was aborted, re-enable the QID's
-		 * inflights.
-		 */
-		if (queue->num_pending_additions == 0)
-			dlb2_ldb_queue_set_inflight_limit(hw, queue);
+	for (i = 0; i < num_words; i++)
+		buf[i] = dlb2_read_vf2pf_mbox(hw, vdev_id, i, false);
 
-		st = DLB2_QUEUE_UNMAPPED;
-		ret = dlb2_port_slot_state_transition(hw, port, queue, i, st);
-		if (ret)
-			return ret;
+	memcpy(data, buf, len);
 
-		goto unmap_qid_done;
-	}
+	return 0;
+}
 
-	/*
-	 * If the queue mapping is on hold pending an unmap, we simply need to
-	 * update the slot's state.
-	 */
-	if (dlb2_port_find_slot_with_pending_map_queue(port, queue, &i)) {
-		st = DLB2_QUEUE_UNMAP_IN_PROG;
-		ret = dlb2_port_slot_state_transition(hw, port, queue, i, st);
-		if (ret)
-			return ret;
+static void dlb2_write_pf2vf_mbox_resp(struct dlb2_hw *hw,
+				       unsigned int vdev_id,
+				       int offs,
+				       u32 data)
+{
+	u32 idx = offs + DLB2_PF2VF_RESP_BASE_WORD;
 
-		goto unmap_qid_done;
-	}
+	if (hw->virt_mode == DLB2_VIRT_SIOV)
+		hw->mbox[vdev_id].pf_to_vdev.mbox[idx] = data;
+	else
+		DLB2_FUNC_WR(hw,
+			     DLB2_PF_PF2VF_MAILBOX(vdev_id, idx),
+			     data);
+}
 
-	st = DLB2_QUEUE_MAPPED;
-	if (!dlb2_port_find_slot_queue(port, st, queue, &i)) {
+/**
+ * dlb2_pf_write_vf_mbox_resp() - (PF only) write a PF->VF mailbox response
+ * @hw: dlb2_hw handle for a particular device.
+ * @vdev_id: vdev ID.
+ * @data: pointer to message data.
+ * @len: size, in bytes, of the data array.
+ *
+ * This function copies the user-provided message data into of the PF's VF->PF
+ * mailboxes.
+ *
+ * Return:
+ * Returns 0 upon success, <0 otherwise.
+ *
+ * EINVAL - len >= DLB2_PF2VF_RESP_BYTES.
+ */
+int dlb2_pf_write_vf_mbox_resp(struct dlb2_hw *hw,
+			       unsigned int vdev_id,
+			       void *data,
+			       int len)
+{
+	u32 buf[DLB2_PF2VF_RESP_BYTES / 4];
+	int num_words;
+	int i;
+
+	if (len > DLB2_PF2VF_RESP_BYTES) {
 		DLB2_HW_ERR(hw,
-			    "[%s()] Internal error: no available CQ slots\n",
-			    __func__);
-		return -EFAULT;
+			    "[%s()] len (%d) > PF->VF mailbox resp size\n",
+			    __func__, len);
+		return -EINVAL;
+	}
+
+	if (hw->virt_mode == DLB2_VIRT_SIOV &&
+	    !hw->mbox[vdev_id].pf_to_vdev.mbox) {
+		DLB2_HW_ERR(hw, "[%s()] No mailbox registered for vdev %d\n",
+			    __func__, vdev_id);
+		return -EINVAL;
 	}
 
+	memcpy(buf, data, len);
+
 	/*
-	 * QID->CQ mapping removal is an asynchronous procedure. It requires
-	 * stopping the DLB2 from scheduling this CQ, draining all inflights
-	 * from the CQ, then unmapping the queue from the CQ. This function
-	 * simply marks the port as needing the queue unmapped, and (if
-	 * necessary) starts the unmapping worker thread.
+	 * Round up len to the nearest 4B boundary, since the mailbox registers
+	 * are 32b wide.
 	 */
-	dlb2_ldb_port_cq_disable(hw, port);
+	num_words = len / 4;
+	if (len % 4 != 0)
+		num_words++;
 
-	st = DLB2_QUEUE_UNMAP_IN_PROG;
-	ret = dlb2_port_slot_state_transition(hw, port, queue, i, st);
-	if (ret)
-		return ret;
+	for (i = 0; i < num_words; i++)
+		dlb2_write_pf2vf_mbox_resp(hw, vdev_id, i, buf[i]);
+
+	return 0;
+}
+
+static void dlb2_write_pf2vf_mbox_req(struct dlb2_hw *hw,
+				      unsigned int vdev_id,
+				      int offs,
+				      u32 data)
+{
+	u32 idx = offs + DLB2_PF2VF_REQ_BASE_WORD;
+
+	if (hw->virt_mode == DLB2_VIRT_SIOV)
+		hw->mbox[vdev_id].pf_to_vdev.mbox[idx] = data;
+	else
+		DLB2_FUNC_WR(hw,
+			     DLB2_PF_PF2VF_MAILBOX(vdev_id, idx),
+			     data);
+}
+
+/**
+ * dlb2_pf_write_vf_mbox_req() - (PF only) write a PF->VF mailbox request
+ * @hw: dlb2_hw handle for a particular device.
+ * @vdev_id: vdev ID.
+ * @data: pointer to message data.
+ * @len: size, in bytes, of the data array.
+ *
+ * This function copies the user-provided message data into of the PF's VF->PF
+ * mailboxes.
+ *
+ * Return:
+ * Returns 0 upon success, <0 otherwise.
+ *
+ * EINVAL - len >= DLB2_PF2VF_REQ_BYTES.
+ */
+int dlb2_pf_write_vf_mbox_req(struct dlb2_hw *hw,
+			      unsigned int vdev_id,
+			      void *data,
+			      int len)
+{
+	u32 buf[DLB2_PF2VF_REQ_BYTES / 4];
+	int num_words;
+	int i;
 
-	/*
-	 * Attempt to finish the unmapping now, in case the port has no
-	 * outstanding inflights. If that's not the case, this will fail and
-	 * the unmapping will be completed at a later time.
-	 */
-	unmap_complete = dlb2_domain_finish_unmap_port(hw, domain, port);
+	if (len > DLB2_PF2VF_REQ_BYTES) {
+		DLB2_HW_ERR(hw,
+			    "[%s()] len (%d) > PF->VF mailbox req size\n",
+			    __func__, len);
+		return -EINVAL;
+	}
+
+	memcpy(buf, data, len);
 
 	/*
-	 * If the unmapping couldn't complete immediately, launch the worker
-	 * thread (if it isn't already launched) to finish it later.
+	 * Round up len to the nearest 4B boundary, since the mailbox registers
+	 * are 32b wide.
 	 */
-	if (!unmap_complete && !os_worker_active(hw))
-		os_schedule_work(hw);
+	num_words = len / 4;
+	if (len % 4 != 0)
+		num_words++;
 
-unmap_qid_done:
-	resp->status = 0;
+	for (i = 0; i < num_words; i++)
+		dlb2_write_pf2vf_mbox_req(hw, vdev_id, i, buf[i]);
 
 	return 0;
 }
 
-static void
-dlb2_log_pending_port_unmaps_args(struct dlb2_hw *hw,
-				  struct dlb2_pending_port_unmaps_args *args,
-				  bool vdev_req,
-				  unsigned int vdev_id)
-{
-	DLB2_HW_DBG(hw, "DLB unmaps in progress arguments:\n");
-	if (vdev_req)
-		DLB2_HW_DBG(hw, "(Request from VF %d)\n", vdev_id);
-	DLB2_HW_DBG(hw, "\tPort ID: %d\n", args->port_id);
-}
-
 /**
- * dlb2_hw_pending_port_unmaps() - returns the number of unmap operations in
- *	progress.
+ * dlb2_vf_read_pf_mbox_resp() - (VF only) read a PF->VF mailbox response
  * @hw: dlb2_hw handle for a particular device.
- * @domain_id: domain ID.
- * @args: number of unmaps in progress args
- * @resp: response structure.
- * @vdev_req: indicates whether this request came from a vdev.
- * @vdev_id: If vdev_req is true, this contains the vdev's ID.
+ * @data: pointer to message data.
+ * @len: size, in bytes, of the data array.
+ *
+ * This function copies the VF's PF->VF mailbox into the array pointed to by
+ * data.
  *
  * Return:
- * Returns 0 upon success, < 0 otherwise. If an error occurs, resp->status is
- * assigned a detailed error code from enum dlb2_error. If successful, resp->id
- * contains the number of unmaps in progress.
+ * Returns 0 upon success, <0 otherwise.
  *
- * Errors:
- * EINVAL - Invalid port ID.
+ * EINVAL - len >= DLB2_PF2VF_RESP_BYTES.
  */
-int dlb2_hw_pending_port_unmaps(struct dlb2_hw *hw,
-				u32 domain_id,
-				struct dlb2_pending_port_unmaps_args *args,
-				struct dlb2_cmd_response *resp,
-				bool vdev_req,
-				unsigned int vdev_id)
+int dlb2_vf_read_pf_mbox_resp(struct dlb2_hw *hw, void *data, int len)
 {
-	struct dlb2_hw_domain *domain;
-	struct dlb2_ldb_port *port;
-
-	dlb2_log_pending_port_unmaps_args(hw, args, vdev_req, vdev_id);
-
-	domain = dlb2_get_domain_from_id(hw, domain_id, vdev_req, vdev_id);
+	u32 buf[DLB2_PF2VF_RESP_BYTES / 4];
+	int num_words;
+	int i;
 
-	if (!domain) {
-		resp->status = DLB2_ST_INVALID_DOMAIN_ID;
+	if (len > DLB2_PF2VF_RESP_BYTES) {
+		DLB2_HW_ERR(hw,
+			    "[%s()] len (%d) > PF->VF mailbox resp size\n",
+			    __func__, len);
 		return -EINVAL;
 	}
 
-	port = dlb2_get_domain_used_ldb_port(args->port_id, vdev_req, domain);
-	if (!port || !port->configured) {
-		resp->status = DLB2_ST_INVALID_PORT_ID;
+	if (len == 0) {
+		DLB2_HW_ERR(hw, "[%s()] invalid len (0)\n", __func__);
 		return -EINVAL;
 	}
 
-	resp->id = port->num_pending_removals;
+	/*
+	 * Round up len to the nearest 4B boundary, since the mailbox registers
+	 * are 32b wide.
+	 */
+	num_words = len / 4;
+	if (len % 4 != 0)
+		num_words++;
+
+	for (i = 0; i < num_words; i++) {
+		u32 idx = i + DLB2_PF2VF_RESP_BASE_WORD;
+
+		buf[i] = DLB2_FUNC_RD(hw, DLB2_VF_PF2VF_MAILBOX(idx));
+	}
+
+	memcpy(data, buf, len);
 
 	return 0;
 }
 
-static int dlb2_verify_start_domain_args(struct dlb2_hw *hw,
-					 u32 domain_id,
-					 struct dlb2_cmd_response *resp,
-					 bool vdev_req,
-					 unsigned int vdev_id,
-					 struct dlb2_hw_domain **out_domain)
+/**
+ * dlb2_vf_read_pf_mbox_req() - (VF only) read a PF->VF mailbox request
+ * @hw: dlb2_hw handle for a particular device.
+ * @data: pointer to message data.
+ * @len: size, in bytes, of the data array.
+ *
+ * This function copies the VF's PF->VF mailbox into the array pointed to by
+ * data.
+ *
+ * Return:
+ * Returns 0 upon success, <0 otherwise.
+ *
+ * EINVAL - len >= DLB2_PF2VF_REQ_BYTES.
+ */
+int dlb2_vf_read_pf_mbox_req(struct dlb2_hw *hw, void *data, int len)
 {
-	struct dlb2_hw_domain *domain;
-
-	domain = dlb2_get_domain_from_id(hw, domain_id, vdev_req, vdev_id);
+	u32 buf[DLB2_PF2VF_REQ_BYTES / 4];
+	int num_words;
+	int i;
 
-	if (!domain) {
-		resp->status = DLB2_ST_INVALID_DOMAIN_ID;
+	if (len > DLB2_PF2VF_REQ_BYTES) {
+		DLB2_HW_ERR(hw,
+			    "[%s()] len (%d) > PF->VF mailbox req size\n",
+			    __func__, len);
 		return -EINVAL;
 	}
 
-	if (!domain->configured) {
-		resp->status = DLB2_ST_DOMAIN_NOT_CONFIGURED;
-		return -EINVAL;
-	}
+	/*
+	 * Round up len to the nearest 4B boundary, since the mailbox registers
+	 * are 32b wide.
+	 */
+	num_words = len / 4;
+	if ((len % 4) != 0)
+		num_words++;
 
-	if (domain->started) {
-		resp->status = DLB2_ST_DOMAIN_STARTED;
-		return -EINVAL;
+	for (i = 0; i < num_words; i++) {
+		u32 idx = i + DLB2_PF2VF_REQ_BASE_WORD;
+
+		buf[i] = DLB2_FUNC_RD(hw, DLB2_VF_PF2VF_MAILBOX(idx));
 	}
 
-	*out_domain = domain;
+	memcpy(data, buf, len);
 
 	return 0;
 }
 
-static void dlb2_log_start_domain(struct dlb2_hw *hw,
-				  u32 domain_id,
-				  bool vdev_req,
-				  unsigned int vdev_id)
+/**
+ * dlb2_vf_write_pf_mbox_req() - (VF only) write a VF->PF mailbox request
+ * @hw: dlb2_hw handle for a particular device.
+ * @data: pointer to message data.
+ * @len: size, in bytes, of the data array.
+ *
+ * This function copies the user-provided message data into of the VF's PF->VF
+ * mailboxes.
+ *
+ * Return:
+ * Returns 0 upon success, <0 otherwise.
+ *
+ * EINVAL - len >= DLB2_VF2PF_REQ_BYTES.
+ */
+int dlb2_vf_write_pf_mbox_req(struct dlb2_hw *hw, void *data, int len)
 {
-	DLB2_HW_DBG(hw, "DLB2 start domain arguments:\n");
-	if (vdev_req)
-		DLB2_HW_DBG(hw, "(Request from vdev %d)\n", vdev_id);
-	DLB2_HW_DBG(hw, "\tDomain ID: %d\n", domain_id);
+	u32 buf[DLB2_VF2PF_REQ_BYTES / 4];
+	int num_words;
+	int i;
+
+	if (len > DLB2_VF2PF_REQ_BYTES) {
+		DLB2_HW_ERR(hw,
+			    "[%s()] len (%d) > VF->PF mailbox req size\n",
+			    __func__, len);
+		return -EINVAL;
+	}
+
+	memcpy(buf, data, len);
+
+	/*
+	 * Round up len to the nearest 4B boundary, since the mailbox registers
+	 * are 32b wide.
+	 */
+	num_words = len / 4;
+	if (len % 4 != 0)
+		num_words++;
+
+	for (i = 0; i < num_words; i++) {
+		u32 idx = i + DLB2_VF2PF_REQ_BASE_WORD;
+
+		DLB2_FUNC_WR(hw, DLB2_VF_VF2PF_MAILBOX(idx), buf[i]);
+	}
+
+	return 0;
 }
 
 /**
- * dlb2_hw_start_domain() - start a scheduling domain
+ * dlb2_vf_write_pf_mbox_resp() - (VF only) write a VF->PF mailbox response
  * @hw: dlb2_hw handle for a particular device.
- * @domain_id: domain ID.
- * @arg: start domain arguments.
- * @resp: response structure.
- * @vdev_req: indicates whether this request came from a vdev.
- * @vdev_id: If vdev_req is true, this contains the vdev's ID.
- *
- * This function starts a scheduling domain, which allows applications to send
- * traffic through it. Once a domain is started, its resources can no longer be
- * configured (besides QID remapping and port enable/disable).
+ * @data: pointer to message data.
+ * @len: size, in bytes, of the data array.
  *
- * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
- * device.
+ * This function copies the user-provided message data into of the VF's PF->VF
+ * mailboxes.
  *
  * Return:
- * Returns 0 upon success, < 0 otherwise. If an error occurs, resp->status is
- * assigned a detailed error code from enum dlb2_error.
+ * Returns 0 upon success, <0 otherwise.
  *
- * Errors:
- * EINVAL - the domain is not configured, or the domain is already started.
+ * EINVAL - len >= DLB2_VF2PF_RESP_BYTES.
  */
-int
-dlb2_hw_start_domain(struct dlb2_hw *hw,
-		     u32 domain_id,
-		     struct dlb2_start_domain_args *args,
-		     struct dlb2_cmd_response *resp,
-		     bool vdev_req,
-		     unsigned int vdev_id)
+int dlb2_vf_write_pf_mbox_resp(struct dlb2_hw *hw, void *data, int len)
 {
-	struct dlb2_list_entry *iter;
-	struct dlb2_dir_pq_pair *dir_queue;
-	struct dlb2_ldb_queue *ldb_queue;
-	struct dlb2_hw_domain *domain;
-	int ret;
-	RTE_SET_USED(args);
-	RTE_SET_USED(iter);
+	u32 buf[DLB2_VF2PF_RESP_BYTES / 4];
+	int num_words;
+	int i;
 
-	dlb2_log_start_domain(hw, domain_id, vdev_req, vdev_id);
+	if (len > DLB2_VF2PF_RESP_BYTES) {
+		DLB2_HW_ERR(hw,
+			    "[%s()] len (%d) > VF->PF mailbox resp size\n",
+			    __func__, len);
+		return -EINVAL;
+	}
 
-	ret = dlb2_verify_start_domain_args(hw,
-					    domain_id,
-					    resp,
-					    vdev_req,
-					    vdev_id,
-					    &domain);
-	if (ret)
-		return ret;
+	memcpy(buf, data, len);
 
 	/*
-	 * Enable load-balanced and directed queue write permissions for the
-	 * queues this domain owns. Without this, the DLB2 will drop all
-	 * incoming traffic to those queues.
+	 * Round up len to the nearest 4B boundary, since the mailbox registers
+	 * are 32b wide.
 	 */
-	DLB2_DOM_LIST_FOR(domain->used_ldb_queues, ldb_queue, iter) {
-		u32 vasqid_v = 0;
-		unsigned int offs;
+	num_words = len / 4;
+	if (len % 4 != 0)
+		num_words++;
 
-		DLB2_BIT_SET(vasqid_v, DLB2_SYS_LDB_VASQID_V_VASQID_V);
+	for (i = 0; i < num_words; i++) {
+		u32 idx = i + DLB2_VF2PF_RESP_BASE_WORD;
 
-		offs = domain->id.phys_id * DLB2_MAX_NUM_LDB_QUEUES +
-			ldb_queue->id.phys_id;
-
-		DLB2_CSR_WR(hw, DLB2_SYS_LDB_VASQID_V(offs), vasqid_v);
+		DLB2_FUNC_WR(hw, DLB2_VF_VF2PF_MAILBOX(idx), buf[i]);
 	}
 
-	DLB2_DOM_LIST_FOR(domain->used_dir_pq_pairs, dir_queue, iter) {
-		u32 vasqid_v = 0;
-		unsigned int offs;
+	return 0;
+}
 
-		DLB2_BIT_SET(vasqid_v, DLB2_SYS_DIR_VASQID_V_VASQID_V);
+/**
+ * dlb2_vdev_is_locked() - check whether the vdev's resources are locked
+ * @hw: dlb2_hw handle for a particular device.
+ * @id: virtual device ID
+ *
+ * This function returns whether or not the vdev's resource assignments are
+ * locked. If locked, no resources can be added to or subtracted from the
+ * group.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ */
+bool dlb2_vdev_is_locked(struct dlb2_hw *hw, unsigned int id)
+{
+	return hw->vdev[id].locked;
+}
 
-		offs = domain->id.phys_id * DLB2_MAX_NUM_DIR_PORTS(hw->ver) +
-			dir_queue->id.phys_id;
+static void dlb2_vf_set_rsrc_virt_ids(struct dlb2_function_resources *rsrcs,
+				      unsigned int id)
+{
+	struct dlb2_list_entry *iter __attribute__((unused));
+	struct dlb2_dir_pq_pair *dir_port;
+	struct dlb2_ldb_queue *ldb_queue;
+	struct dlb2_ldb_port *ldb_port;
+	struct dlb2_hw_domain *domain;
+	int i, j;
+
+	i = 0;
+	DLB2_FUNC_LIST_FOR(rsrcs->avail_domains, domain, iter) {
+		domain->id.virt_id = i;
+		domain->id.vdev_owned = true;
+		domain->id.vdev_id = id;
+		i++;
+	}
+
+	i = 0;
+	DLB2_FUNC_LIST_FOR(rsrcs->avail_ldb_queues, ldb_queue, iter) {
+		ldb_queue->id.virt_id = i;
+		ldb_queue->id.vdev_owned = true;
+		ldb_queue->id.vdev_id = id;
+		i++;
+	}
+
+	i = 0;
+	for (j = 0; j < DLB2_NUM_COS_DOMAINS; j++) {
+		DLB2_FUNC_LIST_FOR(rsrcs->avail_ldb_ports[j], ldb_port, iter) {
+			ldb_port->id.virt_id = i;
+			ldb_port->id.vdev_owned = true;
+			ldb_port->id.vdev_id = id;
+			i++;
+		}
+	}
 
-		DLB2_CSR_WR(hw, DLB2_SYS_DIR_VASQID_V(offs), vasqid_v);
+	i = 0;
+	DLB2_FUNC_LIST_FOR(rsrcs->avail_dir_pq_pairs, dir_port, iter) {
+		dir_port->id.virt_id = i;
+		dir_port->id.vdev_owned = true;
+		dir_port->id.vdev_id = id;
+		i++;
 	}
+}
 
-	dlb2_flush_csr(hw);
+/**
+ * dlb2_lock_vdev() - lock the vdev's resources
+ * @hw: dlb2_hw handle for a particular device.
+ * @id: virtual device ID
+ *
+ * This function sets a flag indicating that the vdev is using its resources.
+ * When the vdev is locked, its resource assignment cannot be changed.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ */
+void dlb2_lock_vdev(struct dlb2_hw *hw, unsigned int id)
+{
+	struct dlb2_function_resources *rsrcs = &hw->vdev[id];
 
-	domain->started = true;
+	rsrcs->locked = true;
 
-	resp->status = 0;
+	dlb2_vf_set_rsrc_virt_ids(rsrcs, id);
+}
 
-	return 0;
+/**
+ * dlb2_unlock_vdev() - unlock the vdev's resources
+ * @hw: dlb2_hw handle for a particular device.
+ * @id: virtual device ID
+ *
+ * This function unlocks the vdev's resource assignment, allowing it to be
+ * modified.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ */
+void dlb2_unlock_vdev(struct dlb2_hw *hw, unsigned int id)
+{
+	hw->vdev[id].locked = false;
 }
 
-static void dlb2_log_get_dir_queue_depth(struct dlb2_hw *hw,
-					 u32 domain_id,
-					 u32 queue_id,
-					 bool vdev_req,
-					 unsigned int vf_id)
+/**
+ * dlb2_reset_vdev_resources() - reassign the vdev's resources to the PF
+ * @hw: dlb2_hw handle for a particular device.
+ * @id: virtual device ID
+ *
+ * This function takes any resources currently assigned to the vdev and
+ * reassigns them to the PF.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ *
+ * Return:
+ * Returns 0 upon success, <0 otherwise.
+ *
+ * Errors:
+ * EINVAL - id is invalid
+ * EPERM  - The vdev's resource assignment is locked and cannot be changed.
+ */
+int dlb2_reset_vdev_resources(struct dlb2_hw *hw, unsigned int id)
 {
-	DLB2_HW_DBG(hw, "DLB get directed queue depth:\n");
-	if (vdev_req)
-		DLB2_HW_DBG(hw, "(Request from VF %d)\n", vf_id);
-	DLB2_HW_DBG(hw, "\tDomain ID: %d\n", domain_id);
-	DLB2_HW_DBG(hw, "\tQueue ID: %d\n", queue_id);
+	if (id >= DLB2_MAX_NUM_VDEVS)
+		return -EINVAL;
+
+	/* If the VF is locked, its resource assignment can't be changed */
+	if (dlb2_vdev_is_locked(hw, id))
+		return -EPERM;
+
+	dlb2_update_vdev_sched_domains(hw, id, 0);
+	dlb2_update_vdev_ldb_queues(hw, id, 0);
+	dlb2_update_vdev_ldb_ports(hw, id, 0);
+	dlb2_update_vdev_dir_ports(hw, id, 0);
+	if (hw->ver == DLB2_HW_V2) {
+		dlb2_update_vdev_ldb_credits(hw, id, 0);
+		dlb2_update_vdev_dir_credits(hw, id, 0);
+	} else {
+		dlb2_update_vdev_credits(hw, id, 0);
+	}
+	dlb2_update_vdev_hist_list_entries(hw, id, 0);
+	dlb2_update_vdev_atomic_inflights(hw, id, 0);
+
+	return 0;
+}
+
+/**
+ * dlb2_clr_pmcsr_disable() - power on bulk of DLB 2.0 logic
+ * @hw: dlb2_hw handle for a particular device.
+ * @ver: device version.
+ *
+ * Clearing the PMCSR must be done at initialization to make the device fully
+ * operational.
+ */
+void dlb2_clr_pmcsr_disable(struct dlb2_hw *hw, enum dlb2_hw_ver ver)
+{
+	u32 pmcsr_dis;
+
+	pmcsr_dis = DLB2_CSR_RD(hw, DLB2_CM_CFG_PM_PMCSR_DISABLE(ver));
+
+	DLB2_BITS_CLR(pmcsr_dis, DLB2_CM_CFG_PM_PMCSR_DISABLE_DISABLE);
+
+	DLB2_CSR_WR(hw, DLB2_CM_CFG_PM_PMCSR_DISABLE(ver), pmcsr_dis);
 }
 
 /**
- * dlb2_hw_get_dir_queue_depth() - returns the depth of a directed queue
+ * dlb2_hw_set_virt_mode() - set the device's virtualization mode
  * @hw: dlb2_hw handle for a particular device.
- * @domain_id: domain ID.
- * @args: queue depth args
- * @resp: response structure.
- * @vdev_req: indicates whether this request came from a vdev.
- * @vdev_id: If vdev_req is true, this contains the vdev's ID.
- *
- * This function returns the depth of a directed queue.
+ * @mode: either none, SR-IOV, or Scalable IOV.
  *
- * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
- * device.
+ * This function sets the virtualization mode of the device. This controls
+ * whether the device uses a software or hardware mailbox.
  *
- * Return:
- * Returns 0 upon success, < 0 otherwise. If an error occurs, resp->status is
- * assigned a detailed error code from enum dlb2_error. If successful, resp->id
- * contains the depth.
+ * This should be called by the PF driver when either SR-IOV or Scalable IOV is
+ * selected as the virtualization mechanism, and by the VF/VDEV driver during
+ * initialization after recognizing itself as an SR-IOV or Scalable IOV device.
  *
  * Errors:
- * EINVAL - Invalid domain ID or queue ID.
+ * EINVAL - Invalid mode.
  */
-int dlb2_hw_get_dir_queue_depth(struct dlb2_hw *hw,
-				u32 domain_id,
-				struct dlb2_get_dir_queue_depth_args *args,
-				struct dlb2_cmd_response *resp,
-				bool vdev_req,
-				unsigned int vdev_id)
+int dlb2_hw_set_virt_mode(struct dlb2_hw *hw, enum dlb2_virt_mode mode)
 {
-	struct dlb2_dir_pq_pair *queue;
-	struct dlb2_hw_domain *domain;
-	int id;
-
-	id = domain_id;
-
-	dlb2_log_get_dir_queue_depth(hw, domain_id, args->queue_id,
-				     vdev_req, vdev_id);
-
-	domain = dlb2_get_domain_from_id(hw, id, vdev_req, vdev_id);
-	if (!domain) {
-		resp->status = DLB2_ST_INVALID_DOMAIN_ID;
-		return -EINVAL;
-	}
-
-	id = args->queue_id;
-
-	queue = dlb2_get_domain_used_dir_pq(hw, id, vdev_req, domain);
-	if (!queue) {
-		resp->status = DLB2_ST_INVALID_QID;
+	if (mode >= NUM_DLB2_VIRT_MODES)
 		return -EINVAL;
-	}
 
-	resp->id = dlb2_dir_queue_depth(hw, queue);
+	hw->virt_mode = mode;
 
 	return 0;
 }
 
-static void dlb2_log_get_ldb_queue_depth(struct dlb2_hw *hw,
-					 u32 domain_id,
-					 u32 queue_id,
-					 bool vdev_req,
-					 unsigned int vf_id)
+/**
+ * dlb2_hw_get_virt_mode() - get the device's virtualization mode
+ * @hw: dlb2_hw handle for a particular device.
+ *
+ * This function gets the virtualization mode of the device.
+ */
+enum dlb2_virt_mode dlb2_hw_get_virt_mode(struct dlb2_hw *hw)
 {
-	DLB2_HW_DBG(hw, "DLB get load-balanced queue depth:\n");
-	if (vdev_req)
-		DLB2_HW_DBG(hw, "(Request from VF %d)\n", vf_id);
-	DLB2_HW_DBG(hw, "\tDomain ID: %d\n", domain_id);
-	DLB2_HW_DBG(hw, "\tQueue ID: %d\n", queue_id);
+	return hw->virt_mode;
 }
 
 /**
- * dlb2_hw_get_ldb_queue_depth() - returns the depth of a load-balanced queue
+ * dlb2_hw_get_ldb_port_phys_id() - get a physical port ID from its virt ID
  * @hw: dlb2_hw handle for a particular device.
- * @domain_id: domain ID.
- * @args: queue depth args
- * @resp: response structure.
- * @vdev_req: indicates whether this request came from a vdev.
- * @vdev_id: If vdev_req is true, this contains the vdev's ID.
- *
- * This function returns the depth of a load-balanced queue.
- *
- * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
- * device.
+ * @id: virtual port ID.
+ * @vdev_id: vdev ID.
  *
  * Return:
- * Returns 0 upon success, < 0 otherwise. If an error occurs, resp->status is
- * assigned a detailed error code from enum dlb2_error. If successful, resp->id
- * contains the depth.
- *
- * Errors:
- * EINVAL - Invalid domain ID or queue ID.
+ * Returns >= 0 upon success, -1 otherwise.
  */
-int dlb2_hw_get_ldb_queue_depth(struct dlb2_hw *hw,
-				u32 domain_id,
-				struct dlb2_get_ldb_queue_depth_args *args,
-				struct dlb2_cmd_response *resp,
-				bool vdev_req,
-				unsigned int vdev_id)
+s32 dlb2_hw_get_ldb_port_phys_id(struct dlb2_hw *hw,
+				 u32 id,
+				 unsigned int vdev_id)
 {
-	struct dlb2_hw_domain *domain;
-	struct dlb2_ldb_queue *queue;
+	struct dlb2_ldb_port *port;
 
-	dlb2_log_get_ldb_queue_depth(hw, domain_id, args->queue_id,
-				     vdev_req, vdev_id);
+	port = dlb2_get_ldb_port_from_id(hw, id, true, vdev_id);
+	if (!port)
+		return -1;
 
-	domain = dlb2_get_domain_from_id(hw, domain_id, vdev_req, vdev_id);
-	if (!domain) {
-		resp->status = DLB2_ST_INVALID_DOMAIN_ID;
-		return -EINVAL;
-	}
+	return port->id.phys_id;
+}
 
-	queue = dlb2_get_domain_ldb_queue(args->queue_id, vdev_req, domain);
-	if (!queue) {
-		resp->status = DLB2_ST_INVALID_QID;
-		return -EINVAL;
-	}
+/**
+ * dlb2_hw_get_dir_port_phys_id() - get a physical port ID from its virt ID
+ * @hw: dlb2_hw handle for a particular device.
+ * @id: virtual port ID.
+ * @vdev_id: vdev ID.
+ *
+ * Return:
+ * Returns >= 0 upon success, -1 otherwise.
+ */
+s32 dlb2_hw_get_dir_port_phys_id(struct dlb2_hw *hw,
+				 u32 id,
+				 unsigned int vdev_id)
+{
+	struct dlb2_dir_pq_pair *port;
 
-	resp->id = dlb2_ldb_queue_depth(hw, queue);
+	port = dlb2_get_dir_pq_from_id(hw, id, true, vdev_id);
+	if (!port)
+		return -1;
 
-	return 0;
+	return port->id.phys_id;
 }
 
 /**
- * dlb2_finish_unmap_qid_procedures() - finish any pending unmap procedures
+ * dlb2_hw_register_sw_mbox() - register a software mailbox
  * @hw: dlb2_hw handle for a particular device.
+ * @vdev_id: vdev ID.
+ * @vdev_to_pf_mbox: pointer to a 4KB memory page for vdev->PF communication.
+ * @pf_to_vdev_mbox: pointer to a 4KB memory page for PF->vdev communication.
+ * @inject: callback function for injecting a PF->vdev interrupt.
+ * @inject_arg: user argument for inject callback.
  *
- * This function attempts to finish any outstanding unmap procedures.
- * This function should be called by the kernel thread responsible for
- * finishing map/unmap procedures.
+ * When Scalable IOV is enabled, the VDCM must register a software mailbox for
+ * every virtual device during vdev creation.
  *
- * Return:
- * Returns the number of procedures that weren't completed.
+ * This function notifies the driver to use a software mailbox using the
+ * provided pointers, instead of the device's hardware mailbox. When the driver
+ * calls mailbox functions like dlb2_pf_write_vf_mbox_req(), the request will
+ * go to the software mailbox instead of the hardware one. This is used in
+ * Scalable IOV virtualization.
  */
-unsigned int dlb2_finish_unmap_qid_procedures(struct dlb2_hw *hw)
+void dlb2_hw_register_sw_mbox(struct dlb2_hw *hw,
+			      unsigned int vdev_id,
+			      u32 *vdev_to_pf_mbox,
+			      u32 *pf_to_vdev_mbox,
+			      void (*inject)(void *),
+			      void *inject_arg)
 {
-	int i, num = 0;
+	u32 offs;
 
-	/* Finish queue unmap jobs for any domain that needs it */
-	for (i = 0; i < DLB2_MAX_NUM_DOMAINS; i++) {
-		struct dlb2_hw_domain *domain = &hw->domains[i];
+	offs = DLB2_VF_VF2PF_MAILBOX_ISR % 0x1000;
 
-		num += dlb2_domain_finish_unmap_qid_procedures(hw, domain);
-	}
+	hw->mbox[vdev_id].vdev_to_pf.mbox = vdev_to_pf_mbox;
+	hw->mbox[vdev_id].vdev_to_pf.isr_in_progress =
+		(u32 *)((u8 *)vdev_to_pf_mbox + offs);
 
-	return num;
+	offs = (DLB2_VF_PF2VF_MAILBOX_ISR % 0x1000);
+
+	hw->mbox[vdev_id].pf_to_vdev.mbox = pf_to_vdev_mbox;
+	hw->mbox[vdev_id].pf_to_vdev.isr_in_progress =
+		(u32 *)((u8 *)pf_to_vdev_mbox + offs);
+
+	hw->mbox[vdev_id].pf_to_vdev_inject = inject;
+	hw->mbox[vdev_id].pf_to_vdev_inject_arg = inject_arg;
 }
 
 /**
- * dlb2_finish_map_qid_procedures() - finish any pending map procedures
+ * dlb2_hw_unregister_sw_mbox() - unregister a software mailbox
  * @hw: dlb2_hw handle for a particular device.
+ * @vdev_id: vdev ID.
  *
- * This function attempts to finish any outstanding map procedures.
- * This function should be called by the kernel thread responsible for
- * finishing map/unmap procedures.
+ * This function notifies the driver to stop using a previously registered
+ * software mailbox.
+ */
+void dlb2_hw_unregister_sw_mbox(struct dlb2_hw *hw, unsigned int vdev_id)
+{
+	hw->mbox[vdev_id].vdev_to_pf.mbox = NULL;
+	hw->mbox[vdev_id].pf_to_vdev.mbox = NULL;
+	hw->mbox[vdev_id].vdev_to_pf.isr_in_progress = NULL;
+	hw->mbox[vdev_id].pf_to_vdev.isr_in_progress = NULL;
+	hw->mbox[vdev_id].pf_to_vdev_inject = NULL;
+	hw->mbox[vdev_id].pf_to_vdev_inject_arg = NULL;
+}
+
+/**
+ * dlb2_hw_setup_cq_ims_entry() - setup a CQ's IMS entry
+ * @hw: dlb2_hw handle for a particular device.
+ * @vdev_id: vdev ID.
+ * @virt_cq_id: virtual CQ ID.
+ * @is_ldb: CQ is load-balanced.
+ * @addr_hi: most-significant 32 bits of address.
+ * @addr_lo: least-significant 32 bits of address.
+ * @data: 32 data bits.
  *
- * Return:
- * Returns the number of procedures that weren't completed.
+ * This sets up the CQ's IMS entry with the provided address and data values.
+ * This function should only be called if the device is configured for Scalable
+ * IOV virtualization. The upper 32 address bits are fixed in hardware and thus
+ * not needed.
  */
-unsigned int dlb2_finish_map_qid_procedures(struct dlb2_hw *hw)
+void dlb2_hw_setup_cq_ims_entry(struct dlb2_hw *hw,
+				unsigned int vdev_id,
+				u32 virt_cq_id,
+				bool is_ldb,
+				u32 addr_hi,
+				u32 addr_lo,
+				u32 data)
 {
-	int i, num = 0;
+	s32 cq_id;
 
-	/* Finish queue map jobs for any domain that needs it */
-	for (i = 0; i < DLB2_MAX_NUM_DOMAINS; i++) {
-		struct dlb2_hw_domain *domain = &hw->domains[i];
+	if (hw->virt_mode != DLB2_VIRT_SIOV) {
+		DLB2_HW_ERR(hw,
+			    "[%s()] Internal error: incorrect virt mode\n",
+			    __func__);
+		return;
+	}
 
-		num += dlb2_domain_finish_map_qid_procedures(hw, domain);
+	/*
+	 * The upper 32 IMS bits are hard-wired to 0 in DLB 2.0 and writable in
+	 * DLB 2.5 to support alternative CQ polling mechanisms.
+	 */
+
+	if (is_ldb) {
+		cq_id = dlb2_hw_get_ldb_port_phys_id(hw, virt_cq_id, vdev_id);
+		if (cq_id < 0) {
+			DLB2_HW_ERR(hw, "[%s()] Failed to lookup CQ ID\n",
+				    __func__);
+			return;
+		}
+
+		DLB2_CSR_WR(hw, DLB2_SYS_LDB_CQ_AI_ADDR_L(cq_id), addr_lo);
+		DLB2_CSR_WR(hw, DLB2_SYS_LDB_CQ_AI_DATA(cq_id), data);
+
+		if (hw->ver != DLB2_HW_V2)
+			DLB2_CSR_WR(hw, DLB2_SYS_LDB_CQ_AI_ADDR_U(cq_id),
+				    addr_hi);
+	} else {
+		cq_id = dlb2_hw_get_dir_port_phys_id(hw, virt_cq_id, vdev_id);
+		if (cq_id < 0) {
+			DLB2_HW_ERR(hw, "[%s()] Failed to lookup CQ ID\n",
+				    __func__);
+			return;
+		}
+
+		DLB2_CSR_WR(hw, DLB2_SYS_DIR_CQ_AI_ADDR(cq_id), addr_lo);
+		DLB2_CSR_WR(hw, DLB2_SYS_DIR_CQ_AI_DATA(cq_id), data);
+
+		if (hw->ver != DLB2_HW_V2)
+			DLB2_CSR_WR(hw, DLB2_SYS_DIR_CQ_AI_ADDR_U(cq_id),
+				    addr_hi);
 	}
 
-	return num;
+	dlb2_flush_csr(hw);
 }
 
 /**
- * dlb2_hw_enable_sparse_dir_cq_mode() - enable sparse mode for directed ports.
+ * dlb2_hw_clear_cq_ims_entry() - clear a CQ's IMS entry
  * @hw: dlb2_hw handle for a particular device.
+ * @vdev_id: vdev ID.
+ * @virt_cq_id: virtual CQ ID.
+ * @is_ldb: CQ is load-balanced.
  *
- * This function must be called prior to configuring scheduling domains.
+ * This clears the CQ's IMS entry, reverting it to its reset state.
  */
+void dlb2_hw_clear_cq_ims_entry(struct dlb2_hw *hw,
+				unsigned int vdev_id,
+				u32 virt_cq_id,
+				bool is_ldb)
+{
+	dlb2_hw_setup_cq_ims_entry(hw, vdev_id, virt_cq_id, is_ldb, 0, 0, 0);
+}
 
-void dlb2_hw_enable_sparse_dir_cq_mode(struct dlb2_hw *hw)
+/**
+ * dlb2_hw_register_pasid() - register a vdev's PASID
+ * @hw: dlb2_hw handle for a particular device.
+ * @vdev_id: vdev ID.
+ * @pasid: the vdev's PASID.
+ *
+ * This function stores the user-supplied PASID, and uses it when configuring
+ * the vdev's CQs.
+ *
+ * Return:
+ * Returns >= 0 upon success, -1 otherwise.
+ */
+int dlb2_hw_register_pasid(struct dlb2_hw *hw,
+			   unsigned int vdev_id,
+			   unsigned int pasid)
 {
-	u32 ctrl;
+	if (vdev_id >= DLB2_MAX_NUM_VDEVS)
+		return -1;
 
-	ctrl = DLB2_CSR_RD(hw, DLB2_CHP_CFG_CHP_CSR_CTRL);
+	hw->pasid[vdev_id] = pasid;
 
-	DLB2_BIT_SET(ctrl,
-		     DLB2_CHP_CFG_CHP_CSR_CTRL_CFG_64BYTES_QE_DIR_CQ_MODE);
+	return 0;
+}
 
-	DLB2_CSR_WR(hw, DLB2_CHP_CFG_CHP_CSR_CTRL, ctrl);
+/**
+ * dlb2_hw_get_cos_bandwidth() - returns the percent of bandwidth allocated
+ *	to a port class-of-service.
+ * @hw: dlb2_hw handle for a particular device.
+ * @cos_id: class-of-service ID.
+ *
+ * Return:
+ * Returns -EINVAL if cos_id is invalid, else the class' bandwidth allocation.
+ */
+int dlb2_hw_get_cos_bandwidth(struct dlb2_hw *hw, u32 cos_id)
+{
+	if (cos_id >= DLB2_NUM_COS_DOMAINS)
+		return -EINVAL;
+
+	return hw->cos_reservation[cos_id];
+}
+
+static void dlb2_log_set_cos_bandwidth(struct dlb2_hw *hw, u32 cos_id, u8 bw)
+{
+	DLB2_HW_DBG(hw, "DLB2 set port CoS bandwidth:\n");
+	DLB2_HW_DBG(hw, "\tCoS ID:    %u\n", cos_id);
+	DLB2_HW_DBG(hw, "\tBandwidth: %u\n", bw);
 }
 
+#define DLB2_MAX_BW_PCT 100
+
 /**
- * dlb2_hw_enable_sparse_ldb_cq_mode() - enable sparse mode for load-balanced
- *	ports.
+ * dlb2_hw_set_cos_bandwidth() - set a bandwidth allocation percentage for a
+ *	port class-of-service.
  * @hw: dlb2_hw handle for a particular device.
+ * @cos_id: class-of-service ID.
+ * @bandwidth: class-of-service bandwidth.
  *
- * This function must be called prior to configuring scheduling domains.
+ * Return:
+ * Returns 0 upon success, < 0 otherwise.
+ *
+ * Errors:
+ * EINVAL - Invalid cos ID, bandwidth is greater than 100, or bandwidth would
+ *	    cause the total bandwidth across all classes of service to exceed
+ *	    100%.
  */
-void dlb2_hw_enable_sparse_ldb_cq_mode(struct dlb2_hw *hw)
+int dlb2_hw_set_cos_bandwidth(struct dlb2_hw *hw, u32 cos_id, u8 bandwidth)
 {
-	u32 ctrl;
+	unsigned int i;
+	u32 reg;
+	u8 total;
+
+	if (cos_id >= DLB2_NUM_COS_DOMAINS)
+		return -EINVAL;
+
+	if (bandwidth > DLB2_MAX_BW_PCT)
+		return -EINVAL;
+
+	total = 0;
+
+	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++)
+		total += (i == cos_id) ? bandwidth : hw->cos_reservation[i];
+
+	if (total > DLB2_MAX_BW_PCT)
+		return -EINVAL;
+
+	reg = DLB2_CSR_RD(hw, DLB2_LSP_CFG_SHDW_RANGE_COS(hw->ver, cos_id));
+
+	/*
+	 * Normalize the bandwidth to a value in the range 0-255. Integer
+	 * division may leave unreserved scheduling slots; these will be
+	 * divided among the 4 classes of service.
+	 */
+	DLB2_BITS_SET(reg, (bandwidth * 256) / 100,
+		      DLB2_LSP_CFG_SHDW_RANGE_COS_BW_RANGE);
+	DLB2_CSR_WR(hw, DLB2_LSP_CFG_SHDW_RANGE_COS(hw->ver, cos_id), reg);
+
+	reg = 0;
+	DLB2_BIT_SET(reg, DLB2_LSP_CFG_SHDW_CTRL_TRANSFER);
+	/* Atomically transfer the newly configured service weight */
+	DLB2_CSR_WR(hw, DLB2_LSP_CFG_SHDW_CTRL(hw->ver), reg);
 
-	ctrl = DLB2_CSR_RD(hw, DLB2_CHP_CFG_CHP_CSR_CTRL);
+	dlb2_log_set_cos_bandwidth(hw, cos_id, bandwidth);
 
-	DLB2_BIT_SET(ctrl,
-		     DLB2_CHP_CFG_CHP_CSR_CTRL_CFG_64BYTES_QE_LDB_CQ_MODE);
+	hw->cos_reservation[cos_id] = bandwidth;
 
-	DLB2_CSR_WR(hw, DLB2_CHP_CFG_CHP_CSR_CTRL, ctrl);
+	return 0;
 }
 
+struct dlb2_wd_config {
+	u32 threshold;
+	u32 interval;
+};
+
 /**
- * dlb2_get_group_sequence_numbers() - return a group's number of SNs per queue
+ * dlb2_hw_enable_wd_timer() - enable the CQ watchdog timers with a
+ *	caller-specified timeout.
  * @hw: dlb2_hw handle for a particular device.
- * @group_id: sequence number group ID.
+ * @tmo: watchdog timeout.
  *
- * This function returns the configured number of sequence numbers per queue
- * for the specified group.
+ * This function should be called during device initialization and after reset.
+ * The watchdog timer interrupt must also be enabled per-CQ, using either
+ * dlb2_hw_enable_dir_cq_wd_int() or dlb2_hw_enable_ldb_cq_wd_int().
  *
  * Return:
- * Returns -EINVAL if group_id is invalid, else the group's SNs per queue.
+ * Returns 0 upon success, < 0 otherwise.
+ *
+ * Errors:
+ * EINVAL - Invalid timeout.
  */
-int dlb2_get_group_sequence_numbers(struct dlb2_hw *hw, u32 group_id)
+int dlb2_hw_enable_wd_timer(struct dlb2_hw *hw, enum dlb2_wd_tmo tmo)
 {
-	if (group_id >= DLB2_MAX_NUM_SEQUENCE_NUMBER_GROUPS)
+	/* Timeout = num_ports * threshold * (sample interval + 1) / 100 MHz */
+	const struct dlb2_wd_config wd_config[NUM_DLB2_WD_TMOS] = {
+		[DLB2_WD_TMO_40S] = {30, 0x1FFFFF},
+		[DLB2_WD_TMO_10S] = {30, 0x7FFFF},
+		[DLB2_WD_TMO_1S]  = {24, 0xFFFF},
+	};
+	u32 dir_thresh = 0;
+	u32 ldb_thresh = 0;
+	u32 dir_en = 0;
+	u32 ldb_en = 0;
+
+	if (tmo >= NUM_DLB2_WD_TMOS)
 		return -EINVAL;
 
-	return hw->rsrcs.sn_groups[group_id].sequence_numbers_per_queue;
+	DLB2_BITS_SET(dir_thresh, wd_config[tmo].threshold,
+		 DLB2_CHP_CFG_DIR_WD_THRESHOLD_WD_THRESHOLD);
+	DLB2_BITS_SET(ldb_thresh, wd_config[tmo].threshold,
+		 DLB2_CHP_CFG_LDB_WD_THRESHOLD_WD_THRESHOLD);
+
+	DLB2_CSR_WR(hw, DLB2_CHP_CFG_DIR_WD_THRESHOLD(hw->ver), dir_thresh);
+	DLB2_CSR_WR(hw, DLB2_CHP_CFG_LDB_WD_THRESHOLD(hw->ver), ldb_thresh);
+
+	DLB2_BITS_SET(dir_en, wd_config[tmo].interval,
+		 DLB2_CHP_CFG_DIR_WD_ENB_INTERVAL_SAMPLE_INTERVAL);
+	DLB2_BITS_SET(ldb_en, wd_config[tmo].interval,
+		 DLB2_CHP_CFG_LDB_WD_ENB_INTERVAL_SAMPLE_INTERVAL);
+	DLB2_BIT_SET(dir_en, DLB2_CHP_CFG_DIR_WD_ENB_INTERVAL_ENB);
+	DLB2_BIT_SET(ldb_en, DLB2_CHP_CFG_LDB_WD_ENB_INTERVAL_ENB);
+
+	/* If running on the emulation platform, adjust accordingly */
+	if (DLB2_HZ == 2000000) {
+		DLB2_BITS_SET(dir_en, (dir_en
+		     & DLB2_CHP_CFG_DIR_WD_ENB_INTERVAL_SAMPLE_INTERVAL) / 400,
+			 DLB2_CHP_CFG_DIR_WD_ENB_INTERVAL_SAMPLE_INTERVAL);
+		DLB2_BITS_SET(ldb_en, (ldb_en
+		     & DLB2_CHP_CFG_LDB_WD_ENB_INTERVAL_SAMPLE_INTERVAL) / 400,
+			 DLB2_CHP_CFG_LDB_WD_ENB_INTERVAL_SAMPLE_INTERVAL);
+	}
+
+	DLB2_CSR_WR(hw, DLB2_CHP_CFG_DIR_WD_ENB_INTERVAL(hw->ver), dir_en);
+	DLB2_CSR_WR(hw, DLB2_CHP_CFG_LDB_WD_ENB_INTERVAL(hw->ver), ldb_en);
+
+	return 0;
 }
 
 /**
- * dlb2_get_group_sequence_number_occupancy() - return a group's in-use slots
+ * dlb2_hw_enable_dir_cq_wd_int() - enable the CQ watchdog interrupt on an
+ *	individual CQ.
  * @hw: dlb2_hw handle for a particular device.
- * @group_id: sequence number group ID.
- *
- * This function returns the group's number of in-use slots (i.e. load-balanced
- * queues using the specified group).
+ * @id: port ID.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
  *
  * Return:
- * Returns -EINVAL if group_id is invalid, else the group's SNs per queue.
+ * Returns 0 upon success, < 0 otherwise.
+ *
+ * Errors:
+ * EINVAL - Invalid directed port ID.
  */
-int dlb2_get_group_sequence_number_occupancy(struct dlb2_hw *hw, u32 group_id)
+int dlb2_hw_enable_dir_cq_wd_int(struct dlb2_hw *hw,
+				 u32 id,
+				 bool vdev_req,
+				 unsigned int vdev_id)
 {
-	if (group_id >= DLB2_MAX_NUM_SEQUENCE_NUMBER_GROUPS)
+	struct dlb2_dir_pq_pair *port;
+	u32 wd_dis = 0;
+	u32 wd_en = 0;
+
+	port = dlb2_get_dir_pq_from_id(hw, id, vdev_req, vdev_id);
+	if (!port)
 		return -EINVAL;
 
-	return dlb2_sn_group_used_slots(&hw->rsrcs.sn_groups[group_id]);
-}
+	DLB2_BIT_SET(wd_en, DLB2_CHP_DIR_CQ_WD_ENB_WD_ENABLE);
 
-static void dlb2_log_set_group_sequence_numbers(struct dlb2_hw *hw,
-						u32 group_id,
-						u32 val)
-{
-	DLB2_HW_DBG(hw, "DLB2 set group sequence numbers:\n");
-	DLB2_HW_DBG(hw, "\tGroup ID: %u\n", group_id);
-	DLB2_HW_DBG(hw, "\tValue:    %u\n", val);
+	DLB2_CSR_WR(hw,
+		    DLB2_CHP_DIR_CQ_WD_ENB(hw->ver, port->id.phys_id), wd_en);
+
+	wd_dis |= 1 << (port->id.phys_id % 32);
+
+	/* WD_DISABLE registers are W1CLR */
+	if (port->id.phys_id < 32)
+		DLB2_CSR_WR(hw, DLB2_CHP_CFG_DIR_WD_DISABLE0(hw->ver), wd_dis);
+	else
+		DLB2_CSR_WR(hw, DLB2_CHP_CFG_DIR_WD_DISABLE1(hw->ver), wd_dis);
+
+	return 0;
 }
 
 /**
- * dlb2_set_group_sequence_numbers() - assign a group's number of SNs per queue
+ * dlb2_hw_enable_ldb_cq_wd_int() - enable the CQ watchdog interrupt on an
+ *	individual CQ.
  * @hw: dlb2_hw handle for a particular device.
- * @group_id: sequence number group ID.
- * @val: requested amount of sequence numbers per queue.
- *
- * This function configures the group's number of sequence numbers per queue.
- * val can be a power-of-two between 32 and 1024, inclusive. This setting can
- * be configured until the first ordered load-balanced queue is configured, at
- * which point the configuration is locked.
+ * @id: port ID.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
  *
  * Return:
- * Returns 0 upon success; -EINVAL if group_id or val is invalid, -EPERM if an
- * ordered queue is configured.
+ * Returns 0 upon success, < 0 otherwise.
+ *
+ * Errors:
+ * EINVAL - Invalid load-balanced port ID.
  */
-int dlb2_set_group_sequence_numbers(struct dlb2_hw *hw,
-				    u32 group_id,
-				    u32 val)
+int dlb2_hw_enable_ldb_cq_wd_int(struct dlb2_hw *hw,
+				 u32 id,
+				 bool vdev_req,
+				 unsigned int vdev_id)
 {
-	const u32 valid_allocations[] = {64, 128, 256, 512, 1024};
-	struct dlb2_sn_group *group;
-	u32 sn_mode = 0;
-	int mode;
+	struct dlb2_ldb_port *port;
+	u32 wd_dis = 0;
+	u32 wd_en = 0;
 
-	if (group_id >= DLB2_MAX_NUM_SEQUENCE_NUMBER_GROUPS)
+	port = dlb2_get_ldb_port_from_id(hw, id, vdev_req, vdev_id);
+	if (!port)
 		return -EINVAL;
 
-	group = &hw->rsrcs.sn_groups[group_id];
+	DLB2_BIT_SET(wd_en, DLB2_CHP_LDB_CQ_WD_ENB_WD_ENABLE);
 
-	/*
-	 * Once the first load-balanced queue using an SN group is configured,
-	 * the group cannot be changed.
-	 */
-	if (group->slot_use_bitmap != 0)
-		return -EPERM;
+	DLB2_CSR_WR(hw,
+		    DLB2_CHP_LDB_CQ_WD_ENB(hw->ver, port->id.phys_id), wd_en);
 
-	for (mode = 0; mode < DLB2_MAX_NUM_SEQUENCE_NUMBER_MODES; mode++)
-		if (val == valid_allocations[mode])
-			break;
+	wd_dis |= 1 << (port->id.phys_id % 32);
 
-	if (mode == DLB2_MAX_NUM_SEQUENCE_NUMBER_MODES)
-		return -EINVAL;
+	/* WD_DISABLE registers are W1CLR */
+	if (port->id.phys_id < 32)
+		DLB2_CSR_WR(hw, DLB2_CHP_CFG_LDB_WD_DISABLE0(hw->ver), wd_dis);
+	else
+		DLB2_CSR_WR(hw, DLB2_CHP_CFG_LDB_WD_DISABLE1(hw->ver), wd_dis);
 
-	group->mode = mode;
-	group->sequence_numbers_per_queue = val;
+	return 0;
+}
 
-	DLB2_BITS_SET(sn_mode, hw->rsrcs.sn_groups[0].mode,
-		 DLB2_RO_GRP_SN_MODE_SN_MODE_0);
-	DLB2_BITS_SET(sn_mode, hw->rsrcs.sn_groups[1].mode,
-		 DLB2_RO_GRP_SN_MODE_SN_MODE_1);
+/**
+ * dlb2_hw_enable_sparse_ldb_cq_mode() - enable sparse mode for load-balanced
+ *	ports.
+ * @hw: dlb2_hw handle for a particular device.
+ *
+ * This function must be called prior to configuring scheduling domains.
+ */
+void dlb2_hw_enable_sparse_ldb_cq_mode(struct dlb2_hw *hw)
+{
+	u32 ctrl;
 
-	DLB2_CSR_WR(hw, DLB2_RO_GRP_SN_MODE(hw->ver), sn_mode);
+	ctrl = DLB2_CSR_RD(hw, DLB2_CHP_CFG_CHP_CSR_CTRL);
 
-	dlb2_log_set_group_sequence_numbers(hw, group_id, val);
+	DLB2_BIT_SET(ctrl,
+		     DLB2_CHP_CFG_CHP_CSR_CTRL_CFG_64BYTES_QE_LDB_CQ_MODE);
 
-	return 0;
+	DLB2_CSR_WR(hw, DLB2_CHP_CFG_CHP_CSR_CTRL, ctrl);
+}
+
+/**
+ * dlb2_hw_enable_sparse_dir_cq_mode() - enable sparse mode for directed ports.
+ * @hw: dlb2_hw handle for a particular device.
+ *
+ * This function must be called prior to configuring scheduling domains.
+ */
+void dlb2_hw_enable_sparse_dir_cq_mode(struct dlb2_hw *hw)
+{
+	u32 ctrl;
+
+	ctrl = DLB2_CSR_RD(hw, DLB2_CHP_CFG_CHP_CSR_CTRL);
+
+	DLB2_BIT_SET(ctrl,
+		     DLB2_CHP_CFG_CHP_CSR_CTRL_CFG_64BYTES_QE_DIR_CQ_MODE);
+
+	DLB2_CSR_WR(hw, DLB2_CHP_CFG_CHP_CSR_CTRL, ctrl);
 }
 
 /**
@@ -6551,24 +10602,36 @@ void dlb2_hw_set_qe_arbiter_weights(struct dlb2_hw *hw, u8 weight[8])
 	DLB2_CSR_WR(hw, DLB2_ATM_CFG_ARB_WEIGHTS_RDY_BIN, reg);
 
 	reg = 0;
-	DLB2_BITS_SET(reg, weight[1], DLB2_NALB_CFG_ARB_WEIGHTS_TQPRI_NALB_0_PRI0);
-	DLB2_BITS_SET(reg, weight[3], DLB2_NALB_CFG_ARB_WEIGHTS_TQPRI_NALB_0_PRI1);
-	DLB2_BITS_SET(reg, weight[5], DLB2_NALB_CFG_ARB_WEIGHTS_TQPRI_NALB_0_PRI2);
-	DLB2_BITS_SET(reg, weight[7], DLB2_NALB_CFG_ARB_WEIGHTS_TQPRI_NALB_0_PRI3);
+	DLB2_BITS_SET(reg, weight[1],
+		      DLB2_NALB_CFG_ARB_WEIGHTS_TQPRI_NALB_0_PRI0);
+	DLB2_BITS_SET(reg, weight[3],
+		      DLB2_NALB_CFG_ARB_WEIGHTS_TQPRI_NALB_0_PRI1);
+	DLB2_BITS_SET(reg, weight[5],
+		      DLB2_NALB_CFG_ARB_WEIGHTS_TQPRI_NALB_0_PRI2);
+	DLB2_BITS_SET(reg, weight[7],
+		      DLB2_NALB_CFG_ARB_WEIGHTS_TQPRI_NALB_0_PRI3);
 	DLB2_CSR_WR(hw, DLB2_NALB_CFG_ARB_WEIGHTS_TQPRI_NALB_0(hw->ver), reg);
 
 	reg = 0;
-	DLB2_BITS_SET(reg, weight[1], DLB2_NALB_CFG_ARB_WEIGHTS_TQPRI_REPLAY_0_PRI0);
-	DLB2_BITS_SET(reg, weight[3], DLB2_NALB_CFG_ARB_WEIGHTS_TQPRI_REPLAY_0_PRI1);
-	DLB2_BITS_SET(reg, weight[5], DLB2_NALB_CFG_ARB_WEIGHTS_TQPRI_REPLAY_0_PRI2);
-	DLB2_BITS_SET(reg, weight[7], DLB2_NALB_CFG_ARB_WEIGHTS_TQPRI_REPLAY_0_PRI3);
+	DLB2_BITS_SET(reg, weight[1],
+		      DLB2_NALB_CFG_ARB_WEIGHTS_TQPRI_REPLAY_0_PRI0);
+	DLB2_BITS_SET(reg, weight[3],
+		      DLB2_NALB_CFG_ARB_WEIGHTS_TQPRI_REPLAY_0_PRI1);
+	DLB2_BITS_SET(reg, weight[5],
+		      DLB2_NALB_CFG_ARB_WEIGHTS_TQPRI_REPLAY_0_PRI2);
+	DLB2_BITS_SET(reg, weight[7],
+		      DLB2_NALB_CFG_ARB_WEIGHTS_TQPRI_REPLAY_0_PRI3);
 	DLB2_CSR_WR(hw, DLB2_NALB_CFG_ARB_WEIGHTS_TQPRI_REPLAY_0(hw->ver), reg);
 
 	reg = 0;
-	DLB2_BITS_SET(reg, weight[1], DLB2_DP_CFG_ARB_WEIGHTS_TQPRI_REPLAY_0_PRI0);
-	DLB2_BITS_SET(reg, weight[3], DLB2_DP_CFG_ARB_WEIGHTS_TQPRI_REPLAY_0_PRI1);
-	DLB2_BITS_SET(reg, weight[5], DLB2_DP_CFG_ARB_WEIGHTS_TQPRI_REPLAY_0_PRI2);
-	DLB2_BITS_SET(reg, weight[7], DLB2_DP_CFG_ARB_WEIGHTS_TQPRI_REPLAY_0_PRI3);
+	DLB2_BITS_SET(reg, weight[1],
+		      DLB2_DP_CFG_ARB_WEIGHTS_TQPRI_REPLAY_0_PRI0);
+	DLB2_BITS_SET(reg, weight[3],
+		      DLB2_DP_CFG_ARB_WEIGHTS_TQPRI_REPLAY_0_PRI1);
+	DLB2_BITS_SET(reg, weight[5],
+		      DLB2_DP_CFG_ARB_WEIGHTS_TQPRI_REPLAY_0_PRI2);
+	DLB2_BITS_SET(reg, weight[7],
+		      DLB2_DP_CFG_ARB_WEIGHTS_TQPRI_REPLAY_0_PRI3);
 	DLB2_CSR_WR(hw, DLB2_DP_CFG_ARB_WEIGHTS_TQPRI_REPLAY_0, reg);
 
 	reg = 0;
@@ -6579,10 +10642,14 @@ void dlb2_hw_set_qe_arbiter_weights(struct dlb2_hw *hw, u8 weight[8])
 	DLB2_CSR_WR(hw, DLB2_DP_CFG_ARB_WEIGHTS_TQPRI_DIR_0, reg);
 
 	reg = 0;
-	DLB2_BITS_SET(reg, weight[1], DLB2_NALB_CFG_ARB_WEIGHTS_TQPRI_ATQ_0_PRI0);
-	DLB2_BITS_SET(reg, weight[3], DLB2_NALB_CFG_ARB_WEIGHTS_TQPRI_ATQ_0_PRI1);
-	DLB2_BITS_SET(reg, weight[5], DLB2_NALB_CFG_ARB_WEIGHTS_TQPRI_ATQ_0_PRI2);
-	DLB2_BITS_SET(reg, weight[7], DLB2_NALB_CFG_ARB_WEIGHTS_TQPRI_ATQ_0_PRI3);
+	DLB2_BITS_SET(reg, weight[1],
+		      DLB2_NALB_CFG_ARB_WEIGHTS_TQPRI_ATQ_0_PRI0);
+	DLB2_BITS_SET(reg, weight[3],
+		      DLB2_NALB_CFG_ARB_WEIGHTS_TQPRI_ATQ_0_PRI1);
+	DLB2_BITS_SET(reg, weight[5],
+		      DLB2_NALB_CFG_ARB_WEIGHTS_TQPRI_ATQ_0_PRI2);
+	DLB2_BITS_SET(reg, weight[7],
+		      DLB2_NALB_CFG_ARB_WEIGHTS_TQPRI_ATQ_0_PRI3);
 	DLB2_CSR_WR(hw, DLB2_NALB_CFG_ARB_WEIGHTS_TQPRI_ATQ_0(hw->ver), reg);
 
 	reg = 0;
@@ -6593,10 +10660,14 @@ void dlb2_hw_set_qe_arbiter_weights(struct dlb2_hw *hw, u8 weight[8])
 	DLB2_CSR_WR(hw, DLB2_ATM_CFG_ARB_WEIGHTS_SCHED_BIN, reg);
 
 	reg = 0;
-	DLB2_BITS_SET(reg, weight[1], DLB2_AQED_CFG_ARB_WEIGHTS_TQPRI_ATM_0_PRI0);
-	DLB2_BITS_SET(reg, weight[3], DLB2_AQED_CFG_ARB_WEIGHTS_TQPRI_ATM_0_PRI1);
-	DLB2_BITS_SET(reg, weight[5], DLB2_AQED_CFG_ARB_WEIGHTS_TQPRI_ATM_0_PRI2);
-	DLB2_BITS_SET(reg, weight[7], DLB2_AQED_CFG_ARB_WEIGHTS_TQPRI_ATM_0_PRI3);
+	DLB2_BITS_SET(reg, weight[1],
+		      DLB2_AQED_CFG_ARB_WEIGHTS_TQPRI_ATM_0_PRI0);
+	DLB2_BITS_SET(reg, weight[3],
+		      DLB2_AQED_CFG_ARB_WEIGHTS_TQPRI_ATM_0_PRI1);
+	DLB2_BITS_SET(reg, weight[5],
+		      DLB2_AQED_CFG_ARB_WEIGHTS_TQPRI_ATM_0_PRI2);
+	DLB2_BITS_SET(reg, weight[7],
+		      DLB2_AQED_CFG_ARB_WEIGHTS_TQPRI_ATM_0_PRI3);
 	DLB2_CSR_WR(hw, DLB2_AQED_CFG_ARB_WEIGHTS_TQPRI_ATM_0, reg);
 }
 
@@ -6616,17 +10687,25 @@ void dlb2_hw_set_qid_arbiter_weights(struct dlb2_hw *hw, u8 weight[8])
 {
 	u32 reg = 0;
 
-	DLB2_BITS_SET(reg, weight[1], DLB2_LSP_CFG_ARB_WEIGHT_LDB_QID_0_PRI0_WEIGHT);
-	DLB2_BITS_SET(reg, weight[3], DLB2_LSP_CFG_ARB_WEIGHT_LDB_QID_0_PRI1_WEIGHT);
-	DLB2_BITS_SET(reg, weight[5], DLB2_LSP_CFG_ARB_WEIGHT_LDB_QID_0_PRI2_WEIGHT);
-	DLB2_BITS_SET(reg, weight[7], DLB2_LSP_CFG_ARB_WEIGHT_LDB_QID_0_PRI3_WEIGHT);
+	DLB2_BITS_SET(reg, weight[1],
+		      DLB2_LSP_CFG_ARB_WEIGHT_LDB_QID_0_PRI0_WEIGHT);
+	DLB2_BITS_SET(reg, weight[3],
+		      DLB2_LSP_CFG_ARB_WEIGHT_LDB_QID_0_PRI1_WEIGHT);
+	DLB2_BITS_SET(reg, weight[5],
+		      DLB2_LSP_CFG_ARB_WEIGHT_LDB_QID_0_PRI2_WEIGHT);
+	DLB2_BITS_SET(reg, weight[7],
+		      DLB2_LSP_CFG_ARB_WEIGHT_LDB_QID_0_PRI3_WEIGHT);
 	DLB2_CSR_WR(hw, DLB2_LSP_CFG_ARB_WEIGHT_LDB_QID_0(hw->ver), reg);
 
 	reg = 0;
-	DLB2_BITS_SET(reg, weight[1], DLB2_LSP_CFG_ARB_WEIGHT_ATM_NALB_QID_0_PRI0_WEIGHT);
-	DLB2_BITS_SET(reg, weight[3], DLB2_LSP_CFG_ARB_WEIGHT_ATM_NALB_QID_0_PRI1_WEIGHT);
-	DLB2_BITS_SET(reg, weight[5], DLB2_LSP_CFG_ARB_WEIGHT_ATM_NALB_QID_0_PRI2_WEIGHT);
-	DLB2_BITS_SET(reg, weight[7], DLB2_LSP_CFG_ARB_WEIGHT_ATM_NALB_QID_0_PRI3_WEIGHT);
+	DLB2_BITS_SET(reg, weight[1],
+		      DLB2_LSP_CFG_ARB_WEIGHT_ATM_NALB_QID_0_PRI0_WEIGHT);
+	DLB2_BITS_SET(reg, weight[3],
+		      DLB2_LSP_CFG_ARB_WEIGHT_ATM_NALB_QID_0_PRI1_WEIGHT);
+	DLB2_BITS_SET(reg, weight[5],
+		      DLB2_LSP_CFG_ARB_WEIGHT_ATM_NALB_QID_0_PRI2_WEIGHT);
+	DLB2_BITS_SET(reg, weight[7],
+		      DLB2_LSP_CFG_ARB_WEIGHT_ATM_NALB_QID_0_PRI3_WEIGHT);
 	DLB2_CSR_WR(hw, DLB2_LSP_CFG_ARB_WEIGHT_ATM_NALB_QID_0(hw->ver), reg);
 }
 
@@ -6637,10 +10716,14 @@ static void dlb2_log_enable_cq_weight(struct dlb2_hw *hw,
 				      unsigned int vdev_id)
 {
 	DLB2_HW_DBG(hw, "DLB2 enable CQ weight arguments:\n");
-	DLB2_HW_DBG(hw, "\tvdev_req %d, vdev_id %d\n", vdev_req, vdev_id);
-	DLB2_HW_DBG(hw, "\tDomain ID: %d\n", domain_id);
-	DLB2_HW_DBG(hw, "\tPort ID:   %d\n", args->port_id);
-	DLB2_HW_DBG(hw, "\tLimit:   %d\n", args->limit);
+	if (vdev_req)
+		DLB2_HW_DBG(hw, "(Request from vdev %d)\n", vdev_id);
+	DLB2_HW_DBG(hw, "\tDomain ID: %d\n",
+		    domain_id);
+	DLB2_HW_DBG(hw, "\tPort ID:   %d\n",
+		    args->port_id);
+	DLB2_HW_DBG(hw, "\tLimit:   %d\n",
+		    args->limit);
 }
 
 static int
@@ -6694,11 +10777,11 @@ dlb2_verify_enable_cq_weight_args(struct dlb2_hw *hw,
 }
 
 int dlb2_hw_enable_cq_weight(struct dlb2_hw *hw,
-			     u32 domain_id,
-			     struct dlb2_enable_cq_weight_args *args,
-			     struct dlb2_cmd_response *resp,
-			     bool vdev_req,
-			     unsigned int vdev_id)
+			  u32 domain_id,
+			  struct dlb2_enable_cq_weight_args *args,
+			  struct dlb2_cmd_response *resp,
+			  bool vdev_req,
+			  unsigned int vdev_id)
 {
 	struct dlb2_hw_domain *domain;
 	struct dlb2_ldb_port *port;
@@ -6733,7 +10816,7 @@ int dlb2_hw_enable_cq_weight(struct dlb2_hw *hw,
 	port = dlb2_get_domain_used_ldb_port(id, vdev_req, domain);
 	if (!port) {
 		DLB2_HW_ERR(hw,
-			    "[%s():	%d] Internal error: port not found\n",
+			    "[%s():%d] Internal error: port not found\n",
 			    __func__, __LINE__);
 		return -EFAULT;
 	}
@@ -6748,68 +10831,165 @@ int dlb2_hw_enable_cq_weight(struct dlb2_hw *hw,
 	return 0;
 }
 
-static void dlb2_log_set_cos_bandwidth(struct dlb2_hw *hw, u32 cos_id, u8 bw)
+int dlb2_hw_set_cq_inflight_ctrl(struct dlb2_hw *hw, u32 domain_id,
+                                 struct dlb2_cq_inflight_ctrl_args *args,
+                                 struct dlb2_cmd_response *resp, bool vdev_req,
+                                 unsigned int vdev_id)
 {
-	DLB2_HW_DBG(hw, "DLB2 set port CoS bandwidth:\n");
-	DLB2_HW_DBG(hw, "\tCoS ID:    %u\n", cos_id);
-	DLB2_HW_DBG(hw, "\tBandwidth: %u\n", bw);
+	struct dlb2_hw_domain *domain;
+	struct dlb2_ldb_port *port;
+	u32 reg = 0;
+	int id;
+
+	domain = dlb2_get_domain_from_id(hw, domain_id, vdev_req, vdev_id);
+	if (!domain) {
+		DLB2_HW_ERR(hw,
+			    "[%s():%d] Internal error: domain not found\n",
+			    __func__, __LINE__);
+		return -EINVAL;
+	}
+
+	id = args->port_id;
+
+	port = dlb2_get_domain_ldb_port(id, vdev_req, domain);
+	if (!port) {
+		DLB2_HW_ERR(hw,
+			    "[%s():%d] Internal error: port not found\n",
+			    __func__, __LINE__);
+		return -EINVAL;
+	}
+
+	DLB2_BITS_SET(reg, args->enable,
+		      DLB2_LSP_CFG_CTRL_GENERAL_0_ENAB_IF_THRESH_V2_5);
+	DLB2_CSR_WR(hw, DLB2_V2_5LSP_CFG_CTRL_GENERAL_0, reg);
+
+	if (args->enable) {
+		reg = 0;
+		DLB2_BITS_SET(reg, args->threshold,
+			      DLB2_LSP_CQ_LDB_INFL_THRESH_THRESH);
+		DLB2_CSR_WR(hw, DLB2_LSP_CQ_LDB_INFL_THRESH(port->id.phys_id),
+			    reg);
+	}
+
+	resp->status = 0;
+
+	return 0;
 }
 
-#define DLB2_MAX_BW_PCT 100
+void dlb2_hw_disable_ldb_sched_perf_ctrl(struct dlb2_hw *hw)
+{
+	u32 reg;
 
-/**
- * dlb2_hw_set_cos_bandwidth() - set a bandwidth allocation percentage for a
- *      port class-of-service.
- * @hw: dlb2_hw handle for a particular device.
- * @cos_id: class-of-service ID.
- * @bandwidth: class-of-service bandwidth.
- *
- * Return:
- * Returns 0 upon success, < 0 otherwise.
- *
- * Errors:
- * EINVAL - Invalid cos ID, bandwidth is greater than 100, or bandwidth would
- *          cause the total bandwidth across all classes of service to exceed
- *          100%.
- */
-int dlb2_hw_set_cos_bandwidth(struct dlb2_hw *hw, u32 cos_id, u8 bandwidth)
+	reg = DLB2_CSR_RD(hw, DLB2_LSP_LDB_SCHED_PERF_CTRL);
+	DLB2_BIT_SET(reg, DLB2_LSP_LDB_SCHED_PERF_CTRL_CLR);
+	DLB2_CSR_WR(hw, DLB2_LSP_LDB_SCHED_PERF_CTRL, reg);
+	dlb2_flush_csr(hw);
+}
+
+void dlb2_hw_enable_ldb_sched_perf_ctrl(struct dlb2_hw *hw)
 {
-	unsigned int i;
 	u32 reg;
-	u8 total;
 
-	if (cos_id >= DLB2_NUM_COS_DOMAINS)
-		return -EINVAL;
+	reg = DLB2_CSR_RD(hw, DLB2_LSP_LDB_SCHED_PERF_CTRL);
+	DLB2_BIT_SET(reg, DLB2_LSP_LDB_SCHED_PERF_CTRL_ENAB);
+	DLB2_CSR_WR(hw, DLB2_LSP_LDB_SCHED_PERF_CTRL, reg);
+	dlb2_flush_csr(hw);
+}
 
-	if (bandwidth > DLB2_MAX_BW_PCT)
-		return -EINVAL;
+static u64 dlb2_hw_read_perf_counter(struct dlb2_hw *hw,
+				     u32 low_offset,
+				     u32 high_offset)
+{
+	u32 low, high, cmp;
 
-	total = 0;
+	high = DLB2_CSR_RD(hw, high_offset);
+	low  = DLB2_CSR_RD(hw, low_offset);
+	cmp  = DLB2_CSR_RD(hw, high_offset);
 
-	for (i = 0; i < DLB2_NUM_COS_DOMAINS; i++)
-		total += (i == cos_id) ? bandwidth : hw->cos_reservation[i];
+	/* Handle the wrap case */
+	if (high != cmp) {
+		high = cmp;
+		low = DLB2_CSR_RD(hw, low_offset);
+	}
+	return ((((u64)high) << 32) | low);
+}
 
-	if (total > DLB2_MAX_BW_PCT)
-		return -EINVAL;
+int dlb2_read_sched_idle_counts(struct dlb2_hw *hw,
+                                 struct dlb2_hw_sched_idle_counts *data)
+{
+	u32 lo, hi;
 
-	reg = DLB2_CSR_RD(hw, DLB2_LSP_CFG_SHDW_RANGE_COS(hw->ver, cos_id));
+	if (hw == NULL || data == NULL)
+		return -1;
 
-	/*
-	 * Normalize the bandwidth to a value in the range 0-255. Integer
-	 * division may leave unreserved scheduling slots; these will be
-	 * divided among the 4 classes of service.
-	 */
-	DLB2_BITS_SET(reg, (bandwidth * 256) / 100, DLB2_LSP_CFG_SHDW_RANGE_COS_BW_RANGE);
-	DLB2_CSR_WR(hw, DLB2_LSP_CFG_SHDW_RANGE_COS(hw->ver, cos_id), reg);
+	memset(data, 0, sizeof(*data));
 
-	reg = 0;
-	DLB2_BIT_SET(reg, DLB2_LSP_CFG_SHDW_CTRL_TRANSFER);
-	/* Atomically transfer the newly configured service weight */
-	DLB2_CSR_WR(hw, DLB2_LSP_CFG_SHDW_CTRL(hw->ver), reg);
+	lo = DLB2_LSP_LDB_SCHED_PERF_0_L;
+	hi = DLB2_LSP_LDB_SCHED_PERF_0_H;
+	data->ldb_perf_nowork_idle_cnt =
+				dlb2_hw_read_perf_counter(hw, lo, hi);
 
-	dlb2_log_set_cos_bandwidth(hw, cos_id, bandwidth);
+	lo = DLB2_LSP_LDB_SCHED_PERF_1_L;
+	hi = DLB2_LSP_LDB_SCHED_PERF_1_H;
+	data->ldb_perf_nospace_idle_cnt =
+				dlb2_hw_read_perf_counter(hw, lo, hi);
 
-	hw->cos_reservation[cos_id] = bandwidth;
+	lo = DLB2_LSP_LDB_SCHED_PERF_2_L;
+	hi = DLB2_LSP_LDB_SCHED_PERF_2_H;
+	data->ldb_perf_sched_cnt =
+				dlb2_hw_read_perf_counter(hw, lo, hi);
+
+	lo = DLB2_LSP_LDB_SCHED_PERF_3_L;
+	hi = DLB2_LSP_LDB_SCHED_PERF_3_H;
+	data->ldb_perf_pfriction_idle_cnt =
+				dlb2_hw_read_perf_counter(hw, lo, hi);
+
+	lo = DLB2_LSP_LDB_SCHED_PERF_5_L;
+	hi = DLB2_LSP_LDB_SCHED_PERF_5_H;
+	data->ldb_perf_pfriction_idle_cnt +=
+				dlb2_hw_read_perf_counter(hw, lo, hi);
+
+	lo = DLB2_LSP_LDB_SCHED_PERF_4_L;
+	hi = DLB2_LSP_LDB_SCHED_PERF_4_H;
+	data->ldb_perf_iflimit_idle_cnt =
+				dlb2_hw_read_perf_counter(hw, lo, hi);
+
+	lo = DLB2_LSP_LDB_SCHED_PERF_6_L;
+	hi = DLB2_LSP_LDB_SCHED_PERF_6_H;
+	data->ldb_perf_fidlimit_idle_cnt =
+				dlb2_hw_read_perf_counter(hw, lo, hi);
+
+	lo = DLB2_CM_PROC_ON_CNT_L;
+	hi = DLB2_CM_PROC_ON_CNT_H;
+	data->perf_proc_on_cnt = dlb2_hw_read_perf_counter(hw, lo, hi);
+
+	lo = DLB2_CM_CLK_ON_CNT_L;
+	hi = DLB2_CM_CLK_ON_CNT_H;
+	data->perf_clk_on_cnt =	dlb2_hw_read_perf_counter(hw, lo, hi);
+
+	lo = DLB2_SYS_CNT_4;
+	hi = DLB2_SYS_CNT_5;
+	data->hcw_err_cnt = dlb2_hw_read_perf_counter(hw, lo, hi);
+
+	lo = DLB2_CHP_CFG_CNTR_CHP_ERR_DROP_L;
+	hi = DLB2_CHP_CFG_CNTR_CHP_ERR_DROP_H;
+	data->hcw_err_cnt += dlb2_hw_read_perf_counter(hw, lo, hi);
 
 	return 0;
 }
+
+void dlb2_hw_set_rate_limit(struct dlb2_hw *hw, int rate_limit)
+{
+	u32 reg;
+
+	reg = DLB2_CSR_RD(hw, DLB2_SYS_WRITE_BUFFER_CTL);
+
+	if (hw->ver == DLB2_HW_V2)
+		DLB2_BITS_SET(reg, rate_limit,
+			      DLB2_SYS_WRITE_BUFFER_CTL_SCH_RATE_LIMIT_V2);
+	else
+		DLB2_BITS_SET(reg, rate_limit,
+			      DLB2_SYS_WRITE_BUFFER_CTL_SCH_RATE_LIMIT_V2_5);
+
+	DLB2_CSR_WR(hw, DLB2_SYS_WRITE_BUFFER_CTL, reg);
+}
diff --git a/drivers/event/dlb2/pf/base/dlb2_resource.h b/drivers/event/dlb2/pf/base/dlb2_resource.h
index 71bd614..011239b 100644
--- a/drivers/event/dlb2/pf/base/dlb2_resource.h
+++ b/drivers/event/dlb2/pf/base/dlb2_resource.h
@@ -6,6 +6,8 @@
 #define __DLB2_RESOURCE_H
 
 #include "dlb2_user.h"
+
+#include "dlb2_hw_types.h"
 #include "dlb2_osdep_types.h"
 
 /**
@@ -37,17 +39,6 @@ int dlb2_resource_init(struct dlb2_hw *hw, enum dlb2_hw_ver ver, const void *pro
  */
 int dlb2_resource_probe(struct dlb2_hw *hw, const void *probe_args);
 
-
-/**
- * dlb2_clr_pmcsr_disable() - power on bulk of DLB 2.0 logic
- * @hw: dlb2_hw handle for a particular device.
- * @ver: device version.
- *
- * Clearing the PMCSR must be done at initialization to make the device fully
- * operational.
- */
-void dlb2_clr_pmcsr_disable(struct dlb2_hw *hw, enum dlb2_hw_ver ver);
-
 /**
  * dlb2_resource_free() - free device state memory
  * @hw: dlb2_hw handle for a particular device.
@@ -71,8 +62,8 @@ void dlb2_resource_reset(struct dlb2_hw *hw);
  * @hw: dlb2_hw handle for a particular device.
  * @args: scheduling domain creation arguments.
  * @resp: response structure.
- * @vdev_request: indicates whether this request came from a vdev.
- * @vdev_id: If vdev_request is true, this contains the vdev's ID.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
  *
  * This function creates a scheduling domain containing the resources specified
  * in args. The individual resources (queues, ports, credits) can be configured
@@ -86,7 +77,7 @@ void dlb2_resource_reset(struct dlb2_hw *hw);
  * assigned a detailed error code from enum dlb2_error. If successful, resp->id
  * contains the domain ID.
  *
- * resp->id contains a virtual ID if vdev_request is true.
+ * resp->id contains a virtual ID if vdev_req is true.
  *
  * Errors:
  * EINVAL - A requested resource is unavailable, or the requested domain name
@@ -96,7 +87,7 @@ void dlb2_resource_reset(struct dlb2_hw *hw);
 int dlb2_hw_create_sched_domain(struct dlb2_hw *hw,
 				struct dlb2_create_sched_domain_args *args,
 				struct dlb2_cmd_response *resp,
-				bool vdev_request,
+				bool vdev_req,
 				unsigned int vdev_id);
 
 /**
@@ -105,8 +96,8 @@ int dlb2_hw_create_sched_domain(struct dlb2_hw *hw,
  * @domain_id: domain ID.
  * @args: queue creation arguments.
  * @resp: response structure.
- * @vdev_request: indicates whether this request came from a vdev.
- * @vdev_id: If vdev_request is true, this contains the vdev's ID.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
  *
  * This function creates a load-balanced queue.
  *
@@ -118,7 +109,7 @@ int dlb2_hw_create_sched_domain(struct dlb2_hw *hw,
  * assigned a detailed error code from enum dlb2_error. If successful, resp->id
  * contains the queue ID.
  *
- * resp->id contains a virtual ID if vdev_request is true.
+ * resp->id contains a virtual ID if vdev_req is true.
  *
  * Errors:
  * EINVAL - A requested resource is unavailable, the domain is not configured,
@@ -130,7 +121,7 @@ int dlb2_hw_create_ldb_queue(struct dlb2_hw *hw,
 			     u32 domain_id,
 			     struct dlb2_create_ldb_queue_args *args,
 			     struct dlb2_cmd_response *resp,
-			     bool vdev_request,
+			     bool vdev_req,
 			     unsigned int vdev_id);
 
 /**
@@ -139,8 +130,8 @@ int dlb2_hw_create_ldb_queue(struct dlb2_hw *hw,
  * @domain_id: domain ID.
  * @args: queue creation arguments.
  * @resp: response structure.
- * @vdev_request: indicates whether this request came from a vdev.
- * @vdev_id: If vdev_request is true, this contains the vdev's ID.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
  *
  * This function creates a directed queue.
  *
@@ -152,7 +143,7 @@ int dlb2_hw_create_ldb_queue(struct dlb2_hw *hw,
  * assigned a detailed error code from enum dlb2_error. If successful, resp->id
  * contains the queue ID.
  *
- * resp->id contains a virtual ID if vdev_request is true.
+ * resp->id contains a virtual ID if vdev_req is true.
  *
  * Errors:
  * EINVAL - A requested resource is unavailable, the domain is not configured,
@@ -163,7 +154,7 @@ int dlb2_hw_create_dir_queue(struct dlb2_hw *hw,
 			     u32 domain_id,
 			     struct dlb2_create_dir_queue_args *args,
 			     struct dlb2_cmd_response *resp,
-			     bool vdev_request,
+			     bool vdev_req,
 			     unsigned int vdev_id);
 
 /**
@@ -173,8 +164,8 @@ int dlb2_hw_create_dir_queue(struct dlb2_hw *hw,
  * @args: port creation arguments.
  * @cq_dma_base: base address of the CQ memory. This can be a PA or an IOVA.
  * @resp: response structure.
- * @vdev_request: indicates whether this request came from a vdev.
- * @vdev_id: If vdev_request is true, this contains the vdev's ID.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
  *
  * This function creates a directed port.
  *
@@ -186,7 +177,7 @@ int dlb2_hw_create_dir_queue(struct dlb2_hw *hw,
  * assigned a detailed error code from enum dlb2_error. If successful, resp->id
  * contains the port ID.
  *
- * resp->id contains a virtual ID if vdev_request is true.
+ * resp->id contains a virtual ID if vdev_req is true.
  *
  * Errors:
  * EINVAL - A requested resource is unavailable, a credit setting is invalid, a
@@ -199,7 +190,7 @@ int dlb2_hw_create_dir_port(struct dlb2_hw *hw,
 			    struct dlb2_create_dir_port_args *args,
 			    uintptr_t cq_dma_base,
 			    struct dlb2_cmd_response *resp,
-			    bool vdev_request,
+			    bool vdev_req,
 			    unsigned int vdev_id);
 
 /**
@@ -209,8 +200,8 @@ int dlb2_hw_create_dir_port(struct dlb2_hw *hw,
  * @args: port creation arguments.
  * @cq_dma_base: base address of the CQ memory. This can be a PA or an IOVA.
  * @resp: response structure.
- * @vdev_request: indicates whether this request came from a vdev.
- * @vdev_id: If vdev_request is true, this contains the vdev's ID.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
  *
  * This function creates a load-balanced port.
  *
@@ -222,7 +213,7 @@ int dlb2_hw_create_dir_port(struct dlb2_hw *hw,
  * assigned a detailed error code from enum dlb2_error. If successful, resp->id
  * contains the port ID.
  *
- * resp->id contains a virtual ID if vdev_request is true.
+ * resp->id contains a virtual ID if vdev_req is true.
  *
  * Errors:
  * EINVAL - A requested resource is unavailable, a credit setting is invalid, a
@@ -235,7 +226,7 @@ int dlb2_hw_create_ldb_port(struct dlb2_hw *hw,
 			    struct dlb2_create_ldb_port_args *args,
 			    uintptr_t cq_dma_base,
 			    struct dlb2_cmd_response *resp,
-			    bool vdev_request,
+			    bool vdev_req,
 			    unsigned int vdev_id);
 
 /**
@@ -244,8 +235,8 @@ int dlb2_hw_create_ldb_port(struct dlb2_hw *hw,
  * @domain_id: domain ID.
  * @args: start domain arguments.
  * @resp: response structure.
- * @vdev_request: indicates whether this request came from a vdev.
- * @vdev_id: If vdev_request is true, this contains the vdev's ID.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
  *
  * This function starts a scheduling domain, which allows applications to send
  * traffic through it. Once a domain is started, its resources can no longer be
@@ -265,7 +256,36 @@ int dlb2_hw_start_domain(struct dlb2_hw *hw,
 			 u32 domain_id,
 			 struct dlb2_start_domain_args *args,
 			 struct dlb2_cmd_response *resp,
-			 bool vdev_request,
+			 bool vdev_req,
+			 unsigned int vdev_id);
+
+/**
+ * dlb2_hw_stop_domain() - stop a scheduling domain
+ * @hw: dlb2_hw handle for a particular device.
+ * @domain_id: domain ID.
+ * @args: stop domain arguments.
+ * @resp: response structure.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
+ *
+ * This function stops a scheduling domain. When stopped applications can no 
+ * longer send traffic through it.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ *
+ * Return:
+ * Returns 0 upon success, < 0 otherwise. If an error occurs, resp->status is
+ * assigned a detailed error code from enum dlb2_error.
+ *
+ * Errors:
+ * EINVAL - the domain is not configured, or the domain is already stopped.
+ */
+int dlb2_hw_stop_domain(struct dlb2_hw *hw,
+			 u32 domain_id,
+			 struct dlb2_stop_domain_args *args,
+			 struct dlb2_cmd_response *resp,
+			 bool vdev_req,
 			 unsigned int vdev_id);
 
 /**
@@ -274,8 +294,8 @@ int dlb2_hw_start_domain(struct dlb2_hw *hw,
  * @domain_id: domain ID.
  * @args: map QID arguments.
  * @resp: response structure.
- * @vdev_request: indicates whether this request came from a vdev.
- * @vdev_id: If vdev_request is true, this contains the vdev's ID.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
  *
  * This function configures the DLB to schedule QEs from the specified queue
  * to the specified port. Each load-balanced port can be mapped to up to 8
@@ -313,7 +333,7 @@ int dlb2_hw_map_qid(struct dlb2_hw *hw,
 		    u32 domain_id,
 		    struct dlb2_map_qid_args *args,
 		    struct dlb2_cmd_response *resp,
-		    bool vdev_request,
+		    bool vdev_req,
 		    unsigned int vdev_id);
 
 /**
@@ -322,8 +342,8 @@ int dlb2_hw_map_qid(struct dlb2_hw *hw,
  * @domain_id: domain ID.
  * @args: unmap QID arguments.
  * @resp: response structure.
- * @vdev_request: indicates whether this request came from a vdev.
- * @vdev_id: If vdev_request is true, this contains the vdev's ID.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
  *
  * This function configures the DLB to stop scheduling QEs from the specified
  * queue to the specified port.
@@ -351,7 +371,7 @@ int dlb2_hw_unmap_qid(struct dlb2_hw *hw,
 		      u32 domain_id,
 		      struct dlb2_unmap_qid_args *args,
 		      struct dlb2_cmd_response *resp,
-		      bool vdev_request,
+		      bool vdev_req,
 		      unsigned int vdev_id);
 
 /**
@@ -386,8 +406,8 @@ unsigned int dlb2_finish_map_qid_procedures(struct dlb2_hw *hw);
  * @domain_id: domain ID.
  * @args: port enable arguments.
  * @resp: response structure.
- * @vdev_request: indicates whether this request came from a vdev.
- * @vdev_id: If vdev_request is true, this contains the vdev's ID.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
  *
  * This function configures the DLB to schedule QEs to a load-balanced port.
  * Ports are enabled by default.
@@ -407,7 +427,7 @@ int dlb2_hw_enable_ldb_port(struct dlb2_hw *hw,
 			    u32 domain_id,
 			    struct dlb2_enable_ldb_port_args *args,
 			    struct dlb2_cmd_response *resp,
-			    bool vdev_request,
+			    bool vdev_req,
 			    unsigned int vdev_id);
 
 /**
@@ -416,8 +436,8 @@ int dlb2_hw_enable_ldb_port(struct dlb2_hw *hw,
  * @domain_id: domain ID.
  * @args: port disable arguments.
  * @resp: response structure.
- * @vdev_request: indicates whether this request came from a vdev.
- * @vdev_id: If vdev_request is true, this contains the vdev's ID.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
  *
  * This function configures the DLB to stop scheduling QEs to a load-balanced
  * port. Ports are enabled by default.
@@ -437,7 +457,7 @@ int dlb2_hw_disable_ldb_port(struct dlb2_hw *hw,
 			     u32 domain_id,
 			     struct dlb2_disable_ldb_port_args *args,
 			     struct dlb2_cmd_response *resp,
-			     bool vdev_request,
+			     bool vdev_req,
 			     unsigned int vdev_id);
 
 /**
@@ -446,8 +466,8 @@ int dlb2_hw_disable_ldb_port(struct dlb2_hw *hw,
  * @domain_id: domain ID.
  * @args: port enable arguments.
  * @resp: response structure.
- * @vdev_request: indicates whether this request came from a vdev.
- * @vdev_id: If vdev_request is true, this contains the vdev's ID.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
  *
  * This function configures the DLB to schedule QEs to a directed port.
  * Ports are enabled by default.
@@ -467,7 +487,7 @@ int dlb2_hw_enable_dir_port(struct dlb2_hw *hw,
 			    u32 domain_id,
 			    struct dlb2_enable_dir_port_args *args,
 			    struct dlb2_cmd_response *resp,
-			    bool vdev_request,
+			    bool vdev_req,
 			    unsigned int vdev_id);
 
 /**
@@ -476,8 +496,8 @@ int dlb2_hw_enable_dir_port(struct dlb2_hw *hw,
  * @domain_id: domain ID.
  * @args: port disable arguments.
  * @resp: response structure.
- * @vdev_request: indicates whether this request came from a vdev.
- * @vdev_id: If vdev_request is true, this contains the vdev's ID.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
  *
  * This function configures the DLB to stop scheduling QEs to a directed port.
  * Ports are enabled by default.
@@ -497,7 +517,7 @@ int dlb2_hw_disable_dir_port(struct dlb2_hw *hw,
 			     u32 domain_id,
 			     struct dlb2_disable_dir_port_args *args,
 			     struct dlb2_cmd_response *resp,
-			     bool vdev_request,
+			     bool vdev_req,
 			     unsigned int vdev_id);
 
 /**
@@ -600,8 +620,8 @@ void dlb2_ack_msix_interrupt(struct dlb2_hw *hw, int vector);
  * @hw: dlb2_hw handle for a particular device.
  * @port_id: port ID
  * @is_ldb: true for load-balanced port, false for a directed port
- * @vdev_request: indicates whether this request came from a vdev.
- * @vdev_id: If vdev_request is true, this contains the vdev's ID.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
  *
  * This function arms the CQ's interrupt. The CQ must be configured prior to
  * calling this function.
@@ -619,7 +639,7 @@ void dlb2_ack_msix_interrupt(struct dlb2_hw *hw, int vector);
 int dlb2_arm_cq_interrupt(struct dlb2_hw *hw,
 			  int port_id,
 			  bool is_ldb,
-			  bool vdev_request,
+			  bool vdev_req,
 			  unsigned int vdev_id);
 
 /**
@@ -809,7 +829,7 @@ bool dlb2_process_ingress_error_interrupt(struct dlb2_hw *hw);
  * Returns -EINVAL if group_id is invalid, else the group's SNs per queue.
  */
 int dlb2_get_group_sequence_numbers(struct dlb2_hw *hw,
-				    unsigned int group_id);
+				    u32 group_id);
 
 /**
  * dlb2_get_group_sequence_number_occupancy() - return a group's in-use slots
@@ -823,7 +843,7 @@ int dlb2_get_group_sequence_numbers(struct dlb2_hw *hw,
  * Returns -EINVAL if group_id is invalid, else the group's SNs per queue.
  */
 int dlb2_get_group_sequence_number_occupancy(struct dlb2_hw *hw,
-					     unsigned int group_id);
+					     u32 group_id);
 
 /**
  * dlb2_set_group_sequence_numbers() - assign a group's number of SNs per queue
@@ -848,8 +868,8 @@ int dlb2_set_group_sequence_numbers(struct dlb2_hw *hw,
  * dlb2_reset_domain() - reset a scheduling domain
  * @hw: dlb2_hw handle for a particular device.
  * @domain_id: domain ID.
- * @vdev_request: indicates whether this request came from a vdev.
- * @vdev_id: If vdev_request is true, this contains the vdev's ID.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
  *
  * This function resets and frees a DLB 2.0 scheduling domain and its associated
  * resources.
@@ -871,7 +891,7 @@ int dlb2_set_group_sequence_numbers(struct dlb2_hw *hw,
  */
 int dlb2_reset_domain(struct dlb2_hw *hw,
 		      u32 domain_id,
-		      bool vdev_request,
+		      bool vdev_req,
 		      unsigned int vdev_id);
 
 /**
@@ -879,8 +899,8 @@ int dlb2_reset_domain(struct dlb2_hw *hw,
  * @hw: dlb2_hw handle for a particular device.
  * @domain_id: domain ID.
  * @port_id: indicates whether this request came from a VF.
- * @vdev_request: indicates whether this request came from a vdev.
- * @vdev_id: If vdev_request is true, this contains the vdev's ID.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
  *
  * This function returns whether a load-balanced port is owned by a specified
  * domain.
@@ -896,7 +916,7 @@ int dlb2_reset_domain(struct dlb2_hw *hw,
 int dlb2_ldb_port_owned_by_domain(struct dlb2_hw *hw,
 				  u32 domain_id,
 				  u32 port_id,
-				  bool vdev_request,
+				  bool vdev_req,
 				  unsigned int vdev_id);
 
 /**
@@ -904,8 +924,8 @@ int dlb2_ldb_port_owned_by_domain(struct dlb2_hw *hw,
  * @hw: dlb2_hw handle for a particular device.
  * @domain_id: domain ID.
  * @port_id: indicates whether this request came from a VF.
- * @vdev_request: indicates whether this request came from a vdev.
- * @vdev_id: If vdev_request is true, this contains the vdev's ID.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
  *
  * This function returns whether a directed port is owned by a specified
  * domain.
@@ -921,15 +941,15 @@ int dlb2_ldb_port_owned_by_domain(struct dlb2_hw *hw,
 int dlb2_dir_port_owned_by_domain(struct dlb2_hw *hw,
 				  u32 domain_id,
 				  u32 port_id,
-				  bool vdev_request,
+				  bool vdev_req,
 				  unsigned int vdev_id);
 
 /**
  * dlb2_hw_get_num_resources() - query the PCI function's available resources
  * @hw: dlb2_hw handle for a particular device.
  * @arg: pointer to resource counts.
- * @vdev_request: indicates whether this request came from a vdev.
- * @vdev_id: If vdev_request is true, this contains the vdev's ID.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
  *
  * This function returns the number of available resources for the PF or for a
  * VF.
@@ -938,20 +958,20 @@ int dlb2_dir_port_owned_by_domain(struct dlb2_hw *hw,
  * device.
  *
  * Return:
- * Returns 0 upon success, -EINVAL if vdev_request is true and vdev_id is
+ * Returns 0 upon success, -EINVAL if vdev_req is true and vdev_id is
  * invalid.
  */
 int dlb2_hw_get_num_resources(struct dlb2_hw *hw,
 			      struct dlb2_get_num_resources_args *arg,
-			      bool vdev_request,
+			      bool vdev_req,
 			      unsigned int vdev_id);
 
 /**
  * dlb2_hw_get_num_used_resources() - query the PCI function's used resources
  * @hw: dlb2_hw handle for a particular device.
  * @arg: pointer to resource counts.
- * @vdev_request: indicates whether this request came from a vdev.
- * @vdev_id: If vdev_request is true, this contains the vdev's ID.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
  *
  * This function returns the number of resources in use by the PF or a VF. It
  * fills in the fields that args points to, except the following:
@@ -964,12 +984,12 @@ int dlb2_hw_get_num_resources(struct dlb2_hw *hw,
  * device.
  *
  * Return:
- * Returns 0 upon success, -EINVAL if vdev_request is true and vdev_id is
+ * Returns 0 upon success, -EINVAL if vdev_req is true and vdev_id is
  * invalid.
  */
 int dlb2_hw_get_num_used_resources(struct dlb2_hw *hw,
 				   struct dlb2_get_num_resources_args *arg,
-				   bool vdev_request,
+				   bool vdev_req,
 				   unsigned int vdev_id);
 
 /**
@@ -1407,6 +1427,30 @@ int dlb2_update_vdev_ldb_credits(struct dlb2_hw *hw, u32 id, u32 num);
 int dlb2_update_vdev_dir_credits(struct dlb2_hw *hw, u32 id, u32 num);
 
 /**
+ * dlb2_update_vdev_credits() - update the vdev's assigned credits
+ * @hw: dlb2_hw handle for a particular device.
+ * @id: virtual device ID
+ * @num: number of credits to assign to this vdev
+ *
+ * This function assigns num credits to the specified vdev. If the vdev
+ * already has credits assigned, this existing assignment is adjusted
+ * accordingly. vdevs are assigned a contiguous chunk of credits, so this
+ * function may fail if a sufficiently large contiguous chunk is not available.
+ *
+ * A vdev can be either an SR-IOV virtual function or a Scalable IOV virtual
+ * device.
+ *
+ * Return:
+ * Returns 0 upon success, <0 otherwise.
+ *
+ * Errors:
+ * EINVAL - id is invalid, or the requested number of resources are
+ *	    unavailable.
+ * EPERM  - The vdev's resource assignment is locked and cannot be changed.
+ */
+int dlb2_update_vdev_credits(struct dlb2_hw *hw, u32 id, u32 num);
+
+/**
  * dlb2_update_vdev_hist_list_entries() - update the vdev's assigned HL entries
  * @hw: dlb2_hw handle for a particular device.
  * @id: virtual device ID
@@ -1508,13 +1552,23 @@ int dlb2_notify_vf(struct dlb2_hw *hw,
 int dlb2_vdev_in_use(struct dlb2_hw *hw, unsigned int id);
 
 /**
+ * dlb2_clr_pmcsr_disable() - power on bulk of DLB 2.0 logic
+ * @hw: dlb2_hw handle for a particular device.
+ * @ver: device version.
+ *
+ * Clearing the PMCSR must be done at initialization to make the device fully
+ * operational.
+ */
+void dlb2_clr_pmcsr_disable(struct dlb2_hw *hw, enum dlb2_hw_ver ver);
+
+/**
  * dlb2_hw_get_ldb_queue_depth() - returns the depth of a load-balanced queue
  * @hw: dlb2_hw handle for a particular device.
  * @domain_id: domain ID.
  * @args: queue depth args
  * @resp: response structure.
- * @vdev_request: indicates whether this request came from a vdev.
- * @vdev_id: If vdev_request is true, this contains the vdev's ID.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
  *
  * This function returns the depth of a load-balanced queue.
  *
@@ -1533,7 +1587,7 @@ int dlb2_hw_get_ldb_queue_depth(struct dlb2_hw *hw,
 				u32 domain_id,
 				struct dlb2_get_ldb_queue_depth_args *args,
 				struct dlb2_cmd_response *resp,
-				bool vdev_request,
+				bool vdev_req,
 				unsigned int vdev_id);
 
 /**
@@ -1542,8 +1596,8 @@ int dlb2_hw_get_ldb_queue_depth(struct dlb2_hw *hw,
  * @domain_id: domain ID.
  * @args: queue depth args
  * @resp: response structure.
- * @vdev_request: indicates whether this request came from a vdev.
- * @vdev_id: If vdev_request is true, this contains the vdev's ID.
+ * @vdev_req: indicates whether this request came from a vdev.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
  *
  * This function returns the depth of a directed queue.
  *
@@ -1562,7 +1616,7 @@ int dlb2_hw_get_dir_queue_depth(struct dlb2_hw *hw,
 				u32 domain_id,
 				struct dlb2_get_dir_queue_depth_args *args,
 				struct dlb2_cmd_response *resp,
-				bool vdev_request,
+				bool vdev_req,
 				unsigned int vdev_id);
 
 enum dlb2_virt_mode {
@@ -1666,6 +1720,7 @@ void dlb2_hw_unregister_sw_mbox(struct dlb2_hw *hw, unsigned int vdev_id);
  * @vdev_id: vdev ID.
  * @virt_cq_id: virtual CQ ID.
  * @is_ldb: CQ is load-balanced.
+ * @addr_hi: most-significant 32 bits of address.
  * @addr_lo: least-significant 32 bits of address.
  * @data: 32 data bits.
  *
@@ -1678,6 +1733,7 @@ void dlb2_hw_setup_cq_ims_entry(struct dlb2_hw *hw,
 				unsigned int vdev_id,
 				u32 virt_cq_id,
 				bool is_ldb,
+				u32 addr_hi,
 				u32 addr_lo,
 				u32 data);
 
@@ -1718,8 +1774,8 @@ int dlb2_hw_register_pasid(struct dlb2_hw *hw,
  * @domain_id: domain ID.
  * @args: number of unmaps in progress args
  * @resp: response structure.
- * @vf_request: indicates whether this request came from a VF.
- * @vf_id: If vf_request is true, this contains the VF's ID.
+ * @vdev_request: indicates whether this request came from a VDEV.
+ * @vdev_id: If vdev_request is true, this contains the VDEV's ID.
  *
  * Return:
  * Returns 0 upon success, < 0 otherwise. If an error occurs, resp->status is
@@ -1733,8 +1789,8 @@ int dlb2_hw_pending_port_unmaps(struct dlb2_hw *hw,
 				u32 domain_id,
 				struct dlb2_pending_port_unmaps_args *args,
 				struct dlb2_cmd_response *resp,
-				bool vf_request,
-				unsigned int vf_id);
+				bool vdev_request,
+				unsigned int vdev_id);
 
 /**
  * dlb2_hw_get_cos_bandwidth() - returns the percent of bandwidth allocated
@@ -1800,7 +1856,7 @@ int dlb2_hw_enable_wd_timer(struct dlb2_hw *hw, enum dlb2_wd_tmo tmo);
  * @hw: dlb2_hw handle for a particular device.
  * @id: port ID.
  * @vdev_req: indicates whether this request came from a vdev.
- * @vdev_id: If vdev_request is true, this contains the vdev's ID.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
  *
  * Return:
  * Returns 0 upon success, < 0 otherwise.
@@ -1819,7 +1875,7 @@ int dlb2_hw_enable_dir_cq_wd_int(struct dlb2_hw *hw,
  * @hw: dlb2_hw handle for a particular device.
  * @id: port ID.
  * @vdev_req: indicates whether this request came from a vdev.
- * @vdev_id: If vdev_request is true, this contains the vdev's ID.
+ * @vdev_id: If vdev_req is true, this contains the vdev's ID.
  *
  * Return:
  * Returns 0 upon success, < 0 otherwise.
@@ -1850,6 +1906,20 @@ void dlb2_hw_enable_sparse_ldb_cq_mode(struct dlb2_hw *hw);
 void dlb2_hw_enable_sparse_dir_cq_mode(struct dlb2_hw *hw);
 
 /**
+ * dlb2_hw_enable_ldb_sched_perf_ctrl() - enable scheduling idle perf counters.
+ * @hw: dlb2_hw handle for a particular device.
+ *
+ */
+void dlb2_hw_enable_ldb_sched_perf_ctrl(struct dlb2_hw *hw);
+
+/**
+ * dlb2_hw_disable_ldb_sched_perf_ctrl() - clear scheduling idle perf counters.
+ * @hw: dlb2_hw handle for a particular device.
+ *
+ */
+void dlb2_hw_disable_ldb_sched_perf_ctrl(struct dlb2_hw *hw);
+
+/**
  * dlb2_hw_set_qe_arbiter_weights() - program QE arbiter weights
  * @hw: dlb2_hw handle for a particular device.
  * @weight: 8-entry array of arbiter weights.
@@ -1924,7 +1994,7 @@ void dlb2_hw_dir_cq_interrupt_set_mode(struct dlb2_hw *hw,
 				       int mode);
 
 /**
- * dlb2_hw_enable_cq_weight() - Enable QE-weight based scheduling on an LDB port.
+ * dlb2_enable_cq_weight() - Enable QE-weight based scheduling on an LDB port.
  * @hw: dlb2_hw handle for a particular device.
  * @domain_id: domain ID.
  * @args: CQ weight enablement arguments.
@@ -1945,15 +2015,51 @@ void dlb2_hw_dir_cq_interrupt_set_mode(struct dlb2_hw *hw,
  *
  * Errors:
  * EINVAL - The domain or port is not configured, the domainhas already been
- *          started, the requested limit exceeds the port's CQ depth, or this
- *          feature is unavailable on the device.
+ *	    started, the requested limit exceeds the port's CQ depth, or this
+ *	    feature is unavailable on the device.
  * EFAULT - Internal error (resp->status not set).
  */
 int dlb2_hw_enable_cq_weight(struct dlb2_hw *hw,
-			     u32 domain_id,
-			     struct dlb2_enable_cq_weight_args *args,
-			     struct dlb2_cmd_response *resp,
-			     bool vdev_request,
-			     unsigned int vdev_id);
+			  u32 domain_id,
+			  struct dlb2_enable_cq_weight_args *args,
+			  struct dlb2_cmd_response *resp,
+			  bool vdev_request,
+			  unsigned int vdev_id);
+/**
+ * This function configures the inflight control threshold for a cq.
+ *
+ * This must be called after creating the port.
+ *
+ * Return:
+ * Returns 0 upon success, < 0 otherwise. If an error occurs, resp->status is
+ * assigned a detailed error code from enum dlb2_error. If successful, resp->id
+ * contains the queue ID.
+ *
+ * Errors:
+ * EINVAL - The domain or port is not configured.
+ */
+int dlb2_hw_set_cq_inflight_ctrl(struct dlb2_hw *hw, u32 domain_id,
+                                 struct dlb2_cq_inflight_ctrl_args *args,
+                                 struct dlb2_cmd_response *resp,
+                                 bool vdev_request, unsigned int vdev_id);
+
+/**
+ * dlb2_read_sched_idle_counts() - read idle perf counter registers.
+ * @hw: dlb2_hw handle for a particular device.
+ * @data: dlb2_hw_sched_idle_counts idle counters structure.
+ *
+ * Return:
+ * Returns 0 upon success, < 0 otherwise.
+ */
+int dlb2_read_sched_idle_counts(struct dlb2_hw *hw,
+				struct dlb2_hw_sched_idle_counts *data);
+
+/**
+ * dlb2_hw_set_rate_limit() - rate limit total throughput.
+ * @hw: dlb2_hw handle for a particular device.
+ * @rate_limit: write buffer schedule rate limit.
+ *
+ */
+void dlb2_hw_set_rate_limit(struct dlb2_hw *hw, int rate_limit);
 
 #endif /* __DLB2_RESOURCE_H */
diff --git a/drivers/event/dlb2/pf/dlb2_main.c b/drivers/event/dlb2/pf/dlb2_main.c
index 717aa4f..4ee847e 100644
--- a/drivers/event/dlb2/pf/dlb2_main.c
+++ b/drivers/event/dlb2/pf/dlb2_main.c
@@ -13,10 +13,9 @@
 #include <rte_malloc.h>
 #include <rte_errno.h>
 
-#include "base/dlb2_regs.h"
-#include "base/dlb2_hw_types.h"
 #include "base/dlb2_resource.h"
 #include "base/dlb2_osdep.h"
+#include "base/dlb2_regs.h"
 #include "dlb2_main.h"
 #include "../dlb2_user.h"
 #include "../dlb2_priv.h"
@@ -46,6 +45,7 @@
 #define DLB2_PCI_CAP_ID_MSIX      0x11
 #define DLB2_PCI_EXT_CAP_ID_PRI   0x13
 #define DLB2_PCI_EXT_CAP_ID_ACS   0xD
+#define DLB2_PCI_EXT_CAP_ID_PASID 0x1B	/* Process Address Space ID */
 
 #define DLB2_PCI_PRI_CTRL_ENABLE         0x1
 #define DLB2_PCI_PRI_ALLOC_REQ           0xC
@@ -64,6 +64,8 @@
 #define DLB2_PCI_ACS_CR                  0x8
 #define DLB2_PCI_ACS_UF                  0x10
 #define DLB2_PCI_ACS_EC                  0x20
+#define DLB2_PCI_PASID_CTRL              0x06    /* PASID control register */
+#define DLB2_PCI_PASID_CAP_OFFSET        0x148   /* PASID capability offset */
 
 static int dlb2_pci_find_capability(struct rte_pci_device *pdev, uint32_t id)
 {
@@ -99,6 +101,11 @@ dlb2_pf_init_driver_state(struct dlb2_dev *dlb2_dev)
 {
 	rte_spinlock_init(&dlb2_dev->resource_mutex);
 
+	if (rte_cpu_get_flag_enabled(RTE_CPUFLAG_MOVDIR64B))
+		dlb2_dev->enqueue_four = dlb2_movdir64b;
+	else
+		dlb2_dev->enqueue_four = dlb2_movntdq;
+
 	return 0;
 }
 
@@ -210,7 +217,7 @@ dlb2_probe(struct rte_pci_device *pdev, const void *probe_args)
 
 	ret = dlb2_resource_probe(&dlb2_dev->hw, probe_args);
 	if (ret)
-		goto resource_probe_fail;
+		goto dlb2_resource_probe_fail;
 
 	ret = dlb2_pf_reset(dlb2_dev);
 	if (ret)
@@ -231,7 +238,7 @@ dlb2_probe(struct rte_pci_device *pdev, const void *probe_args)
 init_driver_state_fail:
 dlb2_reset_fail:
 pci_mmap_bad_addr:
-resource_probe_fail:
+dlb2_resource_probe_fail:
 wait_for_device_ready_fail:
 	rte_free(dlb2_dev);
 dlb2_dev_malloc_fail:
@@ -257,12 +264,14 @@ dlb2_pf_reset(struct dlb2_dev *dlb2_dev)
 	uint16_t rt_ctl_word;
 	uint32_t pri_reqs_dword;
 	uint16_t pri_ctrl_word;
+	uint16_t pasid_ctrl;
 
 	int pcie_cap_offset;
 	int pri_cap_offset;
 	int msix_cap_offset;
 	int err_cap_offset;
 	int acs_cap_offset;
+	int pasid_cap_offset;
 	int wait_count;
 
 	uint16_t devsta_busy_word;
@@ -582,6 +591,28 @@ dlb2_pf_reset(struct dlb2_dev *dlb2_dev)
 		}
 	}
 
+	/* The current Linux kernel vfio driver does not expose PASID capability to
+	 * users. It also enables PASID by default, which breaks DLB PF PMD. We have
+	 * to use the hardcoded offset for now to disable PASID.
+	 */
+	pasid_cap_offset = DLB2_PCI_PASID_CAP_OFFSET;
+
+	off = pasid_cap_offset + DLB2_PCI_PASID_CTRL;
+	if (rte_pci_read_config(pdev, &pasid_ctrl, 2, off) != 2)
+		pasid_ctrl = 0;
+
+	if (pasid_ctrl) {
+		DLB2_INFO(dlb2_dev, "DLB2 disabling pasid...\n");
+
+		pasid_ctrl = 0;
+		ret = rte_pci_write_config(pdev, &pasid_ctrl, 2, off);
+		if (ret != 2) {
+			DLB2_LOG_ERR("[%s()] failed to write the pcie config space at offset %d\n",
+				__func__, (int)off);
+			return ret;
+		}
+	}
+
 	return 0;
 }
 
@@ -657,3 +688,13 @@ dlb2_pf_start_domain(struct dlb2_hw *hw,
 	return dlb2_hw_start_domain(hw, id, args, resp, NOT_VF_REQ,
 				    PF_ID_ZERO);
 }
+
+int
+dlb2_pf_stop_domain(struct dlb2_hw *hw,
+		     u32 id,
+		     struct dlb2_stop_domain_args *args,
+		     struct dlb2_cmd_response *resp)
+{
+	return dlb2_hw_stop_domain(hw, id, args, resp, NOT_VF_REQ,
+				   PF_ID_ZERO);
+}
diff --git a/drivers/event/dlb2/pf/dlb2_main.h b/drivers/event/dlb2/pf/dlb2_main.h
index 4c64d72..f96afa2 100644
--- a/drivers/event/dlb2/pf/dlb2_main.h
+++ b/drivers/event/dlb2/pf/dlb2_main.h
@@ -9,18 +9,30 @@
 #include <rte_log.h>
 #include <rte_spinlock.h>
 #include <rte_pci.h>
-#include <bus_pci_driver.h>
-#include <rte_eal_paging.h>
+
+#ifndef PAGE_SIZE
+#define PAGE_SIZE (sysconf(_SC_PAGESIZE))
+#endif
 
 #include "base/dlb2_hw_types.h"
 #include "../dlb2_user.h"
 
-#define DLB2_EAL_PROBE_CORE 2
+#define DLB2_DEFAULT_UNREGISTER_TIMEOUT_S 5
 #define DLB2_NUM_PROBE_ENQS 1000
 #define DLB2_HCW_MEM_SIZE 8
 #define DLB2_HCW_64B_OFF 4
 #define DLB2_HCW_ALIGN_MASK 0x3F
 
+enum dlb2_device_type {
+	DLB2_PF,
+	DLB2_VF,
+	DLB2_5_PF,
+	DLB2_5_VF,
+};
+
+#define DLB2_IS_PF(dev) (dev->type == DLB2_PF || dev->type == DLB2_5_PF)
+#define DLB2_IS_VF(dev) (dev->type == DLB2_VF || dev->type == DLB2_5_VF)
+
 struct dlb2_dev;
 
 struct dlb2_port_memory {
@@ -34,12 +46,12 @@ struct dlb2_dev {
 	struct dlb2_hw hw;
 	/* struct list_head list; */
 	struct device *dlb2_device;
-	bool domain_reset_failed;
 	/* The enqueue_four function enqueues four HCWs (one cache-line worth)
 	 * to the HQM, using whichever mechanism is supported by the platform
 	 * on which this driver is running.
 	 */
 	void (*enqueue_four)(void *qe4, void *pp_addr);
+	bool domain_reset_failed;
 	/* The resource mutex serializes access to driver data structures and
 	 * hardware registers.
 	 */
@@ -59,7 +71,6 @@ struct dlb2_pp_thread_data {
 
 struct dlb2_dev *dlb2_probe(struct rte_pci_device *pdev, const void *probe_args);
 
-
 int dlb2_pf_reset(struct dlb2_dev *dlb2_dev);
 int dlb2_pf_create_sched_domain(struct dlb2_hw *hw,
 				struct dlb2_create_sched_domain_args *args,
@@ -86,6 +97,10 @@ int dlb2_pf_start_domain(struct dlb2_hw *hw,
 			 u32 domain_id,
 			 struct dlb2_start_domain_args *args,
 			 struct dlb2_cmd_response *resp);
+int dlb2_pf_stop_domain(struct dlb2_hw *hw,
+			u32 domain_id,
+			struct dlb2_stop_domain_args *args,
+			struct dlb2_cmd_response *resp);
 int dlb2_pf_enable_ldb_port(struct dlb2_hw *hw,
 			    u32 domain_id,
 			    struct dlb2_enable_ldb_port_args *args,
diff --git a/drivers/event/dlb2/pf/dlb2_pf.c b/drivers/event/dlb2/pf/dlb2_pf.c
index 3d15250..47f3908 100644
--- a/drivers/event/dlb2/pf/dlb2_pf.c
+++ b/drivers/event/dlb2/pf/dlb2_pf.c
@@ -15,7 +15,7 @@
 
 #include <rte_debug.h>
 #include <rte_log.h>
-#include <dev_driver.h>
+#include <rte_dev.h>
 #include <rte_devargs.h>
 #include <rte_mbuf.h>
 #include <rte_ring.h>
@@ -25,7 +25,7 @@
 #include <rte_cycles.h>
 #include <rte_io.h>
 #include <rte_pci.h>
-#include <bus_pci_driver.h>
+#include <rte_bus_pci.h>
 #include <rte_eventdev.h>
 #include <eventdev_pmd.h>
 #include <eventdev_pmd_pci.h>
@@ -39,6 +39,7 @@
 #include "base/dlb2_hw_types.h"
 #include "base/dlb2_osdep.h"
 #include "base/dlb2_resource.h"
+#include "base/dlb2_regs.h"
 
 static const char *event_dlb2_pf_name = RTE_STR(EVDEV_DLB2_NAME_PMD);
 static unsigned int dlb2_qe_sa_pct = 1;
@@ -48,17 +49,21 @@ static void
 dlb2_pf_low_level_io_init(void)
 {
 	int i;
+	int evdev_id;
+
 	/* Addresses will be initialized at port create */
-	for (i = 0; i < DLB2_MAX_NUM_PORTS(DLB2_HW_V2_5); i++) {
-		/* First directed ports */
-		dlb2_port[i][DLB2_DIR_PORT].pp_addr = NULL;
-		dlb2_port[i][DLB2_DIR_PORT].cq_base = NULL;
-		dlb2_port[i][DLB2_DIR_PORT].mmaped = true;
-
-		/* Now load balanced ports */
-		dlb2_port[i][DLB2_LDB_PORT].pp_addr = NULL;
-		dlb2_port[i][DLB2_LDB_PORT].cq_base = NULL;
-		dlb2_port[i][DLB2_LDB_PORT].mmaped = true;
+	for (evdev_id = 0; evdev_id < RTE_EVENT_MAX_DEVS; evdev_id++) {
+		for (i = 0; i < DLB2_MAX_NUM_PORTS(DLB2_HW_V2_5); i++) {
+			/* First directed ports */
+			dlb2_port[evdev_id][i][DLB2_DIR_PORT].pp_addr = NULL;
+			dlb2_port[evdev_id][i][DLB2_DIR_PORT].cq_base = NULL;
+			dlb2_port[evdev_id][i][DLB2_DIR_PORT].mmaped = true;
+
+			/* Now load balanced ports */
+			dlb2_port[evdev_id][i][DLB2_LDB_PORT].pp_addr = NULL;
+			dlb2_port[evdev_id][i][DLB2_LDB_PORT].cq_base = NULL;
+			dlb2_port[evdev_id][i][DLB2_LDB_PORT].mmaped = true;
+		}
 	}
 }
 
@@ -73,11 +78,13 @@ dlb2_pf_open(struct dlb2_hw_dev *handle, const char *name)
 
 static int
 dlb2_pf_get_device_version(struct dlb2_hw_dev *handle,
-			   uint8_t *revision)
+			   uint8_t *revision,
+			   uint8_t *version)
 {
 	struct dlb2_dev *dlb2_dev = (struct dlb2_dev *)handle->pf_dev;
 
 	*revision = dlb2_dev->revision;
+	*version = dlb2_dev->version;
 
 	return 0;
 }
@@ -102,14 +109,16 @@ static void dlb2_pf_calc_arbiter_weights(u8 *weight,
 		weight[i] = weight[i + 1] - val;
 }
 
-
 static void
 dlb2_pf_hardware_init(struct dlb2_hw_dev *handle)
 {
 	struct dlb2_dev *dlb2_dev = (struct dlb2_dev *)handle->pf_dev;
+	int rate_limit = DLB2_WB_CNTL_RATE_LIMIT;
 
 	dlb2_hw_enable_sparse_ldb_cq_mode(&dlb2_dev->hw);
 	dlb2_hw_enable_sparse_dir_cq_mode(&dlb2_dev->hw);
+	dlb2_hw_enable_ldb_sched_perf_ctrl(&dlb2_dev->hw);
+	dlb2_hw_set_rate_limit(&dlb2_dev->hw, rate_limit);
 
 	/* Configure arbitration weights for QE selection */
 	if (dlb2_qe_sa_pct <= 100) {
@@ -130,7 +139,6 @@ dlb2_pf_hardware_init(struct dlb2_hw_dev *handle)
 
 		dlb2_hw_set_qid_arbiter_weights(&dlb2_dev->hw, weight);
 	}
-
 }
 
 static int
@@ -183,16 +191,13 @@ dlb2_pf_sched_domain_create(struct dlb2_hw_dev *handle,
 	return ret;
 }
 
-static void
+static int
 dlb2_pf_domain_reset(struct dlb2_eventdev *dlb2)
 {
 	struct dlb2_dev *dlb2_dev;
-	int ret;
 
 	dlb2_dev = (struct dlb2_dev *)dlb2->qm_instance.pf_dev;
-	ret = dlb2_pf_reset_domain(&dlb2_dev->hw, dlb2->qm_instance.domain_id);
-	if (ret)
-		DLB2_LOG_ERR("dlb2_pf_reset_domain err %d", ret);
+	return dlb2_pf_reset_domain(&dlb2_dev->hw, dlb2->qm_instance.domain_id);
 }
 
 static int
@@ -301,7 +306,8 @@ dlb2_alloc_coherent_aligned(const struct rte_memzone **mz, uintptr_t *phys,
 static int
 dlb2_pf_ldb_port_create(struct dlb2_hw_dev *handle,
 			struct dlb2_create_ldb_port_args *cfg,
-			enum dlb2_cq_poll_modes poll_mode)
+			enum dlb2_cq_poll_modes poll_mode,
+			uint8_t evdev_id)
 {
 	struct dlb2_dev *dlb2_dev = (struct dlb2_dev *)handle->pf_dev;
 	struct dlb2_cmd_response response = {0};
@@ -329,7 +335,7 @@ dlb2_pf_ldb_port_create(struct dlb2_hw_dev *handle,
 	alloc_sz = RTE_CACHE_LINE_ROUNDUP(alloc_sz);
 
 	port_base = dlb2_alloc_coherent_aligned(&mz, &cq_base, alloc_sz,
-						rte_mem_page_size());
+						PAGE_SIZE);
 	if (port_base == NULL)
 		return -ENOMEM;
 
@@ -347,22 +353,24 @@ dlb2_pf_ldb_port_create(struct dlb2_hw_dev *handle,
 				      cfg,
 				      cq_base,
 				      &response);
+	cfg->response = response;
+
 	if (ret)
 		goto create_port_err;
 
 	pp_base = (uintptr_t)dlb2_dev->hw.func_kva + PP_BASE(is_dir);
-	dlb2_port[response.id][DLB2_LDB_PORT].pp_addr =
-		(void *)(pp_base + (rte_mem_page_size() * response.id));
+	dlb2_port[evdev_id][response.id][DLB2_LDB_PORT].pp_addr =
+		(void *)(pp_base + (PAGE_SIZE * response.id));
+
+	dlb2_port[evdev_id][response.id][DLB2_LDB_PORT].cq_base =
+		(void *)(port_base);
 
-	dlb2_port[response.id][DLB2_LDB_PORT].cq_base = (void *)(port_base);
 	memset(&port_memory, 0, sizeof(port_memory));
 
-	dlb2_port[response.id][DLB2_LDB_PORT].mz = mz;
+	dlb2_port[evdev_id][response.id][DLB2_LDB_PORT].mz = mz;
 
 	dlb2_list_init_head(&port_memory.list);
 
-	cfg->response = response;
-
 	return 0;
 
 create_port_err:
@@ -377,7 +385,8 @@ dlb2_pf_ldb_port_create(struct dlb2_hw_dev *handle,
 static int
 dlb2_pf_dir_port_create(struct dlb2_hw_dev *handle,
 			struct dlb2_create_dir_port_args *cfg,
-			enum dlb2_cq_poll_modes poll_mode)
+			enum dlb2_cq_poll_modes poll_mode,
+			uint8_t evdev_id)
 {
 	struct dlb2_dev *dlb2_dev = (struct dlb2_dev *)handle->pf_dev;
 	struct dlb2_cmd_response response = {0};
@@ -404,7 +413,7 @@ dlb2_pf_dir_port_create(struct dlb2_hw_dev *handle,
 	alloc_sz = RTE_CACHE_LINE_ROUNDUP(alloc_sz);
 
 	port_base = dlb2_alloc_coherent_aligned(&mz, &cq_base, alloc_sz,
-						rte_mem_page_size());
+						PAGE_SIZE);
 	if (port_base == NULL)
 		return -ENOMEM;
 
@@ -422,22 +431,23 @@ dlb2_pf_dir_port_create(struct dlb2_hw_dev *handle,
 				      cfg,
 				      cq_base,
 				      &response);
+	cfg->response = response;
+
 	if (ret)
 		goto create_port_err;
 
 	pp_base = (uintptr_t)dlb2_dev->hw.func_kva + PP_BASE(is_dir);
-	dlb2_port[response.id][DLB2_DIR_PORT].pp_addr =
-		(void *)(pp_base + (rte_mem_page_size() * response.id));
+	dlb2_port[evdev_id][response.id][DLB2_DIR_PORT].pp_addr =
+		(void *)(pp_base + (PAGE_SIZE * response.id));
 
-	dlb2_port[response.id][DLB2_DIR_PORT].cq_base =
+	dlb2_port[evdev_id][response.id][DLB2_DIR_PORT].cq_base =
 		(void *)(port_base);
 	memset(&port_memory, 0, sizeof(port_memory));
 
-	dlb2_port[response.id][DLB2_DIR_PORT].mz = mz;
+	dlb2_port[evdev_id][response.id][DLB2_DIR_PORT].mz = mz;
 
 	dlb2_list_init_head(&port_memory.list);
 
-	cfg->response = response;
 
 	return 0;
 
@@ -573,6 +583,29 @@ dlb2_pf_sched_domain_start(struct dlb2_hw_dev *handle,
 }
 
 static int
+dlb2_pf_sched_domain_stop(struct dlb2_hw_dev *handle,
+			   struct dlb2_stop_domain_args *cfg)
+{
+	struct dlb2_dev *dlb2_dev = (struct dlb2_dev *)handle->pf_dev;
+	struct dlb2_cmd_response response = {0};
+	int ret;
+
+	DLB2_INFO(dev->dlb2_device, "Entering %s()\n", __func__);
+
+	ret = dlb2_pf_stop_domain(&dlb2_dev->hw,
+				   handle->domain_id,
+				   cfg,
+				   &response);
+
+	cfg->response = response;
+
+	DLB2_INFO(dev->dlb2_device, "Exiting %s() with ret=%d\n",
+		  __func__, ret);
+
+	return ret;
+}
+
+static int
 dlb2_pf_get_ldb_queue_depth(struct dlb2_hw_dev *handle,
 			    struct dlb2_get_ldb_queue_depth_args *args)
 {
@@ -623,6 +656,62 @@ dlb2_pf_get_dir_queue_depth(struct dlb2_hw_dev *handle,
 }
 
 static int
+dlb2_pf_get_sched_idle_counts(struct dlb2_hw_dev *handle,
+			      void *idle_counts)
+{
+	struct dlb2_dev *dlb2_dev = (struct dlb2_dev *)handle->pf_dev;
+	struct dlb2_hw_sched_idle_counts *data = idle_counts;
+	struct dlb2_hw_sched_idle_counts prev_data = {0};
+	int ret = 0;
+
+	/* Take one snapshot of the idle counters */
+	ret = dlb2_read_sched_idle_counts(&dlb2_dev->hw, data);
+	memcpy(&prev_data, data, sizeof(prev_data));
+
+	/* Interval between two readings */
+	rte_delay_ms(DLB2_IDLE_CNT_INTERVAL);
+
+	/* Take another snapshot of the counters after the timeout */
+	ret = dlb2_read_sched_idle_counts(&dlb2_dev->hw, data);
+	if (ret != 0) {
+		DLB2_LOG_ERR("Failed to read scheduler idle counters");
+		return -EINVAL;
+	}
+
+	/* Compute diff of the counters */
+	data->ldb_perf_nowork_idle_cnt -= prev_data.ldb_perf_nowork_idle_cnt;
+	data->ldb_perf_nospace_idle_cnt -= prev_data.ldb_perf_nospace_idle_cnt;
+	data->ldb_perf_sched_cnt -= prev_data.ldb_perf_sched_cnt;
+	data->ldb_perf_pfriction_idle_cnt -= prev_data.ldb_perf_pfriction_idle_cnt;
+	data->ldb_perf_iflimit_idle_cnt -= prev_data.ldb_perf_iflimit_idle_cnt;
+	data->ldb_perf_fidlimit_idle_cnt -= prev_data.ldb_perf_fidlimit_idle_cnt;
+	data->perf_proc_on_cnt -= prev_data.perf_proc_on_cnt;
+	data->perf_clk_on_cnt -= prev_data.perf_clk_on_cnt;
+	data->hcw_err_cnt -= prev_data.hcw_err_cnt;
+
+	return ret;
+}
+
+static int
+dlb2_pf_set_cos_bandwidth(struct dlb2_hw_dev *handle,
+			  struct dlb2_set_cos_bw_args *args)
+{
+	struct dlb2_dev *dlb2_dev = (struct dlb2_dev *)handle->pf_dev;
+	int ret = 0;
+
+	DLB2_INFO(dev->dlb2_device, "Entering %s()\n", __func__);
+
+	ret = dlb2_hw_set_cos_bandwidth(&dlb2_dev->hw,
+					args->cos_id,
+					args->bandwidth);
+
+	DLB2_INFO(dev->dlb2_device, "Exiting %s() with ret=%d\n",
+		  __func__, ret);
+
+	return ret;
+}
+
+static int
 dlb2_pf_enable_cq_weight(struct dlb2_hw_dev *handle,
 			 struct dlb2_enable_cq_weight_args *args)
 {
@@ -647,17 +736,58 @@ dlb2_pf_enable_cq_weight(struct dlb2_hw_dev *handle,
 }
 
 static int
-dlb2_pf_set_cos_bandwidth(struct dlb2_hw_dev *handle,
-			  struct dlb2_set_cos_bw_args *args)
+dlb2_pf_set_cq_inflight_ctrl(struct dlb2_hw_dev *handle,
+			     struct dlb2_cq_inflight_ctrl_args *args)
 {
 	struct dlb2_dev *dlb2_dev = (struct dlb2_dev *)handle->pf_dev;
+	struct dlb2_cmd_response response = {0};
 	int ret = 0;
 
 	DLB2_INFO(dev->dlb2_device, "Entering %s()\n", __func__);
 
-	ret = dlb2_hw_set_cos_bandwidth(&dlb2_dev->hw,
-					args->cos_id,
-					args->bandwidth);
+	ret = dlb2_hw_set_cq_inflight_ctrl(&dlb2_dev->hw, handle->domain_id,
+					   args, &response, false, 0);
+	args->response = response;
+
+	DLB2_INFO(dev->dlb2_device, "Exiting %s() with ret=%d\n",
+		  __func__, ret);
+
+	return ret;
+}
+static int dlb2_pf_port_ctrl(struct dlb2_port *qm_port, bool enable)
+{
+	struct dlb2_hw_dev *handle = &qm_port->dlb2->qm_instance;
+	struct dlb2_cmd_response response = {0};
+	struct dlb2_dev *dlb2_dev;
+	int ret = 0;
+
+	dlb2_dev = (struct dlb2_dev *)handle->pf_dev;
+
+	if (PORT_TYPE(qm_port) == DLB2_LDB_PORT) {
+		if (enable) {
+			struct dlb2_enable_ldb_port_args args = {.port_id = qm_port->id};
+
+			ret = dlb2_hw_enable_ldb_port(&dlb2_dev->hw, handle->domain_id,
+						      &args, &response, false, 0);
+		} else {
+			struct dlb2_disable_ldb_port_args args = {.port_id = qm_port->id};
+
+			ret = dlb2_hw_disable_ldb_port(&dlb2_dev->hw, handle->domain_id,
+						      &args, &response, false, 0);
+		}
+	} else {
+		if (enable) {
+			struct dlb2_enable_dir_port_args args = {.port_id = qm_port->id};
+
+			ret = dlb2_hw_enable_dir_port(&dlb2_dev->hw, handle->domain_id,
+						      &args, &response, false, 0);
+		} else {
+			struct dlb2_disable_dir_port_args args = {.port_id = qm_port->id};
+
+			ret = dlb2_hw_disable_dir_port(&dlb2_dev->hw, handle->domain_id,
+						      &args, &response, false, 0);
+		}
+	}
 
 	DLB2_INFO(dev->dlb2_device, "Exiting %s() with ret=%d\n",
 		  __func__, ret);
@@ -685,12 +815,16 @@ dlb2_pf_iface_fn_ptrs_init(void)
 	dlb2_iface_get_ldb_queue_depth = dlb2_pf_get_ldb_queue_depth;
 	dlb2_iface_get_dir_queue_depth = dlb2_pf_get_dir_queue_depth;
 	dlb2_iface_sched_domain_start = dlb2_pf_sched_domain_start;
+	dlb2_iface_sched_domain_stop = dlb2_pf_sched_domain_stop;
 	dlb2_iface_pending_port_unmaps = dlb2_pf_pending_port_unmaps;
 	dlb2_iface_get_sn_allocation = dlb2_pf_get_sn_allocation;
 	dlb2_iface_set_sn_allocation = dlb2_pf_set_sn_allocation;
 	dlb2_iface_get_sn_occupancy = dlb2_pf_get_sn_occupancy;
+	dlb2_iface_get_sched_idle_counts = dlb2_pf_get_sched_idle_counts;
 	dlb2_iface_enable_cq_weight = dlb2_pf_enable_cq_weight;
 	dlb2_iface_set_cos_bw = dlb2_pf_set_cos_bandwidth;
+	dlb2_iface_set_cq_inflight_ctrl = dlb2_pf_set_cq_inflight_ctrl;
+	dlb2_iface_port_ctrl = dlb2_pf_port_ctrl;
 }
 
 /* PCI DEV HOOKS */
@@ -702,19 +836,25 @@ dlb2_eventdev_pci_init(struct rte_eventdev *eventdev)
 	struct dlb2_devargs dlb2_args = {
 		.socket_id = rte_socket_id(),
 		.max_num_events = DLB2_MAX_NUM_LDB_CREDITS,
-		.producer_coremask = NULL,
+		.producer_coremask = {'\0'},
 		.num_dir_credits_override = -1,
 		.qid_depth_thresholds = { {0} },
-		.poll_interval = DLB2_POLL_INTERVAL_DEFAULT,
-		.sw_credit_quanta = DLB2_SW_CREDIT_QUANTA_DEFAULT,
-		.hw_credit_quanta = DLB2_SW_CREDIT_BATCH_SZ,
+		.sw_credit_quanta = {DLB2_SW_CREDIT_QUANTA_DEFAULT,
+			DLB2_SW_CREDIT_P_QUANTA_DEFAULT,
+			DLB2_SW_CREDIT_C_QUANTA_DEFAULT},
+		.hw_credit_quanta = {DLB2_SW_CREDIT_BATCH_SZ,
+			DLB2_SW_CREDIT_P_BATCH_SZ,
+			DLB2_SW_CREDIT_C_BATCH_SZ},
 		.default_depth_thresh = DLB2_DEPTH_THRESH_DEFAULT,
 		.max_cq_depth = DLB2_DEFAULT_CQ_DEPTH,
-		.max_enq_depth = DLB2_MAX_ENQUEUE_DEPTH
+		.max_enq_depth = DLB2_MAX_ENQUEUE_DEPTH,
+		.use_default_hl = true,
+		.alloc_hl_entries = 0
 	};
 	struct dlb2_eventdev *dlb2;
-	int q;
+	struct dlb2_dev *dlb2_dev;
 	const void *probe_args = NULL;
+	int q;
 
 	DLB2_LOG_DBG("Enter with dev_id=%d socket_id=%d",
 		     eventdev->data->dev_id, eventdev->data->socket_id);
@@ -727,8 +867,11 @@ dlb2_eventdev_pci_init(struct rte_eventdev *eventdev)
 	pci_dev = RTE_DEV_TO_PCI(eventdev->dev);
 
 	if (rte_eal_process_type() == RTE_PROC_PRIMARY) {
-		dlb2 = dlb2_pmd_priv(eventdev); /* rte_zmalloc_socket mem */
+		dlb2 = dlb2_pmd_priv(eventdev);
 		dlb2->version = DLB2_HW_DEVICE_FROM_PCI_ID(pci_dev);
+		if (dlb2->version == DLB2_HW_V2_5)
+			dlb2_args.max_num_events =
+				DLB2_MAX_NUM_CREDITS(dlb2->version);
 
 		/* Were we invoked with runtime parameters? */
 		if (pci_dev->device.devargs) {
@@ -754,14 +897,21 @@ dlb2_eventdev_pci_init(struct rte_eventdev *eventdev)
 			goto dlb2_probe_failed;
 		}
 
+		dlb2_dev = (struct dlb2_dev *)dlb2->qm_instance.pf_dev;
+		dlb2_dev->version = dlb2->version;
+
 		ret = dlb2_primary_eventdev_probe(eventdev,
 						  event_dlb2_pf_name,
-						  &dlb2_args);
+						  &dlb2_args,
+						  DLB2_NOT_VDEV);
 	} else {
 		dlb2 = dlb2_pmd_priv(eventdev);
 		dlb2->version = DLB2_HW_DEVICE_FROM_PCI_ID(pci_dev);
+		dlb2_dev = (struct dlb2_dev *)dlb2->qm_instance.pf_dev;
+		dlb2_dev->version = dlb2->version;
 		ret = dlb2_secondary_eventdev_probe(eventdev,
-						    event_dlb2_pf_name);
+						    event_dlb2_pf_name,
+						    DLB2_NOT_VDEV);
 	}
 	if (ret)
 		goto dlb2_probe_failed;
@@ -799,6 +949,8 @@ static const struct rte_pci_id pci_id_dlb2_5_map[] = {
 	},
 };
 
+#define DLB2_PF_EVENT_NAME_LEN 22 /* "dlb2_event" cat PCI addr */
+
 static int
 event_dlb2_pci_probe(struct rte_pci_driver *pci_drv,
 		     struct rte_pci_device *pci_dev)
@@ -806,9 +958,9 @@ event_dlb2_pci_probe(struct rte_pci_driver *pci_drv,
 	int ret;
 
 	ret = rte_event_pmd_pci_probe_named(pci_drv, pci_dev,
-					     sizeof(struct dlb2_eventdev),
-					     dlb2_eventdev_pci_init,
-					     event_dlb2_pf_name);
+					    sizeof(struct dlb2_eventdev),
+					    dlb2_eventdev_pci_init,
+					    pci_dev->device.name);
 	if (ret) {
 		DLB2_LOG_INFO("rte_event_pmd_pci_probe_named() failed, "
 				"ret=%d\n", ret);
@@ -842,7 +994,7 @@ event_dlb2_5_pci_probe(struct rte_pci_driver *pci_drv,
 	ret = rte_event_pmd_pci_probe_named(pci_drv, pci_dev,
 					    sizeof(struct dlb2_eventdev),
 					    dlb2_eventdev_pci_init,
-					    event_dlb2_pf_name);
+					    pci_dev->device.name);
 	if (ret) {
 		DLB2_LOG_INFO("rte_event_pmd_pci_probe_named() failed, "
 				"ret=%d\n", ret);
diff --git a/drivers/event/dlb2/rte_pmd_dlb2.c b/drivers/event/dlb2/rte_pmd_dlb2.c
index 43990e4..c72a42b 100644
--- a/drivers/event/dlb2/rte_pmd_dlb2.c
+++ b/drivers/event/dlb2/rte_pmd_dlb2.c
@@ -33,7 +33,36 @@ rte_pmd_dlb2_set_token_pop_mode(uint8_t dev_id,
 	if (port_id >= dlb2->num_ports || dlb2->ev_ports[port_id].setup_done)
 		return -EINVAL;
 
+	if (dlb2->version == DLB2_HW_V2_5 && mode == DELAYED_POP) {
+		dlb2->ev_ports[port_id].qm_port.enable_inflight_ctrl = true;
+		dlb2->ev_ports[port_id].qm_port.inflight_threshold = 1;
+		mode = AUTO_POP;
+	}
+
 	dlb2->ev_ports[port_id].qm_port.token_pop_mode = mode;
 
 	return 0;
 }
+
+int
+rte_pmd_dlb2_set_port_param(uint8_t dev_id,
+			    uint8_t port_id,
+			    uint64_t flags,
+			    void *val)
+{
+	struct dlb2_eventdev *dlb2;
+	struct rte_eventdev *dev;
+
+	if (val == NULL)
+		return -EINVAL;
+
+	RTE_EVENTDEV_VALID_DEVID_OR_ERR_RET(dev_id, -EINVAL);
+	dev = &rte_eventdevs[dev_id];
+
+	dlb2 = dlb2_pmd_priv(dev);
+
+	if (port_id >= dlb2->num_ports)
+		return -EINVAL;
+
+	return dlb2_set_port_param(dlb2, port_id, flags, val);
+}
diff --git a/drivers/event/dlb2/rte_pmd_dlb2.h b/drivers/event/dlb2/rte_pmd_dlb2.h
index 334c6c3..def63c1 100644
--- a/drivers/event/dlb2/rte_pmd_dlb2.h
+++ b/drivers/event/dlb2/rte_pmd_dlb2.h
@@ -17,7 +17,15 @@ extern "C" {
 
 #include <stdint.h>
 
-#include <rte_compat.h>
+/**
+ * Macros to get/set QID depth and QE weight from rte_event metadata.
+ * Currently 'rsvd' field is used for these. Lower 2 bits are used to store
+ * QID depth while the upper 2 bits are used for QER weight.
+ */
+#define DLB2_GET_QID_DEPTH(x) ((x)->rsvd & 0x3)
+#define DLB2_SET_QID_DEPTH(x, v) ((x)->rsvd = ((x)->rsvd & ~0x3) | (v & 0x3))
+#define DLB2_GET_QE_WEIGHT(x) (((x)->rsvd >> 2) & 0x3)
+#define DLB2_SET_QE_WEIGHT(x, v) ((x)->rsvd = ((x)->rsvd & 0x3) | ((v & 0x3) << 2))
 
 /**
  * @warning
@@ -26,7 +34,7 @@ extern "C" {
  * Selects the token pop mode for a DLB2 port.
  */
 enum dlb2_token_pop_mode {
-	/* Pop the CQ tokens immediately after dequeuing. */
+	/* Pop the CQ tokens immediately after dequeueing. */
 	AUTO_POP,
 	/* Pop CQ tokens after (dequeue_depth - 1) events are released.
 	 * Supported on load-balanced ports only.
@@ -67,6 +75,46 @@ rte_pmd_dlb2_set_token_pop_mode(uint8_t dev_id,
 				uint8_t port_id,
 				enum dlb2_token_pop_mode mode);
 
+/** Set inflight threshold for flow migration */
+#define DLB2_FLOW_MIGRATION_THRESHOLD RTE_BIT64(0)
+
+/** Set port history list */
+#define DLB2_SET_PORT_HL RTE_BIT64(1)
+
+struct dlb2_port_param {
+	uint16_t inflight_threshold : 12;
+	uint16_t port_hl;
+};
+
+/*!
+ * @warning
+ * @b EXPERIMENTAL: this API may change, or be removed, without prior notice
+ *
+ * Configure various port parameters.
+ * AUTO_POP. This function must be called before calling rte_event_port_setup()
+ * for the port, but after calling rte_event_dev_configure().
+ *
+ * @param dev_id
+ *    The identifier of the event device.
+ * @param port_id
+ *    The identifier of the event port.
+ * @param flags
+ *    Bitmask of the parameters being set.
+ * @param val
+ *    Structure coantaining the values of parameters being set.
+ *
+ * @return
+ * - 0: Success
+ * - EINVAL: Invalid dev_id, port_id, or mode
+ * - EINVAL: The DLB2 is not configured, is already running, or the port is
+ *   already setup
+ */
+__rte_experimental
+int
+rte_pmd_dlb2_set_port_param(uint8_t dev_id,
+			    uint8_t port_id,
+			    uint64_t flags,
+			    void *val);
 #ifdef __cplusplus
 }
 #endif
diff --git a/drivers/event/dlb2/version.map b/drivers/event/dlb2/version.map
index 1327e3e..e66c756 100644
--- a/drivers/event/dlb2/version.map
+++ b/drivers/event/dlb2/version.map
@@ -5,5 +5,6 @@ DPDK_23 {
 EXPERIMENTAL {
 	global:
 
+	rte_pmd_dlb2_set_port_param;
 	rte_pmd_dlb2_set_token_pop_mode;
 };
diff --git a/examples/eventdev_pipeline/Makefile b/examples/eventdev_pipeline/Makefile
index 962ff96..93f210f 100644
--- a/examples/eventdev_pipeline/Makefile
+++ b/examples/eventdev_pipeline/Makefile
@@ -23,10 +23,16 @@ shared: build/$(APP)-shared
 static: build/$(APP)-static
 	ln -sf $(APP)-static build/$(APP)
 
+ifneq ($(wildcard $(RTE_SDK)),)
+	ifneq ($(RTE_TARGET),)
+		PKGCONF_OPTS := --define-variable=prefix=$(RTE_SDK)/$(RTE_TARGET)
+	endif
+endif
+
 PC_FILE := $(shell $(PKGCONF) --path libdpdk 2>/dev/null)
-CFLAGS += -O3 $(shell $(PKGCONF) --cflags libdpdk)
-LDFLAGS_SHARED = $(shell $(PKGCONF) --libs libdpdk)
-LDFLAGS_STATIC = $(shell $(PKGCONF) --static --libs libdpdk)
+CFLAGS += -O3 $(shell $(PKGCONF) $(PKGCONF_OPTS) --cflags libdpdk)
+LDFLAGS_SHARED = $(shell $(PKGCONF) $(PKGCONF_OPTS) --libs libdpdk)
+LDFLAGS_STATIC = $(shell $(PKGCONF) $(PKGCONF_OPTS) --static --libs libdpdk)
 
 ifeq ($(MAKECMDGOALS),static)
 # check for broken pkg-config
diff --git a/examples/eventdev_pipeline/pipeline_worker_generic.c b/examples/eventdev_pipeline/pipeline_worker_generic.c
index 783f68c..9e4daeb 100644
--- a/examples/eventdev_pipeline/pipeline_worker_generic.c
+++ b/examples/eventdev_pipeline/pipeline_worker_generic.c
@@ -130,7 +130,7 @@ setup_eventdev_generic(struct worker_data *worker_data)
 	const uint8_t dev_id = 0;
 	/* +1 stages is for a SINGLE_LINK TX stage */
 	const uint8_t nb_queues = cdata.num_stages + 1;
-	const uint8_t nb_ports = cdata.num_workers;
+	const uint8_t nb_ports = cdata.num_workers + 1;
 	struct rte_event_dev_config config = {
 			.nb_event_queues = nb_queues,
 			.nb_event_ports = nb_ports,
@@ -150,7 +150,7 @@ setup_eventdev_generic(struct worker_data *worker_data)
 			.schedule_type = cdata.queue_type,
 			.priority = RTE_EVENT_DEV_PRIORITY_NORMAL,
 			.nb_atomic_flows = 1024,
-			.nb_atomic_order_sequences = 1024,
+			.nb_atomic_order_sequences = 64,
 	};
 	struct rte_event_queue_conf tx_q_conf = {
 			.priority = RTE_EVENT_DEV_PRIORITY_HIGHEST,
@@ -441,6 +441,8 @@ init_adapters(uint16_t nb_ports)
 		rte_exit(EXIT_FAILURE, "failed to create rx adapter[%d]",
 				cdata.rx_adapter_id);
 
+	adptr_p_conf.event_port_cfg = RTE_EVENT_PORT_CFG_SINGLE_LINK;
+
 	ret = rte_event_eth_tx_adapter_create(cdata.tx_adapter_id, evdev_id,
 			&adptr_p_conf);
 	if (ret)
diff --git a/examples/eventdev_pipeline/pipeline_worker_tx.c b/examples/eventdev_pipeline/pipeline_worker_tx.c
index 98a52f3..3cdb0dc 100644
--- a/examples/eventdev_pipeline/pipeline_worker_tx.c
+++ b/examples/eventdev_pipeline/pipeline_worker_tx.c
@@ -479,7 +479,7 @@ setup_eventdev_worker_tx_enq(struct worker_data *worker_data)
 			.schedule_type = cdata.queue_type,
 			.priority = RTE_EVENT_DEV_PRIORITY_NORMAL,
 			.nb_atomic_flows = 1024,
-			.nb_atomic_order_sequences = 1024,
+			.nb_atomic_order_sequences = 64,
 	};
 
 	int ret, ndev = rte_event_dev_count();
diff --git a/examples/ipsec-secgw/Makefile b/examples/ipsec-secgw/Makefile
index 12a2db8..d32b89d 100644
--- a/examples/ipsec-secgw/Makefile
+++ b/examples/ipsec-secgw/Makefile
@@ -36,10 +36,16 @@ shared: build/$(APP)-shared
 static: build/$(APP)-static
 	ln -sf $(APP)-static build/$(APP)
 
+ifneq ($(wildcard $(RTE_SDK)),)
+	ifneq ($(RTE_TARGET),)
+		PKGCONF_OPTS := --define-variable=prefix=$(RTE_SDK)/$(RTE_TARGET)
+	endif
+endif
+
 PC_FILE := $(shell $(PKGCONF) --path libdpdk 2>/dev/null)
-CFLAGS += -O3 $(shell $(PKGCONF) --cflags libdpdk)
-LDFLAGS_SHARED = $(shell $(PKGCONF) --libs libdpdk)
-LDFLAGS_STATIC = $(shell $(PKGCONF) --static --libs libdpdk)
+CFLAGS += -O3 $(shell $(PKGCONF) $(PKGCONF_OPTS) --cflags libdpdk)
+LDFLAGS_SHARED = $(shell $(PKGCONF) $(PKGCONF_OPTS) --libs libdpdk)
+LDFLAGS_STATIC = $(shell $(PKGCONF) $(PKGCONF_OPTS) --static --libs libdpdk)
 
 ifeq ($(MAKECMDGOALS),static)
 # check for broken pkg-config
diff --git a/examples/ipsec-secgw/event_helper.c b/examples/ipsec-secgw/event_helper.c
index 89fb7e6..c3b5478 100644
--- a/examples/ipsec-secgw/event_helper.c
+++ b/examples/ipsec-secgw/event_helper.c
@@ -713,7 +713,7 @@ eh_initialize_eventdev(struct eventmode_conf *em_conf)
 
 			/* Set max atomic flows to 1024 */
 			eventq_conf.nb_atomic_flows = 1024;
-			eventq_conf.nb_atomic_order_sequences = 1024;
+			eventq_conf.nb_atomic_order_sequences = 64;
 
 			/* Setup the queue */
 			ret = rte_event_queue_setup(eventdev_id, j,
diff --git a/examples/ipsec-secgw/ipsec-secgw.c b/examples/ipsec-secgw/ipsec-secgw.c
index a64a26c..47dd131 100644
--- a/examples/ipsec-secgw/ipsec-secgw.c
+++ b/examples/ipsec-secgw/ipsec-secgw.c
@@ -1700,6 +1700,9 @@ cryptodevs_init(enum eh_pkt_transfer_mode mode)
 
 		total_nb_qps += qp;
 		dev_conf.socket_id = rte_cryptodev_socket_id(cdev_id);
+		/* Use the first socket if SOCKET_ID_ANY is returned. */
+		if (dev_conf.socket_id == SOCKET_ID_ANY)
+			dev_conf.socket_id = 0;
 		dev_conf.nb_queue_pairs = qp;
 		dev_conf.ff_disable = RTE_CRYPTODEV_FF_ASYMMETRIC_CRYPTO;
 
diff --git a/examples/l2fwd-event/Makefile b/examples/l2fwd-event/Makefile
index 4d041ba..720fe27 100644
--- a/examples/l2fwd-event/Makefile
+++ b/examples/l2fwd-event/Makefile
@@ -27,10 +27,16 @@ shared: build/$(APP)-shared
 static: build/$(APP)-static
 	ln -sf $(APP)-static build/$(APP)
 
+ifneq ($(wildcard $(RTE_SDK)),)
+	ifneq ($(RTE_TARGET),)
+		PKGCONF_OPTS := --define-variable=prefix=$(RTE_SDK)/$(RTE_TARGET)
+	endif
+endif
+
 PC_FILE := $(shell $(PKGCONF) --path libdpdk 2>/dev/null)
-CFLAGS += -O3 $(shell $(PKGCONF) --cflags libdpdk)
-LDFLAGS_SHARED = $(shell $(PKGCONF) --libs libdpdk)
-LDFLAGS_STATIC = $(shell $(PKGCONF) --static --libs libdpdk)
+CFLAGS += -O3 $(shell $(PKGCONF) $(PKGCONF_OPTS) --cflags libdpdk)
+LDFLAGS_SHARED = $(shell $(PKGCONF) $(PKGCONF_OPTS) --libs libdpdk)
+LDFLAGS_STATIC = $(shell $(PKGCONF) $(PKGCONF_OPTS) --static --libs libdpdk)
 
 ifeq ($(MAKECMDGOALS),static)
 # check for broken pkg-config
diff --git a/examples/l2fwd-event/l2fwd_common.h b/examples/l2fwd-event/l2fwd_common.h
index 07f84cb..fb718ec 100644
--- a/examples/l2fwd-event/l2fwd_common.h
+++ b/examples/l2fwd-event/l2fwd_common.h
@@ -78,6 +78,7 @@ struct l2fwd_resources {
 	uint8_t sched_type;
 	uint8_t mac_updating;
 	uint8_t rx_queue_per_lcore;
+	bool strict_single_link;
 	bool port_pairs;
 	uint16_t nb_rxd;
 	uint16_t nb_txd;
diff --git a/examples/l2fwd-event/l2fwd_event.c b/examples/l2fwd-event/l2fwd_event.c
index 4b5a032..32cd63d 100644
--- a/examples/l2fwd-event/l2fwd_event.c
+++ b/examples/l2fwd-event/l2fwd_event.c
@@ -566,7 +566,16 @@ l2fwd_event_resource_setup(struct l2fwd_resources *rsrc)
 	event_queue_cfg = evt_rsrc->ops.event_device_setup(rsrc);
 
 	/* Event queue configuration */
-	evt_rsrc->ops.event_queue_setup(rsrc, event_queue_cfg);
+	ret = evt_rsrc->ops.event_queue_setup(rsrc, event_queue_cfg);
+	if (ret < 0) {
+		/* TX adapter config error; try with strict single-link configuration */
+		rsrc->strict_single_link = true;
+		event_queue_cfg = evt_rsrc->ops.event_device_setup(rsrc);
+		ret = evt_rsrc->ops.event_queue_setup(rsrc, event_queue_cfg);
+		/* If still error, panic */
+		if (ret < 0)
+			rte_panic("Error in configuring event queue for Tx adapter");
+	}
 
 	/* Event port configuration */
 	evt_rsrc->ops.event_port_setup(rsrc);
diff --git a/examples/l2fwd-event/l2fwd_event.h b/examples/l2fwd-event/l2fwd_event.h
index 78f22e5..5dc69ff 100644
--- a/examples/l2fwd-event/l2fwd_event.h
+++ b/examples/l2fwd-event/l2fwd_event.h
@@ -15,7 +15,7 @@
 
 typedef uint32_t (*event_device_setup_cb)(struct l2fwd_resources *rsrc);
 typedef void (*event_port_setup_cb)(struct l2fwd_resources *rsrc);
-typedef void (*event_queue_setup_cb)(struct l2fwd_resources *rsrc,
+typedef int (*event_queue_setup_cb)(struct l2fwd_resources *rsrc,
 				     uint32_t event_queue_cfg);
 typedef void (*adapter_setup_cb)(struct l2fwd_resources *rsrc);
 typedef void (*event_loop_cb)(struct l2fwd_resources *rsrc);
diff --git a/examples/l2fwd-event/l2fwd_event_generic.c b/examples/l2fwd-event/l2fwd_event_generic.c
index 1977e23..1ccf869 100644
--- a/examples/l2fwd-event/l2fwd_event_generic.c
+++ b/examples/l2fwd-event/l2fwd_event_generic.c
@@ -36,6 +36,9 @@ l2fwd_event_device_setup_generic(struct l2fwd_resources *rsrc)
 	uint16_t port_id;
 	int ret;
 
+	if (rsrc->strict_single_link)
+		event_d_conf.nb_single_link_event_port_queues = 1;
+
 	RTE_ETH_FOREACH_DEV(port_id) {
 		if ((rsrc->enabled_port_mask & (1 << port_id)) == 0)
 			continue;
@@ -83,6 +86,11 @@ l2fwd_event_device_setup_generic(struct l2fwd_resources *rsrc)
 	evt_rsrc->evp.nb_ports = num_workers;
 	evt_rsrc->evq.nb_queues = event_d_conf.nb_event_queues;
 
+	if (rsrc->strict_single_link) {
+		event_d_conf.nb_event_ports++;
+		evt_rsrc->evp.nb_ports++;
+	}
+
 	evt_rsrc->has_burst = !!(dev_info.event_dev_cap &
 				    RTE_EVENT_DEV_CAP_BURST_MODE);
 
@@ -108,6 +116,9 @@ l2fwd_event_port_setup_generic(struct l2fwd_resources *rsrc)
 	struct rte_event_port_conf def_p_conf;
 	uint8_t event_p_id;
 	int32_t ret;
+	uint8_t effective_ports = rsrc->strict_single_link ?
+					evt_rsrc->evp.nb_ports - 1 :
+					evt_rsrc->evp.nb_ports;
 
 	evt_rsrc->evp.event_p_id = (uint8_t *)malloc(sizeof(uint8_t) *
 					evt_rsrc->evp.nb_ports);
@@ -136,8 +147,7 @@ l2fwd_event_port_setup_generic(struct l2fwd_resources *rsrc)
 
 	evt_rsrc->deq_depth = def_p_conf.dequeue_depth;
 
-	for (event_p_id = 0; event_p_id < evt_rsrc->evp.nb_ports;
-								event_p_id++) {
+	for (event_p_id = 0; event_p_id < effective_ports; event_p_id++) {
 		ret = rte_event_port_setup(event_d_id, event_p_id,
 					   &event_p_conf);
 		if (ret < 0)
@@ -160,7 +170,7 @@ l2fwd_event_port_setup_generic(struct l2fwd_resources *rsrc)
 	evt_rsrc->def_p_conf = event_p_conf;
 }
 
-static void
+static int
 l2fwd_event_queue_setup_generic(struct l2fwd_resources *rsrc,
 			  uint32_t event_queue_cfg)
 {
@@ -169,7 +179,7 @@ l2fwd_event_queue_setup_generic(struct l2fwd_resources *rsrc,
 	/* Event queue initialization. 8< */
 	struct rte_event_queue_conf event_q_conf = {
 		.nb_atomic_flows = 1024,
-		.nb_atomic_order_sequences = 1024,
+		.nb_atomic_order_sequences = 64,
 		.event_queue_cfg = event_queue_cfg,
 		.priority = RTE_EVENT_DEV_PRIORITY_NORMAL
 	};
@@ -203,9 +213,12 @@ l2fwd_event_queue_setup_generic(struct l2fwd_resources *rsrc,
 	event_q_conf.event_queue_cfg |= RTE_EVENT_QUEUE_CFG_SINGLE_LINK;
 	event_q_conf.priority = RTE_EVENT_DEV_PRIORITY_HIGHEST,
 	ret = rte_event_queue_setup(event_d_id, event_q_id, &event_q_conf);
+
 	if (ret < 0)
-		rte_panic("Error in configuring event queue for Tx adapter\n");
+		return ret;
+
 	evt_rsrc->evq.event_q_id[event_q_id] = event_q_id;
+	return 0;
 }
 
 static void
@@ -298,9 +311,12 @@ l2fwd_rx_tx_adapter_setup_generic(struct l2fwd_resources *rsrc)
 		free(evt_rsrc->evq.event_q_id);
 		rte_panic("Failed to allocate memery for Rx adapter\n");
 	}
+	if (rsrc->strict_single_link)
+		evt_rsrc->def_p_conf.event_port_cfg |= RTE_EVENT_PORT_CFG_SINGLE_LINK;
 
 	ret = rte_event_eth_tx_adapter_create(tx_adptr_id, event_d_id,
 					      &evt_rsrc->def_p_conf);
+
 	if (ret)
 		rte_panic("Failed to create tx adapter\n");
 
diff --git a/examples/l2fwd-event/l2fwd_event_internal_port.c b/examples/l2fwd-event/l2fwd_event_internal_port.c
index 717a7bc..e057289 100644
--- a/examples/l2fwd-event/l2fwd_event_internal_port.c
+++ b/examples/l2fwd-event/l2fwd_event_internal_port.c
@@ -150,7 +150,7 @@ l2fwd_event_port_setup_internal_port(struct l2fwd_resources *rsrc)
 	evt_rsrc->def_p_conf = event_p_conf;
 }
 
-static void
+static int
 l2fwd_event_queue_setup_internal_port(struct l2fwd_resources *rsrc,
 				uint32_t event_queue_cfg)
 {
@@ -158,7 +158,7 @@ l2fwd_event_queue_setup_internal_port(struct l2fwd_resources *rsrc,
 	uint8_t event_d_id = evt_rsrc->event_d_id;
 	struct rte_event_queue_conf event_q_conf = {
 		.nb_atomic_flows = 1024,
-		.nb_atomic_order_sequences = 1024,
+		.nb_atomic_order_sequences = 64,
 		.event_queue_cfg = event_queue_cfg,
 		.priority = RTE_EVENT_DEV_PRIORITY_NORMAL
 	};
@@ -194,6 +194,7 @@ l2fwd_event_queue_setup_internal_port(struct l2fwd_resources *rsrc,
 			rte_panic("Error in configuring event queue\n");
 		evt_rsrc->evq.event_q_id[event_q_id] = event_q_id;
 	}
+	return 0;
 }
 
 static void
diff --git a/examples/l3fwd/Makefile b/examples/l3fwd/Makefile
index e802f03..e13c78c 100644
--- a/examples/l3fwd/Makefile
+++ b/examples/l3fwd/Makefile
@@ -22,12 +22,18 @@ shared: build/$(APP)-shared
 static: build/$(APP)-static
 	ln -sf $(APP)-static build/$(APP)
 
+ifneq ($(wildcard $(RTE_SDK)),)
+	ifneq ($(RTE_TARGET),)
+		PKGCONF_OPTS := --define-variable=prefix=$(RTE_SDK)/$(RTE_TARGET)
+	endif
+endif
+
 PC_FILE := $(shell $(PKGCONF) --path libdpdk 2>/dev/null)
-CFLAGS += -O3 $(shell $(PKGCONF) --cflags libdpdk)
+CFLAGS += -O3 $(shell $(PKGCONF) $(PKGCONF_OPTS) --cflags libdpdk)
 # Added for 'rte_eth_link_to_str()'
 CFLAGS += -DALLOW_EXPERIMENTAL_API
-LDFLAGS_SHARED = $(shell $(PKGCONF) --libs libdpdk)
-LDFLAGS_STATIC = $(shell $(PKGCONF) --static --libs libdpdk)
+LDFLAGS_SHARED = $(shell $(PKGCONF) $(PKGCONF_OPTS) --libs libdpdk)
+LDFLAGS_STATIC = $(shell $(PKGCONF) $(PKGCONF_OPTS) --static --libs libdpdk)
 
 ifeq ($(MAKECMDGOALS),static)
 # check for broken pkg-config
diff --git a/examples/l3fwd/l3fwd_acl.c b/examples/l3fwd/l3fwd_acl.c
index 401692b..b38e76d 100644
--- a/examples/l3fwd/l3fwd_acl.c
+++ b/examples/l3fwd/l3fwd_acl.c
@@ -247,6 +247,8 @@ struct acl_search_t {
 	int num_ipv6;
 };
 
+#include "l3fwd_acl.h"
+
 static struct {
 	struct rte_acl_ctx *acx_ipv4[NB_SOCKETS];
 	struct rte_acl_ctx *acx_ipv6[NB_SOCKETS];
@@ -263,8 +265,6 @@ static struct rte_acl_rule *acl_base_ipv4, *route_base_ipv4,
 static unsigned int acl_num_ipv4, route_num_ipv4,
 		acl_num_ipv6, route_num_ipv6;
 
-#include "l3fwd_acl.h"
-
 #include "l3fwd_acl_scalar.h"
 
 /*
@@ -875,9 +875,9 @@ dump_acl4_rule(struct rte_mbuf *m, uint32_t sig)
 		rte_pktmbuf_mtod_offset(m, struct rte_ipv4_hdr *,
 					sizeof(struct rte_ether_hdr));
 
-	printf("Packet Src:%s ", inet_ntop(AF_INET, ipv4_hdr->src_addr,
+	printf("Packet Src:%s ", inet_ntop(AF_INET, (void*)(uint64_t)ipv4_hdr->src_addr,
 		abuf, sizeof(abuf)));
-	printf("Dst:%s ", inet_ntop(AF_INET, ipv4_hdr->dst_addr,
+	printf("Dst:%s ", inet_ntop(AF_INET, (void*)(uint64_t)ipv4_hdr->dst_addr,
 		abuf, sizeof(abuf)));
 
 	printf("Src port:%hu,Dst port:%hu ",
diff --git a/examples/l3fwd/l3fwd_acl_scalar.h b/examples/l3fwd/l3fwd_acl_scalar.h
index 542c303..b21ad72 100644
--- a/examples/l3fwd/l3fwd_acl_scalar.h
+++ b/examples/l3fwd/l3fwd_acl_scalar.h
@@ -8,6 +8,11 @@
 #include "l3fwd.h"
 #include "l3fwd_common.h"
 
+#ifdef L3FWDACL_DEBUG
+static inline void dump_acl4_rule(struct rte_mbuf *m, uint32_t sig);
+static inline void dump_acl6_rule(struct rte_mbuf *m, uint32_t sig);
+#endif
+
 static inline void
 l3fwd_acl_prepare_one_packet(struct rte_mbuf **pkts_in, struct acl_search_t *acl,
 	int index)
@@ -93,11 +98,11 @@ l3fwd_acl_send_packets(struct lcore_conf *qconf, struct rte_mbuf *pkts[], uint32
 		} else {
 			dst_port[i] = BAD_PORT;
 #ifdef L3FWDACL_DEBUG
-			if ((res & ACL_DENY_SIGNATURE) != 0) {
+			if (((uint64_t)res & (uint64_t)ACL_DENY_SIGNATURE) != 0) {
 				if (RTE_ETH_IS_IPV4_HDR(pkts[i]->packet_type))
 					dump_acl4_rule(pkts[i], res[i]);
-				else if (RTE_ETH_IS_IPV6_HDR(pkt[i]->packet_type))
-					dump_acl6_rule(pkt[i], res[i]);
+				else if (RTE_ETH_IS_IPV6_HDR(pkts[i]->packet_type))
+					dump_acl6_rule(pkts[i], res[i]);
 			}
 #endif
 		}
diff --git a/examples/l3fwd/l3fwd_event.c b/examples/l3fwd/l3fwd_event.c
index 32906ab..6dacf26 100644
--- a/examples/l3fwd/l3fwd_event.c
+++ b/examples/l3fwd/l3fwd_event.c
@@ -219,6 +219,7 @@ void
 l3fwd_event_resource_setup(struct rte_eth_conf *port_conf)
 {
 	struct l3fwd_event_resources *evt_rsrc = l3fwd_get_eventdev_rsrc();
+	bool strict_single_link = false;
 	const event_loop_cb lpm_event_loop[2][2][2] = {
 		[0][0][0] = lpm_event_main_loop_tx_d,
 		[0][0][1] = lpm_event_main_loop_tx_d_burst,
@@ -265,16 +266,25 @@ l3fwd_event_resource_setup(struct rte_eth_conf *port_conf)
 	l3fwd_eth_dev_port_setup(port_conf);
 
 	/* Event device configuration */
-	event_queue_cfg = evt_rsrc->ops.event_device_setup();
+	event_queue_cfg = evt_rsrc->ops.event_device_setup(strict_single_link);
 
 	/* Event queue configuration */
-	evt_rsrc->ops.event_queue_setup(event_queue_cfg);
+	ret = evt_rsrc->ops.event_queue_setup(event_queue_cfg);
+	if (ret < 0) {
+		/* TX adapter config error; try with strict single-link configuration */
+		strict_single_link = true;
+		event_queue_cfg = evt_rsrc->ops.event_device_setup(strict_single_link);
+		ret = evt_rsrc->ops.event_queue_setup(event_queue_cfg);
+		/* If still error, panic */
+		if (ret < 0)
+			rte_panic("Error in configuring event queue for Tx adapter");
+	}
 
 	/* Event port configuration */
-	evt_rsrc->ops.event_port_setup();
+	evt_rsrc->ops.event_port_setup(strict_single_link);
 
 	/* Rx/Tx adapters configuration */
-	evt_rsrc->ops.adapter_setup();
+	evt_rsrc->ops.adapter_setup(strict_single_link);
 
 	/* Start event device */
 	ret = rte_event_dev_start(evt_rsrc->event_d_id);
diff --git a/examples/l3fwd/l3fwd_event.h b/examples/l3fwd/l3fwd_event.h
index e21817c..0aae82f 100644
--- a/examples/l3fwd/l3fwd_event.h
+++ b/examples/l3fwd/l3fwd_event.h
@@ -19,10 +19,10 @@
 #define L3FWD_EVENT_TX_DIRECT  0x4
 #define L3FWD_EVENT_TX_ENQ     0x8
 
-typedef uint32_t (*event_device_setup_cb)(void);
-typedef void (*event_queue_setup_cb)(uint32_t event_queue_cfg);
-typedef void (*event_port_setup_cb)(void);
-typedef void (*adapter_setup_cb)(void);
+typedef uint32_t (*event_device_setup_cb)(bool strict_single_link);
+typedef int (*event_queue_setup_cb)(uint32_t event_queue_cfg);
+typedef void (*event_port_setup_cb)(bool strict_single_link);
+typedef void (*adapter_setup_cb)(bool strict_single_link);
 typedef int (*event_loop_cb)(void *);
 
 struct l3fwd_event_queues {
diff --git a/examples/l3fwd/l3fwd_event_generic.c b/examples/l3fwd/l3fwd_event_generic.c
index c80573f..46606bf 100644
--- a/examples/l3fwd/l3fwd_event_generic.c
+++ b/examples/l3fwd/l3fwd_event_generic.c
@@ -8,7 +8,7 @@
 #include "l3fwd_event.h"
 
 static uint32_t
-l3fwd_event_device_setup_generic(void)
+l3fwd_event_device_setup_generic(bool strict_single_link)
 {
 	struct l3fwd_event_resources *evt_rsrc = l3fwd_get_eventdev_rsrc();
 	struct rte_event_dev_config event_d_conf = {
@@ -25,6 +25,9 @@ l3fwd_event_device_setup_generic(void)
 	uint16_t port_id;
 	int ret;
 
+	if (strict_single_link)
+		event_d_conf.nb_single_link_event_port_queues = 1;
+
 	RTE_ETH_FOREACH_DEV(port_id) {
 		if ((evt_rsrc->port_mask & (1 << port_id)) == 0)
 			continue;
@@ -42,6 +45,7 @@ l3fwd_event_device_setup_generic(void)
 
 	/* One queue for each ethdev port + one Tx adapter Single link queue. */
 	event_d_conf.nb_event_queues = ethdev_count + 1;
+
 	if (dev_info.max_event_queues < event_d_conf.nb_event_queues)
 		event_d_conf.nb_event_queues = dev_info.max_event_queues;
 
@@ -68,8 +72,14 @@ l3fwd_event_device_setup_generic(void)
 
 	event_d_conf.nb_event_ports = num_workers;
 	evt_rsrc->evp.nb_ports = num_workers;
+
 	evt_rsrc->evq.nb_queues = event_d_conf.nb_event_queues;
 
+	if (strict_single_link) {
+		event_d_conf.nb_event_ports++;
+		evt_rsrc->evp.nb_ports++;
+	}
+
 	evt_rsrc->has_burst = !!(dev_info.event_dev_cap &
 				    RTE_EVENT_DEV_CAP_BURST_MODE);
 
@@ -82,7 +92,7 @@ l3fwd_event_device_setup_generic(void)
 }
 
 static void
-l3fwd_event_port_setup_generic(void)
+l3fwd_event_port_setup_generic(bool strict_single_link)
 {
 	struct l3fwd_event_resources *evt_rsrc = l3fwd_get_eventdev_rsrc();
 	uint8_t event_d_id = evt_rsrc->event_d_id;
@@ -94,6 +104,9 @@ l3fwd_event_port_setup_generic(void)
 	struct rte_event_port_conf def_p_conf;
 	uint8_t event_p_id;
 	int32_t ret;
+	uint8_t effective_ports = strict_single_link ?
+					evt_rsrc->evp.nb_ports - 1 :
+					evt_rsrc->evp.nb_ports;
 
 	evt_rsrc->evp.event_p_id = (uint8_t *)malloc(sizeof(uint8_t) *
 					evt_rsrc->evp.nb_ports);
@@ -122,8 +135,7 @@ l3fwd_event_port_setup_generic(void)
 
 	evt_rsrc->deq_depth = def_p_conf.dequeue_depth;
 
-	for (event_p_id = 0; event_p_id < evt_rsrc->evp.nb_ports;
-								event_p_id++) {
+	for (event_p_id = 0; event_p_id < effective_ports; event_p_id++) {
 		ret = rte_event_port_setup(event_d_id, event_p_id,
 					   &event_p_conf);
 		if (ret < 0)
@@ -139,20 +151,21 @@ l3fwd_event_port_setup_generic(void)
 				  event_p_id);
 		evt_rsrc->evp.event_p_id[event_p_id] = event_p_id;
 	}
+
 	/* init spinlock */
 	rte_spinlock_init(&evt_rsrc->evp.lock);
 
 	evt_rsrc->def_p_conf = event_p_conf;
 }
 
-static void
+static int
 l3fwd_event_queue_setup_generic(uint32_t event_queue_cfg)
 {
 	struct l3fwd_event_resources *evt_rsrc = l3fwd_get_eventdev_rsrc();
 	uint8_t event_d_id = evt_rsrc->event_d_id;
 	struct rte_event_queue_conf event_q_conf = {
 		.nb_atomic_flows = 1024,
-		.nb_atomic_order_sequences = 1024,
+		.nb_atomic_order_sequences = 64,
 		.event_queue_cfg = event_queue_cfg,
 		.priority = RTE_EVENT_DEV_PRIORITY_NORMAL
 	};
@@ -183,15 +196,17 @@ l3fwd_event_queue_setup_generic(uint32_t event_queue_cfg)
 	}
 
 	event_q_conf.event_queue_cfg |= RTE_EVENT_QUEUE_CFG_SINGLE_LINK;
-	event_q_conf.priority = RTE_EVENT_DEV_PRIORITY_HIGHEST,
+	event_q_conf.priority = RTE_EVENT_DEV_PRIORITY_HIGHEST;
 	ret = rte_event_queue_setup(event_d_id, event_q_id, &event_q_conf);
 	if (ret < 0)
-		rte_panic("Error in configuring event queue for Tx adapter\n");
+		return ret;
+
 	evt_rsrc->evq.event_q_id[event_q_id] = event_q_id;
+	return 0;
 }
 
 static void
-l3fwd_rx_tx_adapter_setup_generic(void)
+l3fwd_rx_tx_adapter_setup_generic(bool strict_single_link)
 {
 	struct l3fwd_event_resources *evt_rsrc = l3fwd_get_eventdev_rsrc();
 	struct rte_event_eth_rx_adapter_queue_conf eth_q_conf;
@@ -259,9 +274,12 @@ l3fwd_rx_tx_adapter_setup_generic(void)
 		free(evt_rsrc->evq.event_q_id);
 		rte_panic("Failed to allocate memory for Rx adapter\n");
 	}
+	if (strict_single_link)
+		evt_rsrc->def_p_conf.event_port_cfg |= RTE_EVENT_PORT_CFG_SINGLE_LINK;
 
 	ret = rte_event_eth_tx_adapter_create(tx_adptr_id, event_d_id,
 					      &evt_rsrc->def_p_conf);
+
 	if (ret)
 		rte_panic("Failed to create tx adapter\n");
 
diff --git a/examples/l3fwd/l3fwd_event_internal_port.c b/examples/l3fwd/l3fwd_event_internal_port.c
index 32cf657..0af8904 100644
--- a/examples/l3fwd/l3fwd_event_internal_port.c
+++ b/examples/l3fwd/l3fwd_event_internal_port.c
@@ -8,7 +8,7 @@
 #include "l3fwd_event.h"
 
 static uint32_t
-l3fwd_event_device_setup_internal_port(void)
+l3fwd_event_device_setup_internal_port(__rte_unused bool strict_single_link)
 {
 	struct l3fwd_event_resources *evt_rsrc = l3fwd_get_eventdev_rsrc();
 	struct rte_event_dev_config event_d_conf = {
@@ -81,7 +81,7 @@ l3fwd_event_device_setup_internal_port(void)
 }
 
 static void
-l3fwd_event_port_setup_internal_port(void)
+l3fwd_event_port_setup_internal_port(__rte_unused bool strict_single_link)
 {
 	struct l3fwd_event_resources *evt_rsrc = l3fwd_get_eventdev_rsrc();
 	uint8_t event_d_id = evt_rsrc->event_d_id;
@@ -142,14 +142,14 @@ l3fwd_event_port_setup_internal_port(void)
 	evt_rsrc->def_p_conf = event_p_conf;
 }
 
-static void
+static int
 l3fwd_event_queue_setup_internal_port(uint32_t event_queue_cfg)
 {
 	struct l3fwd_event_resources *evt_rsrc = l3fwd_get_eventdev_rsrc();
 	uint8_t event_d_id = evt_rsrc->event_d_id;
 	struct rte_event_queue_conf event_q_conf = {
 		.nb_atomic_flows = 1024,
-		.nb_atomic_order_sequences = 1024,
+		.nb_atomic_order_sequences = 64,
 		.event_queue_cfg = event_queue_cfg,
 		.priority = RTE_EVENT_DEV_PRIORITY_NORMAL
 	};
@@ -185,10 +185,11 @@ l3fwd_event_queue_setup_internal_port(uint32_t event_queue_cfg)
 			rte_panic("Error in configuring event queue\n");
 		evt_rsrc->evq.event_q_id[event_q_id] = event_q_id;
 	}
+	return 0;
 }
 
 static void
-l3fwd_rx_tx_adapter_setup_internal_port(void)
+l3fwd_rx_tx_adapter_setup_internal_port(__rte_unused bool strict_single_link)
 {
 	struct l3fwd_event_resources *evt_rsrc = l3fwd_get_eventdev_rsrc();
 	struct rte_event_eth_rx_adapter_queue_conf eth_q_conf;
diff --git a/examples/l3fwd/meson.build b/examples/l3fwd/meson.build
index b40244a..368673b 100644
--- a/examples/l3fwd/meson.build
+++ b/examples/l3fwd/meson.build
@@ -7,7 +7,7 @@
 # DPDK instance, use 'make'
 
 allow_experimental_apis = true
-deps += ['acl', 'hash', 'lpm', 'fib', 'eventdev']
+deps += ['acl', 'hash', 'lpm', 'fib', 'eventdev', 'cmdline']
 sources = files(
         'l3fwd_acl.c',
         'l3fwd_em.c',
diff --git a/lib/eal/common/eal_common_options.c b/lib/eal/common/eal_common_options.c
index 2d65357..cd0ce50 100644
--- a/lib/eal/common/eal_common_options.c
+++ b/lib/eal/common/eal_common_options.c
@@ -531,8 +531,16 @@ int
 eal_plugins_init(void)
 {
 	struct shared_driver *solib = NULL;
+	char *env_solib_dir = NULL;
 	struct stat sb;
 
+	env_solib_dir = getenv("DPDK_ENV_PMD_PATH");
+	if (env_solib_dir != NULL) {
+		RTE_LOG(DEBUG, EAL, "overriding default plugin directory "
+			"path with environment value: %s\n", env_solib_dir);
+		default_solib_dir = env_solib_dir;
+	}
+
 	/* If we are not statically linked, add default driver loading
 	 * path if it exists as a directory.
 	 * (Using dlopen with NOLOAD flag on EAL, will return NULL if the EAL
diff --git a/lib/eal/common/eal_options.h b/lib/eal/common/eal_options.h
index 3cc9cb6..ede8941 100644
--- a/lib/eal/common/eal_options.h
+++ b/lib/eal/common/eal_options.h
@@ -108,5 +108,7 @@ int eal_plugins_init(void);
 int eal_save_args(int argc, char **argv);
 int handle_eal_info_request(const char *cmd, const char *params __rte_unused,
 		struct rte_tel_data *d);
+int
+eal_parse_coremask(const char *coremask, int *cores);
 
 #endif /* EAL_OPTIONS_H */
diff --git a/lib/eal/version.map b/lib/eal/version.map
index 7ad12a7..976ba5f 100644
--- a/lib/eal/version.map
+++ b/lib/eal/version.map
@@ -302,6 +302,7 @@ DPDK_23 {
 	rte_vlog;
 	rte_zmalloc;
 	rte_zmalloc_socket;
+	eal_parse_coremask;
 
 	local: *;
 };
@@ -440,6 +441,13 @@ EXPERIMENTAL {
 	rte_thread_detach;
 	rte_thread_equal;
 	rte_thread_join;
+
+	# added as part of RDK
+	rte_rdk_rel_major;
+	rte_rdk_rel_minor;
+	rte_rdk_rel_year;
+	rte_rdk_rel_month;
+	rte_rdk_rel_string;
 };
 
 INTERNAL {
diff --git a/lib/eventdev/rte_eventdev.h b/lib/eventdev/rte_eventdev.h
index a90e23a..0e0992b 100644
--- a/lib/eventdev/rte_eventdev.h
+++ b/lib/eventdev/rte_eventdev.h
@@ -7,7 +7,6 @@
 
 #ifndef _RTE_EVENTDEV_H_
 #define _RTE_EVENTDEV_H_
-
 /**
  * @file
  *
@@ -333,6 +332,7 @@ struct rte_event;
  * @see rte_event_port_link()
  */
 #define RTE_EVENT_DEV_PRIORITY_LOWEST    255
+
 /**< Lowest priority expressed across eventdev subsystem
  * @see rte_event_queue_setup(), rte_event_enqueue_burst()
  * @see rte_event_port_link()
@@ -420,7 +420,7 @@ struct rte_event_dev_info {
 	 */
 	uint8_t max_event_ports;
 	/**< Maximum number of event ports supported by this device */
-	uint8_t max_event_port_dequeue_depth;
+	uint32_t max_event_port_dequeue_depth;
 	/**< Maximum number of events can be dequeued at a time from an
 	 * event port by this device.
 	 * A device that does not support bulk dequeue will set this as 1.
@@ -564,6 +564,9 @@ struct rte_event_dev_config {
 	 */
 };
 
+/* Temporary: provide compatibility with software using the older name */
+#define nb_single_link_event_ports nb_single_link_event_port_queues
+
 /**
  * Configure an event device.
  *
@@ -830,7 +833,7 @@ rte_event_queue_attr_set(uint8_t dev_id, uint8_t queue_id, uint32_t attr_id,
  *
  *  @see rte_event_port_setup()
  */
-
+#define RTE_EVENT_PORT_CFG_RESTORE_DEQ_ORDER   (1ULL << 5)
 /** Event port configuration structure */
 struct rte_event_port_conf {
 	int32_t new_event_threshold;
@@ -963,6 +966,11 @@ rte_event_port_quiesce(uint8_t dev_id, uint8_t port_id,
  * The implicit release disable attribute of the port
  */
 #define RTE_EVENT_PORT_ATTR_IMPLICIT_RELEASE_DISABLE 3
+/**
+ * The restore dequeue order attribute of the port
+ *
+ */
+#define RTE_EVENT_PORT_ATTR_RESTORE_DEQ_ORDER 4
 
 /**
  * Get an attribute from a port.
@@ -1201,6 +1209,8 @@ struct rte_event_vector {
  */
 #define RTE_EVENT_TYPE_ETH_RX_ADAPTER   0x4
 /**< The event generated from event eth Rx adapter */
+#define RTE_EVENT_TYPE_CPPIDEV          0x5
+/**< The event generated from CPPI subsystem */
 #define RTE_EVENT_TYPE_VECTOR           0x8
 /**< Indicates that event is a vector.
  * All vector event types should be a logical OR of EVENT_TYPE_VECTOR.
@@ -1922,12 +1932,19 @@ __rte_event_enqueue_burst(uint8_t dev_id, uint8_t port_id,
 			  const struct rte_event ev[], uint16_t nb_events,
 			  const event_enqueue_burst_t fn)
 {
+#ifdef RTE_LIBRTE_EVENTDEV_DEBUG
+	struct rte_event_fp_ops *fp_ops;
+#else
 	const struct rte_event_fp_ops *fp_ops;
+#endif
+	uint16_t ret;
 	void *port;
 
 	fp_ops = &rte_event_fp_ops[dev_id];
 	port = fp_ops->data[port_id];
 #ifdef RTE_LIBRTE_EVENTDEV_DEBUG
+	struct rte_event_fp_ops *fp_ops2 = &rte_event_fp_ops[dev_id];
+
 	if (dev_id >= RTE_EVENT_MAX_DEVS ||
 	    port_id >= RTE_EVENT_MAX_PORTS_PER_DEV) {
 		rte_errno = EINVAL;
@@ -1938,6 +1955,14 @@ __rte_event_enqueue_burst(uint8_t dev_id, uint8_t port_id,
 		rte_errno = EINVAL;
 		return 0;
 	}
+
+	uint64_t orig = rte_atomic64_exchange((volatile uint64_t *)&fp_ops2->port_user_id[port_id],
+					      rte_lcore_id());
+	if (orig != RTE_UNUSED_LCORE_ID) {
+		printf("[%s()] Error: lcore %u attempted to access port %d while it is in use by lcore %lu\n",
+		       __func__, rte_lcore_id(), port_id, orig);
+		rte_panic("Port sharing detected\n");
+	}
 #endif
 	rte_eventdev_trace_enq_burst(dev_id, port_id, ev, nb_events, (void *)fn);
 	/*
@@ -1945,9 +1970,14 @@ __rte_event_enqueue_burst(uint8_t dev_id, uint8_t port_id,
 	 * requests nb_events as const one
 	 */
 	if (nb_events == 1)
-		return (fp_ops->enqueue)(port, ev);
+		ret = (fp_ops->enqueue)(port, ev);
 	else
-		return fn(port, ev, nb_events);
+		ret = fn(port, ev, nb_events);
+#ifdef RTE_LIBRTE_EVENTDEV_DEBUG
+	rte_atomic64_set(&fp_ops2->port_user_id[port_id], RTE_UNUSED_LCORE_ID);
+#endif
+
+	return ret;
 }
 
 /**
@@ -2178,12 +2208,19 @@ static inline uint16_t
 rte_event_dequeue_burst(uint8_t dev_id, uint8_t port_id, struct rte_event ev[],
 			uint16_t nb_events, uint64_t timeout_ticks)
 {
+#ifdef RTE_LIBRTE_EVENTDEV_DEBUG
+	struct rte_event_fp_ops *fp_ops;
+#else
 	const struct rte_event_fp_ops *fp_ops;
+#endif
+	uint16_t ret;
 	void *port;
 
 	fp_ops = &rte_event_fp_ops[dev_id];
 	port = fp_ops->data[port_id];
 #ifdef RTE_LIBRTE_EVENTDEV_DEBUG
+	struct rte_event_fp_ops *fp_ops2 = &rte_event_fp_ops[dev_id];
+
 	if (dev_id >= RTE_EVENT_MAX_DEVS ||
 	    port_id >= RTE_EVENT_MAX_PORTS_PER_DEV) {
 		rte_errno = EINVAL;
@@ -2194,6 +2231,14 @@ rte_event_dequeue_burst(uint8_t dev_id, uint8_t port_id, struct rte_event ev[],
 		rte_errno = EINVAL;
 		return 0;
 	}
+
+	uint64_t orig = rte_atomic64_exchange((volatile uint64_t *)&fp_ops2->port_user_id[port_id],
+					      rte_lcore_id());
+	if (orig != RTE_UNUSED_LCORE_ID) {
+		printf("[%s()] Error: lcore %u attempted to access port %d while it is in use by lcore %lu\n",
+		       __func__, rte_lcore_id(), port_id, orig);
+		rte_panic("Port sharing detected\n");
+	}
 #endif
 	rte_eventdev_trace_deq_burst(dev_id, port_id, ev, nb_events);
 	/*
@@ -2201,10 +2246,16 @@ rte_event_dequeue_burst(uint8_t dev_id, uint8_t port_id, struct rte_event ev[],
 	 * requests nb_events as const one
 	 */
 	if (nb_events == 1)
-		return (fp_ops->dequeue)(port, ev, timeout_ticks);
+		ret = (fp_ops->dequeue)(port, ev, timeout_ticks);
 	else
-		return (fp_ops->dequeue_burst)(port, ev, nb_events,
+		ret = (fp_ops->dequeue_burst)(port, ev, nb_events,
 					       timeout_ticks);
+
+#ifdef RTE_LIBRTE_EVENTDEV_DEBUG
+	rte_atomic64_set(&fp_ops2->port_user_id[port_id], RTE_UNUSED_LCORE_ID);
+#endif
+
+	return ret;
 }
 
 #define RTE_EVENT_DEV_MAINT_OP_FLUSH          (1 << 0)
diff --git a/usertools/dpdk-devbind.py b/usertools/dpdk-devbind.py
index 4d9c1be..996bd3c 100755
--- a/usertools/dpdk-devbind.py
+++ b/usertools/dpdk-devbind.py
@@ -49,7 +49,7 @@
 hisilicon_dma = {'Class': '08', 'Vendor': '19e5', 'Device': 'a122',
                  'SVendor': None, 'SDevice': None}
 
-intel_dlb = {'Class': '0b', 'Vendor': '8086', 'Device': '270b,2710,2714',
+intel_dlb = {'Class': '0b', 'Vendor': '8086', 'Device': '270b,2710,2711,2714,2715',
              'SVendor': None, 'SDevice': None}
 intel_ioat_bdw = {'Class': '08', 'Vendor': '8086',
                   'Device': '6f20,6f21,6f22,6f23,6f24,6f25,6f26,6f27,6f2e,6f2f',
diff --git a/app/meson.build b/app/meson.build
index e32ea4b..7d05664 100644
--- a/app/meson.build
+++ b/app/meson.build
@@ -11,6 +11,8 @@ if enable_apps.length() == 0
 endif
 
 apps = [
+	'dlb_monitor',
+	'eventdev_dump',
         'dumpcap',
         'pdump',
         'proc-info',
